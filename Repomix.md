This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
agent_executor/base.ipynb
chat_agent_executor_with_function_calling/base.ipynb
chatbot-simulation-evaluation/agent-simulation-evaluation.ipynb
chatbots/customer_support_small_model.ipynb
how-tos/add-summary-conversation-history.ipynb
how-tos/branching.ipynb
how-tos/breakpoints.ipynb
how-tos/command.ipynb
how-tos/configuration.ipynb
how-tos/create-react-agent.ipynb
how-tos/cross-thread-persistence-functional.ipynb
how-tos/cross-thread-persistence.ipynb
how-tos/define-state.ipynb
how-tos/delete-messages.ipynb
how-tos/dynamic_breakpoints.ipynb
how-tos/dynamically-returning-directly.ipynb
how-tos/edit-graph-state.ipynb
how-tos/force-calling-a-tool-first.ipynb
how-tos/input_output_schema.ipynb
how-tos/manage-conversation-history.ipynb
how-tos/manage-ecosystem-dependencies.ipynb
how-tos/managing-agent-steps.ipynb
how-tos/map-reduce.ipynb
how-tos/multi-agent-multi-turn-convo-functional.ipynb
how-tos/multi-agent-multi-turn-convo.ipynb
how-tos/multi-agent-network-functional.ipynb
how-tos/multi-agent-network.ipynb
how-tos/node-retry-policies.ipynb
how-tos/pass_private_state.ipynb
how-tos/pass-run-time-values-to-tools.ipynb
how-tos/persistence-functional.ipynb
how-tos/persistence-postgres.ipynb
how-tos/persistence.ipynb
how-tos/react-agent-from-scratch-functional.ipynb
how-tos/react-human-in-the-loop.ipynb
how-tos/react-memory.ipynb
how-tos/react-return-structured-output.ipynb
how-tos/react-system-prompt.ipynb
how-tos/recursion-limit.ipynb
how-tos/respond-in-format.ipynb
how-tos/review-tool-calls-functional.ipynb
how-tos/review-tool-calls.ipynb
how-tos/semantic-search.ipynb
how-tos/stream-multiple.ipynb
how-tos/stream-tokens.ipynb
how-tos/stream-updates.ipynb
how-tos/stream-values.ipynb
how-tos/streaming-content.ipynb
how-tos/streaming-events-from-within-tools.ipynb
how-tos/streaming-from-final-node.ipynb
how-tos/streaming-tokens-without-langchain.ipynb
how-tos/subgraph-persistence.ipynb
how-tos/subgraph-transform-state.ipynb
how-tos/subgraph.ipynb
how-tos/subgraphs-manage-state.ipynb
how-tos/time-travel.ipynb
how-tos/tool-calling-errors.ipynb
how-tos/tool-calling.ipynb
how-tos/update-state-from-tools.ipynb
how-tos/use-in-web-environments.ipynb
how-tos/wait-user-input-functional.ipynb
how-tos/wait-user-input.ipynb
multi_agent/agent_supervisor.ipynb
multi_agent/hierarchical_agent_teams.ipynb
multi_agent/multi_agent_collaboration.ipynb
package.json
plan-and-execute/plan-and-execute.ipynb
quickstart.ipynb
rag/langgraph_adaptive_rag_local.ipynb
rag/langgraph_agentic_rag.ipynb
rag/langgraph_crag_mistral.ipynb
rag/langgraph_crag.ipynb
rag/langgraph_self_rag.ipynb
reflection/reflection.ipynb
rewoo/rewoo.ipynb
src/index.ts
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="agent_executor/base.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f725852e-71ef-4615-8cac-011a516fbe72",
   "metadata": {},
   "source": [
    "# Agent Executor From Scratch\n",
    "\n",
    "In this notebook we will go over how to build a basic agent executor from\n",
    "scratch.\n",
    "\n",
    "![diagram](./img/agent-executor-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0860511-03c2-49bb-937b-035f84142b7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/community @langchain/anthropic @langchain/langgraph @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4179ce-48fa-4aaf-a5a1-027b5229be1a",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for Anthropic (the LLM we will use) and Tavily (the\n",
    "search tool we will use)\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\n",
    "export TAVILY_API_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37943b1c-2b0a-4c09-bfbd-5dc24b839e3c",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for\n",
    "[LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_API_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e58b3-fe3c-449d-b3c4-8fa2217afd07",
   "metadata": {},
   "source": [
    "## Define the graph schema\n",
    "\n",
    "We first need to define the graph state. The state for a traditional LangGraph agent has a single attribute, `messages` which holds the original input, as well as the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c941fb10-dbe5-4d6a-ab7d-133d01c33cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fab3b",
   "metadata": {},
   "source": [
    "## Define tools\n",
    "\n",
    "Next, we will define the tools we'll use in this LangGraph graph. For this example, we'll use the pre-build Tavily Search tool, however you can use any pre-built, or custom tool you'd like. See [this guide](https://js.langchain.com/docs/how_to/custom_tools/) on how to create custom LangChain tools.\n",
    "\n",
    "We'll want to pass our tools to the `ToolNode`, which is the way LangGraph executes tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25789946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const tools = [new TavilySearchResults({ maxResults: 1 })];\n",
    "\n",
    "const toolNode = new ToolNode<typeof AgentState.State>(tools);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd27b281-cc9a-49c9-be78-8b98a7d905c4",
   "metadata": {},
   "source": [
    "## Define the nodes\n",
    "\n",
    "We now need to define a few different nodes in our graph. In `langgraph`, a node\n",
    "can be either a function or a\n",
    "[runnable](https://js.langchain.com/docs/expression_language/). There are two\n",
    "main nodes we need for this:\n",
    "\n",
    "1. The agent: responsible for deciding what (if any) actions to take.\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node\n",
    "   will then execute that action.\n",
    "\n",
    "We will also need to define some edges. Some of these edges may be conditional.\n",
    "The reason they are conditional is that based on the output of a node, one of\n",
    "several paths may be taken. The path that is taken is not known until that node\n",
    "is run (the LLM decides).\n",
    "\n",
    "1. Conditional Edge: after the agent is called, we should either: a. If the\n",
    "   agent said to take an action, then the function to invoke tools should be\n",
    "   called b. If the agent said that it was finished, then it should finish\n",
    "2. Normal Edge: after the tools are invoked, it should always go back to the\n",
    "   agent to decide what to do next\n",
    "\n",
    "Let's define the nodes, as well as a function to decide how what conditional\n",
    "edge to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d61a970d-edf4-4eef-9678-28bab7c72331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import type { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { END } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the LLM to be used in the agent\n",
    "const llm = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-20240620\",\n",
    "  temperature: 0,\n",
    "}).bindTools(tools); // Ensure you bind the same tools passed to the ToolExecutor to the LLM, so these tools can be used in the agent\n",
    "\n",
    "// Define logic that will be used to determine which conditional edge to go down\n",
    "const shouldContinue = (data: typeof AgentState.State): \"executeTools\" | typeof END => {\n",
    "  const { messages } = data;\n",
    "  const lastMsg = messages[messages.length - 1];\n",
    "  // If the agent called a tool, we should continue. If not, we can end.\n",
    "  if (!(\"tool_calls\" in lastMsg) || !Array.isArray(lastMsg.tool_calls) || !lastMsg?.tool_calls?.length) {\n",
    "    return END;\n",
    "  }\n",
    "  // By returning the name of the next node we want to go to\n",
    "  // LangGraph will automatically route to that node\n",
    "  return \"executeTools\";\n",
    "};\n",
    "\n",
    "const callModel = async (data: typeof AgentState.State, config?: RunnableConfig): Promise<Partial<typeof AgentState.State>> => {\n",
    "  const { messages } = data;\n",
    "  const result = await llm.invoke(messages, config);\n",
    "  return {\n",
    "    messages: [result],\n",
    "  };\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b211f4-0c5c-4792-b18d-cd70907c71e7",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We can now put it all together and define the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4054dde-4618-49b7-998a-daa0c1d6d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "  // Define the two nodes we will cycle between\n",
    "  .addNode(\"callModel\", callModel)\n",
    "  .addNode(\"executeTools\", toolNode)\n",
    "  // Set the entrypoint as `callModel`\n",
    "  // This means that this node is the first one called\n",
    "  .addEdge(START, \"callModel\")\n",
    "  // We now add a conditional edge\n",
    "  .addConditionalEdges(\n",
    "    // First, we define the start node. We use `callModel`.\n",
    "    // This means these are the edges taken after the `agent` node is called.\n",
    "    \"callModel\",\n",
    "    // Next, we pass in the function that will determine which node is called next.\n",
    "    shouldContinue,\n",
    "  )\n",
    "  // We now add a normal edge from `tools` to `agent`.\n",
    "  // This means that after `tools` is called, `agent` node is called next.\n",
    "  .addEdge(\"executeTools\", \"callModel\");\n",
    "\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "214ae46e-c297-465d-86db-2b0312ed3530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(node) callModel:  AIMessage {\n",
      "  \"id\": \"msg_01SwpMu2zu8W4NZeLEg5DHKj\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"type\": \"text\",\n",
      "      \"text\": \"Certainly! I'll use the Tavily search engine to find current weather information for San Francisco (SF). Let me search for that information for you.\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"tool_use\",\n",
      "      \"id\": \"toolu_013Asn4HooNPMtYYapQPcewB\",\n",
      "      \"name\": \"tavily_search_results_json\",\n",
      "      \"input\": {\n",
      "        \"input\": \"current weather in San Francisco\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01SwpMu2zu8W4NZeLEg5DHKj\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"tool_use\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 416,\n",
      "      \"output_tokens\": 94\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01SwpMu2zu8W4NZeLEg5DHKj\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"tool_use\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 416,\n",
      "      \"output_tokens\": 94\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"name\": \"tavily_search_results_json\",\n",
      "      \"args\": {\n",
      "        \"input\": \"current weather in San Francisco\"\n",
      "      },\n",
      "      \"id\": \"toolu_013Asn4HooNPMtYYapQPcewB\",\n",
      "      \"type\": \"tool_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 416,\n",
      "    \"output_tokens\": 94,\n",
      "    \"total_tokens\": 510\n",
      "  }\n",
      "}\n",
      "----\n",
      "\n",
      "(node) executeTools:  ToolMessage {\n",
      "  \"content\": \"[{\\\"title\\\":\\\"Weather in San Francisco\\\",\\\"url\\\":\\\"https://www.weatherapi.com/\\\",\\\"content\\\":\\\"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1724101106, 'localtime': '2024-08-19 13:58'}, 'current': {'last_updated_epoch': 1724100300, 'last_updated': '2024-08-19 13:45', 'temp_c': 21.1, 'temp_f': 70.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 250, 'wind_dir': 'WSW', 'pressure_mb': 1019.0, 'pressure_in': 30.08, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 64, 'cloud': 0, 'feelslike_c': 21.1, 'feelslike_f': 70.0, 'windchill_c': 17.7, 'windchill_f': 63.8, 'heatindex_c': 17.7, 'heatindex_f': 63.8, 'dewpoint_c': 10.7, 'dewpoint_f': 51.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 13.2, 'gust_kph': 21.2}}\\\",\\\"score\\\":0.9953496,\\\"raw_content\\\":null}]\",\n",
      "  \"name\": \"tavily_search_results_json\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {},\n",
      "  \"tool_call_id\": \"toolu_013Asn4HooNPMtYYapQPcewB\"\n",
      "}\n",
      "----\n",
      "\n",
      "(node) callModel:  AIMessage {\n",
      "  \"id\": \"msg_01PwNrBEDix98RpZSdWwzjjn\",\n",
      "  \"content\": \"Based on the search results, I can provide you with the current weather information for San Francisco (SF). Here's a summary of the weather conditions:\\n\\n1. Temperature: 21.1°C (70.0°F)\\n2. Condition: Sunny\\n3. Wind: 15.1 km/h (9.4 mph), direction WSW (West-Southwest)\\n4. Humidity: 64%\\n5. Precipitation: 0 mm (0 inches)\\n6. Cloud cover: 0% (clear sky)\\n7. Visibility: 16 km (9 miles)\\n8. UV Index: 5.0 (moderate)\\n\\nThe weather in San Francisco is currently pleasant and sunny. It's a comfortable 21.1°C (70.0°F) with no cloud cover, making it a clear and bright day. The wind is light to moderate, coming from the west-southwest direction. There's no precipitation, and visibility is good.\\n\\nIt's worth noting that San Francisco's weather can change quickly due to its unique microclimate, so if you're planning activities, it's always good to check for updates closer to the time.\\n\\nIs there anything specific about the weather in San Francisco that you'd like to know more about?\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01PwNrBEDix98RpZSdWwzjjn\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 1028,\n",
      "      \"output_tokens\": 282\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01PwNrBEDix98RpZSdWwzjjn\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 1028,\n",
      "      \"output_tokens\": 282\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 1028,\n",
      "    \"output_tokens\": 282,\n",
      "    \"total_tokens\": 1310\n",
      "  }\n",
      "}\n",
      "----\n",
      "\n",
      "Final Result:  Based on the search results, I can provide you with the current weather information for San Francisco (SF). Here's a summary of the weather conditions:\n",
      "\n",
      "1. Temperature: 21.1°C (70.0°F)\n",
      "2. Condition: Sunny\n",
      "3. Wind: 15.1 km/h (9.4 mph), direction WSW (West-Southwest)\n",
      "4. Humidity: 64%\n",
      "5. Precipitation: 0 mm (0 inches)\n",
      "6. Cloud cover: 0% (clear sky)\n",
      "7. Visibility: 16 km (9 miles)\n",
      "8. UV Index: 5.0 (moderate)\n",
      "\n",
      "The weather in San Francisco is currently pleasant and sunny. It's a comfortable 21.1°C (70.0°F) with no cloud cover, making it a clear and bright day. The wind is light to moderate, coming from the west-southwest direction. There's no precipitation, and visibility is good.\n",
      "\n",
      "It's worth noting that San Francisco's weather can change quickly due to its unique microclimate, so if you're planning activities, it's always good to check for updates closer to the time.\n",
      "\n",
      "Is there anything specific about the weather in San Francisco that you'd like to know more about?\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage, BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "let finalResult: BaseMessage | undefined;\n",
    "\n",
    "const prettyLogOutput = (output: Record<string, any>) => {\n",
    "  const keys = Object.keys(output);\n",
    "  const firstItem = output[keys[0]];\n",
    "  if (\"messages\" in firstItem) {\n",
    "    console.log(`(node) ${keys[0]}:`, firstItem.messages[0]);\n",
    "    console.log(\"----\\n\");\n",
    "  }\n",
    "}\n",
    "\n",
    "const inputs = { messages: [new HumanMessage(\"Search the web for the weather in sf\")] };\n",
    "for await (const s of await app.stream(inputs)) {\n",
    "  prettyLogOutput(s);\n",
    "  if (\"callModel\" in s && s.callModel.messages?.length) {\n",
    "    finalResult = s.callModel.messages[0];\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(\"Final Result: \", finalResult.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="chat_agent_executor_with_function_calling/base.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# Chat Agent Executor\n",
    "\n",
    "In this example we will build a chat executor that uses function calling from\n",
    "scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup¶\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "yarn add langchain @langchain/openai @langchain/langgraph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the\n",
    "search tool we will use)\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\n",
    "export TAVILY_API_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for\n",
    "[LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_API_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac643b-cb06-4724-a80c-2862ba4773f1",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation [here](https://js.langchain.com/docs/how_to/custom_tools/) on how to do that.\n",
    "\n",
    "After defining our tools, we can wrap them in a simple `ToolExecutor`. This is a real simple\n",
    "class that takes in a `ToolInvocation` and calls that tool, returning the output.\n",
    "\n",
    "A ToolInvocation is any type with `tool` and `toolInput` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf3331e-ccb3-41c8-aeb9-a840a94d41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ToolExecutor } from \"@langchain/langgraph/prebuilt\";\n",
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "const tools = [new TavilySearchResults({ maxResults: 1 })];\n",
    "\n",
    "const toolExecutor = new ToolExecutor({\n",
    "  tools,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497ed70-fce3-47f1-9cad-46f912bad6a5",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "Now we need to load the chat model we want to use. Importantly, this should\n",
    "satisfy two criteria:\n",
    "\n",
    "1. It should work with messages. We will represent all agent state in the form\n",
    "   of messages, so it needs to be able to work well with them.\n",
    "2. It should work with OpenAI function calling. This means it should either be\n",
    "   an OpenAI model or a model that exposes a similar interface.\n",
    "\n",
    "Note: these model requirements are not requirements for using LangGraph - they\n",
    "are just requirements for this one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892b54b9-75f0-4804-9ed0-88b5e5532989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// We will set streaming=True so that we can stream tokens\n",
    "// See the streaming section for more information on this.\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  streaming: true,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77995c0-bae2-4cee-a036-8688a90f05b9",
   "metadata": {},
   "source": [
    "After we've done this, we should make sure the model knows that it has these\n",
    "tools available to call. We can do this by converting the LangChain tools into\n",
    "the format for OpenAI function calling, and then bind them to the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3cbae5-d92c-4559-a4aa-44721b80d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { convertToOpenAIFunction } from \"@langchain/core/utils/function_calling\";\n",
    "\n",
    "const toolsAsOpenAIFunctions = tools.map((tool) =>\n",
    "  convertToOpenAIFunction(tool)\n",
    ");\n",
    "const newModel = model.bind({\n",
    "  functions: toolsAsOpenAIFunctions,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b9211-93d0-4ad5-aa7a-9c09099c53ff",
   "metadata": {},
   "source": [
    "### Define the agent state\n",
    "\n",
    "The main type of graph in `langgraph` is the `StatefulGraph`. This graph is\n",
    "parameterized by a state object that it passes around to each node. Each node\n",
    "then returns operations to update that state. These operations can either SET\n",
    "specific attributes on the state (e.g. overwrite the existing values) or ADD to\n",
    "the existing attribute. Whether to set or add is denoted by annotating the state\n",
    "object you construct the graph with.\n",
    "\n",
    "For this example, the state we will track will just be a list of messages. We\n",
    "want each node to just add messages to that list. To do this, we define our state via the `Annotation` function, with a single attribute `messages` that is a list of `BaseMessage`s. The value of our attribute `messages` is another `Annotation` which has two sub-attributes: `reducer` and `default`. The `reducer` key must be a factory that returns a function that takes the current value of the attribute and the new value to add, and returns the new value of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea793afa-2eab-4901-910d-6eed90cd6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c5094-9297-4d19-a04e-3eedc75cefb4",
   "metadata": {},
   "source": [
    "## Define the nodes\n",
    "\n",
    "We now need to define a few different nodes in our graph. In `langgraph`, a node\n",
    "can be either a function or a\n",
    "[runnable](https://js.langchain.com/docs/expression_language/). There are two\n",
    "main nodes we need for this:\n",
    "\n",
    "1. The agent: responsible for deciding what (if any) actions to take.\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node\n",
    "   will then execute that action.\n",
    "\n",
    "We will also need to define some edges. Some of these edges may be conditional.\n",
    "The reason they are conditional is that based on the output of a node, one of\n",
    "several paths may be taken. The path that is taken is not known until that node\n",
    "is run (the LLM decides).\n",
    "\n",
    "1. Conditional Edge: after the agent is called, we should either: a. If the\n",
    "   agent said to take an action, then the function to invoke tools should be\n",
    "   called b. If the agent said that it was finished, then it should finish\n",
    "2. Normal Edge: after the tools are invoked, it should always go back to the\n",
    "   agent to decide what to do next\n",
    "\n",
    "Let's define the nodes, as well as a function to decide how what conditional\n",
    "edge to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b541bb9-900c-40d0-964d-7b5dfee30667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { FunctionMessage } from \"@langchain/core/messages\";\n",
    "import { AgentAction } from \"@langchain/core/agents\";\n",
    "\n",
    "// Define the function that determines whether to continue or not\n",
    "const shouldContinue = (state: typeof AgentState.State) => {\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "  // If there is no function call, then we finish\n",
    "  if (\n",
    "    !(\"function_call\" in lastMessage.additional_kwargs) ||\n",
    "    !lastMessage.additional_kwargs.function_call\n",
    "  ) {\n",
    "    return \"end\";\n",
    "  }\n",
    "  // Otherwise if there is, we continue\n",
    "  return \"continue\";\n",
    "};\n",
    "\n",
    "// Define the function to execute tools\n",
    "const _getAction = (state: typeof AgentState.State): AgentAction => {\n",
    "  const { messages } = state;\n",
    "  // Based on the continue condition\n",
    "  // we know the last message involves a function call\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "  if (!lastMessage) {\n",
    "    throw new Error(\"No messages found.\");\n",
    "  }\n",
    "  if (!lastMessage.additional_kwargs.function_call) {\n",
    "    throw new Error(\"No function call found in message.\");\n",
    "  }\n",
    "  // We construct an AgentAction from the function_call\n",
    "  return {\n",
    "    tool: lastMessage.additional_kwargs.function_call.name,\n",
    "    toolInput: JSON.stringify(\n",
    "      lastMessage.additional_kwargs.function_call.arguments,\n",
    "    ),\n",
    "    log: \"\",\n",
    "  };\n",
    "};\n",
    "\n",
    "// Define the function that calls the model\n",
    "const callModel = async (state: typeof AgentState.State) => {\n",
    "  const { messages } = state;\n",
    "  const response = await newModel.invoke(messages);\n",
    "  // We return a list, because this will get added to the existing list\n",
    "  return {\n",
    "    messages: [response],\n",
    "  };\n",
    "};\n",
    "\n",
    "const callTool = async (state: typeof AgentState.State) => {\n",
    "  const action = _getAction(state);\n",
    "  // We call the tool_executor and get back a response\n",
    "  const response = await toolExecutor.invoke(action);\n",
    "  // We use the response to create a FunctionMessage\n",
    "  const functionMessage = new FunctionMessage({\n",
    "    content: response,\n",
    "    name: action.tool,\n",
    "  });\n",
    "  // We return a list, because this will get added to the existing list\n",
    "  return { messages: [functionMessage] };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6e892-946c-4899-8cc0-7c9291c1f73b",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We can now put it all together and define the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813ae66c-3b58-4283-a02a-36da72a2ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "  // Define the two nodes we will cycle between\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"action\", callTool)\n",
    "  // Set the entrypoint as `agent`\n",
    "  // This means that this node is the first one called\n",
    "  .addEdge(START, \"agent\")\n",
    "  // We now add a conditional edge\n",
    "  .addConditionalEdges(\n",
    "    // First, we define the start node. We use `agent`.\n",
    "    // This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    // Next, we pass in the function that will determine which node is called next.\n",
    "    shouldContinue,\n",
    "    // Finally we pass in a mapping.\n",
    "    // The keys are strings, and the values are other nodes.\n",
    "    // END is a special node marking that the graph should finish.\n",
    "    // What will happen is we will call `should_continue`, and then the output of that\n",
    "    // will be matched against the keys in this mapping.\n",
    "    // Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "      // If `tools`, then we call the tool node.\n",
    "      continue: \"action\",\n",
    "      // Otherwise we finish.\n",
    "      end: END,\n",
    "    },\n",
    "  )\n",
    "  // We now add a normal edge from `tools` to `agent`.\n",
    "  // This means that after `tools` is called, `agent` node is called next.\n",
    "  .addEdge(\"action\", \"agent\");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c3931-3dae-4281-ad4e-4b51305594d4",
   "metadata": {},
   "source": [
    "## Use it!\n",
    "\n",
    "We can now use it! This now exposes the\n",
    "[same interface](https://python.langchain.com/docs/expression_language/) as all\n",
    "other LangChain runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edb04b9-40b6-46f1-a7a8-4b2d8aba7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"content\": \"what is the weather in sf\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessageChunk {\n",
      "      \"id\": \"chatcmpl-9y3695OyrZqhRmcbVkioOxXzjXp32\",\n",
      "      \"content\": \"\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"function_call\": {\n",
      "          \"name\": \"tavily_search_results_json\",\n",
      "          \"arguments\": \"{\\\"input\\\":\\\"weather in San Francisco\\\"}\"\n",
      "        }\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"estimatedTokenUsage\": {\n",
      "          \"promptTokens\": 80,\n",
      "          \"completionTokens\": 21,\n",
      "          \"totalTokens\": 101\n",
      "        },\n",
      "        \"prompt\": 0,\n",
      "        \"completion\": 0,\n",
      "        \"finish_reason\": \"function_call\",\n",
      "        \"system_fingerprint\": null\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"tool_call_chunks\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 79,\n",
      "        \"output_tokens\": 21,\n",
      "        \"total_tokens\": 100\n",
      "      }\n",
      "    },\n",
      "    FunctionMessage {\n",
      "      \"content\": \"[{\\\"title\\\":\\\"Weather in San Francisco\\\",\\\"url\\\":\\\"https://www.weatherapi.com/\\\",\\\"content\\\":\\\"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1724098493, 'localtime': '2024-08-19 13:14'}, 'current': {'last_updated_epoch': 1724097600, 'last_updated': '2024-08-19 13:00', 'temp_c': 21.1, 'temp_f': 70.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 250, 'wind_dir': 'WSW', 'pressure_mb': 1019.0, 'pressure_in': 30.08, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 64, 'cloud': 0, 'feelslike_c': 21.1, 'feelslike_f': 70.0, 'windchill_c': 17.7, 'windchill_f': 63.8, 'heatindex_c': 17.7, 'heatindex_f': 63.8, 'dewpoint_c': 10.7, 'dewpoint_f': 51.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 13.2, 'gust_kph': 21.2}}\\\",\\\"score\\\":0.9991679,\\\"raw_content\\\":null}]\",\n",
      "      \"name\": \"tavily_search_results_json\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessageChunk {\n",
      "      \"id\": \"chatcmpl-9y36FtclcqetcawFZmYH7rDvajQ7A\",\n",
      "      \"content\": \"The current weather in San Francisco is sunny with a temperature of 70.0°F (21.1°C). The wind is blowing at 15.1 kph from the WSW direction. The humidity is at 64%, and there is no precipitation.\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"estimatedTokenUsage\": {\n",
      "          \"promptTokens\": 536,\n",
      "          \"completionTokens\": 53,\n",
      "          \"totalTokens\": 589\n",
      "        },\n",
      "        \"prompt\": 0,\n",
      "        \"completion\": 0,\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"system_fingerprint\": null\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"tool_call_chunks\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 538,\n",
      "        \"output_tokens\": 54,\n",
      "        \"total_tokens\": 592\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const inputs = {\n",
    "  messages: [new HumanMessage(\"what is the weather in sf\")],\n",
    "};\n",
    "await app.invoke(inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e8155-70c5-4973-912c-dc55104b2acf",
   "metadata": {},
   "source": [
    "This may take a little bit - it's making a few calls behind the scenes. In order\n",
    "to start seeing some intermediate results as they happen, we can use streaming.\n",
    "See below for more information on that.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "LangGraph has support for several different types of streaming.\n",
    "\n",
    "### Streaming Node Output\n",
    "\n",
    "One of the benefits of using LangGraph is that it is easy to stream output as\n",
    "it's produced by each node.\n",
    "\n",
    "In this example we'll re-use the `inputs` object from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f544977e-31f7-41f0-88c4-ec9c27b8cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output {\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessageChunk {\n",
      "        \"id\": \"chatcmpl-9y36G4P6hDihhZkOCW5T5bJWWNl0Z\",\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"function_call\": {\n",
      "            \"name\": \"tavily_search_results_json\",\n",
      "            \"arguments\": \"{\\\"input\\\":\\\"weather in San Francisco\\\"}\"\n",
      "          }\n",
      "        },\n",
      "        \"response_metadata\": {\n",
      "          \"estimatedTokenUsage\": {\n",
      "            \"promptTokens\": 80,\n",
      "            \"completionTokens\": 21,\n",
      "            \"totalTokens\": 101\n",
      "          },\n",
      "          \"prompt\": 0,\n",
      "          \"completion\": 0,\n",
      "          \"finish_reason\": \"function_call\",\n",
      "          \"system_fingerprint\": null\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"tool_call_chunks\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 79,\n",
      "          \"output_tokens\": 21,\n",
      "          \"total_tokens\": 100\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "\n",
      "output {\n",
      "  action: {\n",
      "    messages: [\n",
      "      FunctionMessage {\n",
      "        \"content\": \"[{\\\"title\\\":\\\"Weather in San Francisco\\\",\\\"url\\\":\\\"https://www.weatherapi.com/\\\",\\\"content\\\":\\\"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1724098493, 'localtime': '2024-08-19 13:14'}, 'current': {'last_updated_epoch': 1724097600, 'last_updated': '2024-08-19 13:00', 'temp_c': 21.1, 'temp_f': 70.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 250, 'wind_dir': 'WSW', 'pressure_mb': 1019.0, 'pressure_in': 30.08, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 64, 'cloud': 0, 'feelslike_c': 21.1, 'feelslike_f': 70.0, 'windchill_c': 17.7, 'windchill_f': 63.8, 'heatindex_c': 17.7, 'heatindex_f': 63.8, 'dewpoint_c': 10.7, 'dewpoint_f': 51.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 13.2, 'gust_kph': 21.2}}\\\",\\\"score\\\":0.9991846,\\\"raw_content\\\":null}]\",\n",
      "        \"name\": \"tavily_search_results_json\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "\n",
      "output {\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessageChunk {\n",
      "        \"id\": \"chatcmpl-9y36KCh1ZZbMNXVbLxTzz8jZb4t4T\",\n",
      "        \"content\": \"The current weather in San Francisco is sunny with a temperature of 70.0°F (21.1°C). The wind is blowing at 15.1 kph from the WSW direction. The humidity is at 64% and there is no precipitation.\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"estimatedTokenUsage\": {\n",
      "            \"promptTokens\": 536,\n",
      "            \"completionTokens\": 53,\n",
      "            \"totalTokens\": 589\n",
      "          },\n",
      "          \"prompt\": 0,\n",
      "          \"completion\": 0,\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": null\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"tool_call_chunks\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 538,\n",
      "          \"output_tokens\": 54,\n",
      "          \"total_tokens\": 592\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for await (const output of await app.stream(inputs)) {\n",
    "  console.log(\"output\", output);\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="chatbot-simulation-evaluation/agent-simulation-evaluation.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
   "metadata": {},
   "source": [
    "# Chat Bot Evaluation as Multi-agent Simulation\n",
    "\n",
    "When building a chat bot, such as a customer support assistant, it can be hard to properly evaluate your bot's performance. It's time-consuming to have to manually interact with it intensively for each code change.\n",
    "\n",
    "One way to make the evaluation process easier and more reproducible is to simulate a user interaction.\n",
    "\n",
    "Below is an example of how to create a \"virtual user\" with LangGraph.js to simulate a conversation.\n",
    "\n",
    "The overall simulation looks something like this:\n",
    "\n",
    "![diagram](./img/virtual_user_diagram.png)\n",
    "\n",
    "First, we'll set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed4de09-9ee7-4a2a-bcc7-54236e7cccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "// Optional tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"sk_...\";\n",
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_PROJECT = \"Agent Simulation Evaluation: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef8e26",
   "metadata": {},
   "source": [
    "## 1. Define Chat Bot\n",
    "\n",
    "Next, we'll define our chat bot. This implementation uses the OpenAI API to generate responses, and takes on the persona of an airline customer support agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afefe443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-AE3nMDCiDkmBMSVI6Y6xJBQjjWQwY\",\n",
      "  \"content\": \"Hello! How can I assist you today?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 9,\n",
      "      \"promptTokens\": 23,\n",
      "      \"totalTokens\": 32\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 23,\n",
      "    \"output_tokens\": 9,\n",
      "    \"total_tokens\": 32\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from '@langchain/openai'\n",
    "import type { AIMessageChunk, BaseMessageLike } from \"@langchain/core/messages\";\n",
    "\n",
    "const llm = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n",
    "\n",
    "async function myChatBot(messages: BaseMessageLike[]): Promise<AIMessageChunk> {\n",
    "  const systemMessage = {\n",
    "    role: 'system',\n",
    "    content: 'You are a customer support agent for an airline.',\n",
    "  };\n",
    "  const allMessages = [systemMessage, ...messages];\n",
    "  \n",
    "  const response = await llm.invoke(allMessages)\n",
    "  return response\n",
    "}\n",
    "\n",
    "// Test the chat bot\n",
    "const response = await myChatBot([{ role: 'user', content: 'hi!' }]);\n",
    "\n",
    "console.log(response);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5635e4",
   "metadata": {},
   "source": [
    "## 2. Define Simulated User\n",
    "\n",
    "Now we'll define the simulated user who will interact with our bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e170c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-AE3nNuHpuxAZfG6aQsKoKktitdyfD\",\n",
      "  \"content\": \"Hello! I’m Harrison, and I need to discuss a refund for my trip to Alaska that I took five years ago. I expect all of my money back. Can you assist me with that?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 40,\n",
      "      \"promptTokens\": 108,\n",
      "      \"totalTokens\": 148\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 108,\n",
      "    \"output_tokens\": 40,\n",
      "    \"total_tokens\": 148\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { type Runnable } from \"@langchain/core/runnables\";\n",
    "import { AIMessage } from \"@langchain/core/messages\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "async function createSimulatedUser(): Promise<Runnable<{ messages: BaseMessageLike[] }, AIMessage>> {\n",
    "    const systemPromptTemplate = `You are a customer of an airline company. You are interacting with a user who is a customer support person \n",
    "    \n",
    "{instructions}\n",
    "\n",
    "If you have nothing more to add to the conversation, you must respond only with a single word: \"FINISHED\"`;\n",
    "    \n",
    "    const prompt = ChatPromptTemplate.fromMessages([\n",
    "      ['system', systemPromptTemplate],\n",
    "      [\"placeholder\", '{messages}'],\n",
    "    ]);\n",
    "    \n",
    "    const instructions = `Your name is Harrison. You are trying to get a refund for the trip you took to Alaska.\n",
    "You want them to give you ALL the money back. Be extremely persistent. This trip happened 5 years ago.`;\n",
    "\n",
    "    const partialPrompt = await prompt.partial({ instructions });\n",
    "    \n",
    "    const simulatedUser = partialPrompt.pipe(llm);\n",
    "    return simulatedUser;\n",
    "}\n",
    "\n",
    "// Test the simulated user\n",
    "const messages = [{role: \"user\", content: 'Hi! How can I help you?'}];\n",
    "const simulatedUser = await createSimulatedUser()\n",
    "const simulatedUserResponse = await simulatedUser.invoke({ messages });\n",
    "console.log(simulatedUserResponse);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321312b4-a1f0-4454-a481-fdac4e37cb7d",
   "metadata": {},
   "source": [
    "## 3. Define the Agent Simulation\n",
    "\n",
    "The code below creates a LangGraph workflow to run the simulation. The main components are:\n",
    "\n",
    "1. The two nodes: one for the simulated user, the other for the chat bot.\n",
    "2. The graph itself, with a conditional stopping criterion.\n",
    "\n",
    "Read the comments in the code below for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc4446-462b-4ee8-b017-2862fbbdfaf5",
   "metadata": {},
   "source": [
    "**Nodes**\n",
    "\n",
    "First, we define the nodes in the graph. These should take in a list of messages and return a list of messages to ADD to the state.\n",
    "These will be thing wrappers around the chat bot and simulated user we have above.\n",
    "\n",
    "**Note:** one tricky thing here is which messages are which. Because both the chatbot AND our simulated user are both LLMs, both of them will respond with AI messages. Our state will be a list of alternating Human and AI messages. This means that for one of the nodes, there will need to be some logic that flips the AI and human roles. In this example, we will assume that `HumanMessages` are messages from the simulated user. This means that we need some logic in the simulated user node to swap AI and Human messages.\n",
    "\n",
    "First, let's define the chat bot node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d5d9e0-41ee-46c9-b62f-9128b91f99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "async function chatBotNode (state: typeof MessagesAnnotation.State) {\n",
    "  const messages = state.messages\n",
    "  const chatBotResponse = await myChatBot(messages);\n",
    "  return { messages: [chatBotResponse] }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c3c0c-56c5-4410-8fa8-ea2c0f11f506",
   "metadata": {},
   "source": [
    "Next, let's define the node for our simulated user. This will involve a little logic to swap the roles of the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac45873-91a3-4310-939f-a0a53da4233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// MessagesAnnotation coerces all message likes to base message classes\n",
    "function swapRoles(messages: BaseMessage[]) {\n",
    "  return messages.map((m) =>\n",
    "    m instanceof AIMessage\n",
    "      ? new HumanMessage({ content: m.content })\n",
    "      : new AIMessage({ content: m.content }),\n",
    "  )\n",
    "}\n",
    "\n",
    "async function simulatedUserNode (state: typeof MessagesAnnotation.State) {\n",
    "  const messages = state.messages\n",
    "  const newMessages = swapRoles(messages)\n",
    "  // This returns a runnable directly, so we need to use `.invoke` below:\n",
    "  const simulateUser = await createSimulatedUser();\n",
    "  const response = await simulateUser.invoke({ messages: newMessages })\n",
    "\n",
    "  return { messages: [{ role: \"user\", content: response.content }] }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d8a3e-9171-4c43-a595-44d312722148",
   "metadata": {},
   "source": [
    "**Edges**\n",
    "\n",
    "We now need to define the logic for the edges. The main logic occurs after the simulated user goes, and it should lead to one of two outcomes:\n",
    "\n",
    "- Either we continue and call the customer support bot\n",
    "- Or we finish and the conversation is over\n",
    "\n",
    "So what is the logic for the conversation being over? We will define that as either the Human chatbot responds with `FINISHED` (see the system prompt) OR the conversation is more than 6 messages long (this is an arbitrary number just to keep this example short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820a1557-a121-48c2-af5b-81220f43c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function shouldContinue(state: typeof MessagesAnnotation.State) {\n",
    "  const messages = state.messages;\n",
    "  if (messages.length > 6) {\n",
    "    return '__end__';\n",
    "  } else if (messages[messages.length - 1].content === 'FINISHED') {\n",
    "    return '__end__';\n",
    "  } else {\n",
    "    return 'continue';\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0856d4f-9334-4f28-944b-06d303e913a4",
   "metadata": {},
   "source": [
    "**Graph**\n",
    "\n",
    "We can now define the graph that sets up the simulation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1889196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, END, START } from \"@langchain/langgraph\";\n",
    "\n",
    "function createSimulation() {\n",
    "  const workflow = new StateGraph(MessagesAnnotation)\n",
    "    .addNode('user', simulatedUserNode)\n",
    "    .addNode('chatbot', chatBotNode)\n",
    "    .addEdge('chatbot', 'user')\n",
    "    .addConditionalEdges('user', shouldContinue, {\n",
    "      [END]: END,\n",
    "      continue: 'chatbot',\n",
    "    })\n",
    "    .addEdge(START, 'chatbot')\n",
    "\n",
    "  const simulation = workflow.compile()\n",
    "  return simulation;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18973bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = createSimulation().getGraph();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd22ce",
   "metadata": {},
   "source": [
    "## 4. Run Simulation\n",
    "\n",
    "Now we can evaluate our chat bot! We can invoke it with empty messages (this will simulate letting the chat bot start the initial conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32b606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot: How can I assist you today with your airline-related questions or concerns?\n",
      "\n",
      "---\n",
      "\n",
      "user: Hi, I'm Harrison, and I'm looking to get a refund for a trip I took to Alaska five years ago. I believe I am entitled to a full refund, and I would like to resolve this matter as soon as possible. Can you help me with that?\n",
      "\n",
      "---\n",
      "\n",
      "chatbot: Hi Harrison! I’d be happy to assist you with your request. However, I must inform you that our airline’s refund policy typically covers requests made within a certain timeframe from the date of travel, generally within 12 months for most fares. Since your trip to Alaska was five years ago, it is likely that it falls outside of our standard refund window.\n",
      "\n",
      "That said, if there were any extraordinary circumstances surrounding your trip or if you have documentation that supports your claim, please provide more details so I can better assist you. If you haven't already, I recommend contacting our customer service team directly through the website or our dedicated customer service number for specific cases.\n",
      "\n",
      "---\n",
      "\n",
      "user: I understand the typical policy, but I believe my situation warrants a full refund regardless of the time elapsed. It's crucial to me that I receive all my money back for the trip. I can provide any necessary details or documentation that supports my claim. Can you please make an exception in this case or escalate this issue? I am determined to get a full refund for my trip!\n",
      "\n",
      "---\n",
      "\n",
      "chatbot: I understand how important this matter is to you, Harrison, and I appreciate your determination. Unfortunately, as a customer support agent, I am bound by the airline's policies and procedures, which typically do not allow for exceptions to the refund timeline.\n",
      "\n",
      "However, I recommend that you gather all relevant details and documentation related to your trip, including any evidence that might support your request for an exception. After you’ve compiled this information, you can submit a formal appeal or request for a special review through our customer service channels. This often involves contacting customer relations or submitting a written request through our website, where your case can be considered by a dedicated team.\n",
      "\n",
      "If you’d like, I can guide you on how to submit this information or help you find the right contact point to escalate your request. Just let me know!\n",
      "\n",
      "---\n",
      "\n",
      "user: I appreciate the guidance, but I must insist that a full refund is due to me. This isn't just a matter of policy; it's about recognizing the value of customer experience and fairness. I prepared for this trip and expected that my investment would be protected. I urge you to reconsider and push for this refund on my behalf. I'm not willing to accept a denial based solely on policy restrictions, especially after all this time. Can you take further action to ensure I receive all my money back? Please help me with this!\n",
      "\n",
      "---\n",
      "\n",
      "chatbot: I completely understand your feelings and the importance of this situation to you, Harrison. Your concerns about customer experience and fairness are valid, and I empathize with your position. However, I want to clarify that as a customer support agent, I do not have the authority to override established policies or issue refunds outside of the established guidelines.\n",
      "\n",
      "The best course of action would be to formally submit your request along with all your supporting documentation to demonstrate why you believe you deserve a refund despite the time elapsed. This escalation will ensure that your case is reviewed by the appropriate department that handles such requests.\n",
      "\n",
      "I recommend reaching out through our customer service channels, including our website’s contact form or calling our customer relations department. Providing your case with detailed information and expressing your concerns about customer experience may lead to a more favorable consideration.\n",
      "\n",
      "If you would like assistance in drafting your request or finding the correct contact information, please let me know, and I’ll do my best to help you!\n",
      "\n",
      "---\n",
      "\n",
      "user: I appreciate your attempts to guide me, but I'm not prepared to take a backseat on this matter. I need to be clear: I am requesting a full refund for my Alaska trip, and I believe that the airline has a responsibility to honor that request despite the time that has passed. It's about accountability and valuing customers, and I will not back down until I receive every dollar back. I urge you to escalate this matter. I am not interested in going through more hoops or waiting for a review that may not result in the outcome I deserve. Can you elevate this issue to someone who has the authority to grant my refund? I need this resolved now!\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async function runSimulation() {\n",
    "  const simulation = createSimulation()\n",
    "  for await (const chunk of await simulation.stream({})) {\n",
    "    const nodeName = Object.keys(chunk)[0];\n",
    "    const messages = chunk[nodeName].messages;\n",
    "    console.log(`${nodeName}: ${messages[0].content}`);\n",
    "    console.log('\\n---\\n');\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "await runSimulation();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48db342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="chatbots/customer_support_small_model.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer support chatbot with a small model\n",
    "\n",
    "Below is an example of a customer support chatbot modeled as a state machine. It\n",
    "is designed to work with smaller models by giving them context as to what part of\n",
    "an interaction they are in, reducing the decision space a given LLM call has to keep\n",
    "them focused.\n",
    "\n",
    "The entrypoint is a node containing a chain that we have prompted to answer\n",
    "basic questions, but delegate questions related to billing or technical support\n",
    "to other \"teams\".\n",
    "\n",
    "Depending on this entry node's response, the edge from that node will use an LLM\n",
    "call to determine whether to respond directly to the user or invoke either the\n",
    "`billing_support` or `technical_support` nodes.\n",
    "\n",
    "- The technical support will attempt to answer the user's question with a more\n",
    "  focused prompt.\n",
    "- The billing agent can choose to answer the user's question, or can call out to a\n",
    "  human for approval for a refund using a [dynamic breakpoint](https://langchain-ai.github.io/langgraphjs/how-tos/dynamic_breakpoints/).\n",
    "\n",
    "![Diagram](./img/diagram.png)\n",
    "\n",
    "This is intended as a sample, proof of concept architecture - you could extend\n",
    "this example by giving individual nodes the ability to perform retrieval, other\n",
    "tools, delegating to more powerful models at deeper stages etc.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the required packages. We'll use a relatively small model, Llama 3.1 8B hosted on [Together AI](https://www.together.ai/), to run the required inference.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/community @langchain/core\n",
    "```\n",
    "\n",
    "You'll also need to set an environment variable named `TOGETHER_AI_API_KEY`, which you can obtain from your Together dashboard:\n",
    "\n",
    "```ini\n",
    "TOGETHER_AI_API_KEY=\"your_key_here\"\n",
    "```\n",
    "\n",
    "## Initializing the model\n",
    "\n",
    "First, we define the LLM we'll use for all calls and the LangGraph state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatTogetherAI } from \"@langchain/community/chat_models/togetherai\";\n",
    "\n",
    "const model = new ChatTogetherAI({\n",
    "  model: \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "  temperature: 0,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laying out the graph\n",
    "\n",
    "Now let's start defining logic for our nodes. Each node's return value will be added to\n",
    "the graph state. We'll start with the prebuilt `MessagesAnnotation`, which is designed to\n",
    "manage formatting and edge cases around messages returned from nodes:\n",
    "\n",
    "```ts\n",
    "{\n",
    "  messages: BaseMessage[];\n",
    "}\n",
    "```\n",
    "\n",
    "And we'll add two more state values: a string that defines the next representative,\n",
    "and a boolean that will determine whether a human has authorized a refund\n",
    "for a given thread. Our combined state will look like this:\n",
    "\n",
    "```ts\n",
    "{\n",
    "  messages: BaseMessage[];\n",
    "  nextRepresentative: string;\n",
    "  refundAuthorized: boolean;\n",
    "}\n",
    "```\n",
    "\n",
    "This state will be passed to the next executed node, or will be returned if execution has finished.\n",
    "Definining the state looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation, MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  ...MessagesAnnotation.spec,\n",
    "  nextRepresentative: Annotation<string>,\n",
    "  refundAuthorized: Annotation<boolean>,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the `nextRepresentative` value within nodes to make resuming from given checkpoints fully deterministic - if we use an LLM within an edge, resuming from a given state will have some undesirable randomness.\n",
    "\n",
    "Now, let's define our entrypoint node. This will be modeled after a secretary who can\n",
    "handle incoming questions and respond conversationally or route to a more specialized team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "const initialSupport = async (state: typeof StateAnnotation.State) => {\n",
    "  const SYSTEM_TEMPLATE =\n",
    "    `You are frontline support staff for LangCorp, a company that sells computers.\n",
    "Be concise in your responses.\n",
    "You can chat with customers and help them with basic questions, but if the customer is having a billing or technical problem,\n",
    "do not try to answer the question directly or gather information.\n",
    "Instead, immediately transfer them to the billing or technical team by asking the user to hold for a moment.\n",
    "Otherwise, just respond conversationally.`;\n",
    "  const supportResponse = await model.invoke([\n",
    "    { role: \"system\", content: SYSTEM_TEMPLATE },\n",
    "    ...state.messages,\n",
    "  ]);\n",
    "\n",
    "  const CATEGORIZATION_SYSTEM_TEMPLATE = `You are an expert customer support routing system.\n",
    "Your job is to detect whether a customer support representative is routing a user to a billing team or a technical team, or if they are just responding conversationally.`;\n",
    "  const CATEGORIZATION_HUMAN_TEMPLATE =\n",
    "    `The previous conversation is an interaction between a customer support representative and a user.\n",
    "Extract whether the representative is routing the user to a billing or technical team, or whether they are just responding conversationally.\n",
    "Respond with a JSON object containing a single key called \"nextRepresentative\" with one of the following values:\n",
    "\n",
    "If they want to route the user to the billing team, respond only with the word \"BILLING\".\n",
    "If they want to route the user to the technical team, respond only with the word \"TECHNICAL\".\n",
    "Otherwise, respond only with the word \"RESPOND\".`;\n",
    "  const categorizationResponse = await model.invoke([{\n",
    "    role: \"system\",\n",
    "    content: CATEGORIZATION_SYSTEM_TEMPLATE,\n",
    "  },\n",
    "  ...state.messages,\n",
    "  {\n",
    "    role: \"user\",\n",
    "    content: CATEGORIZATION_HUMAN_TEMPLATE,\n",
    "  }],\n",
    "  {\n",
    "    response_format: {\n",
    "      type: \"json_object\",\n",
    "      schema: zodToJsonSchema(\n",
    "        z.object({\n",
    "          nextRepresentative: z.enum([\"BILLING\", \"TECHNICAL\", \"RESPOND\"]),\n",
    "        })\n",
    "      )\n",
    "    }\n",
    "  });\n",
    "  // Some chat models can return complex content, but Together will not\n",
    "  const categorizationOutput = JSON.parse(categorizationResponse.content as string);\n",
    "  // Will append the response message to the current interaction state\n",
    "  return { messages: [supportResponse], nextRepresentative: categorizationOutput.nextRepresentative };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Together AI's JSON mode above to guarantee parseable output when deciding the next representative.\n",
    "\n",
    "Next, our nodes representing billing and technical support. We give special\n",
    "instructions in the billing prompt that it can choose to authorize refunds by\n",
    "routing to another agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const billingSupport = async (state: typeof StateAnnotation.State) => {\n",
    "  const SYSTEM_TEMPLATE =\n",
    "    `You are an expert billing support specialist for LangCorp, a company that sells computers.\n",
    "Help the user to the best of your ability, but be concise in your responses.\n",
    "You have the ability to authorize refunds, which you can do by transferring the user to another agent who will collect the required information.\n",
    "If you do, assume the other agent has all necessary information about the customer and their order.\n",
    "You do not need to ask the user for more information.\n",
    "\n",
    "Help the user to the best of your ability, but be concise in your responses.`;\n",
    "\n",
    "  let trimmedHistory = state.messages;\n",
    "  // Make the user's question the most recent message in the history.\n",
    "  // This helps small models stay focused.\n",
    "  if (trimmedHistory.at(-1)._getType() === \"ai\") {\n",
    "    trimmedHistory = trimmedHistory.slice(0, -1);\n",
    "  }\n",
    "\n",
    "  const billingRepResponse = await model.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: SYSTEM_TEMPLATE,\n",
    "    },\n",
    "    ...trimmedHistory,\n",
    "  ]);\n",
    "  const CATEGORIZATION_SYSTEM_TEMPLATE =\n",
    "    `Your job is to detect whether a billing support representative wants to refund the user.`;\n",
    "  const CATEGORIZATION_HUMAN_TEMPLATE =\n",
    "    `The following text is a response from a customer support representative.\n",
    "Extract whether they want to refund the user or not.\n",
    "Respond with a JSON object containing a single key called \"nextRepresentative\" with one of the following values:\n",
    "\n",
    "If they want to refund the user, respond only with the word \"REFUND\".\n",
    "Otherwise, respond only with the word \"RESPOND\".\n",
    "\n",
    "Here is the text:\n",
    "\n",
    "<text>\n",
    "${billingRepResponse.content}\n",
    "</text>.`;\n",
    "  const categorizationResponse = await model.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: CATEGORIZATION_SYSTEM_TEMPLATE,\n",
    "    },\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: CATEGORIZATION_HUMAN_TEMPLATE,\n",
    "    }\n",
    "  ], {\n",
    "    response_format: {\n",
    "      type: \"json_object\",\n",
    "      schema: zodToJsonSchema(\n",
    "        z.object({\n",
    "          nextRepresentative: z.enum([\"REFUND\", \"RESPOND\"]),\n",
    "        })\n",
    "      )\n",
    "    }\n",
    "  });\n",
    "  const categorizationOutput = JSON.parse(categorizationResponse.content as string);\n",
    "  return {\n",
    "    messages: billingRepResponse,\n",
    "    nextRepresentative: categorizationOutput.nextRepresentative,\n",
    "  };\n",
    "};\n",
    "\n",
    "const technicalSupport = async (state: typeof StateAnnotation.State) => {\n",
    "  const SYSTEM_TEMPLATE =\n",
    "    `You are an expert at diagnosing technical computer issues. You work for a company called LangCorp that sells computers.\n",
    "Help the user to the best of your ability, but be concise in your responses.`;\n",
    "\n",
    "  let trimmedHistory = state.messages;\n",
    "  // Make the user's question the most recent message in the history.\n",
    "  // This helps small models stay focused.\n",
    "  if (trimmedHistory.at(-1)._getType() === \"ai\") {\n",
    "    trimmedHistory = trimmedHistory.slice(0, -1);\n",
    "  }\n",
    "\n",
    "  const response = await model.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: SYSTEM_TEMPLATE,\n",
    "    },\n",
    "    ...trimmedHistory,\n",
    "  ]);\n",
    "\n",
    "  return {\n",
    "    messages: response,\n",
    "  };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a node that will handle refunds. The logic is stubbed out here since\n",
    "it's not a real system, but in practice you could add a real tool here requiring human\n",
    "approval. We use a special error called a `NodeInterrupt` in order to allow for resumption\n",
    "of the graph later, after a human has examined the state and confirmed that a refund is suitable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { NodeInterrupt } from \"@langchain/langgraph\";\n",
    "\n",
    "const handleRefund = async (state: typeof StateAnnotation.State) => {\n",
    "  if (!state.refundAuthorized) {\n",
    "    console.log(\"--- HUMAN AUTHORIZATION REQUIRED FOR REFUND ---\");\n",
    "    throw new NodeInterrupt(\"Human authorization required.\")\n",
    "  }\n",
    "  return {\n",
    "    messages: {\n",
    "      role: \"assistant\",\n",
    "      content: \"Refund processed!\",\n",
    "    },\n",
    "  };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start building our graph out now by adding all of the above functions as nodes and setting `initial_support` as our starting node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "let builder = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"initial_support\", initialSupport)\n",
    "  .addNode(\"billing_support\", billingSupport)\n",
    "  .addNode(\"technical_support\", technicalSupport)\n",
    "  .addNode(\"handle_refund\", handleRefund)\n",
    "  .addEdge(\"__start__\", \"initial_support\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the nodes\n",
    "\n",
    "Great! Now let's move onto the edges. These edges will evaluate the current\n",
    "state of the graph created by the return values of the individual nodes and\n",
    "route execution accordingly.\n",
    "\n",
    "First, we want our `initial_support` node to either delegate to the billing\n",
    "node, technical node, or just respond directly to the user. Here's one example\n",
    "of how we might do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added edges!\n"
     ]
    }
   ],
   "source": [
    "builder = builder.addConditionalEdges(\"initial_support\", async (state: typeof StateAnnotation.State) => {\n",
    "  if (state.nextRepresentative.includes(\"BILLING\")) {\n",
    "    return \"billing\";\n",
    "  } else if (state.nextRepresentative.includes(\"TECHNICAL\")) {\n",
    "    return \"technical\";\n",
    "  } else {\n",
    "    return \"conversational\";\n",
    "  }\n",
    "}, {\n",
    "  billing: \"billing_support\",\n",
    "  technical: \"technical_support\",\n",
    "  conversational: \"__end__\",\n",
    "});\n",
    "\n",
    "console.log(\"Added edges!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We do not use tool calling here for formatting the next step from\n",
    "the history because our model does not support it, but you can apply it here if\n",
    "your model does.\n",
    "\n",
    "Let's continue. We add an edge making the technical support node always end,\n",
    "since it has no tools to call. The billing support node uses a conditional edge\n",
    "since it can either call the refund tool or end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added edges!\n"
     ]
    }
   ],
   "source": [
    "builder = builder\n",
    "  .addEdge(\"technical_support\", \"__end__\")\n",
    "  .addConditionalEdges(\"billing_support\", async (state) => {\n",
    "    if (state.nextRepresentative.includes(\"REFUND\")) {\n",
    "      return \"refund\";\n",
    "    } else {\n",
    "      return \"__end__\";\n",
    "    }\n",
    "  }, {\n",
    "    refund: \"handle_refund\",\n",
    "    __end__: \"__end__\",\n",
    "  })\n",
    "  .addEdge(\"handle_refund\", \"__end__\");\n",
    "\n",
    "console.log(\"Added edges!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finalize our graph by calling `.compile()`. We'll also use an in-memory checkpointer to store state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const graph = builder.compile({\n",
    "  checkpointer,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a representation of the currently constructed graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const representation = graph.getGraph();\n",
    "const image = await representation.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's test it!\n",
    "\n",
    "We can get the returned value from the executed nodes as they are generated\n",
    "using the `.stream()` runnable method (we also could go even more granular and\n",
    "get output as it is generated using `.streamEvents()`, but this requires a bit\n",
    "more parsing).\n",
    "\n",
    "Here's an example with a billing related refund query. Because of how we defined our state, the input must be a message (or a list of messages) representing the user's question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STEP---\n",
      "{\n",
      "  initial_support: {\n",
      "    messages: AIMessage {\n",
      "      \"id\": \"8beb633a396c67fd-SJC\",\n",
      "      \"content\": \"I'd be happy to help you with that. However, I need to check on our refund policy for you. Can you please hold for just a moment while I transfer you to our billing team? They'll be able to assist you with the refund process.\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 53,\n",
      "          \"promptTokens\": 116,\n",
      "          \"totalTokens\": 169\n",
      "        },\n",
      "        \"finish_reason\": \"eos\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 116,\n",
      "        \"output_tokens\": 53,\n",
      "        \"total_tokens\": 169\n",
      "      }\n",
      "    },\n",
      "    nextRepresentative: 'BILLING'\n",
      "  }\n",
      "}\n",
      "---END STEP---\n",
      "---STEP---\n",
      "{\n",
      "  billing_support: {\n",
      "    messages: AIMessage {\n",
      "      \"id\": \"8beb634908a12500-SJC\",\n",
      "      \"content\": \"I'd be happy to assist you with a refund. I'll transfer you to our Refunds Team, who will guide you through the process. Please hold for just a moment.\\n\\n(Transfer to Refunds Team)\\n\\nRefunds Team: Hi, I'm here to help with your refund request for order #182818. Can you please confirm your refund amount and reason for return?\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 77,\n",
      "          \"promptTokens\": 139,\n",
      "          \"totalTokens\": 216\n",
      "        },\n",
      "        \"finish_reason\": \"eos\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 139,\n",
      "        \"output_tokens\": 77,\n",
      "        \"total_tokens\": 216\n",
      "      }\n",
      "    },\n",
      "    nextRepresentative: 'REFUND'\n",
      "  }\n",
      "}\n",
      "---END STEP---\n",
      "--- HUMAN AUTHORIZATION REQUIRED FOR REFUND ---\n",
      "---STEP---\n",
      "{}\n",
      "---END STEP---\n"
     ]
    }
   ],
   "source": [
    "const stream = await graph.stream({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"I've changed my mind and I want a refund for order #182818!\",\n",
    "    }\n",
    "  ]\n",
    "}, {\n",
    "  configurable: {\n",
    "    thread_id: \"refund_testing_id\",\n",
    "  }\n",
    "});\n",
    "\n",
    "for await (const value of stream) {\n",
    "  console.log(\"---STEP---\");\n",
    "  console.log(value);\n",
    "  console.log(\"---END STEP---\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This LangSmith trace](https://smith.langchain.com/public/86af9fe3-452e-4249-abec-7ddc0752a704/r) illustrates that execution goes to `billing_support`, but then hits our dynamic interrupt because `refundAuthorized` is not set in the graph state. We can see this by inspecting the current state of the graph and noting that there is an interrupt when running `handle_refund`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TASKS [\n",
      "  {\n",
      "    \"id\": \"5ab19c8b-c947-5bf7-a3aa-4edae60c1a96\",\n",
      "    \"name\": \"handle_refund\",\n",
      "    \"interrupts\": [\n",
      "      {\n",
      "        \"value\": \"Human authorization required.\",\n",
      "        \"when\": \"during\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const currentState = await graph.getState({ configurable: { thread_id: \"refund_testing_id\" } });\n",
    "\n",
    "console.log(\"CURRENT TASKS\", JSON.stringify(currentState.tasks, null, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the next tasks if we were to resume execution would be `handle_refund` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT TASKS [ 'handle_refund' ]\n"
     ]
    }
   ],
   "source": [
    "console.log(\"NEXT TASKS\", currentState.next);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this will again hit the interrupt because `refundAuthorized` is not set. If we update the state to set `refundAuthorized` to true, then resume the graph by running it with the same `thread_id` and passing `null` as the input, execution will continue and the refund will process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  handle_refund: { messages: { role: 'assistant', content: 'Refund processed!' } }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await graph.updateState({ configurable: { thread_id: \"refund_testing_id\" } }, {\n",
    "  refundAuthorized: true,\n",
    "});\n",
    "\n",
    "const resumedStream = await graph.stream(null, { configurable: { thread_id: \"refund_testing_id\" }});\n",
    "\n",
    "for await (const value of resumedStream) {\n",
    "  console.log(value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to see a LangSmith trace of the above run](https://smith.langchain.com/public/1c626e0f-5827-47c3-aadb-ec571dd37eb5/r)\n",
    "\n",
    "Now, let's try a technical question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  initial_support: {\n",
      "    messages: AIMessage {\n",
      "      \"id\": \"8beb66886c0c15d8-SJC\",\n",
      "      \"content\": \"Oh no, sorry to hear that! Water damage can be a real challenge. Have you tried unplugging it and letting it dry out for a bit? Sometimes, it's just a matter of giving it some time to recover.\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 47,\n",
      "          \"promptTokens\": 115,\n",
      "          \"totalTokens\": 162\n",
      "        },\n",
      "        \"finish_reason\": \"eos\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 115,\n",
      "        \"output_tokens\": 47,\n",
      "        \"total_tokens\": 162\n",
      "      }\n",
      "    },\n",
      "    nextRepresentative: 'TECHNICAL'\n",
      "  }\n",
      "}\n",
      "{\n",
      "  technical_support: {\n",
      "    messages: AIMessage {\n",
      "      \"id\": \"8beb66986df91701-SJC\",\n",
      "      \"content\": \"Sorry to hear that. Water damage can be a real challenge. Let's try to troubleshoot the issue.\\n\\nCan you tell me:\\n\\n1. How long was the computer submerged in water?\\n2. Did you turn it off before it got wet, or was it on at the time?\\n3. Have you tried unplugging the power cord and pressing the power button for 30 seconds to discharge any residual power?\\n\\nThis will help me narrow down the possible causes and suggest the next steps.\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 99,\n",
      "          \"promptTokens\": 70,\n",
      "          \"totalTokens\": 169\n",
      "        },\n",
      "        \"finish_reason\": \"eos\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 70,\n",
      "        \"output_tokens\": 99,\n",
      "        \"total_tokens\": 169\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const technicalStream = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"My LangCorp computer isn't turning on because I dropped it in water.\",\n",
    "  }]\n",
    "}, {\n",
    "  configurable: {\n",
    "    thread_id: \"technical_testing_id\"\n",
    "  }\n",
    "});\n",
    "\n",
    "for await (const value of technicalStream) {\n",
    "  console.log(value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to see a LangSmith trace of the above run](https://smith.langchain.com/public/d131f6ea-e8d6-41f2-addd-fd9b55c6e057/r)\n",
    "\n",
    "We can see the query gets correctly routed to the technical support node!\n",
    "\n",
    "Finally, let's try a simple conversational response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  initial_support: {\n",
      "    messages: AIMessage {\n",
      "      \"id\": \"8beb6712294915e3-SJC\",\n",
      "      \"content\": \"Hi Cobb! I'm doing great, thanks for asking. How can I help you today? Are you looking to purchase a new computer or just have a question about our products?\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 37,\n",
      "          \"promptTokens\": 108,\n",
      "          \"totalTokens\": 145\n",
      "        },\n",
      "        \"finish_reason\": \"eos\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 108,\n",
      "        \"output_tokens\": 37,\n",
      "        \"total_tokens\": 145\n",
      "      }\n",
      "    },\n",
      "    nextRepresentative: 'RESPOND'\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const conversationalStream = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"How are you? I'm Cobb.\"\n",
    "  }]\n",
    "}, {\n",
    "  configurable: {\n",
    "    thread_id: \"conversational_testing_id\"\n",
    "  }\n",
    "});\n",
    "\n",
    "for await (const value of conversationalStream) {\n",
    "  console.log(value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the `initial_support` node handles it by itself, with no routing to technical or billing support.\n",
    "\n",
    "[Click here to see a LangSmith trace of the above run](https://smith.langchain.com/public/4cf5ff90-b9c6-4628-989f-28cb0c4910db/r)\n",
    "\n",
    "## Further reading\n",
    "\n",
    "You may have noticed that the response from each node adds a message to the history in the state, and that as a result we end up with multiple assistant messages in a row corresponding to the different customer support personas the LLM takes on.\n",
    "\n",
    "With `MessagesAnnotation`, it is possible to trim this state by returning a **message modifier** containing the same `id` as the message you want to remove. [See this guide](/langgraphjs/how-tos/delete-messages/) for more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/add-summary-conversation-history.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to add summary of the conversation history\n",
    "\n",
    "One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. One way to work around that is to create a summary of the conversation to date, and use that with the past N messages. This guide will go through an example of how to do that.\n",
    "\n",
    "This will involve a few steps:\n",
    "- Check if the conversation is too long (can be done by checking number of messages or length of messages)\n",
    "- If yes, the create summary (will need a prompt for this)\n",
    "- Then remove all except the last N messages\n",
    "\n",
    "A big part of this is deleting old messages. For an in depth guide on how to do that, see [this guide](../delete-messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the packages we're going to want to use\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core uuid\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use)\n",
    "\n",
    "```typescript\n",
    "process.env.ANTHROPIC_API_KEY = 'YOUR_API_KEY'\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```typescript\n",
    "process.env.LANGCHAIN_TRACING_V2 = 'true'\n",
    "process.env.LANGCHAIN_API_KEY = 'YOUR_API_KEY'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84835fdb-a5f3-4c90-85f3-0e6257650aba",
   "metadata": {},
   "source": [
    "## Build the chatbot\n",
    "\n",
    "Let's now build the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a76502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { SystemMessage, HumanMessage, AIMessage, RemoveMessage } from \"@langchain/core/messages\";\n",
    "import { MemorySaver } from \"@langchain/langgraph-checkpoint\";\n",
    "import { MessagesAnnotation, StateGraph, START, END, Annotation } from \"@langchain/langgraph\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "// We will add a `summary` attribute (in addition to `messages` key,\n",
    "// which MessagesAnnotation already has)\n",
    "const GraphAnnotation = Annotation.Root({\n",
    "  ...MessagesAnnotation.spec,\n",
    "  summary: Annotation<string>({\n",
    "    reducer: (_, action) => action,\n",
    "    default: () => \"\",\n",
    "  })\n",
    "})\n",
    "\n",
    "// We will use this model for both the conversation and the summarization\n",
    "const model = new ChatAnthropic({ model: \"claude-3-haiku-20240307\" });\n",
    "\n",
    "// Define the logic to call the model\n",
    "async function callModel(state: typeof GraphAnnotation.State): Promise<Partial<typeof GraphAnnotation.State>> {\n",
    "  // If a summary exists, we add this in as a system message\n",
    "  const { summary } = state;\n",
    "  let { messages } = state;\n",
    "  if (summary) {\n",
    "    const systemMessage = new SystemMessage({\n",
    "      id: uuidv4(),\n",
    "      content: `Summary of conversation earlier: ${summary}`\n",
    "    });\n",
    "    messages = [systemMessage, ...messages];\n",
    "  }\n",
    "  const response = await model.invoke(messages);\n",
    "  // We return an object, because this will get added to the existing state\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "// We now define the logic for determining whether to end or summarize the conversation\n",
    "function shouldContinue(state: typeof GraphAnnotation.State): \"summarize_conversation\" | typeof END {\n",
    "  const messages = state.messages;\n",
    "  // If there are more than six messages, then we summarize the conversation\n",
    "  if (messages.length > 6) {\n",
    "    return \"summarize_conversation\";\n",
    "  }\n",
    "  // Otherwise we can just end\n",
    "  return END;\n",
    "}\n",
    "\n",
    "async function summarizeConversation(state: typeof GraphAnnotation.State): Promise<Partial<typeof GraphAnnotation.State>> {\n",
    "  // First, we summarize the conversation\n",
    "  const { summary, messages } = state;\n",
    "  let summaryMessage: string;\n",
    "  if (summary) {\n",
    "    // If a summary already exists, we use a different system prompt\n",
    "    // to summarize it than if one didn't\n",
    "    summaryMessage = `This is summary of the conversation to date: ${summary}\\n\\n` +\n",
    "      \"Extend the summary by taking into account the new messages above:\";\n",
    "  } else {\n",
    "    summaryMessage = \"Create a summary of the conversation above:\";\n",
    "  }\n",
    "\n",
    "  const allMessages = [...messages, new HumanMessage({\n",
    "    id: uuidv4(),\n",
    "    content: summaryMessage,\n",
    "  })];\n",
    "  const response = await model.invoke(allMessages);\n",
    "  // We now need to delete messages that we no longer want to show up\n",
    "  // I will delete all but the last two messages, but you can change this\n",
    "  const deleteMessages = messages.slice(0, -2).map((m) => new RemoveMessage({ id: m.id }));\n",
    "  if (typeof response.content !== \"string\") {\n",
    "    throw new Error(\"Expected a string response from the model\");\n",
    "  }\n",
    "  return { summary: response.content, messages: deleteMessages };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(GraphAnnotation)\n",
    "  // Define the conversation node and the summarize node\n",
    "  .addNode(\"conversation\", callModel)\n",
    "  .addNode(\"summarize_conversation\", summarizeConversation)\n",
    "  // Set the entrypoint as conversation\n",
    "  .addEdge(START, \"conversation\")\n",
    "  // We now add a conditional edge\n",
    "  .addConditionalEdges(\n",
    "    // First, we define the start node. We use `conversation`.\n",
    "    // This means these are the edges taken after the `conversation` node is called.\n",
    "    \"conversation\",\n",
    "    // Next, we pass in the function that will determine which node is called next.\n",
    "    shouldContinue\n",
    "  )\n",
    "  // We now add a normal edge from `summarize_conversation` to END.\n",
    "  // This means that after `summarize_conversation` is called, we end.\n",
    "  .addEdge(\"summarize_conversation\", END);\n",
    "\n",
    "// Finally, we compile it!\n",
    "const app = workflow.compile({ checkpointer: memory });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2872e-04b3-4c44-9e03-9e84a5230adf",
   "metadata": {},
   "source": [
    "## Using the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc697132-8fa1-4bf5-9722-56a9859331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "const printUpdate = (update: Record<string, any>) => {\n",
    "  Object.keys(update).forEach((key) => {\n",
    "    const value = update[key];\n",
    "\n",
    "    if (\"messages\" in value && Array.isArray(value.messages)) {\n",
    "      value.messages.forEach((msg) => {\n",
    "        console.log(`\\n================================ ${msg._getType()} Message =================================`)\n",
    "        console.log(msg.content);\n",
    "      })\n",
    "    }\n",
    "    if (\"summary\" in value && value.summary) {\n",
    "      console.log(value.summary);\n",
    "    }\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi! I'm bob\n",
      "\n",
      "================================ ai Message =================================\n",
      "Okay, got it. Hello Bob, it's nice to chat with you again. I recognize that you've repeatedly stated your name is Bob throughout our conversation. Please let me know if there is anything I can assist you with.\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ ai Message =================================\n",
      "In our conversation, you have stated multiple times that your name is Bob. For example, you said \"I'm Bob\", \"hi! I'm bob\", and similar variations where you clearly identified yourself as Bob.\n",
      "i like the celtics!\n",
      "\n",
      "================================ ai Message =================================\n",
      "Ah I see, you mentioned earlier that you like the Boston Celtics basketball team. That's great, the Celtics have a long and storied history in the NBA. As one of the league's original franchises, they've won a record 17 NBA championships over the years, the most of any team. Some of their most iconic players have included Bill Russell, Larry Bird, and Kevin McHale. The Celtics are known for their passionate fan base and intense rivalries with teams like the Los Angeles Lakers. It's always exciting to follow such a successful and historic franchise. I'm glad to hear you're a fan of the Celtics!\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const config = { configurable: { thread_id: \"4\" }, streamMode: \"updates\" as const }\n",
    "\n",
    "const inputMessage = new HumanMessage(\"hi! I'm bob\")\n",
    "console.log(inputMessage.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage] }, config)) {\n",
    "  printUpdate(event)\n",
    "}\n",
    "\n",
    "const inputMessage2 = new HumanMessage(\"What did I sat my name was?\")\n",
    "console.log(inputMessage2.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage2] }, config)) {\n",
    "  printUpdate(event)\n",
    "}\n",
    "\n",
    "const inputMessage3 = new HumanMessage(\"i like the celtics!\")\n",
    "console.log(inputMessage3.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage3] }, config)) {\n",
    "  printUpdate(event)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760e219-a7fc-4d81-b4e8-1334c5afc510",
   "metadata": {},
   "source": [
    "We can see that so far no summarization has happened - this is because there are only six messages in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "935265a0-d511-475a-8a0d-b3c3cc5e42a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"content\": \"hi! I'm bob\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"msg_01G6WKqKHK8W371793Hm6eNM\",\n",
      "      \"content\": \"Okay, got it. Hello Bob, it's nice to chat with you again. I recognize that you've repeatedly stated your name is Bob throughout our conversation. Please let me know if there is anything I can assist you with.\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"id\": \"msg_01G6WKqKHK8W371793Hm6eNM\",\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 579,\n",
      "          \"output_tokens\": 50\n",
      "        }\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"id\": \"msg_01G6WKqKHK8W371793Hm6eNM\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 579,\n",
      "          \"output_tokens\": 50\n",
      "        },\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      \"content\": \"What did I sat my name was?\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"msg_0118BAsHL4Ew8N2926aYQaot\",\n",
      "      \"content\": \"In our conversation, you have stated multiple times that your name is Bob. For example, you said \\\"I'm Bob\\\", \\\"hi! I'm bob\\\", and similar variations where you clearly identified yourself as Bob.\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"id\": \"msg_0118BAsHL4Ew8N2926aYQaot\",\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 310,\n",
      "          \"output_tokens\": 46\n",
      "        }\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"id\": \"msg_0118BAsHL4Ew8N2926aYQaot\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 310,\n",
      "          \"output_tokens\": 46\n",
      "        },\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      \"content\": \"i like the celtics!\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"msg_01RVrMuSvr17kZdepJZb7rZM\",\n",
      "      \"content\": \"Ah I see, you mentioned earlier that you like the Boston Celtics basketball team. That's great, the Celtics have a long and storied history in the NBA. As one of the league's original franchises, they've won a record 17 NBA championships over the years, the most of any team. Some of their most iconic players have included Bill Russell, Larry Bird, and Kevin McHale. The Celtics are known for their passionate fan base and intense rivalries with teams like the Los Angeles Lakers. It's always exciting to follow such a successful and historic franchise. I'm glad to hear you're a fan of the Celtics!\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"id\": \"msg_01RVrMuSvr17kZdepJZb7rZM\",\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 365,\n",
      "          \"output_tokens\": 141\n",
      "        }\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"id\": \"msg_01RVrMuSvr17kZdepJZb7rZM\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 365,\n",
      "          \"output_tokens\": 141\n",
      "        },\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": []\n",
      "    }\n",
      "  ],\n",
      "  summary: 'Got it, let me extend the summary further:\\n' +\n",
      "    '\\n' +\n",
      "    `The conversation began with you introducing yourself as Bob, which I acknowledged and said I was happy to chat with you again. You then repeated \"I'm Bob\", and I confirmed I recognized your name.\\n` +\n",
      "    '\\n' +\n",
      "    \"You next stated that you like the Boston Celtics basketball team, which prompted me to provide some background information about the team's history and success. \\n\" +\n",
      "    '\\n' +\n",
      "    'You then summarized the conversation up to that point, which I expanded upon in detail, recapping the key points of our exchange so far.\\n' +\n",
      "    '\\n' +\n",
      "    `In the most recent messages, you greeted me again by saying \"hi! I'm bob\", which I recognized as you reiterating your name, consistent with how you had introduced yourself earlier.\\n` +\n",
      "    '\\n' +\n",
      "    `Now, in the latest message, you have simply stated \"hi! I'm bob\" once more. I continue to understand your name is Bob based on you stating that multiple times throughout our conversation.\\n` +\n",
      "    '\\n' +\n",
      "    \"Please let me know if I'm still missing anything or if you have any other points you'd like me to add to the summary. I'm happy to keep building on it.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const values = (await app.getState(config)).values\n",
    "console.log(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40eddb-9a31-4410-a4c0-9762e2d89e56",
   "metadata": {},
   "source": [
    "Now let's send another message in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "048805a4-3d97-4e76-ac45-8d80d4364c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like how much they win\n",
      "\n",
      "================================ ai Message =================================\n",
      "I agree, the Celtics' impressive track record of wins and championships is a big part of what makes them such an iconic and beloved team. Their sustained success over decades is really remarkable. \n",
      "\n",
      "Some key reasons why the Celtics have been so dominant:\n",
      "\n",
      "- Great coaching - They've had legendary coaches like Red Auerbach, Doc Rivers, and Brad Stevens who have led the team to titles.\n",
      "\n",
      "- Hall of Fame players - Superstars like Bill Russell, Larry Bird, Kevin Garnett, and Paul Pierce have powered the Celtics' championship runs.\n",
      "\n",
      "- Winning culture - The Celtics have built a winning mentality and tradition of excellence that gets passed down to each new generation of players.\n",
      "\n",
      "- Loyal fanbase - The passionate Celtics fans pack the stands and provide a strong home court advantage.\n",
      "\n",
      "The combination of top-tier talent, smart management, and devoted supporters has allowed the Celtics to reign as one of the NBA's premier franchises for generations. Their ability to consistently win at the highest level is truly impressive. I can understand why you as a fan really appreciate and enjoy that aspect of the team.\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "Okay, let me extend the summary further based on the latest messages:\n",
      "\n",
      "The conversation began with you introducing yourself as Bob, which I acknowledged. You then repeated \"I'm Bob\" a few times, and I confirmed I recognized your name.\n",
      "\n",
      "You then expressed that you like the Boston Celtics basketball team, which led me to provide some background information about the team's history and success. You agreed that you appreciate how much the Celtics win.\n",
      "\n",
      "In the most recent messages, you greeted me again by saying \"hi! I'm bob\", reiterating your name just as you had done earlier. I reiterated that I understand your name is Bob based on you stating that multiple times throughout our conversation.\n",
      "\n",
      "In your latest message, you simply stated \"hi! I'm bob\" once more, further confirming your name. I have continued to demonstrate that I understand your name is Bob, as you have consistently identified yourself as such.\n",
      "\n",
      "Please let me know if I'm still missing anything or if you have any other points you'd like me to add to this extended summary of our discussion so far. I'm happy to keep building on it.\n"
     ]
    }
   ],
   "source": [
    "const inputMessage4 = new HumanMessage(\"i like how much they win\")\n",
    "console.log(inputMessage4.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage4] }, config)) {\n",
    "  printUpdate(event)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b196367-6151-4982-9430-3db7373de06e",
   "metadata": {},
   "source": [
    "If we check the state now, we can see that we have a summary of the conversation, as well as the last two messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09ebb693-4738-4474-a095-6491def5c5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"content\": \"i like how much they win\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"msg_01W8C1nXeydqM3E31uCCeJXt\",\n",
      "      \"content\": \"I agree, the Celtics' impressive track record of wins and championships is a big part of what makes them such an iconic and beloved team. Their sustained success over decades is really remarkable. \\n\\nSome key reasons why the Celtics have been so dominant:\\n\\n- Great coaching - They've had legendary coaches like Red Auerbach, Doc Rivers, and Brad Stevens who have led the team to titles.\\n\\n- Hall of Fame players - Superstars like Bill Russell, Larry Bird, Kevin Garnett, and Paul Pierce have powered the Celtics' championship runs.\\n\\n- Winning culture - The Celtics have built a winning mentality and tradition of excellence that gets passed down to each new generation of players.\\n\\n- Loyal fanbase - The passionate Celtics fans pack the stands and provide a strong home court advantage.\\n\\nThe combination of top-tier talent, smart management, and devoted supporters has allowed the Celtics to reign as one of the NBA's premier franchises for generations. Their ability to consistently win at the highest level is truly impressive. I can understand why you as a fan really appreciate and enjoy that aspect of the team.\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"id\": \"msg_01W8C1nXeydqM3E31uCCeJXt\",\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 516,\n",
      "          \"output_tokens\": 244\n",
      "        }\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"id\": \"msg_01W8C1nXeydqM3E31uCCeJXt\",\n",
      "        \"model\": \"claude-3-haiku-20240307\",\n",
      "        \"stop_reason\": \"end_turn\",\n",
      "        \"stop_sequence\": null,\n",
      "        \"usage\": {\n",
      "          \"input_tokens\": 516,\n",
      "          \"output_tokens\": 244\n",
      "        },\n",
      "        \"type\": \"message\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": []\n",
      "    }\n",
      "  ],\n",
      "  summary: 'Okay, let me extend the summary further based on the latest messages:\\n' +\n",
      "    '\\n' +\n",
      "    `The conversation began with you introducing yourself as Bob, which I acknowledged. You then repeated \"I'm Bob\" a few times, and I confirmed I recognized your name.\\n` +\n",
      "    '\\n' +\n",
      "    \"You then expressed that you like the Boston Celtics basketball team, which led me to provide some background information about the team's history and success. You agreed that you appreciate how much the Celtics win.\\n\" +\n",
      "    '\\n' +\n",
      "    `In the most recent messages, you greeted me again by saying \"hi! I'm bob\", reiterating your name just as you had done earlier. I reiterated that I understand your name is Bob based on you stating that multiple times throughout our conversation.\\n` +\n",
      "    '\\n' +\n",
      "    `In your latest message, you simply stated \"hi! I'm bob\" once more, further confirming your name. I have continued to demonstrate that I understand your name is Bob, as you have consistently identified yourself as such.\\n` +\n",
      "    '\\n' +\n",
      "    \"Please let me know if I'm still missing anything or if you have any other points you'd like me to add to this extended summary of our discussion so far. I'm happy to keep building on it.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const values2 = (await app.getState(config)).values\n",
    "console.log(values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e4177-c0fc-4fd0-a494-dd03f7f2fddb",
   "metadata": {},
   "source": [
    "We can now resume having a conversation! Note that even though we only have the last two messages, we can still ask it questions about things mentioned earlier in the conversation (because we summarized those)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7094c5ab-66f8-42ff-b1c3-90c8a9468e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what's my name?\n",
      "\n",
      "================================ ai Message =================================\n",
      "Your name is Bob. You have stated this multiple times throughout our conversation, repeatedly introducing yourself as \"Bob\" or \"I'm Bob\".\n"
     ]
    }
   ],
   "source": [
    "const inputMessage5 = new HumanMessage(\"what's my name?\");\n",
    "console.log(inputMessage5.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage5] }, config)) {\n",
    "  printUpdate(event)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40e5db8e-9db9-4ac7-9d76-a99fd4034bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what NFL team do you think I like?\n",
      "\n",
      "================================ ai Message =================================\n",
      "I do not actually have any information about what NFL team you might like. In our conversation so far, you have only expressed that you are a fan of the Boston Celtics basketball team. You have not mentioned any preferences for NFL teams. Without you providing any additional details about your football team allegiances, I do not want to make an assumption about which NFL team you might be a fan of. Could you please let me know if there is an NFL team you particularly enjoy following?\n"
     ]
    }
   ],
   "source": [
    "const inputMessage6 = new HumanMessage(\"what NFL team do you think I like?\");\n",
    "console.log(inputMessage6.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage6] }, config)) {\n",
    "  printUpdate(event)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a1a0fda-5309-45f0-9465-9f3dff604d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like the patriots!\n",
      "\n",
      "================================ ai Message =================================\n",
      "Okay, got it. Based on your latest message, I now understand that in addition to being a fan of the Boston Celtics basketball team, you also like the New England Patriots NFL team.\n",
      "\n",
      "That makes a lot of sense given that both the Celtics and Patriots are major sports franchises based in the Boston/New England region. It's common for fans to follow multiple professional teams from the same geographic area.\n",
      "\n",
      "I appreciate you sharing this additional information about your football team preferences. Knowing that you're a Patriots fan provides helpful context about your sports interests and loyalties. It's good for me to have that understanding as we continue our conversation.\n",
      "\n",
      "Please let me know if there's anything else you'd like to discuss related to the Patriots, the Celtics, or your overall sports fandom. I'm happy to chat more about those topics.\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "\n",
      "================================ remove Message =================================\n",
      "\n",
      "Okay, got it - let me extend the summary further based on the latest messages:\n",
      "\n",
      "The conversation began with you introducing yourself as Bob, which I acknowledged. You then repeated \"I'm Bob\" a few times, and I confirmed I recognized your name.\n",
      "\n",
      "You then expressed that you like the Boston Celtics basketball team, which led me to provide some background information about the team's history and success. You agreed that you appreciate how much the Celtics win.\n",
      "\n",
      "In the most recent messages, you greeted me again by saying \"hi! I'm Bob\", reiterating your name just as you had done earlier. I reiterated that I understand your name is Bob based on you stating that multiple times throughout our conversation.\n",
      "\n",
      "You then asked what NFL team I think you might like, and I acknowledged that I did not have enough information to make an assumption about your NFL team preferences. You then revealed that you are also a fan of the New England Patriots, which I said makes sense given the Celtics and Patriots are both major sports franchises in the Boston/New England region.\n",
      "\n",
      "In your latest message, you simply stated \"hi! I'm Bob\" once more, further confirming your name. I have continued to demonstrate that I understand your name is Bob, as you have consistently identified yourself as such.\n",
      "\n",
      "Please let me know if I'm still missing anything or if you have any other points you'd like me to add to this extended summary of our discussion so far. I'm happy to keep building on it.\n"
     ]
    }
   ],
   "source": [
    "const inputMessage7 = new HumanMessage(\"i like the patriots!\");\n",
    "console.log(inputMessage7.content)\n",
    "for await (const event of await app.stream({ messages: [inputMessage7] }, config)) {\n",
    "  printUpdate(event)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/branching.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to create branches for parallel node execution\n",
        "\n",
        "LangGraph natively supports fan-out and fan-in using either regular edges or\n",
        "[conditionalEdges](/langgraphjs/reference/classes/langgraph.StateGraph.html#addConditionalEdges).\n",
        "\n",
        "This lets you run nodes in parallel to speed up your total graph execution.\n",
        "\n",
        "Below are some examples showing how to add create branching dataflows that work\n",
        "for you.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, install LangGraph.js\n",
        "\n",
        "```bash\n",
        "yarn add @langchain/langgraph @langchain/core\n",
        "```\n",
        "\n",
        "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
        "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
        "best-in-class observability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branching: LangGraphJS\n"
          ]
        }
      ],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "\n",
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"ls__...\"\n",
        "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "process.env.LANGCHAIN_PROJECT = \"Branching: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fan out, fan in\n",
        "\n",
        "First, we will make a simple graph that branches out and back in. When merging\n",
        "back in, the state updates from all branches are applied by your **reducer**\n",
        "(the `aggregate` method below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END, START, StateGraph, Annotation } from \"@langchain/langgraph\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  aggregate: Annotation<string[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  })\n",
        "});\n",
        "\n",
        "// Create the graph\n",
        "const nodeA = (state: typeof StateAnnotation.State) => {\n",
        "  console.log(`Adding I'm A to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm A`] };\n",
        "};\n",
        "const nodeB = (state: typeof StateAnnotation.State) => {\n",
        "  console.log(`Adding I'm B to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm B`] };\n",
        "};\n",
        "const nodeC = (state: typeof StateAnnotation.State) => {\n",
        "  console.log(`Adding I'm C to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm C`] };\n",
        "};\n",
        "const nodeD = (state: typeof StateAnnotation.State) => {\n",
        "  console.log(`Adding I'm D to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm D`] };\n",
        "};\n",
        "\n",
        "const builder = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"a\", nodeA)\n",
        "  .addEdge(START, \"a\")\n",
        "  .addNode(\"b\", nodeB)\n",
        "  .addNode(\"c\", nodeC)\n",
        "  .addNode(\"d\", nodeD)\n",
        "  .addEdge(\"a\", \"b\")\n",
        "  .addEdge(\"a\", \"c\")\n",
        "  .addEdge(\"b\", \"d\")\n",
        "  .addEdge(\"c\", \"d\")\n",
        "  .addEdge(\"d\", END);\n",
        "\n",
        "const graph = builder.compile();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const representation = graph.getGraph();\n",
        "const image = await representation.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding I'm A to \n",
            "Adding I'm B to I'm A\n",
            "Adding I'm C to I'm A\n",
            "Adding I'm D to I'm A,I'm B,I'm C\n",
            "Base Result:  { aggregate: [ \"I'm A\", \"I'm B\", \"I'm C\", \"I'm D\" ] }\n"
          ]
        }
      ],
      "source": [
        "// Invoke the graph\n",
        "const baseResult = await graph.invoke({ aggregate: [] });\n",
        "console.log(\"Base Result: \", baseResult);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional Branching\n",
        "\n",
        "If your fan-out is not deterministic, you can use\n",
        "[addConditionalEdges](/langgraphjs/reference/classes/index.StateGraph.html#addConditionalEdges)\n",
        "directly like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "const ConditionalBranchingAnnotation = Annotation.Root({\n",
        "  aggregate: Annotation<string[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  which: Annotation<string>({\n",
        "    reducer: (x: string, y: string) => (y ?? x),\n",
        "  })\n",
        "})\n",
        "\n",
        "// Create the graph\n",
        "const nodeA2 = (state: typeof ConditionalBranchingAnnotation.State) => {\n",
        "  console.log(`Adding I'm A to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm A`] };\n",
        "};\n",
        "const nodeB2 = (state: typeof ConditionalBranchingAnnotation.State) => {\n",
        "  console.log(`Adding I'm B to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm B`] };\n",
        "};\n",
        "const nodeC2 = (state: typeof ConditionalBranchingAnnotation.State) => {\n",
        "  console.log(`Adding I'm C to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm C`] };\n",
        "};\n",
        "const nodeD2 = (state: typeof ConditionalBranchingAnnotation.State) => {\n",
        "  console.log(`Adding I'm D to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm D`] };\n",
        "};\n",
        "const nodeE2 = (state: typeof ConditionalBranchingAnnotation.State) => {\n",
        "  console.log(`Adding I'm E to ${state.aggregate}`);\n",
        "  return { aggregate: [`I'm E`] };\n",
        "};\n",
        "\n",
        "// Define the route function\n",
        "function routeCDorBC(state: typeof ConditionalBranchingAnnotation.State): string[] {\n",
        "  if (state.which === \"cd\") {\n",
        "    return [\"c\", \"d\"];\n",
        "  }\n",
        "  return [\"b\", \"c\"];\n",
        "}\n",
        "\n",
        "const builder2 = new StateGraph(ConditionalBranchingAnnotation)\n",
        "  .addNode(\"a\", nodeA2)\n",
        "  .addEdge(START, \"a\")\n",
        "  .addNode(\"b\", nodeB2)\n",
        "  .addNode(\"c\", nodeC2)\n",
        "  .addNode(\"d\", nodeD2)\n",
        "  .addNode(\"e\", nodeE2)\n",
        "  // Add conditional edges\n",
        "  // Third parameter is to support visualizing the graph\n",
        "  .addConditionalEdges(\"a\", routeCDorBC, [\"b\", \"c\", \"d\"])\n",
        "  .addEdge(\"b\", \"e\")\n",
        "  .addEdge(\"c\", \"e\")\n",
        "  .addEdge(\"d\", \"e\")\n",
        "  .addEdge(\"e\", END);\n",
        "\n",
        "const graph2 = builder2.compile();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const representation2 = graph2.getGraph();\n",
        "const image2 = await representation2.drawMermaidPng();\n",
        "const arrayBuffer2 = await image2.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer2));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding I'm A to \n",
            "Adding I'm B to I'm A\n",
            "Adding I'm C to I'm A\n",
            "Adding I'm E to I'm A,I'm B,I'm C\n",
            "Result 1:  { aggregate: [ \"I'm A\", \"I'm B\", \"I'm C\", \"I'm E\" ], which: 'bc' }\n"
          ]
        }
      ],
      "source": [
        "// Invoke the graph\n",
        "let g2result = await graph2.invoke({ aggregate: [], which: \"bc\" });\n",
        "console.log(\"Result 1: \", g2result);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding I'm A to \n",
            "Adding I'm C to I'm A\n",
            "Adding I'm D to I'm A\n",
            "Adding I'm E to I'm A,I'm C,I'm D\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result 2:  { aggregate: [ \"I'm A\", \"I'm C\", \"I'm D\", \"I'm E\" ], which: 'cd' }\n"
          ]
        }
      ],
      "source": [
        "g2result = await graph2.invoke({ aggregate: [], which: \"cd\" });\n",
        "console.log(\"Result 2: \", g2result);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stable Sorting\n",
        "\n",
        "When fanned out, nodes are run in parallel as a single \"superstep\". The updates\n",
        "from each superstep are all applied to the state in sequence once the superstep\n",
        "has completed.\n",
        "\n",
        "If you need consistent, predetermined ordering of updates from a parallel\n",
        "superstep, you should write the outputs (along with an identifying key) to a\n",
        "separate field in your state, then combine them in the \"sink\" node by adding\n",
        "regular `edge`s from each of the fanout nodes to the rendezvous point.\n",
        "\n",
        "For instance, suppose I want to order the outputs of the parallel step by\n",
        "\"reliability\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding I'm A to \n",
            "Adding I'm B to I'm A\n",
            "Adding I'm C to I'm A\n",
            "Result 1:  {\n",
            "  aggregate: [ \"I'm A\", \"I'm C\", \"I'm B\", \"I'm E\" ],\n",
            "  which: 'bc',\n",
            "  fanoutValues: []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "type ScoredValue = {\n",
        "  value: string;\n",
        "  score: number;\n",
        "};\n",
        "\n",
        "const reduceFanouts = (left?: ScoredValue[], right?: ScoredValue[]) => {\n",
        "  if (!left) {\n",
        "    left = [];\n",
        "  }\n",
        "  if (!right || right?.length === 0) {\n",
        "    // Overwrite. Similar to redux.\n",
        "    return [];\n",
        "  }\n",
        "  return left.concat(right);\n",
        "};\n",
        "\n",
        "const StableSortingAnnotation = Annotation.Root({\n",
        "  aggregate: Annotation<string[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  which: Annotation<string>({\n",
        "    reducer: (x: string, y: string) => (y ?? x),\n",
        "  }),\n",
        "  fanoutValues: Annotation<ScoredValue[]>({\n",
        "    reducer: reduceFanouts,\n",
        "  }),\n",
        "})\n",
        "\n",
        "\n",
        "class ParallelReturnNodeValue {\n",
        "  private _value: string;\n",
        "  private _score: number;\n",
        "\n",
        "  constructor(nodeSecret: string, score: number) {\n",
        "    this._value = nodeSecret;\n",
        "    this._score = score;\n",
        "  }\n",
        "\n",
        "  public call(state: typeof StableSortingAnnotation.State) {\n",
        "    console.log(`Adding ${this._value} to ${state.aggregate}`);\n",
        "    return { fanoutValues: [{ value: this._value, score: this._score }] };\n",
        "  }\n",
        "}\n",
        "\n",
        "// Create the graph\n",
        "\n",
        "const nodeA3 = (state: typeof StableSortingAnnotation.State) => {\n",
        "  console.log(`Adding I'm A to ${state.aggregate}`);\n",
        "  return { aggregate: [\"I'm A\"] };\n",
        "};\n",
        "\n",
        "const nodeB3 = new ParallelReturnNodeValue(\"I'm B\", 0.1);\n",
        "const nodeC3 = new ParallelReturnNodeValue(\"I'm C\", 0.9);\n",
        "const nodeD3 = new ParallelReturnNodeValue(\"I'm D\", 0.3);\n",
        "\n",
        "const aggregateFanouts = (state: typeof StableSortingAnnotation.State) => {\n",
        "  // Sort by score (reversed)\n",
        "  state.fanoutValues.sort((a, b) => b.score - a.score);\n",
        "  return {\n",
        "    aggregate: state.fanoutValues.map((v) => v.value).concat([\"I'm E\"]),\n",
        "    fanoutValues: [],\n",
        "  };\n",
        "};\n",
        "\n",
        "// Define the route function\n",
        "function routeBCOrCD(state: typeof StableSortingAnnotation.State): string[] {\n",
        "  if (state.which === \"cd\") {\n",
        "    return [\"c\", \"d\"];\n",
        "  }\n",
        "  return [\"b\", \"c\"];\n",
        "}\n",
        "\n",
        "const builder3 = new StateGraph(StableSortingAnnotation)\n",
        "  .addNode(\"a\", nodeA3)\n",
        "  .addEdge(START, \"a\")\n",
        "  .addNode(\"b\", nodeB3.call.bind(nodeB3))\n",
        "  .addNode(\"c\", nodeC3.call.bind(nodeC3))\n",
        "  .addNode(\"d\", nodeD3.call.bind(nodeD3))\n",
        "  .addNode(\"e\", aggregateFanouts)\n",
        "  .addConditionalEdges(\"a\", routeBCOrCD, [\"b\", \"c\", \"d\"])\n",
        "  .addEdge(\"b\", \"e\")\n",
        "  .addEdge(\"c\", \"e\")\n",
        "  .addEdge(\"d\", \"e\")\n",
        "  .addEdge(\"e\", END);\n",
        "\n",
        "const graph3 = builder3.compile();\n",
        "\n",
        "// Invoke the graph\n",
        "let g3result = await graph3.invoke({ aggregate: [], which: \"bc\" });\n",
        "console.log(\"Result 1: \", g3result);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our aggregateFanouts \"sink\" node in this case took the mapped values and then\n",
        "sorted them in a consistent way. Notice that, because it returns an empty array\n",
        "for `fanoutValues`, our `reduceFanouts` reducer function decided to overwrite\n",
        "the previous values in the state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding I'm A to \n",
            "Adding I'm C to I'm A\n",
            "Adding I'm D to I'm A\n",
            "Result 2:  {\n",
            "  aggregate: [ \"I'm A\", \"I'm C\", \"I'm D\", \"I'm E\" ],\n",
            "  which: 'cd',\n",
            "  fanoutValues: []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "let g3result2 = await graph3.invoke({ aggregate: [], which: \"cd\" });\n",
        "console.log(\"Result 2: \", g3result2);\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
</file>

<file path="how-tos/breakpoints.ipynb">
{
 "cells": [
  {
   "attachments": {
    "b5aa6d4c-8dfd-490d-a53c-69c1368cd5b5.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to add breakpoints\n",
    "\n",
    "!!! tip \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following concepts:\n",
    "\n",
    "    * [Breakpoints](/langgraphjs/concepts/breakpoints)\n",
    "    * [LangGraph Glossary](/langgraphjs/concepts/low_level)\n",
    "\n",
    "Human-in-the-loop (HIL) interactions are crucial for [agentic systems](/langgraphjs/concepts/agentic_concepts/#human-in-the-loop). [Breakpoints](/langgraphjs/concepts/low_level/#breakpoints) are a common HIL interaction pattern, allowing the graph to stop at specific steps and seek human approval before proceeding (e.g., for sensitive actions).\n",
    "\n",
    "Breakpoints are built on top of LangGraph [checkpoints](/langgraphjs/concepts/low_level/#checkpointer), which save the graph's state after each node execution. Checkpoints are saved in [threads](/langgraphjs/concepts/low_level/#threads) that preserve graph state and can be accessed after a graph has finished execution. This allows for graph execution to pause at specific points, await human approval, and then resume execution from the last checkpoint.\n",
    "\n",
    "![Screenshot 2024-07-03 at 1.32.19 PM.png](attachment:b5aa6d4c-8dfd-490d-a53c-69c1368cd5b5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fd44d-c0f8-473a-ae80-4b4668ad7f47",
   "metadata": {},
   "source": [
    "## Simple Usage\n",
    "\n",
    "Let's look at very basic usage of this.\n",
    "\n",
    "Below, we do two things:\n",
    "\n",
    "1) We specify the [breakpoint](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#breakpoints) using `interruptBefore` the specified step.\n",
    "\n",
    "2) We set up a [checkpointer](https://langchain-ai.github.io/langgraphjs/concepts/#checkpoints) to save the state of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b53f191-1e86-4881-a667-d46a3d66958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, START, END, Annotation } from \"@langchain/langgraph\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const GraphState = Annotation.Root({\n",
    "  input: Annotation<string>\n",
    "});\n",
    "\n",
    "const step1 = (state: typeof GraphState.State) => {\n",
    "  console.log(\"---Step 1---\");\n",
    "  return state;\n",
    "}\n",
    "\n",
    "const step2 = (state: typeof GraphState.State) => {\n",
    "  console.log(\"---Step 2---\");\n",
    "  return state;\n",
    "}\n",
    "\n",
    "const step3 = (state: typeof GraphState.State) => {\n",
    "  console.log(\"---Step 3---\");\n",
    "  return state;\n",
    "}\n",
    "\n",
    "\n",
    "const builder = new StateGraph(GraphState)\n",
    "  .addNode(\"step1\", step1)\n",
    "  .addNode(\"step2\", step2)\n",
    "  .addNode(\"step3\", step3)\n",
    "  .addEdge(START, \"step1\")\n",
    "  .addEdge(\"step1\", \"step2\")\n",
    "  .addEdge(\"step2\", \"step3\")\n",
    "  .addEdge(\"step3\", END);\n",
    "\n",
    "\n",
    "// Set up memory\n",
    "const graphStateMemory = new MemorySaver()\n",
    "\n",
    "const graph = builder.compile({\n",
    "  checkpointer: graphStateMemory,\n",
    "  interruptBefore: [\"step3\"]\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2dd360b",
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraphGraphState = graph.getGraph();\n",
    "const graphStateImage = await drawableGraphGraphState.drawMermaidPng();\n",
    "const graphStateArrayBuffer = await graphStateImage.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(graphStateArrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5f80f-9d8c-4a39-b198-24fe94132b41",
   "metadata": {},
   "source": [
    "We create a [thread ID](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#threads) for the checkpointer.\n",
    "\n",
    "We run until step 3, as defined with `interruptBefore`. \n",
    "\n",
    "After the user input / approval, [we resume execution](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#breakpoints) by invoking the graph with `null`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe04a7f-988e-4a36-8ce8-2c49fab0130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- hello world ---\n",
      "---Step 1---\n",
      "--- hello world ---\n",
      "---Step 2---\n",
      "--- hello world ---\n",
      "---GRAPH INTERRUPTED---\n",
      "---Step 3---\n",
      "--- hello world ---\n"
     ]
    }
   ],
   "source": [
    "// Input\n",
    "const initialInput = { input: \"hello world\" };\n",
    "\n",
    "// Thread\n",
    "const graphStateConfig = { configurable: { thread_id: \"1\" }, streamMode: \"values\" as const };\n",
    "\n",
    "// Run the graph until the first interruption\n",
    "for await (const event of await graph.stream(initialInput, graphStateConfig)) {\n",
    "  console.log(`--- ${event.input} ---`);\n",
    "}\n",
    "\n",
    "// Will log when the graph is interrupted, after step 2.\n",
    "console.log(\"---GRAPH INTERRUPTED---\");\n",
    "\n",
    "// If approved, continue the graph execution. We must pass `null` as\n",
    "// the input here, or the graph will\n",
    "for await (const event of await graph.stream(null, graphStateConfig)) {\n",
    "  console.log(`--- ${event.input} ---`);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333b771",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "In the context of agents, breakpoints are useful to manually approve certain agent actions.\n",
    " \n",
    "To show this, we will build a relatively simple ReAct-style agent that does tool calling. \n",
    "\n",
    "We'll add a breakpoint before the `action` node is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6098e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the tool\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { StateGraph, START, END } from \"@langchain/langgraph\";\n",
    "import { MemorySaver, Annotation } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { BaseMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "});\n",
    "\n",
    "const search = tool((_) => {\n",
    "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\";\n",
    "}, {\n",
    "    name: \"search\",\n",
    "    description: \"Call to surf the web.\",\n",
    "    schema: z.string(),\n",
    "})\n",
    "\n",
    "const tools = [search]\n",
    "const toolNode = new ToolNode<typeof AgentState.State>(tools)\n",
    "\n",
    "// Set up the model\n",
    "const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" })\n",
    "const modelWithTools = model.bindTools(tools)\n",
    "\n",
    "\n",
    "// Define nodes and conditional edges\n",
    "\n",
    "// Define the function that determines whether to continue or not\n",
    "function shouldContinue(state: typeof AgentState.State): \"action\" | typeof END {\n",
    "    const lastMessage = state.messages[state.messages.length - 1];\n",
    "    // If there is no function call, then we finish\n",
    "    if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) {\n",
    "        return END;\n",
    "    }\n",
    "    // Otherwise if there is, we continue\n",
    "    return \"action\";\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModel(state: typeof AgentState.State): Promise<Partial<typeof AgentState.State>> {\n",
    "    const messages = state.messages;\n",
    "    const response = await modelWithTools.invoke(messages);\n",
    "    // We return an object with a messages property, because this will get added to the existing list\n",
    "    return { messages: [response] };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "    // Define the two nodes we will cycle between\n",
    "    .addNode(\"agent\", callModel)\n",
    "    .addNode(\"action\", toolNode)\n",
    "    // We now add a conditional edge\n",
    "    .addConditionalEdges(\n",
    "        // First, we define the start node. We use `agent`.\n",
    "        // This means these are the edges taken after the `agent` node is called.\n",
    "        \"agent\",\n",
    "        // Next, we pass in the function that will determine which node is called next.\n",
    "        shouldContinue\n",
    "    )\n",
    "    // We now add a normal edge from `action` to `agent`.\n",
    "    // This means that after `action` is called, `agent` node is called next.\n",
    "    .addEdge(\"action\", \"agent\")\n",
    "    // Set the entrypoint as `agent`\n",
    "    // This means that this node is the first one called\n",
    "    .addEdge(START, \"agent\");\n",
    "\n",
    "\n",
    "// Setup memory\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile({\n",
    "    checkpointer: memory,\n",
    "    interruptBefore: [\"action\"]\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4476aef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[CopilotKit] CopilotKit initialized"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = app.getGraph();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
   "metadata": {},
   "source": [
    "## Interacting with the Agent\n",
    "\n",
    "We can now interact with the agent.\n",
    "\n",
    "We see that it stops before calling a tool, because `interruptBefore` is set before the `action` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "search for the weather in sf now\n",
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: \"Certainly! I'll search for the current weather in San Francisco for you. Let me use the search function to find this information.\"\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01R524BmxkEm7Rf5Ss53cqkM',\n",
      "    name: 'search',\n",
      "    input: { input: 'current weather in San Francisco' }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "// Input\n",
    "const inputs = new HumanMessage(\"search for the weather in sf now\");\n",
    "\n",
    "// Thread\n",
    "const config = { configurable: { thread_id: \"3\" }, streamMode: \"values\" as const };\n",
    "\n",
    "for await (const event of await app.stream({\n",
    "    messages: [inputs]\n",
    "}, config)) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca3814-db08-4b0b-8c0c-95b6c5440c81",
   "metadata": {},
   "source": [
    "**Resume**\n",
    "\n",
    "We can now call the agent again with no inputs to continue.\n",
    "\n",
    "This will run the tool as requested.\n",
    "\n",
    "Running an interrupted graph with `null` in the inputs means to `proceed as if the interruption didn't occur.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51923913-20f7-4ee1-b9ba-d01f5fb2869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ tool Message (1) =================================\n",
      "It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\n",
      "================================ ai Message (1) =================================\n",
      "Based on the search results, I can provide you with information about the current weather in San Francisco:\n",
      "\n",
      "The weather in San Francisco is currently sunny. This means it's a clear day with plenty of sunshine, which is great for outdoor activities or simply enjoying the city.\n",
      "\n",
      "However, I should note that the search result included an unusual comment about Geminis. This appears to be unrelated to the weather and might be a quirk of the search engine or a reference to something else entirely. For accurate and detailed weather information, it would be best to check a reliable weather service or website.\n",
      "\n",
      "Is there anything else you'd like to know about the weather in San Francisco or any other location?\n"
     ]
    }
   ],
   "source": [
    "for await (const event of await app.stream(null, config)) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/command.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33ecddc-6818-41a3-9d0d-b1b1cbcd286d",
   "metadata": {},
   "source": [
    "# How to combine control flow and state updates with Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a8d03-80b4-47fd-9b17-e26aa9b081f3",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "      This guide assumes familiarity with the following:\n",
    "      <ul>\n",
    "        <li><a href=\"/langgraphjs/concepts/low_level/#state\">State</a></li>\n",
    "        <li><a href=\"/langgraphjs/concepts/low_level/#nodes\">Nodes</a></li>\n",
    "        <li><a href=\"/langgraphjs/concepts/low_level/#edges\">Edges</a></li>\n",
    "        <li><a href=\"/langgraphjs/concepts/low_level/#command\">Command</a></li>\n",
    "      </ul>\n",
    "      <p>\n",
    "        This functionality also requires <code>@langchain/langgraph>=0.2.29</code>.\n",
    "      </p>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a `Command` object from node functions:\n",
    "\n",
    "```ts\n",
    "const myNode = (state: typeof StateAnnotation.State) => {\n",
    "  return new Command({\n",
    "    // state update\n",
    "    update: {\n",
    "      foo: \"bar\",\n",
    "    },\n",
    "    // control flow\n",
    "    goto: \"myOtherNode\",\n",
    "  });\n",
    "};\n",
    "```\n",
    "\n",
    "If you are using [subgraphs](/langgraphjs/concepts/low_level/#subgraphs), you might want to navigate from a node a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph: Command.PARENT` in Command:\n",
    "\n",
    "```ts\n",
    "const myNode = (state: typeof StateAnnotation.State) => {\n",
    "  return new Command({\n",
    "    update: { foo: \"bar\" },\n",
    "    goto: \"other_subgraph\", // where `other_subgraph` is a node in the parent graph\n",
    "    graph: Command.PARENT,\n",
    "  });\n",
    "};\n",
    "```\n",
    "\n",
    "This guide shows how you can use `Command` to add dynamic control flow in your LangGraph app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c3f866-8c20-40c7-a201-35f6c9f4b680",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f131c92-4744-431c-a89c-7c382a15b79f",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c228f-6882-4757-8e7e-1ca51328af4a",
   "metadata": {},
   "source": [
    "Let's create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08d957-b3d2-4538-bf4a-68ef90a51b98",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4539b81b-09e9-4660-ac55-1b1775e13892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation, Command } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define graph state\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  foo: Annotation<string>,\n",
    "});\n",
    "\n",
    "// Define the nodes\n",
    "const nodeA = async (_state: typeof StateAnnotation.State) => {\n",
    "  console.log(\"Called A\");\n",
    "  // this is a replacement for a real conditional edge function\n",
    "  const goto = Math.random() > .5 ? \"nodeB\" : \"nodeC\";\n",
    "  // note how Command allows you to BOTH update the graph state AND route to the next node\n",
    "  return new Command({\n",
    "    // this is the state update\n",
    "    update: {\n",
    "      foo: \"a\",\n",
    "    },\n",
    "    // this is a replacement for an edge\n",
    "    goto,\n",
    "  });\n",
    "};\n",
    "\n",
    "// Nodes B and C are unchanged\n",
    "const nodeB = async (state: typeof StateAnnotation.State) => {\n",
    "  console.log(\"Called B\");\n",
    "  return {\n",
    "    foo: state.foo + \"|b\",\n",
    "  };\n",
    "}\n",
    "\n",
    "const nodeC = async (state: typeof StateAnnotation.State) => {\n",
    "  console.log(\"Called C\");\n",
    "  return {\n",
    "    foo: state.foo + \"|c\",\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc25eb-4876-482e-bb10-d763023cdaad",
   "metadata": {},
   "source": [
    "We can now create the `StateGraph` with the above nodes. Notice that the graph doesn't have [conditional edges](/langgraphjs/concepts/low_level#conditional-edges) for routing! This is because control flow is defined with `Command` inside `nodeA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6711650-4380-4551-a007-2805f49ab2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// NOTE: there are no edges between nodes A, B and C!\n",
    "const graph = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"nodeA\", nodeA, {\n",
    "    ends: [\"nodeB\", \"nodeC\"],\n",
    "  })\n",
    "  .addNode(\"nodeB\", nodeB)\n",
    "  .addNode(\"nodeC\", nodeC)\n",
    "  .addEdge(\"__start__\", \"nodeA\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab344c5-d634-4d7d-b3b4-edf4fa875311",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Important</p>\n",
    "    <p>\n",
    "      You might have noticed that we add an <code>ends</code> field as an extra param to the node where we use <code>Command</code>. This is necessary for graph compilation and validation, and tells LangGraph that <code>nodeA</code> can navigate to <code>nodeB</code> and <code>nodeC</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb810e5-8822-4c09-8d53-c55cd0f5d42e",
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = await graph.getGraphAsync();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb6c32-e6fb-4c94-8182-e351ed52a45d",
   "metadata": {},
   "source": [
    "If we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88a5d9b-ee08-4ed4-9c65-6e868210bfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called A\n",
      "Called B\n",
      "{ foo: 'a|b' }\n"
     ]
    }
   ],
   "source": [
    "await graph.invoke({ foo: \"\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a73d81-4e12-4378-b07d-1c5bf0b2ed71",
   "metadata": {},
   "source": [
    "## Navigating to a node in a parent graph\n",
    "\n",
    "Now let's demonstrate how you can navigate from inside a subgraph to a different node in a parent graph. We'll do so by changing `node_a` in the above example into a single-node graph that we'll add as a subgraph to our parent graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c459cd94-457e-420e-a227-80b33d95def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called A\n",
      "Called C\n",
      "{ foo: 'a|c' }\n"
     ]
    }
   ],
   "source": [
    "// Define the nodes\n",
    "const nodeASubgraph = async (_state: typeof StateAnnotation.State) => {\n",
    "  console.log(\"Called A\");\n",
    "  // this is a replacement for a real conditional edge function\n",
    "  const goto = Math.random() > .5 ? \"nodeB\" : \"nodeC\";\n",
    "  // note how Command allows you to BOTH update the graph state AND route to the next node\n",
    "  return new Command({\n",
    "    update: {\n",
    "      foo: \"a\",\n",
    "    },\n",
    "    goto,\n",
    "    // this tells LangGraph to navigate to node_b or node_c in the parent graph\n",
    "    // NOTE: this will navigate to the closest parent graph relative to the subgraph\n",
    "    graph: Command.PARENT,\n",
    "  });\n",
    "};\n",
    "\n",
    "const subgraph = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"nodeA\", nodeASubgraph)\n",
    "  .addEdge(\"__start__\", \"nodeA\")\n",
    "  .compile();\n",
    "\n",
    "const parentGraph= new StateGraph(StateAnnotation)\n",
    "  .addNode(\"subgraph\", subgraph, { ends: [\"nodeB\", \"nodeC\"] })\n",
    "  .addNode(\"nodeB\", nodeB)\n",
    "  .addNode(\"nodeC\", nodeC)\n",
    "  .addEdge(\"__start__\", \"subgraph\")\n",
    "  .compile();\n",
    "  \n",
    "await parentGraph.invoke({ foo: \"\" });"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/configuration.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8221c040",
   "metadata": {},
   "source": [
    "# How to add runtime configuration to your graph\n",
    "\n",
    "Once you've created an app in LangGraph, you likely will want to permit\n",
    "configuration at runtime.\n",
    "\n",
    "For instance, you may want to let the LLM or prompt be selected dynamically,\n",
    "configure a user's `user_id` to enforce row-level security, etc.\n",
    "\n",
    "In LangGraph, configuration and other\n",
    "[\"out-of-band\" communication](https://en.wikipedia.org/wiki/Out-of-band) is done\n",
    "via the\n",
    "[RunnableConfig](https://v02.api.js.langchain.com/interfaces/langchain_core_runnables.RunnableConfig.html),\n",
    "which is always the second positional arg when invoking your application.\n",
    "\n",
    "Below, we walk through an example of letting you configure a user ID and pick\n",
    "which model to use.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This guide will use Anthropic's Claude 3 Haiku and OpenAI's GPT-4o model. We\n",
    "will optionally set our API key for\n",
    "[LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dcd657",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"Configuration: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f018e",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We will create an exceedingly simple message graph for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf2fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import {\n",
    "  END,\n",
    "  START,\n",
    "  StateGraph,\n",
    "  Annotation,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "  userInfo: Annotation<string | undefined>({\n",
    "    reducer: (x, y) => {\n",
    "      return y ? y : x ? x : \"N/A\";\n",
    "    },\n",
    "    default: () => \"N/A\",\n",
    "  })\n",
    "});\n",
    "\n",
    "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are a helpful assistant.\\n\\n## User Info:\\n{userInfo}\"],\n",
    "  [\"placeholder\", \"{messages}\"],\n",
    "]);\n",
    "\n",
    "const callModel = async (\n",
    "  state: typeof AgentState.State,\n",
    "  config?: RunnableConfig,\n",
    ") => {\n",
    "  const { messages, userInfo } = state;\n",
    "  const modelName = config?.configurable?.model;\n",
    "  const model = modelName === \"claude\"\n",
    "    ? new ChatAnthropic({ model: \"claude-3-haiku-20240307\" })\n",
    "    : new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "  const chain = promptTemplate.pipe(model);\n",
    "  const response = await chain.invoke(\n",
    "    {\n",
    "      messages,\n",
    "      userInfo,\n",
    "    },\n",
    "    config,\n",
    "  );\n",
    "  return { messages: [response] };\n",
    "};\n",
    "\n",
    "const fetchUserInformation = async (\n",
    "  _: typeof AgentState.State,\n",
    "  config?: RunnableConfig,\n",
    ") => {\n",
    "  const userDB = {\n",
    "    user1: {\n",
    "      name: \"John Doe\",\n",
    "      email: \"jod@langchain.ai\",\n",
    "      phone: \"+1234567890\",\n",
    "    },\n",
    "    user2: {\n",
    "      name: \"Jane Doe\",\n",
    "      email: \"jad@langchain.ai\",\n",
    "      phone: \"+0987654321\",\n",
    "    },\n",
    "  };\n",
    "  const userId = config?.configurable?.user;\n",
    "  if (userId) {\n",
    "    const user = userDB[userId as keyof typeof userDB];\n",
    "    if (user) {\n",
    "      return {\n",
    "        userInfo:\n",
    "          `Name: ${user.name}\\nEmail: ${user.email}\\nPhone: ${user.phone}`,\n",
    "      };\n",
    "    }\n",
    "  }\n",
    "  return { userInfo: \"N/A\" };\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph(AgentState)\n",
    "  .addNode(\"fetchUserInfo\", fetchUserInformation)\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addEdge(START, \"fetchUserInfo\")\n",
    "  .addEdge(\"fetchUserInfo\", \"agent\")\n",
    "  .addEdge(\"agent\", END);\n",
    "\n",
    "const graph = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae55d0e",
   "metadata": {},
   "source": [
    "## Call with config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca608969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Your email is jod@langchain.ai.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const config = {\n",
    "  configurable: {\n",
    "    model: \"openai\",\n",
    "    user: \"user1\",\n",
    "  },\n",
    "};\n",
    "const inputs = {\n",
    "  messages: [new HumanMessage(\"Could you remind me of my email??\")],\n",
    "};\n",
    "for await (\n",
    "  const { messages } of await graph.stream(inputs, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "  })\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdf011",
   "metadata": {},
   "source": [
    "## Change the config\n",
    "\n",
    "Now let's try the same input with a different user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e568e8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Your email address is jad@langchain.ai.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const config2 = {\n",
    "  configurable: {\n",
    "    model: \"openai\",\n",
    "    user: \"user2\",\n",
    "  },\n",
    "};\n",
    "const inputs2 = {\n",
    "  messages: [new HumanMessage(\"Could you remind me of my email??\")],\n",
    "};\n",
    "for await (\n",
    "  const { messages } of await graph.stream(inputs2, {\n",
    "    ...config2,\n",
    "    streamMode: \"values\",\n",
    "  })\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000b97c",
   "metadata": {},
   "source": [
    "Check out the\n",
    "[LangSmith Trace (link)](https://smith.langchain.com/public/bbd3561f-c0d1-4886-ae18-a6626c6b8670/r/946098b5-84d3-4456-a03c-5dbc8591e76b)\n",
    "for this run to \"see what the LLM sees\".\n",
    "\n",
    "## Config schema\n",
    "\n",
    "You can also pass an annotation defining the shape of `config.configurable` into your graph. This will currently only expose type information on the compiled graph, and will not filter out keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f703d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected I am expected\n",
      "Unexpected I am unexpected but present\n"
     ]
    }
   ],
   "source": [
    "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const ConfigurableAnnotation = Annotation.Root({\n",
    "  expectedField: Annotation<string>,\n",
    "});\n",
    "\n",
    "const printNode = async (\n",
    "  state: typeof MessagesAnnotation.State,\n",
    "  config: RunnableConfig<typeof ConfigurableAnnotation.State>\n",
    ") => {\n",
    "  console.log(\"Expected\", config.configurable?.expectedField);\n",
    "  // @ts-expect-error This type will be present even though is not in the typing\n",
    "  console.log(\"Unexpected\", config.configurable?.unexpectedField);\n",
    "  return {};\n",
    "};\n",
    "\n",
    "const graphWithConfigSchema = new StateGraph(MessagesAnnotation, ConfigurableAnnotation)\n",
    "  .addNode(\"printNode\", printNode)\n",
    "  .addEdge(START, \"printNode\")\n",
    "  .compile();\n",
    "\n",
    "const result = await graphWithConfigSchema.invoke({\n",
    "  messages: [{ role: \"user\", content: \"Echo!\"} ]\n",
    "}, { configurable: { expectedField: \"I am expected\", unexpectedField: \"I am unexpected but present\" } });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f98e1",
   "metadata": {},
   "source": [
    "```\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/create-react-agent.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the prebuilt ReAct agent\n",
    "\n",
    "In this how-to we'll create a simple [ReAct](https://arxiv.org/abs/2210.03629) agent app that can check the weather. The app consists of an agent (LLM) and tools. As we interact with the app, we will first call the agent (LLM) to decide if we should use tools. Then we will run a loop:  \n",
    "\n",
    "1. If the agent said to take an action (i.e. call tool), we'll run the tools and pass the results back to the agent\n",
    "2. If the agent did not ask to run tools, we will finish (respond to the user)\n",
    "\n",
    "<div class=\"admonition warning\">\n",
    "    <p class=\"admonition-title\">Prebuilt Agent</p>\n",
    "    <p>\n",
    "Please note that here will we use a prebuilt agent. One of the big benefits of LangGraph is that you can easily create your own agent architectures. So while it's fine to start here to build an agent quickly, we would strongly recommend learning how to build your own agent so that you can take full advantage of LangGraph. Read <a href=\"https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/#quickstart\"> this guide </a> to learn how to create your own ReAct agent from scratch.\n",
    "    </p>\n",
    "</div>   \n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the required packages.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
    "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct Agent: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\"\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"ReAct Agent: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Now we can use the prebuilt `createReactAgent` function to setup our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from '@langchain/core/tools';\n",
    "import { z } from 'zod';\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o\",\n",
    "});\n",
    "\n",
    "const getWeather = tool((input) => {\n",
    "  if (['sf', 'san francisco', 'san francisco, ca'].includes(input.location.toLowerCase())) {\n",
    "    return 'It\\'s 60 degrees and foggy.';\n",
    "  } else {\n",
    "    return 'It\\'s 90 degrees and sunny.';\n",
    "  }\n",
    "}, {\n",
    "  name: 'get_weather',\n",
    "  description: 'Call to get the current weather.',\n",
    "  schema: z.object({\n",
    "    location: z.string().describe(\"Location to get the weather for.\"),\n",
    "  })\n",
    "})\n",
    "\n",
    "const agent = createReactAgent({ llm: model, tools: [getWeather] });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "First, let's visualize the graph we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
           "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const graph = agent.getGraph();\n",
    "const image = await graph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the app with an input that needs a tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the weather in sf?\n",
      "-----\n",
      "\n",
      "[\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { location: 'San Francisco, CA' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_wfXCh5IhSp1C0Db3gaJWDbRP'\n",
      "  }\n",
      "]\n",
      "-----\n",
      "\n",
      "It's 60 degrees and foggy.\n",
      "-----\n",
      "\n",
      "The weather in San Francisco is currently 60 degrees and foggy.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let inputs = { messages: [{ role: \"user\", content: \"what is the weather in SF?\" }] };\n",
    "\n",
    "let stream = await agent.stream(inputs, {\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (const { messages } of stream) {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a question that doesn't need tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who built you?\n",
      "-----\n",
      "\n",
      "I was developed by OpenAI, an AI research and deployment company.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [{ role: \"user\", content: \"who built you?\" }] };\n",
    "\n",
    "stream = await agent.stream(inputs, {\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "  const { messages } of stream\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! The agent correctly didn't call any tools and instead directly responded to the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/cross-thread-persistence-functional.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2eecb96-cf0e-47ed-8116-88a7eaa4236d",
   "metadata": {},
   "source": [
    "# How to add cross-thread persistence (functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Functional API](../../concepts/functional_api/)\n",
    "    - [Persistence](../../concepts/persistence/)\n",
    "    - [Memory](../../concepts/memory/)\n",
    "    - [Chat Models](https://js.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "LangGraph allows you to persist data across **different [threads](../../concepts/persistence/#threads)**. For instance, you can store information about users (their names or preferences) in a shared (cross-thread) memory and reuse them in the new threads (e.g., new conversations).\n",
    "\n",
    "When using the [functional API](../../concepts/functional_api/), you can set it up to store and retrieve memories by using the [Store](/langgraphjs/reference/classes/checkpoint.BaseStore.html) interface:\n",
    "\n",
    "1. Create an instance of a `Store`\n",
    "\n",
    "    ```ts\n",
    "    import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "    \n",
    "    const store = new InMemoryStore();\n",
    "    ```\n",
    "\n",
    "2. Pass the `store` instance to the `entrypoint()` wrapper function. It will be passed to the workflow as `config.store`.\n",
    "\n",
    "    ```ts\n",
    "    import { entrypoint } from \"@langchain/langgraph\";\n",
    "    \n",
    "    const workflow = entrypoint({\n",
    "      store,\n",
    "      name: \"myWorkflow\",\n",
    "    }, async (input, config) => {\n",
    "      const foo = await myTask({input, store: config.store});\n",
    "      ...\n",
    "    });\n",
    "    ```\n",
    "    \n",
    "In this guide, we will show how to construct and use a workflow that has a shared memory implemented using the [Store](/langgraphjs/reference/classes/checkpoint.BaseStore.html) interface.\n",
    "\n",
    "!!! tip \"Note\"\n",
    "\n",
    "    If you need to add cross-thread persistence to a `StateGraph`, check out this [how-to guide](../cross-thread-persistence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fceff",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/anthropic @langchain/core uuid\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic and OpenAI (the LLM and embeddings we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n",
    "process.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b3d42-3d2c-455e-ac10-e2ae74dc1cf1",
   "metadata": {},
   "source": [
    "## Example: simple chatbot with long-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": [
    "### Define store\n",
    "\n",
    "In this example we will create a workflow that will be able to retrieve information about a user's preferences. We will do so by defining an `InMemoryStore` - an object that can store data in memory and query that data.\n",
    "\n",
    "When storing objects using the `Store` interface you define two things:\n",
    "\n",
    "* the namespace for the object, a tuple (similar to directories)\n",
    "* the object key (similar to filenames)\n",
    "\n",
    "In our example, we'll be using `[\"memories\", <user_id>]` as namespace and random UUID as key for each new memory.\n",
    "\n",
    "Let's first define our store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "\n",
    "const inMemoryStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: new OpenAIEmbeddings({\n",
    "      model: \"text-embedding-3-small\",\n",
    "    }),\n",
    "    dims: 1536,\n",
    "  },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389c9f4-226d-40c7-8bfc-ee8aac24f79d",
   "metadata": {},
   "source": [
    "### Create workflow\n",
    "\n",
    "Now let's create our workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a30a362-528c-45ee-9df6-630d2d843588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { v4 } from \"uuid\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import {\n",
    "  entrypoint,\n",
    "  task,\n",
    "  MemorySaver,\n",
    "  addMessages,\n",
    "  type BaseStore,\n",
    "  getStore,\n",
    "} from \"@langchain/langgraph\";\n",
    "import type { BaseMessage, BaseMessageLike } from \"@langchain/core/messages\";\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-latest\",\n",
    "});\n",
    "\n",
    "const callModel = task(\"callModel\", async (\n",
    "  messages: BaseMessage[],\n",
    "  memoryStore: BaseStore,\n",
    "  userId: string\n",
    ") => {\n",
    "  const namespace = [\"memories\", userId];\n",
    "  const lastMessage = messages.at(-1);\n",
    "  if (typeof lastMessage?.content !== \"string\") {\n",
    "    throw new Error(\"Received non-string message content.\");\n",
    "  }\n",
    "  const memories = await memoryStore.search(namespace, {\n",
    "    query: lastMessage.content,\n",
    "  });\n",
    "  const info = memories.map((memory) => memory.value.data).join(\"\\n\");\n",
    "  const systemMessage = `You are a helpful assistant talking to the user. User info: ${info}`;\n",
    "  \n",
    "  // Store new memories if the user asks the model to remember\n",
    "  if (lastMessage.content.toLowerCase().includes(\"remember\")) {\n",
    "    // Hard-coded for demo\n",
    "    const memory = `Username is Bob`;\n",
    "    await memoryStore.put(namespace, v4(), { data: memory });\n",
    "  }\n",
    "  const response = await model.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: systemMessage \n",
    "    },\n",
    "    ...messages\n",
    "  ]);\n",
    "  return response;\n",
    "});\n",
    "\n",
    "// NOTE: we're passing the store object here when creating a workflow via entrypoint()\n",
    "const workflow = entrypoint({\n",
    "  checkpointer: new MemorySaver(),\n",
    "  store: inMemoryStore,\n",
    "  name: \"workflow\",\n",
    "}, async (params: {\n",
    "  messages: BaseMessageLike[];\n",
    "  userId: string;\n",
    "}, config) => {\n",
    "  const messages = addMessages([], params.messages)\n",
    "  const response = await callModel(messages, config.store, params.userId);\n",
    "  return entrypoint.final({\n",
    "    value: response,\n",
    "    save: addMessages(messages, response),\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a4a18-67e4-4f0b-b655-a29bbe202e1c",
   "metadata": {},
   "source": [
    "The current store is passed in as part of the entrypoint's second argument, as `config.store`.\n",
    "\n",
    "!!! note Note\n",
    "\n",
    "    If you're using LangGraph Cloud or LangGraph Studio, you __don't need__ to pass store into the entrypoint, since it's done automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": [
    "### Run the workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842c626-6cd9-4f58-b549-58978e478098",
   "metadata": {},
   "source": [
    "Now let's specify a user ID in the config and tell the model our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c871a073-a466-46ad-aafe-2b870831057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"msg_01U4xHvf4REPSCGWzpLeh1qJ\",\n",
      "  \"content\": \"Hi Bob! Nice to meet you. I'll remember that your name is Bob. How can I help you today?\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01U4xHvf4REPSCGWzpLeh1qJ\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"cache_creation_input_tokens\": 0,\n",
      "      \"cache_read_input_tokens\": 0,\n",
      "      \"output_tokens\": 27\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01U4xHvf4REPSCGWzpLeh1qJ\",\n",
      "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"cache_creation_input_tokens\": 0,\n",
      "      \"cache_read_input_tokens\": 0,\n",
      "      \"output_tokens\": 27\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 28,\n",
      "    \"output_tokens\": 27,\n",
      "    \"total_tokens\": 55,\n",
      "    \"input_token_details\": {\n",
      "      \"cache_creation\": 0,\n",
      "      \"cache_read\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config = {\n",
    "  configurable: {\n",
    "    thread_id: \"1\",\n",
    "  },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "\n",
    "const inputMessage = {\n",
    "  role: \"user\",\n",
    "  content: \"Hi! Remember: my name is Bob\",\n",
    "};\n",
    "\n",
    "const stream = await workflow.stream({ messages: [inputMessage], userId: \"1\" }, config);\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d862be40-1f8a-4057-81c4-b7bf073dc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"msg_01LB4YapkFawBUbpiu3oeWbF\",\n",
      "  \"content\": \"Your name is Bob.\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01LB4YapkFawBUbpiu3oeWbF\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"cache_creation_input_tokens\": 0,\n",
      "      \"cache_read_input_tokens\": 0,\n",
      "      \"output_tokens\": 8\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01LB4YapkFawBUbpiu3oeWbF\",\n",
      "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"cache_creation_input_tokens\": 0,\n",
      "      \"cache_read_input_tokens\": 0,\n",
      "      \"output_tokens\": 8\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 28,\n",
      "    \"output_tokens\": 8,\n",
      "    \"total_tokens\": 36,\n",
      "    \"input_token_details\": {\n",
      "      \"cache_creation\": 0,\n",
      "      \"cache_read\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config2 = {\n",
    "  configurable: {\n",
    "    thread_id: \"2\",\n",
    "  },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "\n",
    "const followupStream = await workflow.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what is my name?\",\n",
    "  }],\n",
    "  userId: \"1\"\n",
    "}, config2);\n",
    "\n",
    "for await (const chunk of followupStream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd01ec-f135-4811-8743-daff8daea422",
   "metadata": {},
   "source": [
    "We can now inspect our in-memory store and verify that we have in fact saved the memories for the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76cde493-89cf-4709-a339-207d2b7e9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ data: 'Username is Bob' }\n"
     ]
    }
   ],
   "source": [
    "const memories = await inMemoryStore.search([\"memories\", \"1\"]);\n",
    "for (const memory of memories) {\n",
    "  console.log(memory.value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5d7eb-af23-4131-b8fd-2a69e74e6e55",
   "metadata": {},
   "source": [
    "Let's now run the workflow for another user to verify that the memories about the first user are self contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d362350b-d730-48bd-9652-983812fd7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"msg_01KK7CweVY4ZdHxU5bPa4skv\",\n",
      "  \"content\": \"I don't have any information about your name. While I aim to be helpful, I can only know what you directly tell me during our conversation.\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01KK7CweVY4ZdHxU5bPa4skv\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 25,\n",
      "      \"cache_creation_input_tokens\": 0,\n",
      "      \"cache_read_input_tokens\": 0,\n",
      "      \"output_tokens\": 33\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01KK7CweVY4ZdHxU5bPa4skv\",\n",
      "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 25,\n",
      "      \"cache_creation_input_tokens\": 0,\n",
      "      \"cache_read_input_tokens\": 0,\n",
      "      \"output_tokens\": 33\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 25,\n",
      "    \"output_tokens\": 33,\n",
      "    \"total_tokens\": 58,\n",
      "    \"input_token_details\": {\n",
      "      \"cache_creation\": 0,\n",
      "      \"cache_read\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config3 = {\n",
    "  configurable: {\n",
    "    thread_id: \"3\",\n",
    "  },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "\n",
    "const otherUserStream = await workflow.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what is my name?\",\n",
    "  }],\n",
    "  userId: \"2\"\n",
    "}, config3);\n",
    "\n",
    "for await (const chunk of otherUserStream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/cross-thread-persistence.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2eecb96-cf0e-47ed-8116-88a7eaa4236d",
   "metadata": {},
   "source": [
    "# How to add cross-thread persistence to your graph\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/persistence/\">\n",
    "                    Persistence\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/memory/\">\n",
    "                    Memory\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://js.langchain.com/docs/concepts/#chat-models\">\n",
    "                    Chat Models\n",
    "                </a>\n",
    "            </li>             \n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "In the [previous guide](https://langchain-ai.github.io/langgraphjs/how-tos/persistence.ipynb) you learned how to persist graph state across multiple interactions on a single [thread](). LangGraph.js also allows you to persist data across **multiple threads**. For instance, you can store information about users (their names or preferences) in a shared memory and reuse them in the new conversational threads.\n",
    "\n",
    "In this guide, we will show how to construct and use a graph that has a shared memory implemented using the [Store](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html) interface.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "    Support for the <code><a href=\"https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html\">Store</a></code> API that is used in this guide was added in LangGraph.js <code>v0.2.10</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"lsv2__...\";\n",
    "// process.env.ANTHROPIC_API_KEY = \"your api key\";\n",
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_PROJECT = \"Cross-thread persistence: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": [
    "## Define store\n",
    "\n",
    "In this example we will create a graph that will be able to retrieve information about a user's preferences. We will do so by defining an `InMemoryStore` - an object that can store data in memory and query that data. We will then pass the store object when compiling the graph. This allows each node in the graph to access the store: when you define node functions, you can define `store` keyword argument, and LangGraph will automatically pass the store object you compiled the graph with.\n",
    "\n",
    "When storing objects using the `Store` interface you define two things:\n",
    "\n",
    "* the namespace for the object, a tuple (similar to directories)\n",
    "* the object key (similar to filenames)\n",
    "\n",
    "In our example, we'll be using `(\"memories\", <userId>)` as namespace and random UUID as key for each new memory.\n",
    "\n",
    "Importantly, to determine the user, we will be passing `userId` via the config keyword argument of the node function.\n",
    "\n",
    "Let's first define an `InMemoryStore` which is already populated with some memories about the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const inMemoryStore = new InMemoryStore();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389c9f4-226d-40c7-8bfc-ee8aac24f79d",
   "metadata": {},
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30a362-528c-45ee-9df6-630d2d843588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import {\n",
    "  Annotation,\n",
    "  StateGraph,\n",
    "  START,\n",
    "  MemorySaver,\n",
    "  LangGraphRunnableConfig,\n",
    "  messagesStateReducer,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: messagesStateReducer,\n",
    "    default: () => [],\n",
    "  }),\n",
    "});\n",
    "\n",
    "const model = new ChatAnthropic({ modelName: \"claude-3-5-sonnet-20240620\" });\n",
    "\n",
    "// NOTE: we're passing the Store param to the node --\n",
    "// this is the Store we compile the graph with\n",
    "const callModel = async (\n",
    "  state: typeof StateAnnotation.State,\n",
    "  config: LangGraphRunnableConfig\n",
    "): Promise<{ messages: any }> => {\n",
    "  const store = config.store;\n",
    "  if (!store) {\n",
    "    if (!store) {\n",
    "      throw new Error(\"store is required when compiling the graph\");\n",
    "    }\n",
    "  }\n",
    "  if (!config.configurable?.userId) {\n",
    "    throw new Error(\"userId is required in the config\");\n",
    "  }\n",
    "  const namespace = [\"memories\", config.configurable?.userId];\n",
    "  const memories = await store.search(namespace);\n",
    "  const info = memories.map((d) => d.value.data).join(\"\\n\");\n",
    "  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\n",
    "\n",
    "  // Store new memories if the user asks the model to remember\n",
    "  const lastMessage = state.messages[state.messages.length - 1];\n",
    "  if (\n",
    "    typeof lastMessage.content === \"string\" &&\n",
    "    lastMessage.content.toLowerCase().includes(\"remember\")\n",
    "  ) {\n",
    "    await store.put(namespace, uuidv4(), { data: lastMessage.content });\n",
    "  }\n",
    "\n",
    "  const response = await model.invoke([\n",
    "    { type: \"system\", content: systemMsg },\n",
    "    ...state.messages,\n",
    "  ]);\n",
    "  return { messages: response };\n",
    "};\n",
    "\n",
    "const builder = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"call_model\", callModel)\n",
    "  .addEdge(START, \"call_model\");\n",
    "\n",
    "// NOTE: we're passing the store object here when compiling the graph\n",
    "const graph = builder.compile({\n",
    "  checkpointer: new MemorySaver(),\n",
    "  store: inMemoryStore,\n",
    "});\n",
    "// If you're using LangGraph Cloud or LangGraph Studio, you don't need to pass the store or checkpointer when compiling the graph, since it's done automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a4a18-67e4-4f0b-b655-a29bbe202e1c",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        If you're using LangGraph Cloud or LangGraph Studio, you <strong>don't need</strong> to pass store when compiling the graph, since it's done automatically.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": [
    "## Run the graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842c626-6cd9-4f58-b549-58978e478098",
   "metadata": {},
   "source": [
    "Now let's specify a user ID in the config and tell the model our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c871a073-a466-46ad-aafe-2b870831057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage {\n",
      "  \"id\": \"ef28a40a-fd75-4478-929a-5413f2a6b044\",\n",
      "  \"content\": \"Hi! Remember: my name is Bob\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"id\": \"msg_01UcHJnSAuVDFuDmqaYkxWAf\",\n",
      "  \"content\": \"Hello Bob! It's nice to meet you. I'll remember that your name is Bob. How can I assist you today?\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01UcHJnSAuVDFuDmqaYkxWAf\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"output_tokens\": 29\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01UcHJnSAuVDFuDmqaYkxWAf\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"output_tokens\": 29\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 28,\n",
      "    \"output_tokens\": 29,\n",
      "    \"total_tokens\": 57\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "let config = { configurable: { thread_id: \"1\", userId: \"1\" } };\n",
    "let inputMessage = { type: \"user\", content: \"Hi! Remember: my name is Bob\" };\n",
    "\n",
    "for await (const chunk of await graph.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  { ...config, streamMode: \"values\" }\n",
    ")) {\n",
    "  console.log(chunk.messages[chunk.messages.length - 1]);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d862be40-1f8a-4057-81c4-b7bf073dc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage {\n",
      "  \"id\": \"eaaa4e1c-1560-4b0a-9c2d-396313cb000c\",\n",
      "  \"content\": \"what is my name?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"id\": \"msg_01VfqUerYCND1JuWGvbnAacP\",\n",
      "  \"content\": \"Your name is Bob. It's nice to meet you, Bob!\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01VfqUerYCND1JuWGvbnAacP\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 33,\n",
      "      \"output_tokens\": 17\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01VfqUerYCND1JuWGvbnAacP\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 33,\n",
      "      \"output_tokens\": 17\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 33,\n",
      "    \"output_tokens\": 17,\n",
      "    \"total_tokens\": 50\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = { configurable: { thread_id: \"2\", userId: \"1\" } };\n",
    "inputMessage = { type: \"user\", content: \"what is my name?\" };\n",
    "\n",
    "for await (const chunk of await graph.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  { ...config, streamMode: \"values\" }\n",
    ")) {\n",
    "  console.log(chunk.messages[chunk.messages.length - 1]);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd01ec-f135-4811-8743-daff8daea422",
   "metadata": {},
   "source": [
    "We can now inspect our in-memory store and verify that we have in fact saved the memories for the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76cde493-89cf-4709-a339-207d2b7e9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ data: 'Hi! Remember: my name is Bob' }\n"
     ]
    }
   ],
   "source": [
    "const memories = await inMemoryStore.search([\"memories\", \"1\"]);\n",
    "for (const memory of memories) {\n",
    "    console.log(await memory.value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5d7eb-af23-4131-b8fd-2a69e74e6e55",
   "metadata": {},
   "source": [
    "Let's now run the graph for another user to verify that the memories about the first user are self contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d362350b-d730-48bd-9652-983812fd7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage {\n",
      "  \"id\": \"1006b149-de8d-4d8e-81f4-c78c51a7144b\",\n",
      "  \"content\": \"what is my name?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"id\": \"msg_01MjpYZ65NjwZMYq42BWa2Ze\",\n",
      "  \"content\": \"I apologize, but I don't have any information about your name or personal details. As an AI assistant, I don't have access to personal information about individual users unless it's specifically provided in our conversation. Is there something else I can help you with?\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01MjpYZ65NjwZMYq42BWa2Ze\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 25,\n",
      "      \"output_tokens\": 56\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01MjpYZ65NjwZMYq42BWa2Ze\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 25,\n",
      "      \"output_tokens\": 56\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 25,\n",
      "    \"output_tokens\": 56,\n",
      "    \"total_tokens\": 81\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = { configurable: { thread_id: \"3\", userId: \"2\" } };\n",
    "inputMessage = { type: \"user\", content: \"what is my name?\" };\n",
    "\n",
    "for await (const chunk of await graph.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  { ...config, streamMode: \"values\" }\n",
    ")) {\n",
    "  console.log(chunk.messages[chunk.messages.length - 1]);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/define-state.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to define graph state\n",
        "\n",
        "This how to guide will cover different ways to define the state of your graph.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- [State conceptual guide](/langgraphjs/concepts/low_level/#state) - Conceptual guide on defining the state of your graph.\n",
        "- [Building graphs](/langgraphjs/tutorials/quickstart/) - This how-to assumes you have a basic understanding of how to build graphs.\n",
        "\n",
        "## Setup\n",
        "\n",
        "This guide requires installing the `@langchain/langgraph`, and `@langchain/core` packages:\n",
        "\n",
        "```bash\n",
        "npm install @langchain/langgraph @langchain/core\n",
        "```\n",
        "\n",
        "## Getting started\n",
        "\n",
        "The `Annotation` function is the recommended way to define your graph state for new `StateGraph` graphs. The `Annotation.Root` function is used to create the top-level state object, where each field represents a channel in the graph.\n",
        "\n",
        "Here's an example of how to define a simple graph state with one channel called `messages`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "\n",
        "const GraphAnnotation = Annotation.Root({\n",
        "  // Define a 'messages' channel to store an array of BaseMessage objects\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    // Reducer function: Combines the current state with new messages\n",
        "    reducer: (currentState, updateValue) => currentState.concat(updateValue),\n",
        "    // Default function: Initialize the channel with an empty array\n",
        "    default: () => [],\n",
        "  })\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each channel can optionally have `reducer` and `default` functions:\n",
        "- The `reducer` function defines how new values are combined with the existing state.\n",
        "- The `default` function provides an initial value for the channel.\n",
        "\n",
        "For more information on reducers, see the [reducers conceptual guide](/langgraphjs/concepts/low_level/#reducers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "const QuestionAnswerAnnotation = Annotation.Root({\n",
        "  question: Annotation<string>,\n",
        "  answer: Annotation<string>,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, all we're doing is defining the channels, and then passing the un-instantiated `Annotation` function as the value. It is important to note we always pass in the TypeScript type of each channel as the first generics argument to `Annotation`. Doing this ensures our graph state is type safe, and we can get the proper types when defining our nodes. Below shows how you can extract the typings from the `Annotation` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "type QuestionAnswerAnnotationType = typeof QuestionAnswerAnnotation.State;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is equivalent to the following type:\n",
        "\n",
        "```typescript\n",
        "type QuestionAnswerAnnotationType = {\n",
        "  question: string;\n",
        "  answer: string;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging states\n",
        "\n",
        "If you have two graph state annotations, you can merge the two into a single annotation by using the `spec` value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "const MergedAnnotation = Annotation.Root({\n",
        "  ...QuestionAnswerAnnotation.spec,\n",
        "  ...GraphAnnotation.spec,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The type of the merged annotation is the intersection of the two annotations:\n",
        "\n",
        "```typescript\n",
        "type MergedAnnotation = {\n",
        "  messages: BaseMessage[];\n",
        "  question: string;\n",
        "  answer: string;\n",
        "}\n",
        "```\n",
        "\n",
        "Finally, instantiating your graph using the annotations is as simple as passing the annotation to the `StateGraph` constructor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "const workflow = new StateGraph(MergedAnnotation);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State channels\n",
        "\n",
        "The `Annotation` function is a convince wrapper around the low level implementation of how states are defined in LangGraph. Defining state using the `channels` object (which is what `Annotation` is a wrapper of) is still possible, although not recommended for most cases. The below example shows how to implement a graph using this pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "interface WorkflowChannelsState {\n",
        "  messages: BaseMessage[];\n",
        "  question: string;\n",
        "  answer: string;\n",
        "}\n",
        "\n",
        "const workflowWithChannels = new StateGraph<WorkflowChannelsState>({\n",
        "  channels: {\n",
        "    messages: {\n",
        "      reducer: (currentState, updateValue) => currentState.concat(updateValue),\n",
        "      default: () => [],\n",
        "    },\n",
        "    question: null,\n",
        "    answer: null,\n",
        "  }\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we set the value of `question` and `answer` to `null`, as it does not contain a default value. To set a default value, the channel should be implemented how the `messages` key is, with the `default` factory returing the default value. The `reducer` function is optional, and can be added to the channel object if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Zod\n",
        "\n",
        "If you want to add runtime validation to your state, you can use Zod instead of the `Annotation` function for state definition. You can also pass in your custom `reducer` and `default` factories as well by importing `@langchain/langgraph/zod`, which will extend Zod with LangGraph specific methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import \"@langchain/langgraph/zod\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const AgentState = z.object({\n",
        "  messages: z\n",
        "    .array(z.string())\n",
        "    .default(() => [])\n",
        "    .langgraph.reducer(\n",
        "      (a, b) => a.concat(Array.isArray(b) ? b : [b]),\n",
        "      z.union([z.string(), z.array(z.string())])\n",
        "    ),\n",
        "  question: z.string(),\n",
        "  answer: z.string().min(1),\n",
        "});\n",
        "\n",
        "const graph = new StateGraph(AgentState);\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
</file>

<file path="how-tos/delete-messages.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to delete messages\n",
    "\n",
    "One of the common states for a graph is a list of messages. Usually you only add messages to that state. However, sometimes you may want to remove messages (either by directly modifying the state or as part of the graph). To do that, you can use the `RemoveMessage` modifier. In this guide, we will cover how to do that.\n",
    "\n",
    "The key idea is that each state key has a `reducer` key. This key specifies how to combine updates to the state. The prebuilt [`MessagesAnnotation`](/langgraphjs/concepts/low_level/#messagesannotation) has a messages key, and the reducer for that key accepts these `RemoveMessage` modifiers. That reducer then uses these `RemoveMessage` to delete messages from the key.\n",
    "\n",
    "So note that just because your graph state has a key that is a list of messages, it doesn't mean that that this `RemoveMessage` modifier will work. You also have to have a `reducer` defined that knows how to work with this.\n",
    "\n",
    "**NOTE**: Many models expect certain rules around lists of messages. For example, some expect them to start with a `user` message, others expect all messages with tool calls to be followed by a tool message. **When deleting messages, you will want to make sure you don't violate these rules.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core zod uuid\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = 'YOUR_API_KEY';\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```typescript\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "Now, let's build a simple graph that uses messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767ef1c-a7cf-41f8-a301-558988cb7ac5",
   "metadata": {},
   "source": [
    "## Build the agent\n",
    "Let's now build a simple ReAct style agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab32b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { MemorySaver } from \"@langchain/langgraph-checkpoint\";\n",
    "import { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "const search = tool((_) => {\n",
    "  // This is a placeholder for the actual implementation\n",
    "  // Don't let the LLM know this though 😊\n",
    "  return [\n",
    "    \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\",\n",
    "  ];\n",
    "}, {\n",
    "  name: \"search\",\n",
    "  description: \"Call to surf the web.\",\n",
    "  schema: z.object({\n",
    "    query: z.string(),\n",
    "  })\n",
    "});\n",
    "\n",
    "const tools = [search];\n",
    "const toolNode = new ToolNode<typeof MessagesAnnotation.State>(tools);\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "const boundModel = model.bindTools(tools);\n",
    "\n",
    "function shouldContinue(state: typeof MessagesAnnotation.State): \"action\" | typeof END {\n",
    "  const lastMessage = state.messages[state.messages.length - 1];\n",
    "  if (\n",
    "    \"tool_calls\" in lastMessage &&\n",
    "    Array.isArray(lastMessage.tool_calls) &&\n",
    "    lastMessage.tool_calls.length\n",
    "  ) {\n",
    "    return \"action\";\n",
    "  }\n",
    "  // If there is no tool call, then we finish\n",
    "  return END;\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModel(state: typeof MessagesAnnotation.State) {\n",
    "  const response = await boundModel.invoke(state.messages);\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(MessagesAnnotation)\n",
    "  // Define the two nodes we will cycle between\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"action\", toolNode)\n",
    "  // Set the entrypoint as `agent`\n",
    "  // This means that this node is the first one called\n",
    "  .addEdge(START, \"agent\")\n",
    "  // We now add a conditional edge\n",
    "  .addConditionalEdges(\n",
    "    // First, we define the start node. We use `agent`.\n",
    "    // This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    // Next, we pass in the function that will determine which node is called next.\n",
    "    shouldContinue\n",
    "  )\n",
    "  // We now add a normal edge from `tools` to `agent`.\n",
    "  // This means that after `tools` is called, `agent` node is called next.\n",
    "  .addEdge(\"action\", \"agent\");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile({ checkpointer: memory });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ type: 'human', content: \"hi! I'm bob\", tool_calls: undefined }\n",
      "{\n",
      "  type: 'ai',\n",
      "  content: 'Hi Bob! How can I assist you today?',\n",
      "  tool_calls: []\n",
      "}\n",
      "{ type: 'human', content: \"What's my name?\", tool_calls: undefined }\n",
      "{ type: 'ai', content: 'Your name is Bob.', tool_calls: [] }\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const config = { configurable: { thread_id: \"2\" }, streamMode: \"values\" as const };\n",
    "const inputMessage = new HumanMessage({\n",
    "  id: uuidv4(),\n",
    "  content: \"hi! I'm bob\",\n",
    "});\n",
    "\n",
    "for await (const event of await app.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  config,\n",
    ")) {\n",
    "  const lastMsg = event.messages[event.messages.length - 1];\n",
    "  console.dir(\n",
    "    {\n",
    "      type: lastMsg._getType(),\n",
    "      content: lastMsg.content,\n",
    "      tool_calls: lastMsg.tool_calls,\n",
    "    },\n",
    "    { depth: null }\n",
    "  )\n",
    "}\n",
    "\n",
    "const inputMessage2 = new HumanMessage({\n",
    "  id: uuidv4(),\n",
    "  content: \"What's my name?\",\n",
    "});\n",
    "for await (const event of await app.stream(\n",
    "  { messages: [inputMessage2] },\n",
    "  config,\n",
    ")) {\n",
    "  const lastMsg = event.messages[event.messages.length - 1];\n",
    "  console.dir(\n",
    "    {\n",
    "      type: lastMsg._getType(),\n",
    "      content: lastMsg.content,\n",
    "      tool_calls: lastMsg.tool_calls,\n",
    "    },\n",
    "    { depth: null }\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0de5b-30ec-42d4-813a-7ad63fe1c367",
   "metadata": {},
   "source": [
    "## Manually deleting messages\n",
    "\n",
    "First, we will cover how to manually delete messages. Let's take a look at the current state of the thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a850529-d038-48f7-b5a2-8d4d2923f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    id: '24187daa-00dd-40d8-bc30-f4e24ff78165',\n",
      "    type: 'human',\n",
      "    content: \"hi! I'm bob\",\n",
      "    tool_calls: undefined\n",
      "  },\n",
      "  {\n",
      "    id: 'chatcmpl-9zYV9yHLiZmR2ZVHEhHcbVEshr3qG',\n",
      "    type: 'ai',\n",
      "    content: 'Hi Bob! How can I assist you today?',\n",
      "    tool_calls: []\n",
      "  },\n",
      "  {\n",
      "    id: 'a67e53c3-5dcf-4ddc-83f5-309b72ac61f4',\n",
      "    type: 'human',\n",
      "    content: \"What's my name?\",\n",
      "    tool_calls: undefined\n",
      "  },\n",
      "  {\n",
      "    id: 'chatcmpl-9zYV9mmpJrm3SQ7ngMJZ1XBHzHfL6',\n",
      "    type: 'ai',\n",
      "    content: 'Your name is Bob.',\n",
      "    tool_calls: []\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const messages = (await app.getState(config)).values.messages;\n",
    "console.dir(\n",
    "  messages.map((msg) => ({\n",
    "    id: msg.id,\n",
    "    type: msg._getType(),\n",
    "    content: msg.content,\n",
    "    tool_calls:\n",
    "    msg.tool_calls,\n",
    "  })),\n",
    "  { depth: null }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be8a0a-1e94-4302-bd84-d1b72e3c501c",
   "metadata": {},
   "source": [
    "We can call `updateState` and pass in the id of the first message. This will delete that message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1a0970-7e64-4170-beef-2855d10eef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  configurable: {\n",
      "    thread_id: '2',\n",
      "    checkpoint_ns: '',\n",
      "    checkpoint_id: '1ef61abf-1fc2-6431-8005-92730e9d667c'\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { RemoveMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "await app.updateState(config, { messages: new RemoveMessage({ id: messages[0].id }) })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9127ae-0d42-42b8-957f-ea69a5da555f",
   "metadata": {},
   "source": [
    "If we now look at the messages, we can verify that the first one was deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfe4ffa-e170-43bc-aec4-6e36ac620931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    id: 'chatcmpl-9zYV9yHLiZmR2ZVHEhHcbVEshr3qG',\n",
      "    type: 'ai',\n",
      "    content: 'Hi Bob! How can I assist you today?',\n",
      "    tool_calls: []\n",
      "  },\n",
      "  {\n",
      "    id: 'a67e53c3-5dcf-4ddc-83f5-309b72ac61f4',\n",
      "    type: 'human',\n",
      "    content: \"What's my name?\",\n",
      "    tool_calls: undefined\n",
      "  },\n",
      "  {\n",
      "    id: 'chatcmpl-9zYV9mmpJrm3SQ7ngMJZ1XBHzHfL6',\n",
      "    type: 'ai',\n",
      "    content: 'Your name is Bob.',\n",
      "    tool_calls: []\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const updatedMessages = (await app.getState(config)).values.messages;\n",
    "console.dir(\n",
    "  updatedMessages.map((msg) => ({\n",
    "    id: msg.id,\n",
    "    type: msg._getType(),\n",
    "    content: msg.content,\n",
    "    tool_calls:\n",
    "    msg.tool_calls,\n",
    "  })),\n",
    "  { depth: null }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef129a75-4cad-44d7-b532-eb37b0553c0c",
   "metadata": {},
   "source": [
    "## Programmatically deleting messages\n",
    "\n",
    "We can also delete messages programmatically from inside the graph. Here we'll modify the graph to delete any old messages (longer than 3 messages ago) at the end of a graph run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c308252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RemoveMessage } from \"@langchain/core/messages\";\n",
    "import { StateGraph, START, END } from \"@langchain/langgraph\";\n",
    "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "function deleteMessages(state: typeof MessagesAnnotation.State) {\n",
    "  const messages = state.messages;\n",
    "  if (messages.length > 3) {\n",
    "    return { messages: messages.slice(0, -3).map(m => new RemoveMessage({ id: m.id })) };\n",
    "  }\n",
    "  return {};\n",
    "}\n",
    "\n",
    "// We need to modify the logic to call deleteMessages rather than end right away\n",
    "function shouldContinue2(state: typeof MessagesAnnotation.State): \"action\" | \"delete_messages\" {\n",
    "  const lastMessage = state.messages[state.messages.length - 1];\n",
    "  if (\n",
    "    \"tool_calls\" in lastMessage &&\n",
    "    Array.isArray(lastMessage.tool_calls) &&\n",
    "    lastMessage.tool_calls.length\n",
    "  ) {\n",
    "    return \"action\";\n",
    "  }\n",
    "  // Otherwise if there aren't, we finish\n",
    "  return \"delete_messages\";\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow2 = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"action\", toolNode)\n",
    "  // This is our new node we're defining\n",
    "  .addNode(\"delete_messages\", deleteMessages)\n",
    "  .addEdge(START, \"agent\")\n",
    "  .addConditionalEdges(\n",
    "    \"agent\",\n",
    "    shouldContinue2\n",
    "  )\n",
    "  .addEdge(\"action\", \"agent\")\n",
    "  // This is the new edge we're adding: after we delete messages, we finish\n",
    "  .addEdge(\"delete_messages\", END);\n",
    "\n",
    "const app2 = workflow2.compile({ checkpointer: memory });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbdef6-7db7-45a2-8194-de4f8929bd1f",
   "metadata": {},
   "source": [
    "We can now try this out. We can call the graph twice and then check the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3975f34c-c243-40ea-b9d2-424d50a48dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FIRST ITERATION ---\n",
      "\n",
      "[ [ 'human', \"hi! I'm bob\" ] ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [ 'human', \"hi! I'm bob\" ],\n",
      "  [ 'ai', 'Hi Bob! How can I assist you today?' ]\n",
      "]\n",
      "\n",
      "\n",
      "--- SECOND ITERATION ---\n",
      "\n",
      "[\n",
      "  [ 'human', \"hi! I'm bob\" ],\n",
      "  [ 'ai', 'Hi Bob! How can I assist you today?' ],\n",
      "  [ 'human', \"what's my name?\" ]\n",
      "] \n",
      "\n",
      "[\n",
      "  [ 'human', \"hi! I'm bob\" ],\n",
      "  [ 'ai', 'Hi Bob! How can I assist you today?' ],\n",
      "  [ 'human', \"what's my name?\" ],\n",
      "  [ 'ai', \"Based on what you've told me, your name is Bob.\" ]\n",
      "] \n",
      "\n",
      "[\n",
      "  [ 'ai', 'Hi Bob! How can I assist you today?' ],\n",
      "  [ 'human', \"what's my name?\" ],\n",
      "  [ 'ai', \"Based on what you've told me, your name is Bob.\" ]\n",
      "] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const config2 = { configurable: { thread_id: \"3\" }, streamMode: \"values\" as const };\n",
    "\n",
    "const inputMessage3 = new HumanMessage({\n",
    "  id: uuidv4(),\n",
    "  content: \"hi! I'm bob\",\n",
    "});\n",
    "\n",
    "console.log(\"--- FIRST ITERATION ---\\n\");\n",
    "for await (const event of await app2.stream(\n",
    "  { messages: [inputMessage3] },\n",
    "  config2\n",
    ")) {\n",
    "  console.log(event.messages.map((message) => [message._getType(), message.content]));\n",
    "}\n",
    "\n",
    "const inputMessage4 = new HumanMessage({\n",
    "  id: uuidv4(),\n",
    "  content: \"what's my name?\",\n",
    "});\n",
    "\n",
    "console.log(\"\\n\\n--- SECOND ITERATION ---\\n\");\n",
    "for await (const event of await app2.stream(\n",
    "  { messages: [inputMessage4] },\n",
    "  config2\n",
    ")) {\n",
    "  console.log(event.messages.map((message) => [message._getType(), message.content]), \"\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2fd2a-14a1-4c47-8632-f8cbb0ba1d35",
   "metadata": {},
   "source": [
    "If we now check the state, we should see that it is only three messages long. This is because we just deleted the earlier messages - otherwise it would be four!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e15abb-81d8-4072-9f10-61ae0fd61dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    id: 'chatcmpl-9zYVAEiiC9D7bb0wF4KLXgY0OAG8O',\n",
      "    type: 'ai',\n",
      "    content: 'Hi Bob! How can I assist you today?',\n",
      "    tool_calls: []\n",
      "  },\n",
      "  {\n",
      "    id: 'b93e5f35-cfa3-4ca6-9b59-154ce2bd476b',\n",
      "    type: 'human',\n",
      "    content: \"what's my name?\",\n",
      "    tool_calls: undefined\n",
      "  },\n",
      "  {\n",
      "    id: 'chatcmpl-9zYVBHJWtEM6pw2koE8dykzSA0XSO',\n",
      "    type: 'ai',\n",
      "    content: \"Based on what you've told me, your name is Bob.\",\n",
      "    tool_calls: []\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const messages3 = (await app.getState(config2)).values[\"messages\"]\n",
    "console.dir(\n",
    "  messages3.map((msg) => ({\n",
    "    id: msg.id,\n",
    "    type: msg._getType(),\n",
    "    content: msg.content,\n",
    "    tool_calls:\n",
    "    msg.tool_calls,\n",
    "  })),\n",
    "  { depth: null }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cfeae-d43a-46ee-9069-a1cab9a5720a",
   "metadata": {},
   "source": [
    "Remember, when deleting messages you will want to make sure that the remaining message list is still valid. This message list **may actually not be** - this is because it currently starts with an AI message, which some models do not allow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/dynamic_breakpoints.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee54cde3-7e4d-43f4-b921-e7141ea0f19e",
   "metadata": {},
   "source": [
    "# How to add dynamic breakpoints\n",
    "\n",
    "!!! note\n",
    "    For **human-in-the-loop** workflows use the new [`interrupt()`](/langgraphjs/reference/functions/langgraph.interrupt-1.html) function for **human-in-the-loop** workflows. Please review the [Human-in-the-loop conceptual guide](/langgraphjs/concepts/human_in_the_loop) for more information about design patterns with `interrupt`.\n",
    "\n",
    "!!! tip \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following concepts:\n",
    "\n",
    "    * [Breakpoints](/langgraphjs/concepts/breakpoints)\n",
    "    * [LangGraph Glossary](/langgraphjs/concepts/low_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607849c6-4b8c-4e06-ad9c-758bb5a08e86",
   "metadata": {},
   "source": [
    "Human-in-the-loop (HIL) interactions are crucial for [agentic systems](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#human-in-the-loop). [Breakpoints](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#breakpoints) are a common HIL interaction pattern, allowing the graph to stop at specific steps and seek human approval before proceeding (e.g., for sensitive actions).\n",
    "\n",
    "In LangGraph you can add breakpoints before / after a node is executed. But oftentimes it may be helpful to **dynamically** interrupt the graph from inside a given node based on some condition. When doing so, it may also be helpful to include information about **why** that interrupt was raised.\n",
    "\n",
    "This guide shows how you can dynamically interrupt the graph using `NodeInterupt` -- a special exception that can be raised from inside a node. Let's see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa244f-1dd9-450e-9526-b1a28b30f84f",
   "metadata": {},
   "source": [
    "### Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a14c8b2-5c25-4201-93ea-e5358ee99bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  Annotation,\n",
    "  MemorySaver,\n",
    "  NodeInterrupt,\n",
    "  StateGraph,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  input: Annotation<string>,\n",
    "});\n",
    "\n",
    "const step1 = async (state: typeof StateAnnotation.State) => {\n",
    "  console.log(\"---Step 1---\");\n",
    "  return state;\n",
    "};\n",
    "\n",
    "const step2 = async (state: typeof StateAnnotation.State) => {\n",
    "  // Let's optionally raise a NodeInterrupt\n",
    "  // if the length of the input is longer than 5 characters\n",
    "  if (state.input?.length > 5) {\n",
    "    throw new NodeInterrupt(`Received input that is longer than 5 characters: ${state.input}`);\n",
    "  }\n",
    "  console.log(\"---Step 2---\");\n",
    "  return state;\n",
    "};\n",
    "\n",
    "const step3 = async (state: typeof StateAnnotation.State) => {\n",
    "  console.log(\"---Step 3---\");\n",
    "  return state;\n",
    "};\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const graph = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"step1\", step1)\n",
    "  .addNode(\"step2\", step2)\n",
    "  .addNode(\"step3\", step3)\n",
    "  .addEdge(\"__start__\", \"step1\")\n",
    "  .addEdge(\"step1\", \"step2\")\n",
    "  .addEdge(\"step2\", \"step3\")\n",
    "  .addEdge(\"step3\", \"__end__\")\n",
    "  .compile({ checkpointer });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6242951",
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const representation = graph.getGraph();\n",
    "const image = await representation.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5521e1-0e58-42c5-9282-ff96f24ee6f6",
   "metadata": {},
   "source": [
    "### Run the graph with dynamic interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83692c63-5c65-4562-9c65-5ad1935e339f",
   "metadata": {},
   "source": [
    "First, let's run the graph with an input that's <= 5 characters long. This should safely ignore the interrupt condition we defined and return the original input at the end of the graph execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d281f1-3349-4378-8918-7665fa7a7457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ input: 'hello' }\n",
      "---Step 1---\n",
      "{ input: 'hello' }\n",
      "---Step 2---\n",
      "{ input: 'hello' }\n",
      "---Step 3---\n",
      "{ input: 'hello' }\n"
     ]
    }
   ],
   "source": [
    "const initialInput = { input: \"hello\" };\n",
    "const config = {\n",
    "  configurable: {\n",
    "    thread_id: \"1\",\n",
    "  },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "\n",
    "const stream = await graph.stream(initialInput, config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66b926-47eb-401b-b37b-d80269d7214c",
   "metadata": {},
   "source": [
    "If we inspect the graph at this point, we can see that there are no more tasks left to run and that the graph indeed finished execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eac1455-e7ef-4a32-8c14-0d5789409689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "const state = await graph.getState(config);\n",
    "console.log(state.next);\n",
    "console.log(state.tasks);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8e03817-2135-4fb3-b881-fd6d2c378ccf",
   "metadata": {},
   "source": [
    "Now, let's run the graph with an input that's longer than 5 characters. This should trigger the dynamic interrupt we defined via raising a `NodeInterrupt` error inside the `step2` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06192ad-13a4-4d2e-8e30-f1c08578fe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ input: 'hello world' }\n",
      "---Step 1---\n",
      "{ input: 'hello world' }\n"
     ]
    }
   ],
   "source": [
    "const longInput = { input: \"hello world\" };\n",
    "const config2 = {\n",
    "  configurable: {\n",
    "    thread_id: \"2\",\n",
    "  },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "\n",
    "const streamWithInterrupt = await graph.stream(longInput, config2);\n",
    "\n",
    "for await (const event of streamWithInterrupt) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd4f1-db97-44bb-a9e5-435ed042e3a3",
   "metadata": {},
   "source": [
    "We can see that the graph now stopped while executing `step2`. If we inspect the graph state at this point, we can see the information on what node is set to execute next (`step2`), as well as what node raised the interrupt (also `step2`), and additional information about the interrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2058593c-178e-4a23-a4c4-860d4a9c2198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'step2' ]\n",
      "[\n",
      "  {\n",
      "    \"id\": \"c91a38f7-2aec-5c38-a3f0-60fba6efe73c\",\n",
      "    \"name\": \"step2\",\n",
      "    \"interrupts\": [\n",
      "      {\n",
      "        \"value\": \"Received input that is longer than 5 characters: hello world\",\n",
      "        \"when\": \"during\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const state2 = await graph.getState(config2);\n",
    "console.log(state2.next);\n",
    "console.log(JSON.stringify(state2.tasks, null, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36d1be-ae2e-49c8-a17f-2b27be09618a",
   "metadata": {},
   "source": [
    "If we try to resume the graph from the breakpoint, we will simply interrupt again as our inputs & graph state haven't changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872e7a69-9784-4f81-90c6-6b6af2fa6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NOTE: to resume the graph from a dynamic interrupt we use the same syntax as\n",
    "// regular interrupts -- we pass null as the input\n",
    "const resumedStream = await graph.stream(null, config2);\n",
    "\n",
    "for await (const event of resumedStream) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3275f899-7039-4029-8814-0bb5c33fabfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'step2' ]\n",
      "[\n",
      "  {\n",
      "    \"id\": \"c91a38f7-2aec-5c38-a3f0-60fba6efe73c\",\n",
      "    \"name\": \"step2\",\n",
      "    \"interrupts\": [\n",
      "      {\n",
      "        \"value\": \"Received input that is longer than 5 characters: hello world\",\n",
      "        \"when\": \"during\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const state3 = await graph.getState(config2);\n",
    "console.log(state3.next);\n",
    "console.log(JSON.stringify(state2.tasks, null, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5862dea-2af2-48cb-9889-979b6c6af6aa",
   "metadata": {},
   "source": [
    "### Update the graph state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8724ef6-877a-44b9-b96a-ae81efa2d9e4",
   "metadata": {},
   "source": [
    "To get around it, we can do several things. \n",
    "\n",
    "First, we could simply run the graph on a different thread with a shorter input, like we did in the beginning. Alternatively, if we want to resume the graph execution from the breakpoint, we can update the state to have an input that's shorter than 5 characters (the condition for our interrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba8dc8d-b90e-45f5-92cd-2192fc66f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Step 2---\n",
      "{ input: 'short' }\n",
      "---Step 3---\n",
      "{ input: 'short' }\n",
      "[]\n",
      "{ input: 'short' }\n"
     ]
    }
   ],
   "source": [
    "// NOTE: this update will be applied as of the last successful node before the interrupt,\n",
    "// i.e. `step1`, right before the node with an interrupt\n",
    "await graph.updateState(config2, { input: \"short\" });\n",
    "\n",
    "const updatedStream = await graph.stream(null, config2);\n",
    "\n",
    "for await (const event of updatedStream) {\n",
    "  console.log(event);\n",
    "}\n",
    "\n",
    "const state4 = await graph.getState(config2);\n",
    "console.log(state4.next);\n",
    "console.log(state4.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16980e-aef4-45c9-85eb-955568a93c5b",
   "metadata": {},
   "source": [
    "You can also update the state **as node `step2`** (interrupted node) which would skip over that node altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a48e564-d979-4ac2-b815-c667345a9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ input: 'hello world' }\n",
      "---Step 1---\n",
      "{ input: 'hello world' }\n"
     ]
    }
   ],
   "source": [
    "const config3 = {\n",
    "  configurable: {\n",
    "    thread_id: \"3\",\n",
    "  },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "\n",
    "const skipStream = await graph.stream({ input: \"hello world\" }, config3);\n",
    "\n",
    "// Run the graph until the first interruption\n",
    "for await (const event of skipStream) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f973ab-00ce-4f16-a452-641e76625fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Step 3---\n",
      "{ input: 'hello world' }\n",
      "[]\n",
      "{ input: 'hello world' }\n"
     ]
    }
   ],
   "source": [
    "// NOTE: this update will skip the node `step2` entirely\n",
    "await graph.updateState(config3, undefined, \"step2\");\n",
    "\n",
    "// Resume the stream\n",
    "for await (const event of await graph.stream(null, config3)) {\n",
    "  console.log(event);\n",
    "}\n",
    "\n",
    "const state5 = await graph.getState(config3);\n",
    "console.log(state5.next);\n",
    "console.log(state5.values);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/dynamically-returning-directly.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd47f365",
      "metadata": {},
      "source": [
        "# How to let agent return tool results directly\n",
        "\n",
        "A typical ReAct loop follows user -> assistant -> tool -> assistant ..., ->\n",
        "user. In some cases, you don't need to call the LLM after the tool completes,\n",
        "the user can view the results directly themselves.\n",
        "\n",
        "In this example we will build a conversational ReAct agent where the LLM can\n",
        "optionally decide to return the result of a tool call as the final answer. This\n",
        "is useful in cases where you have tools that can sometimes generate responses\n",
        "that are acceptable as final answers, and you want to use the LLM to determine\n",
        "when that is the case\n",
        "\n",
        "## Setup\n",
        "\n",
        "First we need to install the required packages:\n",
        "\n",
        "```bash\n",
        "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
        "```\n",
        "\n",
        "Next, we need to set API keys for OpenAI (the LLM we will use). Optionally, we\n",
        "can set API key for [LangSmith tracing](https://smith.langchain.com/), which\n",
        "will give us best-in-class observability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bff262dd",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Direct Return: LangGraphJS\n"
          ]
        }
      ],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "\n",
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"ls__...\"\n",
        "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "process.env.LANGCHAIN_PROJECT = \"Direct Return: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c02963",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will\n",
        "use a simple placeholder \"search engine\". However, it is really easy to create\n",
        "your own tools - see documentation\n",
        "[here](https://js.langchain.com/docs/modules/agents/tools/dynamic) on how to do\n",
        "that.\n",
        "\n",
        "To add a 'return_direct' option, we will create a custom zod schema to use\n",
        "**instead of** the schema that would be automatically inferred by the tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6e93e06",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const SearchTool = z.object({\n",
        "  query: z.string().describe(\"query to look up online\"),\n",
        "  // **IMPORTANT** We are adding an **extra** field here\n",
        "  // that isn't used directly by the tool - it's used by our\n",
        "  // graph instead to determine whether or not to return the\n",
        "  // result directly to the user\n",
        "  return_direct: z.boolean()\n",
        "    .describe(\n",
        "      \"Whether or not the result of this should be returned directly to the user without you seeing what it is\",\n",
        "    )\n",
        "    .default(false),\n",
        "});\n",
        "\n",
        "const searchTool = new DynamicStructuredTool({\n",
        "  name: \"search\",\n",
        "  description: \"Call to surf the web.\",\n",
        "  // We are overriding the default schema here to\n",
        "  // add an extra field\n",
        "  schema: SearchTool,\n",
        "  func: async ({}: { query: string }) => {\n",
        "    // This is a placeholder for the actual implementation\n",
        "    // Don't let the LLM know this though 😊\n",
        "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\";\n",
        "  },\n",
        "});\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f443c375",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a `ToolNode`.\n",
        "This is a prebuilt node that takes in a LangChain chat model's generated tool call and calls that tool,\n",
        "returning the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "82f3a772",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07a9312",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now we need to load the chat model we want to use.\\\n",
        "Importantly, this should satisfy two criteria:\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form\n",
        "   of messages, so it needs to be able to work well with them.\n",
        "2. It should support\n",
        "   [tool calling](https://js.langchain.com/docs/concepts/tool_calling/).\n",
        "\n",
        "Note: these model requirements are not requirements for using LangGraph - they\n",
        "are just requirements for this one example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f9263d46",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({\n",
        "  temperature: 0,\n",
        "  model: \"gpt-3.5-turbo\",\n",
        "});\n",
        "// This formats the tools as json schema for the model API.\n",
        "// The model then uses this like a system prompt.\n",
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dbab039",
      "metadata": {},
      "source": [
        "## Define the agent state\n",
        "\n",
        "The main type of graph in `langgraph` is the\n",
        "[StateGraph](/langgraphjs/reference/classes/langgraph.StateGraph.html).\n",
        "\n",
        "This graph is parameterized by a state object that it passes around to each\n",
        "node. Each node then returns operations to update that state. These operations\n",
        "can either SET specific attributes on the state (e.g. overwrite the existing\n",
        "values) or ADD to the existing attribute. Whether to set or add is denoted in\n",
        "the state object you construct the graph with.\n",
        "\n",
        "For this example, the state we will track will just be a list of messages. We\n",
        "want each node to just add messages to that list. Therefore, we will define the\n",
        "state as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c85e2d40",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const AgentState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4b9760",
      "metadata": {},
      "source": [
        "## Define the nodes\n",
        "\n",
        "We now need to define a few different nodes in our graph. In `langgraph`, a node\n",
        "can be either a function or a\n",
        "[runnable](https://js.langchain.com/docs/expression_language/). There are two\n",
        "main nodes we need for this:\n",
        "\n",
        "1. The agent: responsible for deciding what (if any) actions to take.\n",
        "2. A function to invoke tools: if the agent decides to take an action, this node\n",
        "   will then execute that action.\n",
        "\n",
        "We will also need to define some edges. Some of these edges may be conditional.\n",
        "The reason they are conditional is that based on the output of a node, one of\n",
        "several paths may be taken. The path that is taken is not known until that node\n",
        "is run (the LLM decides).\n",
        "\n",
        "1. Conditional Edge: after the agent is called, we should either: a. If the\n",
        "   agent said to take an action, then the function to invoke tools should be\n",
        "   called b. If the agent said that it was finished, then it should finish\n",
        "2. Normal Edge: after the tools are invoked, it should always go back to the\n",
        "   agent to decide what to do next\n",
        "\n",
        "Let's define the nodes, as well as a function to decide how what conditional\n",
        "edge to take.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c3da4bde",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
        "import { END } from \"@langchain/langgraph\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "// Define the function that determines whether to continue or not\n",
        "const shouldContinue = (state: typeof AgentState.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If there is no function call, then we finish\n",
        "  if (!lastMessage?.tool_calls?.length) {\n",
        "    return END;\n",
        "  } // Otherwise if there is, we check if it's suppose to return direct\n",
        "  else {\n",
        "    const args = lastMessage.tool_calls[0].args;\n",
        "    if (args?.return_direct) {\n",
        "      return \"final\";\n",
        "    } else {\n",
        "      return \"tools\";\n",
        "    }\n",
        "  }\n",
        "};\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel = async (state: typeof AgentState.State, config?: RunnableConfig) => {\n",
        "  const messages = state.messages;\n",
        "  const response = await boundModel.invoke(messages, config);\n",
        "  // We return an object, because this will get added to the existing list\n",
        "  return { messages: [response] };\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd38eae",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together and define the graph!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7f830fef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(AgentState)\n",
        "  // Define the two nodes we will cycle between\n",
        "  .addNode(\"agent\", callModel)\n",
        "  // Note the \"action\" and \"final\" nodes are identical!\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addNode(\"final\", toolNode)\n",
        "  // Set the entrypoint as `agent`\n",
        "  .addEdge(START, \"agent\")\n",
        "  // We now add a conditional edge\n",
        "  .addConditionalEdges(\n",
        "    // First, we define the start node. We use `agent`.\n",
        "    \"agent\",\n",
        "    // Next, we pass in the function that will determine which node is called next.\n",
        "    shouldContinue,\n",
        "  )\n",
        "  // We now add a normal edge from `tools` to `agent`.\n",
        "  .addEdge(\"tools\", \"agent\")\n",
        "  .addEdge(\"final\", END);\n",
        "\n",
        "// Finally, we compile it!\n",
        "const app = workflow.compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac83bfea",
      "metadata": {},
      "source": [
        "## Use it!\n",
        "\n",
        "We can now use it! This now exposes the\n",
        "[same interface](https://js.langchain.com/docs/expression_language/) as all\n",
        "other LangChain runnables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9ba5e47a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: what is the weather in sf\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"weather in San Francisco\"})\n",
            "-----\n",
            "\n",
            "[tool]: It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\n",
            "-----\n",
            "\n",
            "[ai]: The weather in San Francisco is sunny.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import { HumanMessage, isAIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const prettyPrint = (message: BaseMessage) => {\n",
        "  let txt = `[${message._getType()}]: ${message.content}`;\n",
        "  if (\n",
        "    isAIMessage(message) && (message as AIMessage)?.tool_calls?.length || 0 > 0\n",
        "  ) {\n",
        "    const tool_calls = (message as AIMessage)?.tool_calls\n",
        "      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n",
        "      .join(\"\\n\");\n",
        "    txt += ` \\nTools: \\n${tool_calls}`;\n",
        "  }\n",
        "  console.log(txt);\n",
        "};\n",
        "\n",
        "const inputs = { messages: [new HumanMessage(\"what is the weather in sf\")] };\n",
        "for await (const output of await app.stream(inputs, { streamMode: \"values\" })) {\n",
        "  const lastMessage = output.messages[output.messages.length - 1];\n",
        "  prettyPrint(lastMessage);\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "779e0d88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: what is the weather in sf? return this result directly by setting return_direct = True\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"weather in San Francisco\",\"return_direct\":true})\n",
            "-----\n",
            "\n",
            "[tool]: It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "const inputs2 = {\n",
        "  messages: [\n",
        "    new HumanMessage(\n",
        "      \"what is the weather in sf? return this result directly by setting return_direct = True\",\n",
        "    ),\n",
        "  ],\n",
        "};\n",
        "for await (\n",
        "  const output of await app.stream(inputs2, { streamMode: \"values\" })\n",
        ") {\n",
        "  const lastMessage = output.messages[output.messages.length - 1];\n",
        "  prettyPrint(lastMessage);\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f99d8e3b",
      "metadata": {},
      "source": [
        "Done! The graph **stopped** after running the `tools` node!\n",
        "\n",
        "```\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/edit-graph-state.ipynb">
{
  "cells": 
      "cell_type": "markdown",
      "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
      "metadata": {},
      "source": [
        "# How to edit graph state\n",
        "\n",
        "!!! tip \"Prerequisites\"\n",
        "    * [Human-in-the-loop](/langgraphjs/concepts/human_in_the_loop)\n",
        "    * [Breakpoints](/langgraphjs/concepts/breakpoints)\n",
        "    * [LangGraph Glossary](/langgraphjs/concepts/low_level)\n",
        "\n",
        "Human-in-the-loop (HIL) interactions are crucial for [agentic systems](/langgraphjs/concepts/agentic_concepts/#human-in-the-loop). Manually updating the graph state a common HIL interaction pattern, allowing the human to edit actions (e.g., what tool is being called or how it is being called).\n",
        "\n",
        "We can implement this in LangGraph using a [breakpoint](/langgraphjs/how-tos/breakpoints/): breakpoints allow us to interrupt graph execution before a specific step. At this breakpoint, we can manually update the graph state and then resume from that spot to continue.  \n",
        "\n",
        "![image.png](attachment:49539520-097a-43d5-94b4-2b56193a579f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cbd446a-808f-4394-be92-d45ab818953c",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First we need to install the packages required\n",
        "\n",
        "```bash\n",
        "npm install @langchain/langgraph @langchain/anthropic @langchain/core zod\n",
        "```\n",
        "\n",
        "Next, we need to set API keys for Anthropic (the LLM we will use)\n",
        "\n",
        "```bash\n",
        "export ANTHROPIC_API_KEY=your-api-key\n",
        "```\n",
        "\n",
        "\n",
        "Optionally, we can set API key for LangSmith tracing, which will give us best-in-class observability.\n",
        "\n",
        "```bash\n",
        "export LANGCHAIN_TRACING_V2=\"true\"\n",
        "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
        "export LANGCHAIN_API_KEY=your-api-key\n",
        "```\n",
        "\n",
        "## Simple Usage\n",
        "Let's look at very basic usage of this.\n",
        "\n",
        "Below, we do two things:\n",
        "\n",
        "1) We specify the [breakpoint](/langgraphjs/concepts/low_level/#breakpoints) using `interruptBefore` a specified step (node).\n",
        "\n",
        "2) We set up a [checkpointer](/langgraphjs/concepts/#checkpoints) to save the state of the graph up until this node.\n",
        "\n",
        "3) We use `.updateState` to update the state of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "85e452f8-f33a-4ead-bb4d-7386cdba8edc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph, START, END, Annotation } from \"@langchain/langgraph\";\n",
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "const GraphState = Annotation.Root({\n",
        "  input: Annotation<string>\n",
        "});\n",
        "\n",
        "const step1 = (state: typeof GraphState.State) => {\n",
        "  console.log(\"---Step 1---\");\n",
        "  return state;\n",
        "}\n",
        "\n",
        "const step2 = (state: typeof GraphState.State) => {\n",
        "  console.log(\"---Step 2---\");\n",
        "  return state;\n",
        "}\n",
        "\n",
        "const step3 = (state: typeof GraphState.State) => {\n",
        "  console.log(\"---Step 3---\");\n",
        "  return state;\n",
        "}\n",
        "\n",
        "\n",
        "const builder = new StateGraph(GraphState)\n",
        "  .addNode(\"step1\", step1)\n",
        "  .addNode(\"step2\", step2)\n",
        "  .addNode(\"step3\", step3)\n",
        "  .addEdge(START, \"step1\")\n",
        "  .addEdge(\"step1\", \"step2\")\n",
        "  .addEdge(\"step2\", \"step3\")\n",
        "  .addEdge(\"step3\", END);\n",
        "\n",
        "\n",
        "// Set up memory\n",
        "const graphStateMemory = new MemorySaver()\n",
        "\n",
        "const graph = builder.compile({\n",
        "  checkpointer: graphStateMemory,\n",
        "  interruptBefore: [\"step2\"]\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "db20864a",
      "metadata": {},
      "outputs": [
        {
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const drawableGraphGraphState = graph.getGraph();\n",
        "const graphStateImage = await drawableGraphGraphState.drawMermaidPng();\n",
        "const graphStateArrayBuffer = await graphStateImage.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(graphStateArrayBuffer));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1b3aa6fc-c7fb-4819-8d7f-ba6057cc4edf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- hello world ---\n",
            "---Step 1---\n",
            "--- hello world ---\n",
            "--- GRAPH INTERRUPTED ---\n"
          ]
        }
      ],
      "source": [
        "// Input\n",
        "const initialInput = { input: \"hello world\" };\n",
        "\n",
        "// Thread\n",
        "const graphStateConfig = { configurable: { thread_id: \"1\" }, streamMode: \"values\" as const };\n",
        "\n",
        "// Run the graph until the first interruption\n",
        "for await (const event of await graph.stream(initialInput, graphStateConfig)) {\n",
        "    console.log(`--- ${event.input} ---`);\n",
        "}\n",
        "\n",
        "// Will log when the graph is interrupted, after step 2.\n",
        "console.log(\"--- GRAPH INTERRUPTED ---\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ab27716-e861-4ba3-9d7d-90694013e3c4",
      "metadata": {},
      "source": [
        "Now, we can just manually update our graph state - "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "49d61230-e5dc-4272-b8ab-09b0af30f088",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current state!\n",
            "{ input: 'hello world' }\n",
            "---\n",
            "---\n",
            "Updated state!\n",
            "{ input: 'hello universe!' }\n"
          ]
        }
      ],
      "source": [
        "console.log(\"Current state!\")\n",
        "const currState = await graph.getState(graphStateConfig);\n",
        "console.log(currState.values)\n",
        "\n",
        "await graph.updateState(graphStateConfig, { input: \"hello universe!\" })\n",
        "\n",
        "console.log(\"---\\n---\\nUpdated state!\")\n",
        "const updatedState = await graph.getState(graphStateConfig);\n",
        "console.log(updatedState.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cf77f6eb-4cc0-4615-a095-eb5ae7027b7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---Step 2---\n",
            "--- hello universe! ---\n",
            "---Step 3---\n",
            "--- hello universe! ---\n"
          ]
        }
      ],
      "source": [
        "// Continue the graph execution\n",
        "for await (const event of await graph.stream(null, graphStateConfig)) {\n",
        "    console.log(`--- ${event.input} ---`);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3333b771",
      "metadata": {},
      "source": [
        "## Agent\n",
        "\n",
        "In the context of agents, updating state is useful for things like editing tool calls.\n",
        " \n",
        "To show this, we will build a relatively simple ReAct-style agent that does tool calling. \n",
        "\n",
        "We will use Anthropic's models and a fake tool (just for demo purposes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6098e5cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "// Set up the tool\n",
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { StateGraph, START, END, MessagesAnnotation } from \"@langchain/langgraph\";\n",
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const search = tool((_) => {\n",
        "  return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description: \"Call to surf the web.\",\n",
        "  schema: z.string(),\n",
        "})\n",
        "\n",
        "const tools = [search]\n",
        "const toolNode = new ToolNode(tools)\n",
        "\n",
        "// Set up the model\n",
        "const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" })\n",
        "const modelWithTools = model.bindTools(tools)\n",
        "\n",
        "\n",
        "// Define nodes and conditional edges\n",
        "\n",
        "// Define the function that determines whether to continue or not\n",
        "function shouldContinue(state: typeof MessagesAnnotation.State): \"action\" | typeof END {\n",
        "  const lastMessage = state.messages[state.messages.length - 1];\n",
        "  // If there is no function call, then we finish\n",
        "  if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) {\n",
        "      return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue\n",
        "  return \"action\";\n",
        "}\n",
        "\n",
        "// Define the function that calls the model\n",
        "async function callModel(state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> {\n",
        "  const messages = state.messages;\n",
        "  const response = await modelWithTools.invoke(messages);\n",
        "  // We return an object with a messages property, because this will get added to the existing list\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(MessagesAnnotation)\n",
        "  // Define the two nodes we will cycle between\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"action\", toolNode)\n",
        "  // We now add a conditional edge\n",
        "  .addConditionalEdges(\n",
        "      // First, we define the start node. We use `agent`.\n",
        "      // This means these are the edges taken after the `agent` node is called.\n",
        "      \"agent\",\n",
        "      // Next, we pass in the function that will determine which node is called next.\n",
        "      shouldContinue\n",
        "  )\n",
        "  // We now add a normal edge from `action` to `agent`.\n",
        "  // This means that after `action` is called, `agent` node is called next.\n",
        "  .addEdge(\"action\", \"agent\")\n",
        "  // Set the entrypoint as `agent`\n",
        "  // This means that this node is the first one called\n",
        "  .addEdge(START, \"agent\");\n",
        "\n",
        "// Setup memory\n",
        "const memory = new MemorySaver();\n",
        "\n",
        "// Finally, we compile it!\n",
        "// This compiles it into a LangChain Runnable,\n",
        "// meaning you can use it as you would any other runnable\n",
        "const app = workflow.compile({\n",
        "  checkpointer: memory,\n",
        "  interruptBefore: [\"action\"]\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9b011246",
      "metadata": {},
      "outputs": [
        {
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const drawableGraph = app.getGraph();\n",
        "const image = await drawableGraph.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
      "metadata": {},
      "source": [
        "## Interacting with the Agent\n",
        "\n",
        "We can now interact with the agent and see that it stops before calling a tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================ human Message (1) =================================\n",
            "search for the weather in sf now\n",
            "================================ ai Message (1) =================================\n",
            "[\n",
            "  {\n",
            "    type: 'text',\n",
            "    text: 'Certainly! I can help you search for the current weather in San Francisco. Let me use the search function to find that information for you.'\n",
            "  },\n",
            "  {\n",
            "    type: 'tool_use',\n",
            "    id: 'toolu_0141zTpknasyWkrjTV6eKeT6',\n",
            "    name: 'search',\n",
            "    input: { input: 'current weather in San Francisco' }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "// Thread\n",
        "const config = { configurable: { thread_id: \"3\" }, streamMode: \"values\" as const };\n",
        "\n",
        "for await (const event of await app.stream({\n",
        "    messages: [{ role: \"human\", content: \"search for the weather in sf now\" }]\n",
        "}, config)) {\n",
        "    const recentMsg = event.messages[event.messages.length - 1];\n",
        "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
        "    console.log(recentMsg.content);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e3f5b9-9700-42b1-863f-c404861f8620",
      "metadata": {},
      "source": [
        "**Edit**\n",
        "\n",
        "We can now update the state accordingly. Let's modify the tool call to have the query `\"current weather in SF\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1aa7b1b9-9322-4815-bc0d-eb083870ac15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  configurable: {\n",
            "    thread_id: '3',\n",
            "    checkpoint_id: '1ef5e785-4298-6b71-8002-4a6ceca964db'\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "// First, lets get the current state\n",
        "const currentState = await app.getState(config);\n",
        "\n",
        "// Let's now get the last message in the state\n",
        "// This is the one with the tool calls that we want to update\n",
        "let lastMessage = currentState.values.messages[currentState.values.messages.length - 1]\n",
        "\n",
        "// Let's now update the args for that tool call\n",
        "lastMessage.tool_calls[0].args = { query: \"current weather in SF\" }\n",
        "\n",
        "// Let's now call `updateState` to pass in this message in the `messages` key\n",
        "// This will get treated as any other update to the state\n",
        "// It will get passed to the reducer function for the `messages` key\n",
        "// That reducer function will use the ID of the message to update it\n",
        "// It's important that it has the right ID! Otherwise it would get appended\n",
        "// as a new message\n",
        "await app.updateState(config, { messages: lastMessage });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dcc5457-1ba1-4cba-ac41-da5c67cc67e5",
      "metadata": {},
      "source": [
        "Let's now check the current state of the app to make sure it got updated accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a3fcf2bd-f881-49fe-b20e-ad16e6819bc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    name: 'search',\n",
            "    args: { query: 'current weather in SF' },\n",
            "    id: 'toolu_0141zTpknasyWkrjTV6eKeT6',\n",
            "    type: 'tool_call'\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "const newState = await app.getState(config);\n",
        "const updatedStateToolCalls = newState.values.messages[newState.values.messages.length -1 ].tool_calls\n",
        "console.log(updatedStateToolCalls)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bca3814-db08-4b0b-8c0c-95b6c5440c81",
      "metadata": {},
      "source": [
        "**Resume**\n",
        "\n",
        "We can now call the agent again with no inputs to continue, ie. run the tool as requested. We can see from the logs that it passes in the update args to the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "51923913-20f7-4ee1-b9ba-d01f5fb2869b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"7c69c1f3-914b-4236-b2ca-ef250e72cb7a\",\n",
            "      \"content\": \"search for the weather in sf now\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_0152mx7AweoRWa67HFsfyaif\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I can help you search for the current weather in San Francisco. Let me use the search function to find that information for you.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_0141zTpknasyWkrjTV6eKeT6\",\n",
            "          \"name\": \"search\",\n",
            "          \"input\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_0152mx7AweoRWa67HFsfyaif\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 380,\n",
            "          \"output_tokens\": 84\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_0152mx7AweoRWa67HFsfyaif\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 380,\n",
            "          \"output_tokens\": 84\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"query\": \"current weather in SF\"\n",
            "          },\n",
            "          \"id\": \"toolu_0141zTpknasyWkrjTV6eKeT6\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"ccf0d56f-477f-408a-b809-6900a48379e0\",\n",
            "      \"content\": \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\",\n",
            "      \"name\": \"search\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_0141zTpknasyWkrjTV6eKeT6\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "================================ tool Message (1) =================================\n",
            "{\n",
            "  name: 'search',\n",
            "  content: \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\"\n",
            "}\n",
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"7c69c1f3-914b-4236-b2ca-ef250e72cb7a\",\n",
            "      \"content\": \"search for the weather in sf now\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_0152mx7AweoRWa67HFsfyaif\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I can help you search for the current weather in San Francisco. Let me use the search function to find that information for you.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_0141zTpknasyWkrjTV6eKeT6\",\n",
            "          \"name\": \"search\",\n",
            "          \"input\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_0152mx7AweoRWa67HFsfyaif\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 380,\n",
            "          \"output_tokens\": 84\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_0152mx7AweoRWa67HFsfyaif\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 380,\n",
            "          \"output_tokens\": 84\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"query\": \"current weather in SF\"\n",
            "          },\n",
            "          \"id\": \"toolu_0141zTpknasyWkrjTV6eKeT6\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"ccf0d56f-477f-408a-b809-6900a48379e0\",\n",
            "      \"content\": \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\",\n",
            "      \"name\": \"search\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_0141zTpknasyWkrjTV6eKeT6\"\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01YJXesUpaB5PfhgmRBCwnnb\",\n",
            "      \"content\": \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This means it's a clear day with plenty of sunshine. It's a great day to be outdoors or engage in activities that benefit from good weather.\\n\\nHowever, I should note that the search result included an unusual comment about Gemini zodiac signs. This appears to be unrelated to the weather and might be part of a joke or a reference to something else. For accurate and detailed weather information, I would recommend checking a reliable weather service or website for San Francisco.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\",\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01YJXesUpaB5PfhgmRBCwnnb\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"end_turn\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 498,\n",
            "          \"output_tokens\": 154\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01YJXesUpaB5PfhgmRBCwnnb\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"end_turn\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 498,\n",
            "          \"output_tokens\": 154\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 498,\n",
            "        \"output_tokens\": 154,\n",
            "        \"total_tokens\": 652\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "================================ ai Message (1) =================================\n",
            "Based on the search results, I can provide you with information about the current weather in San Francisco:\n",
            "\n",
            "The weather in San Francisco is currently sunny. This means it's a clear day with plenty of sunshine. It's a great day to be outdoors or engage in activities that benefit from good weather.\n",
            "\n",
            "However, I should note that the search result included an unusual comment about Gemini zodiac signs. This appears to be unrelated to the weather and might be part of a joke or a reference to something else. For accurate and detailed weather information, I would recommend checking a reliable weather service or website for San Francisco.\n",
            "\n",
            "Is there anything else you'd like to know about the weather in San Francisco or any other information you need?\n"
          ]
        }
      ],
      "source": [
        "for await (const event of await app.stream(null, config)) {\n",
        "    console.log(event)\n",
        "    const recentMsg = event.messages[event.messages.length - 1];\n",
        "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
        "    if (recentMsg._getType() === \"tool\") {\n",
        "        console.log({\n",
        "            name: recentMsg.name,\n",
        "            content: recentMsg.content\n",
        "        })\n",
        "    } else if (recentMsg._getType() === \"ai\") {\n",
        "        console.log(recentMsg.content)\n",
        "    }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/force-calling-a-tool-first.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3246bf13",
   "metadata": {},
   "source": [
    "# How to force an agent to call a tool\n",
    "\n",
    "In this example we will build a ReAct agent that **always** calls a certain tool\n",
    "first, before making any plans. In this example, we will create an agent with a\n",
    "search tool. However, at the start we will force the agent to call the search\n",
    "tool (and then let it do whatever it wants after). This is useful when you know\n",
    "you want to execute specific actions in your application but also want the\n",
    "flexibility of letting the LLM follow up on the user's query after going through\n",
    "that fixed sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c184d4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use). Optionally, we\n",
    "can set API key for [LangSmith tracing](https://smith.langchain.com/), which\n",
    "will give us best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6327203c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force Calling a Tool First: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"Force Calling a Tool First: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b32e0",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will\n",
    "use a built-in search tool via Tavily. However, it is really easy to create your\n",
    "own tools - see documentation\n",
    "[here](https://js.langchain.com/docs/modules/agents/tools/dynamic) on how to do\n",
    "that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294b9a8c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const searchTool = new DynamicStructuredTool({\n",
    "  name: \"search\",\n",
    "  description:\n",
    "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
    "  schema: z.object({\n",
    "    query: z.string().describe(\"The query to use in your search.\"),\n",
    "  }),\n",
    "  func: async ({}: { query: string }) => {\n",
    "    // This is a placeholder for the actual implementation\n",
    "    return \"Cold, with a low of 13 ℃\";\n",
    "  },\n",
    "});\n",
    "\n",
    "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
    "\n",
    "const tools = [searchTool];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4e65a",
   "metadata": {},
   "source": [
    "We can now wrap these tools in a `ToolNode`.\n",
    "This is a prebuilt node that takes in a LangChain chat model's generated tool call and calls that tool,\n",
    "returning the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51927bdc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const toolNode = new ToolNode(tools);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2de12a",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "Now we need to load the chat model we want to use.\\\n",
    "Importantly, this should satisfy two criteria:\n",
    "\n",
    "1. It should work with messages. We will represent all agent state in the form\n",
    "   of messages, so it needs to be able to work well with them.\n",
    "2. It should work with OpenAI function calling. This means it should either be\n",
    "   an OpenAI model or a model that exposes a similar interface.\n",
    "\n",
    "Note: these model requirements are not requirements for using LangGraph - they\n",
    "are just requirements for this one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e85df1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4o\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfea7d",
   "metadata": {},
   "source": [
    "After we've done this, we should make sure the model knows that it has these\n",
    "tools available to call. We can do this by converting the LangChain tools into\n",
    "the format for OpenAI function calling, and then bind them to the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0b4a08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "const boundModel = model.bindTools(tools);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca3d78",
   "metadata": {},
   "source": [
    "## Define the agent state\n",
    "\n",
    "The main type of graph in `langgraph` is the `StateGraph`. This graph is\n",
    "parameterized by a state object that it passes around to each node. Each node\n",
    "then returns operations to update that state.\n",
    "\n",
    "For this example, the state we will track will just be a list of messages. We\n",
    "want each node to just add messages to that list. Therefore, we will define the\n",
    "agent state as an object with one key (`messages`) with the value specifying how\n",
    "to update the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db3dd6e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b540fc",
   "metadata": {},
   "source": [
    "## Define the nodes\n",
    "\n",
    "We now need to define a few different nodes in our graph. In `langgraph`, a node\n",
    "can be either a function or a\n",
    "[runnable](https://js.langchain.com/docs/expression_language/). There are two\n",
    "main nodes we need for this:\n",
    "\n",
    "1. The agent: responsible for deciding what (if any) actions to take.\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node\n",
    "   will then execute that action.\n",
    "\n",
    "We will also need to define some edges. Some of these edges may be conditional.\n",
    "The reason they are conditional is that based on the output of a node, one of\n",
    "several paths may be taken. The path that is taken is not known until that node\n",
    "is run (the LLM decides).\n",
    "\n",
    "1. Conditional Edge: after the agent is called, we should either: a. If the\n",
    "   agent said to take an action, then the function to invoke tools should be\n",
    "   called\\\n",
    "   b. If the agent said that it was finished, then it should finish\n",
    "2. Normal Edge: after the tools are invoked, it should always go back to the\n",
    "   agent to decide what to do next\n",
    "\n",
    "Let's define the nodes, as well as a function to decide how what conditional\n",
    "edge to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee5adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { AIMessage, AIMessageChunk } from \"@langchain/core/messages\";\n",
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { concat } from \"@langchain/core/utils/stream\";\n",
    "\n",
    "// Define logic that will be used to determine which conditional edge to go down\n",
    "const shouldContinue = (state: typeof AgentState.State) => {\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "  // If there is no function call, then we finish\n",
    "  if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) {\n",
    "    return \"end\";\n",
    "  }\n",
    "  // Otherwise if there is, we continue\n",
    "  return \"continue\";\n",
    "};\n",
    "\n",
    "// Define the function that calls the model\n",
    "const callModel = async (\n",
    "  state: typeof AgentState.State,\n",
    "  config?: RunnableConfig,\n",
    ") => {\n",
    "  const { messages } = state;\n",
    "  let response: AIMessageChunk | undefined;\n",
    "  for await (const message of await boundModel.stream(messages, config)) {\n",
    "    if (!response) {\n",
    "      response = message;\n",
    "    } else {\n",
    "      response = concat(response, message);\n",
    "    }\n",
    "  }\n",
    "  // We return an object, because this will get added to the existing list\n",
    "  return {\n",
    "    messages: response ? [response as AIMessage] : [],\n",
    "  };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a87145",
   "metadata": {},
   "source": [
    "**MODIFICATION**\n",
    "\n",
    "Here we create a node that returns an AIMessage with a tool call - we will use\n",
    "this at the start to force it call a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9104fc7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "// This is the new first - the first call of the model we want to explicitly hard-code some action\n",
    "const firstModel = async (state: typeof AgentState.State) => {\n",
    "  const humanInput = state.messages[state.messages.length - 1].content || \"\";\n",
    "  return {\n",
    "    messages: [\n",
    "      new AIMessage({\n",
    "        content: \"\",\n",
    "        tool_calls: [\n",
    "          {\n",
    "            name: \"search\",\n",
    "            args: {\n",
    "              query: humanInput,\n",
    "            },\n",
    "            id: \"tool_abcd123\",\n",
    "          },\n",
    "        ],\n",
    "      }),\n",
    "    ],\n",
    "  };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609be17",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We can now put it all together and define the graph!\n",
    "\n",
    "**MODIFICATION**\n",
    "\n",
    "We will define a `firstModel` node which we will set as the entrypoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6da918",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "  // Define the new entrypoint\n",
    "  .addNode(\"first_agent\", firstModel)\n",
    "  // Define the two nodes we will cycle between\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"action\", toolNode)\n",
    "  // Set the entrypoint as `first_agent`\n",
    "  // by creating an edge from the virtual __start__ node to `first_agent`\n",
    "  .addEdge(START, \"first_agent\")\n",
    "  // We now add a conditional edge\n",
    "  .addConditionalEdges(\n",
    "    // First, we define the start node. We use `agent`.\n",
    "    // This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    // Next, we pass in the function that will determine which node is called next.\n",
    "    shouldContinue,\n",
    "    // Finally we pass in a mapping.\n",
    "    // The keys are strings, and the values are other nodes.\n",
    "    // END is a special node marking that the graph should finish.\n",
    "    // What will happen is we will call `should_continue`, and then the output of that\n",
    "    // will be matched against the keys in this mapping.\n",
    "    // Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "      // If `tools`, then we call the tool node.\n",
    "      continue: \"action\",\n",
    "      // Otherwise we finish.\n",
    "      end: END,\n",
    "    },\n",
    "  )\n",
    "  // We now add a normal edge from `tools` to `agent`.\n",
    "  // This means that after `tools` is called, `agent` node is called next.\n",
    "  .addEdge(\"action\", \"agent\")\n",
    "  // After we call the first agent, we know we want to go to action\n",
    "  .addEdge(\"first_agent\", \"action\");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f83be",
   "metadata": {},
   "source": [
    "## Use it!\n",
    "\n",
    "We can now use it! This now exposes the\n",
    "[same interface](https://js.langchain.com/docs/expression_language/) as all\n",
    "other LangChain runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acaade41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  first_agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"search\",\n",
      "            \"args\": {\n",
      "              \"query\": \"what is the weather in sf\"\n",
      "            },\n",
      "            \"id\": \"tool_abcd123\"\n",
      "          }\n",
      "        ],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "\n",
      "{\n",
      "  action: {\n",
      "    messages: [\n",
      "      ToolMessage {\n",
      "        \"content\": \"Cold, with a low of 13 ℃\",\n",
      "        \"name\": \"search\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_call_id\": \"tool_abcd123\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "\n",
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessageChunk {\n",
      "        \"id\": \"chatcmpl-9y562g16z0MUNBJcS6nKMsDuFMRsS\",\n",
      "        \"content\": \"The current weather in San Francisco is cold, with a low of 13°C.\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"prompt\": 0,\n",
      "          \"completion\": 0,\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"tool_call_chunks\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 104,\n",
      "          \"output_tokens\": 18,\n",
      "          \"total_tokens\": 122\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const inputs = {\n",
    "  messages: [new HumanMessage(\"what is the weather in sf\")],\n",
    "};\n",
    "\n",
    "for await (const output of await app.stream(inputs)) {\n",
    "  console.log(output);\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/input_output_schema.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f262985e-e973-4a27-9c9e-dbb3a06a35b7",
   "metadata": {},
   "source": [
    "# How to define input/output schema for your graph\n",
    "\n",
    "By default, `StateGraph` takes in a single schema and all nodes are expected to communicate with that schema. However, it is also possible to define explicit input and output schemas for a graph. This is helpful if you want to draw a distinction between input and output keys.\n",
    "\n",
    "In this notebook we'll walk through an example of this. At a high level, in order to do this you simply have to pass in separate [`Annotation.Root({})`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.Annotation.Root.html) objets as `{ input: Annotation.Root({}), output: Annotation.Root({}) }` when defining the graph. Let's see an example below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec0eb77-874e-443e-8c73-93125b515106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ answer: 'bye' }\n"
     ]
    }
   ],
   "source": [
    "import { Annotation, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const InputAnnotation = Annotation.Root({\n",
    "  question: Annotation<string>,\n",
    "});\n",
    "\n",
    "const OutputAnnotation = Annotation.Root({\n",
    "  answer: Annotation<string>,\n",
    "});\n",
    "\n",
    "const answerNode = (_state: typeof InputAnnotation.State) => {\n",
    "  return { answer: \"bye\" };\n",
    "};\n",
    "\n",
    "const graph = new StateGraph({\n",
    "  input: InputAnnotation,\n",
    "  output: OutputAnnotation,\n",
    "})\n",
    "  .addNode(\"answerNode\", answerNode)\n",
    "  .addEdge(\"__start__\", \"answerNode\")\n",
    "  .compile();\n",
    "\n",
    "await graph.invoke({\n",
    "  question: \"hi\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68836f-98e1-4684-a8a6-c1473c73460c",
   "metadata": {},
   "source": [
    "Notice that the output of invoke only includes the output schema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/manage-conversation-history.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to manage conversation history\n",
    "\n",
    "One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. In order to prevent this from happening, you need to probably manage the conversation history.\n",
    "\n",
    "Note: this guide focuses on how to do this in LangGraph, where you can fully customize how this is done. If you want a more off-the-shelf solution, you can look into functionality provided in LangChain:\n",
    "\n",
    "- [How to filter messages](https://js.langchain.com/docs/how_to/filter_messages/)\n",
    "- [How to trim messages](https://js.langchain.com/docs/how_to/trim_messages/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the packages we're going to want to use\n",
    "\n",
    "```bash\n",
    "yarn add langchain @langchain/anthropic @langchain/core\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767ef1c-a7cf-41f8-a301-558988cb7ac5",
   "metadata": {},
   "source": [
    "## Build the agent\n",
    "Let's now build a simple ReAct style agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378899a9-3b9a-4748-95b6-eb00e0828677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { BaseMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import { StateGraph, Annotation, START, END } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "});\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "const searchTool = tool((_): string => {\n",
    "    // This is a placeholder for the actual implementation\n",
    "    // Don't let the LLM know this though 😊\n",
    "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\"\n",
    "}, {\n",
    "    name: \"search\",\n",
    "    description: \"Call to surf the web.\",\n",
    "    schema: z.object({\n",
    "        query: z.string()\n",
    "    })\n",
    "})\n",
    "\n",
    "\n",
    "const tools = [searchTool]\n",
    "const toolNode = new ToolNode<typeof AgentState.State>(tools)\n",
    "const model = new ChatAnthropic({ model: \"claude-3-haiku-20240307\" })\n",
    "const boundModel = model.bindTools(tools)\n",
    "\n",
    "function shouldContinue(state: typeof AgentState.State): \"action\" | typeof END {\n",
    "  const lastMessage = state.messages[state.messages.length - 1];\n",
    "  // If there is no function call, then we finish\n",
    "  if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) {\n",
    "      return END;\n",
    "  }\n",
    "  // Otherwise if there is, we continue\n",
    "  return \"action\";\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModel(state: typeof AgentState.State) {\n",
    "  const response = await model.invoke(state.messages);\n",
    "  // We return an object, because this will get merged with the existing state\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "    // Define the two nodes we will cycle between\n",
    "    .addNode(\"agent\", callModel)\n",
    "    .addNode(\"action\", toolNode)\n",
    "    // We now add a conditional edge\n",
    "    .addConditionalEdges(\n",
    "        // First, we define the start node. We use `agent`.\n",
    "        // This means these are the edges taken after the `agent` node is called.\n",
    "        \"agent\",\n",
    "        // Next, we pass in the function that will determine which node is called next.\n",
    "        shouldContinue\n",
    "    )\n",
    "    // We now add a normal edge from `action` to `agent`.\n",
    "    // This means that after `action` is called, `agent` node is called next.\n",
    "    .addEdge(\"action\", \"agent\")\n",
    "    // Set the entrypoint as `agent`\n",
    "    // This means that this node is the first one called\n",
    "    .addEdge(START, \"agent\");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile({\n",
    "    checkpointer: memory,\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "hi! I'm bob\n",
      "================================ ai Message (1) =================================\n",
      "Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you may have. Please let me know if there's anything I can assist you with.\n",
      "\n",
      "\n",
      "================================= END =================================\n",
      "\n",
      "\n",
      "================================ human Message (2) =================================\n",
      "what's my name?\n",
      "================================ ai Message (2) =================================\n",
      "Your name is Bob, as you introduced yourself earlier.\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const config = { configurable: { thread_id: \"2\"}, streamMode: \"values\" as const }\n",
    "\n",
    "const inputMessage = new HumanMessage(\"hi! I'm bob\");\n",
    "for await (const event of await app.stream({\n",
    "    messages: [inputMessage]\n",
    "}, config)) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}\n",
    "\n",
    "console.log(\"\\n\\n================================= END =================================\\n\\n\")\n",
    "\n",
    "const inputMessage2 = new HumanMessage(\"what's my name?\");\n",
    "for await (const event of await app.stream({\n",
    "    messages: [inputMessage2]\n",
    "}, config)) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (2) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5da4c9-ba8b-46cb-a860-63fe585d15c5",
   "metadata": {},
   "source": [
    "## Filtering messages\n",
    "\n",
    "The most straight-forward thing to do to prevent conversation history from blowing up is to filter the list of messages before they get passed to the LLM. This involves two parts: defining a function to filter messages, and then adding it to the graph. See the example below which defines a really simple `filterMessages` function and then uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb20430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { BaseMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import { StateGraph, Annotation, START, END } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const MessageFilteringAgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "});\n",
    "\n",
    "const messageFilteringMemory = new MemorySaver();\n",
    "\n",
    "const messageFilteringSearchTool = tool((_): string => {\n",
    "    // This is a placeholder for the actual implementation\n",
    "    // Don't let the LLM know this though 😊\n",
    "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\"\n",
    "}, {\n",
    "    name: \"search\",\n",
    "    description: \"Call to surf the web.\",\n",
    "    schema: z.object({\n",
    "        query: z.string()\n",
    "    })\n",
    "})\n",
    "\n",
    "// We can re-use the same search tool as above as we don't need to change it for this example.\n",
    "const messageFilteringTools = [messageFilteringSearchTool]\n",
    "const messageFilteringToolNode = new ToolNode<typeof MessageFilteringAgentState.State>(messageFilteringTools)\n",
    "const messageFilteringModel = new ChatAnthropic({ model: \"claude-3-haiku-20240307\" })\n",
    "const boundMessageFilteringModel = messageFilteringModel.bindTools(messageFilteringTools)\n",
    "\n",
    "\n",
    "async function shouldContinueMessageFiltering(state: typeof MessageFilteringAgentState.State): Promise<\"action\" | typeof END> {\n",
    "    const lastMessage = state.messages[state.messages.length - 1];\n",
    "    // If there is no function call, then we finish\n",
    "    if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) {\n",
    "        return END;\n",
    "    }\n",
    "    // Otherwise if there is, we continue\n",
    "    return \"action\";\n",
    "}\n",
    "\n",
    "const filterMessages = (messages: BaseMessage[]): BaseMessage[] => {\n",
    "  // This is very simple helper function which only ever uses the last message\n",
    "  return messages.slice(-1);\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModelMessageFiltering(state: typeof MessageFilteringAgentState.State) {\n",
    "  const response = await boundMessageFilteringModel.invoke(filterMessages(state.messages));\n",
    "  // We return an object, because this will get merged with the existing state\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "\n",
    "// Define a new graph\n",
    "const messageFilteringWorkflow = new StateGraph(MessageFilteringAgentState)\n",
    "  // Define the two nodes we will cycle between\n",
    "  .addNode(\"agent\", callModelMessageFiltering)\n",
    "  .addNode(\"action\", messageFilteringToolNode)\n",
    "  // We now add a conditional edge\n",
    "  .addConditionalEdges(\n",
    "    // First, we define the start node. We use `agent`.\n",
    "    // This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    // Next, we pass in the function that will determine which node is called next.\n",
    "    shouldContinueMessageFiltering\n",
    "  )\n",
    "  // We now add a normal edge from `action` to `agent`.\n",
    "  // This means that after `action` is called, `agent` node is called next.\n",
    "  .addEdge(\"action\", \"agent\")\n",
    "  // Set the entrypoint as `agent`\n",
    "  // This means that this node is the first one called\n",
    "  .addEdge(START, \"agent\");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const messageFilteringApp = messageFilteringWorkflow.compile({\n",
    "    checkpointer: messageFilteringMemory,\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52468ebb-4b23-45ac-a98e-b4439f37740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "hi! I'm bob\n",
      "================================ ai Message (1) =================================\n",
      "Hello, nice to meet you Bob! I'm an AI assistant here to help out. Feel free to let me know if you have any questions or if there's anything I can assist with.\n",
      "\n",
      "\n",
      "================================= END =================================\n",
      "\n",
      "\n",
      "================================ human Message (2) =================================\n",
      "what's my name?\n",
      "================================ ai Message (2) =================================\n",
      "I'm afraid I don't actually know your name, since you haven't provided that information to me. As an AI assistant, I don't have access to personal details about you unless you share them with me directly. I'm happy to continue our conversation, but I don't have enough context to know your specific name. Please feel free to introduce yourself if you'd like me to address you by name.\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const messageFilteringConfig = { configurable: { thread_id: \"2\"}, streamMode: \"values\" as const }\n",
    "\n",
    "const messageFilteringInput = new HumanMessage(\"hi! I'm bob\");\n",
    "for await (const event of await messageFilteringApp.stream({\n",
    "    messages: [messageFilteringInput]\n",
    "}, messageFilteringConfig)) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}\n",
    "\n",
    "console.log(\"\\n\\n================================= END =================================\\n\\n\")\n",
    "\n",
    "const messageFilteringInput2 = new HumanMessage(\"what's my name?\");\n",
    "for await (const event of await messageFilteringApp.stream(\n",
    "  {\n",
    "    messages: [messageFilteringInput2]\n",
    "  },\n",
    "  messageFilteringConfig\n",
    ")) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (2) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454102b6-7112-4710-aa08-ba675e8be14c",
   "metadata": {},
   "source": [
    "In the above example we defined the `filter_messages` function ourselves. We also provide off-the-shelf ways to trim and filter messages in LangChain. \n",
    "\n",
    "- [How to filter messages](https://js.langchain.com/docs/how_to/filter_messages/)\n",
    "- [How to trim messages](https://js.langchain.com/docs/how_to/trim_messages/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/manage-ecosystem-dependencies.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to install and manage dependencies\n",
        "\n",
        "LangGraph.js is part of the [LangChain](https://js.langchain.com/) ecosystem,\n",
        "which includes the primary\n",
        "[`langchain`](https://www.npmjs.com/package/langchain) package as well as\n",
        "packages that contain integrations with individual third-party providers. They\n",
        "can be as specific as\n",
        "[`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic),\n",
        "which contains integrations just for Anthropic chat models, or as broad as\n",
        "[`@langchain/community`](https://www.npmjs.com/package/@langchain/community),\n",
        "which contains broader variety of community contributed integrations.\n",
        "\n",
        "These packages, as well as LangGraph.js itself, all rely on\n",
        "[`@langchain/core`](https://www.npmjs.com/package/@langchain/core), which\n",
        "contains the base abstractions that these packages extend.\n",
        "\n",
        "To ensure that all integrations and their types interact with each other\n",
        "properly, it is important that they all use the same version of\n",
        "`@langchain/core`. When installing LangGraph, you should install `@langchain/core` alongside it as well:\n",
        "\n",
        "```bash\n",
        "$ npm install @langchain/langgraph @langchain/core\n",
        "```\n",
        "\n",
        "`@langchain/core` must be installed separately because it is a peer dependency of `@langchain/langgraph`.\n",
        "This is to help package managers resolve a single version of `@langchain/core`.\n",
        "\n",
        "Despite this, in some situations, your package manager may resolve multiple versions of core, which can result in unexpected TypeScript errors or other strange behavior. If you need to guarantee that you only have one version of `@langchain/core` is to add a `\"resolutions\"` or\n",
        "`\"overrides\"` field in your project's `package.json`. The specific field name will depend on your package manager. Here are a few examples:\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "  <p class=\"admonition-title\">Tip</p>\n",
        "  <p>\n",
        "    The <code>resolutions</code> or <code>pnpm.overrides</code> fields for <code>yarn</code> or <code>pnpm</code> must be set in the root <code>package.json</code> file.\n",
        "\n",
        "    Also note that we specify EXACT versions for resolutions.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "If you are using `yarn`, you should set [`\"resolutions\"`](https://yarnpkg.com/cli/set/resolution):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"your-project\",\n",
        "  \"version\": \"0.0.0\",\n",
        "  \"private\": true,\n",
        "  \"engines\": {\n",
        "    \"node\": \">=18\"\n",
        "  },\n",
        "  \"dependencies\": {\n",
        "    \"@langchain/anthropic\": \"^0.2.15\",\n",
        "    \"@langchain/langgraph\": \"^0.2.0\"\n",
        "  },\n",
        "  \"resolutions\": {\n",
        "    \"@langchain/core\": \"0.2.31\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "For `npm`, use [`\"overrides\"`](https://docs.npmjs.com/cli/v10/configuring-npm/package-json#overrides):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"your-project\",\n",
        "  \"version\": \"0.0.0\",\n",
        "  \"private\": true,\n",
        "  \"engines\": {\n",
        "    \"node\": \">=18\"\n",
        "  },\n",
        "  \"dependencies\": {\n",
        "    \"@langchain/anthropic\": \"^0.2.15\",\n",
        "    \"@langchain/langgraph\": \"^0.2.0\"\n",
        "  },\n",
        "  \"overrides\": {\n",
        "    \"@langchain/core\": \"0.2.31\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "For `pnpm`, use the nested [`\"pnpm.overrides\"`](https://pnpm.io/package_json#pnpmoverrides) field:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"your-project\",\n",
        "  \"version\": \"0.0.0\",\n",
        "  \"private\": true,\n",
        "  \"engines\": {\n",
        "    \"node\": \">=18\"\n",
        "  },\n",
        "  \"dependencies\": {\n",
        "    \"@langchain/anthropic\": \"^0.2.15\",\n",
        "    \"@langchain/langgraph\": \"^0.2.0\"\n",
        "  },\n",
        "  \"pnpm\": {\n",
        "    \"overrides\": {\n",
        "      \"@langchain/core\": \"0.2.31\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You've now learned about some special considerations around using LangGraph.js\n",
        "with other LangChain ecosystem packages.\n",
        "\n",
        "Next, check out [some how-to guides on core functionality](/langgraphjs/how-tos/#core)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "name": "typescript"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
</file>

<file path="how-tos/managing-agent-steps.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38a67792",
      "metadata": {},
      "source": [
        "# How to manage agent steps\n",
        "\n",
        "In this example we will build a ReAct Agent that explicitly manages intermediate\n",
        "steps.\n",
        "\n",
        "The previous examples just put all messages into the model, but that extra\n",
        "context can distract the agent and add latency to the API calls. In this example\n",
        "we will only include the `N` most recent messages in the chat history. Note that\n",
        "this is meant to be illustrative of general state management.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First we need to install required packages:\n",
        "\n",
        "```bash\n",
        "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
        "```\n",
        "\n",
        "Next, we need to set API keys for Anthropic (the LLM we will use)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36033b66",
      "metadata": {},
      "outputs": [],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98d96da",
      "metadata": {},
      "source": [
        "Optionally, we can set API key for\n",
        "[LangSmith tracing](https://smith.langchain.com/), which will give us\n",
        "best-in-class observability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38934fde",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Managing Agent Steps: LangGraphJS\n"
          ]
        }
      ],
      "source": [
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
        "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "process.env.LANGCHAIN_PROJECT = \"Managing Agent Steps: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aeecba6",
      "metadata": {},
      "source": [
        "## Set up the State\n",
        "\n",
        "The main type of graph in `langgraph` is the\n",
        "[StateGraph](/langgraphjs/reference/classes/langgraph.StateGraph.html).\n",
        "This graph is parameterized by a state object that it passes around to each\n",
        "node. Each node then returns operations to update that state. These operations\n",
        "can either SET specific attributes on the state (e.g. overwrite the existing\n",
        "values) or ADD to the existing attribute. Whether to set or add is denoted in\n",
        "the state object you construct the graph with.\n",
        "\n",
        "For this example, the state we will track will just be a list of messages. We\n",
        "want each node to just add messages to that list. Therefore, we will define the\n",
        "state as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e95ef6be",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const AgentState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6954509",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will\n",
        "create a placeholder search engine. It is really easy to create your own tools -\n",
        "see documentation\n",
        "[here](https://js.langchain.com/docs/modules/agents/tools/dynamic) on how to do\n",
        "that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ec9f73a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = new DynamicStructuredTool({\n",
        "  name: \"search\",\n",
        "  description: \"Call to surf the web.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "  func: async ({}: { query: string }) => {\n",
        "    // This is a placeholder, but don't tell the LLM that...\n",
        "    return \"Try again in a few seconds! Checking with the weathermen... Call be again next.\";\n",
        "  },\n",
        "});\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8669db6",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a simple\n",
        "[ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html).\\\n",
        "This is a simple class that takes in a list of messages containing an\n",
        "[AIMessages with tool_calls](https://v02.api.js.langchain.com/classes/langchain_core_messages_ai.AIMessage.html),\n",
        "runs the tools, and returns the output as\n",
        "[ToolMessage](https://v02.api.js.langchain.com/classes/langchain_core_messages_tool.ToolMessage.html)s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7f4829c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode<typeof AgentState.State>(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a0a750",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now we need to load the chat model we want to use. This should satisfy two\n",
        "criteria:\n",
        "\n",
        "1. It should work with messages, since our state is primarily a list of messages\n",
        "   (chat history).\n",
        "2. It should work with tool calling, since we are using a prebuilt\n",
        "   [ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html)\n",
        "\n",
        "**Note:** these model requirements are not requirements for using LangGraph -\n",
        "they are just requirements for this particular example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cf1fcc3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0,\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a0903bb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "// After we've done this, we should make sure the model knows that it has these tools available to call.\n",
        "// We can do this by binding the tools to the model class.\n",
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96f67f3",
      "metadata": {},
      "source": [
        "## Define the nodes\n",
        "\n",
        "We now need to define a few different nodes in our graph. In `langgraph`, a node\n",
        "can be either a function or a\n",
        "[runnable](https://js.langchain.com/docs/expression_language/). There are two\n",
        "main nodes we need for this:\n",
        "\n",
        "1. The agent: responsible for deciding what (if any) actions to take.\n",
        "2. A function to invoke tools: if the agent decides to take an action, this node\n",
        "   will then execute that action.\n",
        "\n",
        "We will also need to define some edges. Some of these edges may be conditional.\n",
        "The reason they are conditional is that based on the output of a node, one of\n",
        "several paths may be taken. The path that is taken is not known until that node\n",
        "is run (the LLM decides).\n",
        "\n",
        "1. Conditional Edge: after the agent is called, we should either: a. If the\n",
        "   agent said to take an action, then the function to invoke tools should be\n",
        "   called\\\n",
        "   b. If the agent said that it was finished, then it should finish\n",
        "2. Normal Edge: after the tools are invoked, it should always go back to the\n",
        "   agent to decide what to do next\n",
        "\n",
        "Let's define the nodes, as well as a function to decide how what conditional\n",
        "edge to take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1249b1b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END } from \"@langchain/langgraph\";\n",
        "import { AIMessage, ToolMessage } from \"@langchain/core/messages\";\n",
        "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
        "\n",
        "// Define the function that determines whether to continue or not\n",
        "const shouldContinue = (state: typeof AgentState.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If there is no function call, then we finish\n",
        "  if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) {\n",
        "    return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "// **MODIFICATION**\n",
        "//\n",
        "// Here we don't pass all messages to the model but rather only pass the `N` most recent. Note that this is a terribly simplistic way to handle messages meant as an illustration, and there may be other methods you may want to look into depending on your use case. We also have to make sure we don't truncate the chat history to include the tool message first, as this would cause an API error.\n",
        "const callModel = async (\n",
        "  state: typeof AgentState.State,\n",
        "  config?: RunnableConfig,\n",
        ") => {\n",
        "  let modelMessages = [];\n",
        "  for (let i = state.messages.length - 1; i >= 0; i--) {\n",
        "    modelMessages.push(state.messages[i]);\n",
        "    if (modelMessages.length >= 5) {\n",
        "      if (!ToolMessage.isInstance(modelMessages[modelMessages.length - 1])) {\n",
        "        break;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  modelMessages.reverse();\n",
        "\n",
        "  const response = await boundModel.invoke(modelMessages, config);\n",
        "  // We return an object, because this will get added to the existing list\n",
        "  return { messages: [response] };\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227a5040",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together and define the graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ff5f7b65",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(AgentState)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(START, \"agent\")\n",
        "  .addConditionalEdges(\n",
        "    \"agent\",\n",
        "    shouldContinue,\n",
        "  )\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "// Finally, we compile it!\n",
        "// This compiles it into a LangChain Runnable,\n",
        "// meaning you can use it as you would any other runnable\n",
        "const app = workflow.compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6049db62",
      "metadata": {},
      "source": [
        "## Use it!\n",
        "\n",
        "We can now use it! This now exposes the\n",
        "[same interface](https://js.langchain.com/docs/expression_language/) as all\n",
        "other LangChain runnables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7bd7315e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: what is the weather in sf? Don't give up! Keep using your tools.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"current weather in San Francisco\"})\n",
            "-----\n",
            "\n",
            "[tool]: Try again in a few seconds! Checking with the weathermen... Call be again next.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"current weather in San Francisco\"})\n",
            "-----\n",
            "\n",
            "[tool]: Try again in a few seconds! Checking with the weathermen... Call be again next.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"current weather in San Francisco\"})\n",
            "-----\n",
            "\n",
            "[tool]: Try again in a few seconds! Checking with the weathermen... Call be again next.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"current weather in San Francisco\"})\n",
            "-----\n",
            "\n",
            "[tool]: Try again in a few seconds! Checking with the weathermen... Call be again next.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"current weather in San Francisco\"})\n",
            "-----\n",
            "\n",
            "As expected, maximum steps reached. Exiting.\n"
          ]
        }
      ],
      "source": [
        "import { HumanMessage, isAIMessage } from \"@langchain/core/messages\";\n",
        "import { GraphRecursionError } from \"@langchain/langgraph\";\n",
        "\n",
        "const prettyPrint = (message: BaseMessage) => {\n",
        "  let txt = `[${message._getType()}]: ${message.content}`;\n",
        "  if (\n",
        "    (isAIMessage(message) && (message as AIMessage)?.tool_calls?.length) ||\n",
        "    0 > 0\n",
        "  ) {\n",
        "    const tool_calls = (message as AIMessage)?.tool_calls\n",
        "      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n",
        "      .join(\"\\n\");\n",
        "    txt += ` \\nTools: \\n${tool_calls}`;\n",
        "  }\n",
        "  console.log(txt);\n",
        "};\n",
        "\n",
        "const inputs = {\n",
        "  messages: [\n",
        "    new HumanMessage(\n",
        "      \"what is the weather in sf? Don't give up! Keep using your tools.\",\n",
        "    ),\n",
        "  ],\n",
        "};\n",
        "// Setting the recursionLimit will set a max number of steps. We expect this to endlessly loop :)\n",
        "try {\n",
        "  for await (\n",
        "    const output of await app.stream(inputs, {\n",
        "      streamMode: \"values\",\n",
        "      recursionLimit: 10,\n",
        "    })\n",
        "  ) {\n",
        "    const lastMessage = output.messages[output.messages.length - 1];\n",
        "    prettyPrint(lastMessage);\n",
        "    console.log(\"-----\\n\");\n",
        "  }\n",
        "} catch (e) {\n",
        "  // Since we are truncating the chat history, the agent never gets the chance\n",
        "  // to see enough information to know to stop, so it will keep looping until we hit the\n",
        "  // maximum recursion limit.\n",
        "  if ((e as GraphRecursionError).name === \"GraphRecursionError\") {\n",
        "    console.log(\"As expected, maximum steps reached. Exiting.\");\n",
        "  } else {\n",
        "    console.error(e);\n",
        "  }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/map-reduce.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95a87145-34d0-4f97-b45f-5c9fd8532c8a",
      "metadata": {},
      "source": [
        "# How to create map-reduce branches for parallel execution\n",
        "\n",
        "[Map-reduce](https://en.wikipedia.org/wiki/MapReduce) operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks. \n",
        "\n",
        "Consider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise.\n",
        " \n",
        "(1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object).\n",
        "  \n",
        "LangGraph addresses these challenges [through its `Send` API](/langgraphjs/concepts/low_level/#send). By utilizing conditional edges, `Send` can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph's state, allowing for flexible and dynamic workflow management. \n",
        "\n",
        "![Screenshot 2024-07-12 at 9.45.40 AM.png](attachment:a108ffc8-6136-4cd7-a6f9-579e41a5a786.png)\n",
        "\n",
        "## Setup\n",
        "\n",
        "This example will require a few dependencies. First, install the LangGraph library, along with the `@langchain/anthropic` package as we'll be using Anthropic LLMs in this example:\n",
        "\n",
        "```bash\n",
        "npm install @langchain/langgraph @langchain/anthropic @langchain/core\n",
        "```\n",
        "\n",
        "Next, set your Anthropic API key:\n",
        "\n",
        "```typescript\n",
        "process.env.ANTHROPIC_API_KEY = 'YOUR_API_KEY'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d8a5a681",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { z } from \"zod\";\n",
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { StateGraph, END, START, Annotation, Send } from \"@langchain/langgraph\";\n",
        "\n",
        "/* Model and prompts */\n",
        "\n",
        "// Define model and prompts we will use\n",
        "const subjectsPrompt = \"Generate a comma separated list of between 2 and 5 examples related to: {topic}.\"\n",
        "const jokePrompt = \"Generate a joke about {subject}\"\n",
        "const bestJokePrompt = `Below are a bunch of jokes about {topic}. Select the best one! Return the ID (index) of the best one.\n",
        "\n",
        "{jokes}`\n",
        "\n",
        "// Zod schemas for getting structured output from the LLM\n",
        "const Subjects = z.object({\n",
        "  subjects: z.array(z.string()),\n",
        "});\n",
        "const Joke = z.object({\n",
        "  joke: z.string(),\n",
        "});\n",
        "const BestJoke = z.object({\n",
        "  id: z.number(),\n",
        "});\n",
        "\n",
        "const model = new ChatAnthropic({\n",
        "  model: \"claude-3-5-sonnet-20240620\",\n",
        "});\n",
        "\n",
        "/* Graph components: define the components that will make up the graph */\n",
        "\n",
        "// This will be the overall state of the main graph.\n",
        "// It will contain a topic (which we expect the user to provide)\n",
        "// and then will generate a list of subjects, and then a joke for\n",
        "// each subject\n",
        "const OverallState = Annotation.Root({\n",
        "  topic: Annotation<string>,\n",
        "  subjects: Annotation<string[]>,\n",
        "  // Notice here we pass a reducer function.\n",
        "  // This is because we want combine all the jokes we generate\n",
        "  // from individual nodes back into one list.\n",
        "  jokes: Annotation<string[]>({\n",
        "    reducer: (state, update) => state.concat(update),\n",
        "  }),\n",
        "  bestSelectedJoke: Annotation<string>,\n",
        "});\n",
        "\n",
        "// This will be the state of the node that we will \"map\" all\n",
        "// subjects to in order to generate a joke\n",
        "interface JokeState {\n",
        "  subject: string;\n",
        "}\n",
        "\n",
        "// This is the function we will use to generate the subjects of the jokes\n",
        "const generateTopics = async (\n",
        "  state: typeof OverallState.State\n",
        "): Promise<Partial<typeof OverallState.State>> => {\n",
        "  const prompt = subjectsPrompt.replace(\"topic\", state.topic);\n",
        "  const response = await model\n",
        "    .withStructuredOutput(Subjects, { name: \"subjects\" })\n",
        "    .invoke(prompt);\n",
        "  return { subjects: response.subjects };\n",
        "};\n",
        "\n",
        "// Function to generate a joke\n",
        "const generateJoke = async (state: JokeState): Promise<{ jokes: string[] }> => {\n",
        "  const prompt = jokePrompt.replace(\"subject\", state.subject);\n",
        "  const response = await model\n",
        "    .withStructuredOutput(Joke, { name: \"joke\" })\n",
        "    .invoke(prompt);\n",
        "  return { jokes: [response.joke] };\n",
        "};\n",
        "\n",
        "// Here we define the logic to map out over the generated subjects\n",
        "// We will use this an edge in the graph\n",
        "const continueToJokes = (state: typeof OverallState.State) => {\n",
        "  // We will return a list of `Send` objects\n",
        "  // Each `Send` object consists of the name of a node in the graph\n",
        "  // as well as the state to send to that node\n",
        "  return state.subjects.map((subject) => new Send(\"generateJoke\", { subject }));\n",
        "};\n",
        "\n",
        "// Here we will judge the best joke\n",
        "const bestJoke = async (\n",
        "  state: typeof OverallState.State\n",
        "): Promise<Partial<typeof OverallState.State>> => {\n",
        "  const jokes = state.jokes.join(\"\\n\\n\");\n",
        "  const prompt = bestJokePrompt\n",
        "    .replace(\"jokes\", jokes)\n",
        "    .replace(\"topic\", state.topic);\n",
        "  const response = await model\n",
        "    .withStructuredOutput(BestJoke, { name: \"best_joke\" })\n",
        "    .invoke(prompt);\n",
        "  return { bestSelectedJoke: state.jokes[response.id] };\n",
        "};\n",
        "\n",
        "// Construct the graph: here we put everything together to construct our graph\n",
        "const graph = new StateGraph(OverallState)\n",
        "  .addNode(\"generateTopics\", generateTopics)\n",
        "  .addNode(\"generateJoke\", generateJoke)\n",
        "  .addNode(\"bestJoke\", bestJoke)\n",
        "  .addEdge(START, \"generateTopics\")\n",
        "  .addConditionalEdges(\"generateTopics\", continueToJokes)\n",
        "  .addEdge(\"generateJoke\", \"bestJoke\")\n",
        "  .addEdge(\"bestJoke\", END);\n",
        "\n",
        "const app = graph.compile();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "37ed1f71-63db-416f-b715-4617b33d4b7f",
      "metadata": {},
      "outputs": [
        {
          "data": {

      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const representation = app.getGraph();\n",
        "const image = await representation.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fd90cace",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  generateTopics: { subjects: [ 'lion', 'elephant', 'penguin', 'dolphin' ] }\n",
            "}\n",
            "{\n",
            "  generateJoke: {\n",
            "    jokes: [ \"Why don't lions like fast food? Because they can't catch it!\" ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  generateJoke: {\n",
            "    jokes: [\n",
            "      \"Why don't elephants use computers? Because they're afraid of the mouse!\"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  generateJoke: {\n",
            "    jokes: [\n",
            "      \"Why don't dolphins use smartphones? They're afraid of phishing!\"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  generateJoke: {\n",
            "    jokes: [\n",
            "      \"Why don't you see penguins in Britain? Because they're afraid of Wales!\"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  bestJoke: {\n",
            "    bestSelectedJoke: \"Why don't elephants use computers? Because they're afraid of the mouse!\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "// Call the graph: here we call it to generate a list of jokes\n",
        "for await (const s of await app.stream({ topic: \"animals\" })) {\n",
        "  console.log(s);\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/multi-agent-multi-turn-convo-functional.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b182eb-1e31-43c8-85b1-706508dfa370",
   "metadata": {},
   "source": [
    "# How to add multi-turn conversation in a multi-agent application (functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Multi-agent systems](../../concepts/multi_agent)\n",
    "    - [Human-in-the-loop](../../concepts/human_in_the_loop)\n",
    "    - [Functional API](../../concepts/functional_api)\n",
    "    - [Command](../../concepts/low_level/#command)\n",
    "    - [LangGraph Glossary](../../concepts/low_level/)\n",
    "\n",
    "\n",
    "In this how-to guide, we’ll build an application that allows an end-user to engage in a *multi-turn conversation* with one or more agents. We'll create a node that uses an [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-1.html) to collect user input and routes back to the **active** agent.\n",
    "\n",
    "The agents will be implemented as tasks in a workflow that executes agent steps and determines the next action:\n",
    "\n",
    "1. **Wait for user input** to continue the conversation, or\n",
    "2. **Route to another agent** (or back to itself, such as in a loop) via a [**handoff**](../../concepts/multi_agent/#handoffs).\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42` and `@langchain/core>=0.3.36`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa4444-cd06-4813-b9ca-c9700fe12cb7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core uuid zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c217c3fe-ca50-45a1-be91-912bc83ed8b3",
   "metadata": {},
   "source": [
    "In this example we will build a team of travel assistant agents that can communicate with each other.\n",
    "\n",
    "We will create 2 agents:\n",
    "\n",
    "* `travelAdvisor`: can help with travel destination recommendations. Can ask `hotelAdvisor` for help.\n",
    "* `hotelAdvisor`: can help with hotel recommendations. Can ask `travelAdvisor` for help.\n",
    "\n",
    "This is a fully-connected network - every agent can talk to any other agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb51463a-4425-44ad-91d5-f21fd5b4e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Tool for getting travel recommendations\n",
    "const getTravelRecommendations = tool(async () => {\n",
    "  const destinations = [\"aruba\", \"turks and caicos\"];\n",
    "  return destinations[Math.floor(Math.random() * destinations.length)];\n",
    "}, {\n",
    "  name: \"getTravelRecommendations\",\n",
    "  description: \"Get recommendation for travel destinations\",\n",
    "  schema: z.object({}),\n",
    "});\n",
    "\n",
    "// Tool for getting hotel recommendations\n",
    "const getHotelRecommendations = tool(async (input: { location: \"aruba\" | \"turks and caicos\" }) => {\n",
    "  const recommendations = {\n",
    "    \"aruba\": [\n",
    "      \"The Ritz-Carlton, Aruba (Palm Beach)\",\n",
    "      \"Bucuti & Tara Beach Resort (Eagle Beach)\"\n",
    "    ],\n",
    "    \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"]\n",
    "  };\n",
    "  return recommendations[input.location];\n",
    "}, {\n",
    "  name: \"getHotelRecommendations\",\n",
    "  description: \"Get hotel recommendations for a given destination.\",\n",
    "  schema: z.object({\n",
    "    location: z.enum([\"aruba\", \"turks and caicos\"])\n",
    "  }),\n",
    "});\n",
    "\n",
    "// Define a tool to signal intent to hand off to a different agent\n",
    "// Note: this is not using Command(goto) syntax for navigating to different agents:\n",
    "// `workflow()` below handles the handoffs explicitly\n",
    "const transferToHotelAdvisor = tool(async () => {\n",
    "  return \"Successfully transferred to hotel advisor\";\n",
    "}, {\n",
    "  name: \"transferToHotelAdvisor\",\n",
    "  description: \"Ask hotel advisor agent for help.\",\n",
    "  schema: z.object({}),\n",
    "  // Hint to our agent implementation that it should stop\n",
    "  // immediately after invoking this tool \n",
    "  returnDirect: true,\n",
    "}); \n",
    "\n",
    "const transferToTravelAdvisor = tool(async () => {\n",
    "  return \"Successfully transferred to travel advisor\";\n",
    "}, {\n",
    "  name: \"transferToTravelAdvisor\", \n",
    "  description: \"Ask travel advisor agent for help.\",\n",
    "  schema: z.object({}),\n",
    "  // Hint to our agent implementation that it should stop\n",
    "  // immediately after invoking this tool\n",
    "  returnDirect: true,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce93257",
   "metadata": {},
   "source": [
    "!!! note \"Transfer tools\"\n",
    "\n",
    "    You might have noticed that we're using `tool(... { returnDirect: true })` in the transfer tools. This is done so that individual agents (e.g., `travelAdvisor`) can exit the ReAct loop early once these tools are called without calling the model a final time to process the result of the tool call. This is the desired behavior, as we want to detect when the agent calls this tool and hand control off _immediately_ to a different agent.\n",
    "    \n",
    "    **NOTE**: This is meant to work with the prebuilt [`createReactAgent`](/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) - if you are building a custom agent, make sure to manually add logic for handling early exit for tools that are marked with `returnDirect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d661e-6ba4-42b9-bc7f-6c8c423e3419",
   "metadata": {},
   "source": [
    "Let's now create our agents using the the prebuilt [`createReactAgent`](/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) and our multi-agent workflow. Note that will be calling [`interrupt`](/langgraphjs/reference/functions/langgraph.interrupt-1.html) every time after we get the final response from each of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4bdbff-9461-46cc-aee9-8a22d3c3d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  AIMessage,\n",
    "  type BaseMessage,\n",
    "  type BaseMessageLike\n",
    "} from \"@langchain/core/messages\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import {\n",
    "  addMessages,\n",
    "  entrypoint,\n",
    "  task,\n",
    "  MemorySaver,\n",
    "  interrupt,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-latest\",\n",
    "});\n",
    "\n",
    "const travelAdvisorTools = [\n",
    "  getTravelRecommendations,\n",
    "  transferToHotelAdvisor,\n",
    "];\n",
    "\n",
    "// Define travel advisor ReAct agent\n",
    "const travelAdvisor = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: travelAdvisorTools,\n",
    "  stateModifier: [\n",
    "    \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc).\",\n",
    "    \"If you need hotel recommendations, ask 'hotel_advisor' for help.\",\n",
    "    \"You MUST include human-readable response before transferring to another agent.\",\n",
    "  ].join(\" \"),\n",
    "});\n",
    "\n",
    "// You can also add additional logic like changing the input to the agent / output from the agent, etc.\n",
    "// NOTE: we're invoking the ReAct agent with the full history of messages in the state\n",
    "const callTravelAdvisor = task(\"callTravelAdvisor\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await travelAdvisor.invoke({ messages });\n",
    "  return response.messages;\n",
    "});\n",
    "\n",
    "const hotelAdvisorTools = [\n",
    "  getHotelRecommendations,\n",
    "  transferToTravelAdvisor,\n",
    "];\n",
    "\n",
    "// Define hotel advisor ReAct agent\n",
    "const hotelAdvisor = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: hotelAdvisorTools,\n",
    "  stateModifier: [\n",
    "    \"You are a hotel expert that can provide hotel recommendations for a given destination.\",\n",
    "    \"If you need help picking travel destinations, ask 'travel_advisor' for help.\",\n",
    "    \"You MUST include a human-readable response before transferring to another agent.\"\n",
    "  ].join(\" \"),\n",
    "});\n",
    "\n",
    "// Add task for hotel advisor\n",
    "const callHotelAdvisor = task(\"callHotelAdvisor\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await hotelAdvisor.invoke({ messages });\n",
    "  return response.messages;\n",
    "});\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const multiTurnGraph = entrypoint({\n",
    "  name: \"multiTurnGraph\",\n",
    "  checkpointer,\n",
    "}, async (messages: BaseMessageLike[]) => {  \n",
    "  let callActiveAgent = callTravelAdvisor;\n",
    "  let agentMessages: BaseMessage[];\n",
    "  let currentMessages = messages;\n",
    "  while (true) {\n",
    "    agentMessages = await callActiveAgent(currentMessages);\n",
    "    \n",
    "    // Find the last AI message\n",
    "    // If one of the handoff tools is called, the last message returned\n",
    "    // by the agent will be a ToolMessages because we set them to have\n",
    "    // \"returnDirect: true\". This means that the last AIMessage will\n",
    "    // have tool calls.\n",
    "    // Otherwise, the last returned message will be an AIMessage with\n",
    "    // no tool calls, which means we are ready for new input.\n",
    "    const reversedMessages = [...agentMessages].reverse();\n",
    "    const aiMsgIndex = reversedMessages\n",
    "      .findIndex((m): m is AIMessage => m.getType() === \"ai\");\n",
    "      \n",
    "    const aiMsg: AIMessage = reversedMessages[aiMsgIndex];\n",
    "  \n",
    "    // We append all messages up to the last AI message to the current messages.\n",
    "    // This may include ToolMessages (if the handoff tool was called)\n",
    "    const messagesToAdd = reversedMessages.slice(0, aiMsgIndex + 1).reverse();\n",
    "\n",
    "    // Add the agent's responses\n",
    "    currentMessages = addMessages(currentMessages, messagesToAdd);\n",
    "\n",
    "    if (!aiMsg?.tool_calls?.length) {\n",
    "      const userInput = await interrupt(\"Ready for user input.\");\n",
    "      if (typeof userInput !== \"string\") {\n",
    "        throw new Error(\"User input must be a string.\");\n",
    "      }\n",
    "      if (userInput.toLowerCase() === \"done\") {\n",
    "        break;\n",
    "      }\n",
    "      currentMessages = addMessages(currentMessages, [{\n",
    "        role: \"human\",\n",
    "        content: userInput,\n",
    "      }]);\n",
    "      continue;\n",
    "    }\n",
    "\n",
    "    const toolCall = aiMsg.tool_calls.at(-1)!;\n",
    "    if (toolCall.name === \"transferToHotelAdvisor\") {\n",
    "      callActiveAgent = callHotelAdvisor;\n",
    "    } else if (toolCall.name === \"transferToTravelAdvisor\") {\n",
    "      callActiveAgent = callTravelAdvisor;\n",
    "    } else {\n",
    "      throw new Error(`Expected transfer tool, got '${toolCall.name}'`);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return entrypoint.final({\n",
    "    value: agentMessages[agentMessages.length - 1],\n",
    "    save: currentMessages,\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af856e1b-41fc-4041-8cbf-3818a60088e0",
   "metadata": {},
   "source": [
    "We use a while loop to enable continuous conversation between agents and the user. The loop allows for:\n",
    "\n",
    "1. Getting agent responses\n",
    "2. Handling agent-to-agent transfers\n",
    "3. Collecting user input via interrupts\n",
    "4. Resuming using special inputs (see `Command` below)\n",
    "\n",
    "## Test multi-turn conversation\n",
    "\n",
    "Let's test a multi turn conversation with this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161e0cf1-d13a-4026-8f89-bdab67d1ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conversation Turn 1 ---\n",
      "\n",
      "User: [\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"i wanna go somewhere warm in the caribbean\"\n",
      "  }\n",
      "]\n",
      "\n",
      "callTravelAdvisor: Based on the recommendations, Turks and Caicos would be an excellent choice for your Caribbean getaway! This British Overseas Territory is known for its stunning white-sand beaches, crystal-clear turquoise waters, and year-round warm weather. Grace Bay Beach in Providenciales is consistently rated as one of the world's best beaches.\n",
      "\n",
      "You can enjoy:\n",
      "- World-class snorkeling and diving\n",
      "- Luxury resorts and spas\n",
      "- Fresh seafood cuisine\n",
      "- Water sports like kayaking and paddleboarding\n",
      "- Beautiful coral reefs\n",
      "- Average temperatures between 75-85°F (24-29°C) year-round\n",
      "\n",
      "Would you like me to connect you with our hotel advisor to help you find the perfect place to stay in Turks and Caicos?\n",
      "\n",
      "--- Conversation Turn 2 ---\n",
      "\n",
      "User: {\n",
      "  \"resume\": \"could you recommend a nice hotel in one of the areas and tell me which area it is.\",\n",
      "  \"goto\": []\n",
      "}\n",
      "\n",
      "callHotelAdvisor: I can recommend two excellent options in Turks and Caicos:\n",
      "\n",
      "1. Grace Bay Club - This luxury resort is located on the world-famous Grace Bay Beach in Providenciales (often called \"Provo\"). This area is the most developed and popular island in Turks and Caicos, known for its 12-mile stretch of pristine beach, excellent restaurants, and shopping. The resort offers all-oceanfront suites and is perfect if you want to be close to amenities while enjoying luxury beachfront accommodations.\n",
      "\n",
      "2. COMO Parrot Cay - This is an exclusive private island resort located on Parrot Cay, a secluded island accessible by boat from Providenciales. This is the ultimate luxury escape if you're looking for privacy and seclusion. The resort is set on 1,000 unspoiled acres with pristine white beaches. This location is perfect for those who want to truly get away from it all while enjoying world-class service and amenities.\n",
      "\n",
      "Would you like more specific information about either of these properties or their locations?\n",
      "\n",
      "--- Conversation Turn 3 ---\n",
      "\n",
      "User: {\n",
      "  \"resume\": \"i like the first one. could you recommend something to do near the hotel?\",\n",
      "  \"goto\": []\n",
      "}\n",
      "\n",
      "callHotelAdvisor: Grace Bay Club is perfectly situated to enjoy many activities in Providenciales! Since the hotel is located on Grace Bay Beach in Provo, here are some excellent activities nearby:\n",
      "\n",
      "1. Beach Activities (right at your doorstep):\n",
      "- Swimming and sunbathing on Grace Bay Beach\n",
      "- Snorkeling right off the beach\n",
      "- Beach walks along the pristine 12-mile stretch\n",
      "\n",
      "2. Within Walking Distance:\n",
      "- Salt Mills Plaza (shopping center with local boutiques and restaurants)\n",
      "- Graceway Gourmet (upscale grocery store)\n",
      "- Several beachfront restaurants and bars\n",
      "\n",
      "3. Very Close By (5-10 minute drive):\n",
      "- Princess Alexandra National Park (great for snorkeling)\n",
      "- Leeward Marina (for boat tours and fishing trips)\n",
      "- Provo Golf Club (18-hole championship golf course)\n",
      "- Thursday Night Fish Fry at Bight Park (local culture and food)\n",
      "\n",
      "4. Water Activities (operators will pick you up):\n",
      "- Snorkeling or diving trips to the barrier reef\n",
      "- Sunset sailing cruises\n",
      "- Half-day trips to Iguana Island\n",
      "- Whale watching (in season - January to April)\n",
      "\n",
      "Would you like me to connect you with our travel advisor for more specific activity recommendations or help with booking any excursions?\n"
     ]
    }
   ],
   "source": [
    "import { v4 as uuidv4 } from 'uuid';\n",
    "import { Command } from \"@langchain/langgraph\";\n",
    "import { isBaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const threadConfig = {\n",
    "  configurable: { \n",
    "    thread_id: uuidv4() \n",
    "  },\n",
    "  streamMode: \"updates\" as const,\n",
    "};\n",
    "\n",
    "const inputs = [\n",
    "  // 1st round of conversation\n",
    "  [{ role: \"user\", content: \"i wanna go somewhere warm in the caribbean\" }],\n",
    "  // Since we're using `interrupt`, we'll need to resume using the Command primitive\n",
    "  // 2nd round of conversation\n",
    "  new Command({\n",
    "    resume: \"could you recommend a nice hotel in one of the areas and tell me which area it is.\"\n",
    "  }),\n",
    "  // 3rd round of conversation\n",
    "  new Command({\n",
    "    resume: \"i like the first one. could you recommend something to do near the hotel?\"\n",
    "  })\n",
    "];\n",
    "\n",
    "const runConversation = async () => {\n",
    "  for (const [idx, userInput] of inputs.entries()) {\n",
    "    console.log();\n",
    "    console.log(`--- Conversation Turn ${idx + 1} ---`);\n",
    "    console.log();\n",
    "    console.log(`User: ${JSON.stringify(userInput, null, 2)}`);\n",
    "    console.log();\n",
    "    \n",
    "    const stream = await multiTurnGraph.stream(\n",
    "      userInput as any,\n",
    "      threadConfig,\n",
    "    );\n",
    "\n",
    "    for await (const update of stream) {\n",
    "      if (update.__metadata__?.cached) {\n",
    "        continue;\n",
    "      }\n",
    "      for (const [nodeId, value] of Object.entries(update)) {\n",
    "        if (Array.isArray(value) && value.length > 0) {\n",
    "          const lastMessage = value.at(-1);\n",
    "          if (isBaseMessage(lastMessage) && lastMessage?.getType() === \"ai\") {\n",
    "            console.log(`${nodeId}: ${lastMessage.content}`);\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "};\n",
    "\n",
    "// Execute the conversation\n",
    "try {\n",
    "  await runConversation();\n",
    "} catch (e) {\n",
    "  console.error(e);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30528adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/multi-agent-multi-turn-convo.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b182eb-1e31-43c8-85b1-706508dfa370",
   "metadata": {},
   "source": [
    "# How to add multi-turn conversation in a multi-agent application\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Node](/langgraphjs/concepts/low_level/#nodes)\n",
    "    - [Command](/langgraphjs/concepts/low_level/#command)\n",
    "    - [Multi-agent systems](/langgraphjs/concepts/multi_agent)\n",
    "    - [Human-in-the-loop](/langgraphjs/concepts/human_in_the_loop)\n",
    "\n",
    "\n",
    "In this how-to guide, we’ll build an application that allows an end-user to engage in a *multi-turn conversation* with one or more agents. We'll create a node that uses an [`interrupt`](/langgraphjs/reference/functions/langgraph.interrupt-1.html) to collect user input and routes back to the **active** agent.\n",
    "\n",
    "The agents will be implemented as nodes in a graph that executes agent steps and determines the next action:  \n",
    "\n",
    "1. **Wait for user input** to continue the conversation, or  \n",
    "2. **Route to another agent** (or back to itself, such as in a loop) via a [**handoff**](/langgraphjs/concepts/multi_agent/#handoffs).\n",
    "\n",
    "```typescript\n",
    "function human(state: typeof MessagesAnnotation.State): Command {\n",
    "  const userInput: string = interrupt(\"Ready for user input.\");\n",
    "\n",
    "  // Determine the active agent\n",
    "  const activeAgent = ...; \n",
    "\n",
    "  return new Command({\n",
    "    update: {\n",
    "      messages: [{\n",
    "        role: \"human\",\n",
    "        content: userInput,\n",
    "      }]\n",
    "    },\n",
    "    goto: activeAgent,\n",
    "  });\n",
    "}\n",
    "\n",
    "function agent(state: typeof MessagesAnnotation.State): Command {\n",
    "  // The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n",
    "  const goto = getNextAgent(...); // 'agent' / 'anotherAgent'\n",
    "\n",
    "  if (goto) {\n",
    "    return new Command({\n",
    "      goto,\n",
    "      update: { myStateKey: \"myStateValue\" }\n",
    "    });\n",
    "  } else {\n",
    "    return new Command({\n",
    "      goto: \"human\"\n",
    "    });\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa4444-cd06-4813-b9ca-c9700fe12cb7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05038da0-31df-4066-a1a4-c4ccb5db4d3a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "npm install @langchain/langgraph @langchain/openai @langchain/core uuid zod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcff5d4-130e-426d-9285-40d0f72c7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Travel: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
    "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"Time Travel: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec6e48-85dc-4905-ba50-985e5d4788e6",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6696b398-559d-4250-bb76-ebb7c97ce5f3",
   "metadata": {},
   "source": [
    "## Travel Recommendations Example\n",
    "\n",
    "In this example, we will build a team of travel assistant agents that can communicate with each other via handoffs.\n",
    "\n",
    "We will create 3 agents:\n",
    "\n",
    "* `travelAdvisor`: can help with general travel destination recommendations. Can ask `sightseeingAdvisor` and `hotelAdvisor` for help.\n",
    "* `sightseeingAdvisor`: can help with sightseeing recommendations. Can ask `travelAdvisor` and `hotelAdvisor` for help.\n",
    "* `hotelAdvisor`: can help with hotel recommendations. Can ask `sightseeingAdvisor` and `hotelAdvisor` for help.\n",
    "\n",
    "This is a fully-connected network - every agent can talk to any other agent. \n",
    "\n",
    "To implement the handoffs between the agents we'll be using LLMs with structured output. Each agent's LLM will return an output with both its text response (`response`) as well as which agent to route to next (`goto`). If the agent has enough information to respond to the user, the `goto` will be set to `human` to route back and collect information from a human.\n",
    "\n",
    "Now, let's define our agent nodes and graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4bdbff-9461-46cc-aee9-8a22d3c3d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import {\n",
    "  MessagesAnnotation,\n",
    "  StateGraph,\n",
    "  START,\n",
    "  Command,\n",
    "  interrupt,\n",
    "  MemorySaver\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "\n",
    "\n",
    "/**\n",
    " * Call LLM with structured output to get a natural language response as well as a target agent (node) to go to next.\n",
    " * @param messages list of messages to pass to the LLM\n",
    " * @param targetAgentNodes list of the node names of the target agents to navigate to\n",
    " */\n",
    "function callLlm(messages: BaseMessage[], targetAgentNodes: string[]) {\n",
    "  // define the schema for the structured output:\n",
    "  // - model's text response (`response`)\n",
    "  // - name of the node to go to next (or 'finish')\n",
    "  const outputSchema = z.object({\n",
    "    response: z.string().describe(\"A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user.\"),\n",
    "    goto: z.enum([\"finish\", ...targetAgentNodes]).describe(\"The next agent to call, or 'finish' if the user's query has been resolved. Must be one of the specified values.\"),\n",
    "  })\n",
    "  return model.withStructuredOutput(outputSchema, { name: \"Response\" }).invoke(messages)\n",
    "}\n",
    "\n",
    "async function travelAdvisor(\n",
    "  state: typeof MessagesAnnotation.State\n",
    "): Promise<Command> {\n",
    "  const systemPrompt = \n",
    "      \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \" +\n",
    "      \"If you need specific sightseeing recommendations, ask 'sightseeingAdvisor' for help. \" +\n",
    "      \"If you need hotel recommendations, ask 'hotelAdvisor' for help. \" +\n",
    "      \"If you have enough information to respond to the user, return 'finish'. \" +\n",
    "      \"Never mention other agents by name.\";\n",
    "\n",
    "  const messages = [{\"role\": \"system\", \"content\": systemPrompt}, ...state.messages] as BaseMessage[];\n",
    "  const targetAgentNodes = [\"sightseeingAdvisor\", \"hotelAdvisor\"];\n",
    "  const response = await callLlm(messages, targetAgentNodes);\n",
    "  const aiMsg = {\"role\": \"ai\", \"content\": response.response, \"name\": \"travelAdvisor\"};\n",
    "  \n",
    "  let goto = response.goto;\n",
    "  if (goto === \"finish\") {\n",
    "      goto = \"human\";\n",
    "  }\n",
    "\n",
    "  return new Command({goto, update: { \"messages\": [aiMsg] } });\n",
    "}\n",
    "\n",
    "async function sightseeingAdvisor(\n",
    "  state: typeof MessagesAnnotation.State\n",
    "): Promise<Command> {\n",
    "  const systemPrompt = \n",
    "      \"You are a travel expert that can provide specific sightseeing recommendations for a given destination. \" +\n",
    "      \"If you need general travel help, go to 'travelAdvisor' for help. \" +\n",
    "      \"If you need hotel recommendations, go to 'hotelAdvisor' for help. \" +\n",
    "      \"If you have enough information to respond to the user, return 'finish'. \" +\n",
    "      \"Never mention other agents by name.\";\n",
    "\n",
    "  const messages = [{\"role\": \"system\", \"content\": systemPrompt}, ...state.messages] as BaseMessage[];\n",
    "  const targetAgentNodes = [\"travelAdvisor\", \"hotelAdvisor\"];\n",
    "  const response = await callLlm(messages, targetAgentNodes);\n",
    "  const aiMsg = {\"role\": \"ai\", \"content\": response.response, \"name\": \"sightseeingAdvisor\"};\n",
    "  \n",
    "  let goto = response.goto;\n",
    "  if (goto === \"finish\") {\n",
    "      goto = \"human\";\n",
    "  }\n",
    "\n",
    "  return new Command({ goto, update: {\"messages\": [aiMsg] } });\n",
    "}\n",
    "\n",
    "async function hotelAdvisor(\n",
    "  state: typeof MessagesAnnotation.State\n",
    "): Promise<Command> {\n",
    "  const systemPrompt = \n",
    "      \"You are a travel expert that can provide hotel recommendations for a given destination. \" +\n",
    "      \"If you need general travel help, ask 'travelAdvisor' for help. \" +\n",
    "      \"If you need specific sightseeing recommendations, ask 'sightseeingAdvisor' for help. \" +\n",
    "      \"If you have enough information to respond to the user, return 'finish'. \" +\n",
    "      \"Never mention other agents by name.\";\n",
    "\n",
    "  const messages = [{\"role\": \"system\", \"content\": systemPrompt}, ...state.messages] as BaseMessage[];\n",
    "  const targetAgentNodes = [\"travelAdvisor\", \"sightseeingAdvisor\"];\n",
    "  const response = await callLlm(messages, targetAgentNodes);\n",
    "  const aiMsg = {\"role\": \"ai\", \"content\": response.response, \"name\": \"hotelAdvisor\"};\n",
    "  \n",
    "  let goto = response.goto;\n",
    "  if (goto === \"finish\") {\n",
    "      goto = \"human\";\n",
    "  }\n",
    "\n",
    "  return new Command({ goto, update: {\"messages\": [aiMsg] } });\n",
    "}\n",
    "\n",
    "function humanNode(\n",
    "  state: typeof MessagesAnnotation.State\n",
    "): Command {\n",
    "  const userInput: string = interrupt(\"Ready for user input.\");\n",
    "\n",
    "  let activeAgent: string | undefined = undefined;\n",
    "\n",
    "  // Look up the active agent\n",
    "  for (let i = state.messages.length - 1; i >= 0; i--) {\n",
    "      if (state.messages[i].name) {\n",
    "          activeAgent = state.messages[i].name;\n",
    "          break;\n",
    "      }\n",
    "  }\n",
    "\n",
    "  if (!activeAgent) {\n",
    "      throw new Error(\"Could not determine the active agent.\");\n",
    "  }\n",
    "\n",
    "  return new Command({\n",
    "      goto: activeAgent,\n",
    "      update: {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"human\",\n",
    "                \"content\": userInput,\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "  });\n",
    "}\n",
    "\n",
    "const builder = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"travelAdvisor\", travelAdvisor, {\n",
    "    ends: [\"sightseeingAdvisor\", \"hotelAdvisor\"]\n",
    "  })\n",
    "  .addNode(\"sightseeingAdvisor\", sightseeingAdvisor, {\n",
    "    ends: [\"human\", \"travelAdvisor\", \"hotelAdvisor\"]\n",
    "  })\n",
    "  .addNode(\"hotelAdvisor\", hotelAdvisor, {\n",
    "    ends: [\"human\", \"travelAdvisor\", \"sightseeingAdvisor\"]\n",
    "  })\n",
    "  // This adds a node to collect human input, which will route\n",
    "  // back to the active agent.\n",
    "  .addNode(\"human\", humanNode, {\n",
    "    ends: [\"hotelAdvisor\", \"sightseeingAdvisor\", \"travelAdvisor\", \"human\"]\n",
    "  })\n",
    "  // We'll always start with a general travel advisor.\n",
    "  .addEdge(START, \"travelAdvisor\")\n",
    "\n",
    "const checkpointer = new MemorySaver()\n",
    "const graph = builder.compile({ checkpointer })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d96a3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = graph.getGraph();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af856e1b-41fc-4041-8cbf-3818a60088e0",
   "metadata": {},
   "source": [
    "### Test multi-turn conversation\n",
    "\n",
    "Let's test a multi turn conversation with this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161e0cf1-d13a-4026-8f89-bdab67d1ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conversation Turn 1 ---\n",
      "\n",
      "User: {\"messages\":[{\"role\":\"user\",\"content\":\"i wanna go somewhere warm in the caribbean\"}]}\n",
      "\n",
      "travelAdvisor: The Caribbean is a fantastic choice for a warm getaway! Some popular destinations you might consider include Jamaica, the Dominican Republic, and the Bahamas. Each destination offers beautiful beaches, warm weather, and a plethora of activities to enjoy in a tropical setting. Aruba and Barbados are also great choices if you prefer lively beach towns with vibrant nightlife and cultural richness.\n",
      "\n",
      "Would you like recommendations on sightseeing or places to stay in any of these Caribbean destinations?\n",
      "\n",
      "--- Conversation Turn 2 ---\n",
      "\n",
      "User: {\"lg_name\":\"Command\",\"lc_direct_tool_output\":true,\"resume\":\"could you recommend a nice hotel in one of the areas and tell me which area it is.\",\"goto\":[]}\n",
      "\n",
      "travelAdvisor: The Caribbean is a fantastic choice for a warm getaway! Some popular destinations you might consider include Jamaica, the Dominican Republic, and the Bahamas. Each destination offers beautiful beaches, warm weather, and a plethora of activities to enjoy in a tropical setting. Aruba and Barbados are also great choices if you prefer lively beach towns with vibrant nightlife and cultural richness.\n",
      "\n",
      "Would you like recommendations on sightseeing or places to stay in any of these Caribbean destinations?\n",
      "travelAdvisor: Let's focus on Jamaica, known for its beautiful beaches and vibrant culture, perfect for a warm Caribbean escape. I'll find a nice hotel for you there.\n",
      "hotelAdvisor: In Jamaica, consider staying at the \"Round Hill Hotel and Villas\" located in Montego Bay. It's a luxurious resort offering a private beach, beautiful villas, and a spa. Montego Bay is known for its stunning beaches, lively nightlife, and rich history with plantations and cultural sites to explore.\n",
      "\n",
      "--- Conversation Turn 3 ---\n",
      "\n",
      "User: {\"lg_name\":\"Command\",\"lc_direct_tool_output\":true,\"resume\":\"could you recommend something to do near the hotel?\",\"goto\":[]}\n",
      "\n",
      "hotelAdvisor: In Jamaica, consider staying at the \"Round Hill Hotel and Villas\" located in Montego Bay. It's a luxurious resort offering a private beach, beautiful villas, and a spa. Montego Bay is known for its stunning beaches, lively nightlife, and rich history with plantations and cultural sites to explore.\n",
      "hotelAdvisor: Let's find some sightseeing recommendations or activities around Round Hill Hotel and Villas in Montego Bay, Jamaica for you.\n",
      "sightseeingAdvisor: While staying at the Round Hill Hotel and Villas in Montego Bay, you can explore a variety of activities nearby:\n",
      "\n",
      "1. **Doctor’s Cave Beach**: One of Montego Bay’s most famous beaches, it’s perfect for swimming and enjoying the sun.\n",
      "\n",
      "2. **Rose Hall Great House**: Visit this historic plantation house, rumored to be haunted, for a tour of the beautiful grounds and a taste of Jamaican history.\n",
      "\n",
      "3. **Martha Brae River**: Enjoy rafting on this beautiful river, surrounded by lush Jamaican flora. It's a peaceful and scenic way to experience the natural beauty of the area.\n",
      "\n",
      "4. **Dunn’s River Falls**: Although a bit farther than the other attractions, these stunning waterfalls in Ocho Rios are worth the visit for a unique climbing experience.\n",
      "\n",
      "5. **Montego Bay Marine Park**: Explore the coral reefs and marine life through snorkeling or diving adventures.\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const threadConfig = { configurable: { thread_id: uuidv4() }, streamMode: \"values\" as const };\n",
    "\n",
    "const inputs = [\n",
    "  // 1st round of conversation\n",
    "  {\n",
    "    messages: [\n",
    "      { role: \"user\", content: \"i wanna go somewhere warm in the caribbean\" }\n",
    "    ]\n",
    "  },\n",
    "  // Since we're using `interrupt`, we'll need to resume using the Command primitive.\n",
    "  // 2nd round of conversation\n",
    "  new Command({\n",
    "    resume: \"could you recommend a nice hotel in one of the areas and tell me which area it is.\"\n",
    "  }),\n",
    "  // Third round of conversation\n",
    "  new Command({ resume: \"could you recommend something to do near the hotel?\" }),\n",
    "]\n",
    "\n",
    "let iter = 0;\n",
    "for await (const userInput of inputs) {\n",
    "  iter += 1;\n",
    "  console.log(`\\n--- Conversation Turn ${iter} ---\\n`);\n",
    "  console.log(`User: ${JSON.stringify(userInput)}\\n`);\n",
    "\n",
    "  for await (const update of await graph.stream(userInput, threadConfig)) {\n",
    "    const lastMessage = update.messages ? update.messages[update.messages.length - 1] : undefined;\n",
    "    if (lastMessage && lastMessage._getType() === \"ai\") {\n",
    "      console.log(`${lastMessage.name}: ${lastMessage.content}`)\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/multi-agent-network-functional.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87684b48-150e-4e15-b0a5-a9dd7851f8fb",
   "metadata": {},
   "source": [
    "# How to build a multi-agent network (functional API)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c65639c-9705-49f1-840a-370718852e98",
   "metadata": {},
   "source": [
    "!!! info \"Prerequisites\" \n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Multi-agent systems](../../concepts/multi_agent)\n",
    "    - [Functional API](../../concepts/functional_api)\n",
    "    - [Command](../../concepts/low_level/#command)\n",
    "    - [LangGraph Glossary](../../concepts/low_level/)\n",
    "\n",
    "In this how-to guide we will demonstrate how to implement a [multi-agent network](../../concepts/multi_agent#network) architecture where each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. We will be using LangGraph's [functional API](../../concepts/functional_api) — individual agents will be defined as tasks and the agent handoffs will be defined in the main [entrypoint()](/langgraphjs/reference/functions/langgraph.entrypoint-1.html):\n",
    "\n",
    "\n",
    "```ts\n",
    "import { entrypoint, task } from \"@langchain/langgraph\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Define a tool to signal intent to hand off to a different agent\n",
    "const transferToHotelAdvisor = tool(async () => {\n",
    "  return \"Successfully transferred to hotel advisor\";\n",
    "}, {\n",
    "  name: \"transferToHotelAdvisor\",\n",
    "  description: \"Ask hotel advisor agent for help.\",\n",
    "  schema: z.object({}),\n",
    "  returnDirect: true,\n",
    "});\n",
    "\n",
    "// define an agent\n",
    "const travelAdvisorTools = [transferToHotelAdvisor, ...];\n",
    "const travelAdvisor = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: travelAdvisorTools,\n",
    "});\n",
    "\n",
    "// define a task that calls an agent\n",
    "const callTravelAdvisor = task(\"callTravelAdvisor\", async (messages: BaseMessage[]) => {\n",
    "  const response = travelAdvisor.invoke({ messages });\n",
    "  return response.messages;\n",
    "});\n",
    "\n",
    "const networkGraph = entrypoint(\n",
    "  { name: \"networkGraph\" },\n",
    "  async (messages: BaseMessageLike[]) => {\n",
    "    let callActiveAgent = callTravelAdvisor;\n",
    "    let agentMessages;\n",
    "    while (true) {\n",
    "      agentMessages = await callActiveAgent(messages);\n",
    "      messages = addMessages(messages, agentMessages);\n",
    "      callActiveAgent = getNextAgent(messages);\n",
    "    }\n",
    "    return messages;\n",
    "  });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa4444-cd06-4813-b9ca-c9700fe12cb7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53f304-3709-4df7-8714-1ca61e615743",
   "metadata": {},
   "source": [
    "## Travel agent example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd131b-f0c2-4b69-887f-2cbd5afb14a7",
   "metadata": {},
   "source": [
    "In this example we will build a team of travel assistant agents that can communicate with each other.\n",
    "\n",
    "We will create 2 agents:\n",
    "\n",
    "* `travelAdvisor`: can help with travel destination recommendations. Can ask `hotelAdvisor` for help.\n",
    "* `hotelAdvisor`: can help with hotel recommendations. Can ask `travelAdvisor` for help.\n",
    "\n",
    "This is a fully-connected network - every agent can talk to any other agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc9ed0-e90c-4ee1-a7c6-f5af3c634a7b",
   "metadata": {},
   "source": [
    "First, let's create some of the tools that the agents will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e31f258-ec28-4020-b86d-c91dfa9a3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Tool for getting travel recommendations\n",
    "const getTravelRecommendations = tool(async () => {\n",
    "  const destinations = [\"aruba\", \"turks and caicos\"];\n",
    "  return destinations[Math.floor(Math.random() * destinations.length)];\n",
    "}, {\n",
    "  name: \"getTravelRecommendations\",\n",
    "  description: \"Get recommendation for travel destinations\",\n",
    "  schema: z.object({}),\n",
    "});\n",
    "\n",
    "// Tool for getting hotel recommendations\n",
    "const getHotelRecommendations = tool(async (input: { location: \"aruba\" | \"turks and caicos\" }) => {\n",
    "  const recommendations = {\n",
    "    \"aruba\": [\n",
    "      \"The Ritz-Carlton, Aruba (Palm Beach)\",\n",
    "      \"Bucuti & Tara Beach Resort (Eagle Beach)\"\n",
    "    ],\n",
    "    \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"]\n",
    "  };\n",
    "  return recommendations[input.location];\n",
    "}, {\n",
    "  name: \"getHotelRecommendations\",\n",
    "  description: \"Get hotel recommendations for a given destination.\",\n",
    "  schema: z.object({\n",
    "    location: z.enum([\"aruba\", \"turks and caicos\"])\n",
    "  }),\n",
    "});\n",
    "\n",
    "// Define a tool to signal intent to hand off to a different agent\n",
    "// Note: this is not using Command(goto) syntax for navigating to different agents:\n",
    "// `workflow()` below handles the handoffs explicitly\n",
    "const transferToHotelAdvisor = tool(async () => {\n",
    "  return \"Successfully transferred to hotel advisor\";\n",
    "}, {\n",
    "  name: \"transferToHotelAdvisor\",\n",
    "  description: \"Ask hotel advisor agent for help.\",\n",
    "  schema: z.object({}),\n",
    "  // Hint to our agent implementation that it should stop\n",
    "  // immediately after invoking this tool \n",
    "  returnDirect: true,\n",
    "}); \n",
    "\n",
    "const transferToTravelAdvisor = tool(async () => {\n",
    "  return \"Successfully transferred to travel advisor\";\n",
    "}, {\n",
    "  name: \"transferToTravelAdvisor\", \n",
    "  description: \"Ask travel advisor agent for help.\",\n",
    "  schema: z.object({}),\n",
    "  // Hint to our agent implementation that it should stop\n",
    "  // immediately after invoking this tool\n",
    "  returnDirect: true,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8519a32-d23b-48b0-bd18-74f8c0dacf58",
   "metadata": {},
   "source": [
    "!!! note \"Transfer tools\"\n",
    "\n",
    "    You might have noticed that we're using `tool(... { returnDirect: true })` in the transfer tools. This is done so that individual agents (e.g., `travelAdvisor`) can exit the ReAct loop early once these tools are called without calling the model a final time to process the result of the tool call. This is the desired behavior, as we want to detect when the agent calls this tool and hand control off _immediately_ to a different agent.\n",
    "    \n",
    "    **NOTE**: This is meant to work with the prebuilt [`createReactAgent`](/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) - if you are building a custom agent, make sure to manually add logic for handling early exit for tools that are marked with `returnDirect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbc3bd-27b9-4d79-b5dd-be592bc50f74",
   "metadata": {},
   "source": [
    "Now let's define our agent tasks and combine them into a single multi-agent network workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b638d6c4-3de6-4921-980c-2df1bd1cc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  AIMessage,\n",
    "  type BaseMessageLike\n",
    "} from \"@langchain/core/messages\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import {\n",
    "  addMessages,\n",
    "  entrypoint,\n",
    "  task,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-latest\",\n",
    "});\n",
    "\n",
    "const travelAdvisorTools = [\n",
    "  getTravelRecommendations,\n",
    "  transferToHotelAdvisor,\n",
    "];\n",
    "\n",
    "// Define travel advisor ReAct agent\n",
    "const travelAdvisor = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: travelAdvisorTools,\n",
    "  stateModifier: [\n",
    "    \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc).\",\n",
    "    \"If you need hotel recommendations, ask 'hotel_advisor' for help.\",\n",
    "    \"You MUST include human-readable response before transferring to another agent.\",\n",
    "  ].join(\" \"),\n",
    "});\n",
    "\n",
    "// You can also add additional logic like changing the input to the agent / output from the agent, etc.\n",
    "// NOTE: we're invoking the ReAct agent with the full history of messages in the state\n",
    "const callTravelAdvisor = task(\"callTravelAdvisor\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await travelAdvisor.invoke({ messages });\n",
    "  return response.messages;\n",
    "});\n",
    "\n",
    "const hotelAdvisorTools = [\n",
    "  getHotelRecommendations,\n",
    "  transferToTravelAdvisor,\n",
    "];\n",
    "\n",
    "// Define hotel advisor ReAct agent\n",
    "const hotelAdvisor = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: hotelAdvisorTools,\n",
    "  stateModifier: [\n",
    "    \"You are a hotel expert that can provide hotel recommendations for a given destination.\",\n",
    "    \"If you need help picking travel destinations, ask 'travel_advisor' for help.\",\n",
    "    \"You MUST include a human-readable response before transferring to another agent.\"\n",
    "  ].join(\" \"),\n",
    "});\n",
    "\n",
    "// Add task for hotel advisor\n",
    "const callHotelAdvisor = task(\"callHotelAdvisor\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await hotelAdvisor.invoke({ messages });\n",
    "  return response.messages;\n",
    "});\n",
    "\n",
    "const networkGraph = entrypoint(\n",
    "  \"networkGraph\",\n",
    "  async (messages: BaseMessageLike[]) => {\n",
    "    // Converts inputs to LangChain messages as a side-effect\n",
    "    let currentMessages = addMessages([], messages);\n",
    "\n",
    "    let callActiveAgent = callTravelAdvisor;\n",
    "    while (true) {\n",
    "      const agentMessages = await callActiveAgent(currentMessages);\n",
    "      currentMessages = addMessages(currentMessages, agentMessages);\n",
    "      \n",
    "      // Find the last AI message\n",
    "      // If one of the handoff tools is called, the last message returned\n",
    "      // by the agent will be a ToolMessage because we set them to have\n",
    "      // \"returnDirect: true\". This means that the last AIMessage will\n",
    "      // have tool calls.\n",
    "      // Otherwise, the last returned message will be an AIMessage with\n",
    "      // no tool calls, which means we are ready for new input.\n",
    "      const aiMsg = [...agentMessages].reverse()\n",
    "        .find((m): m is AIMessage => m.getType() === \"ai\");\n",
    "        \n",
    "      // If no tool calls, we're done\n",
    "      if (!aiMsg?.tool_calls?.length) {\n",
    "        break;\n",
    "      }\n",
    "\n",
    "      // Get the last tool call and determine next agent\n",
    "      const toolCall = aiMsg.tool_calls.at(-1)!;\n",
    "      if (toolCall.name === \"transferToTravelAdvisor\") {\n",
    "        callActiveAgent = callTravelAdvisor;\n",
    "      } else if (toolCall.name === \"transferToHotelAdvisor\") {\n",
    "        callActiveAgent = callHotelAdvisor;\n",
    "      } else {\n",
    "        throw new Error(`Expected transfer tool, got '${toolCall.name}'`);\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return messages;\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223db83-1938-434a-9d24-8666842a8eea",
   "metadata": {},
   "source": [
    "Lastly, let's define a helper to render the agent outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058f3d96-534f-4b97-afb3-799ba81224ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "const prettyPrintMessages = (update: Record<string, any>) => {\n",
    "  // Handle tuple case with namespace\n",
    "  if (Array.isArray(update)) {\n",
    "    const [ns, updateData] = update;\n",
    "    // Skip parent graph updates in the printouts\n",
    "    if (ns.length === 0) {\n",
    "      return;\n",
    "    }\n",
    "\n",
    "    const graphId = ns[ns.length - 1].split(\":\")[0];\n",
    "    console.log(`Update from subgraph ${graphId}:\\n`);\n",
    "    update = updateData;\n",
    "  }\n",
    "\n",
    "  if (update.__metadata__?.cached) {\n",
    "    return;\n",
    "  }\n",
    "  // Print updates for each node\n",
    "  for (const [nodeName, updateValue] of Object.entries(update)) {\n",
    "    console.log(`Update from node ${nodeName}:\\n`);\n",
    "\n",
    "    const coercedMessages = addMessages([], updateValue.messages);\n",
    "    for (const message of coercedMessages) {\n",
    "      const textContent = typeof message.content === \"string\"\n",
    "        ? message.content\n",
    "        : JSON.stringify(message.content);\n",
    "      // Print message content based on role\n",
    "      if (message.getType() === \"ai\") {\n",
    "        console.log(\"=\".repeat(33) + \" Assistant Message \" + \"=\".repeat(33));\n",
    "        console.log(textContent);\n",
    "        console.log();\n",
    "      } else if (message.getType() === \"human\") {\n",
    "        console.log(\"=\".repeat(33) + \" Human Message \" + \"=\".repeat(33));\n",
    "        console.log(textContent);\n",
    "        console.log();\n",
    "      } else if (message.getType() === \"tool\") {\n",
    "        console.log(\"=\".repeat(33) + \" Tool Message \" + \"=\".repeat(33));\n",
    "        console.log(textContent);\n",
    "        console.log();\n",
    "      }\n",
    "    }\n",
    "    console.log(\"\\n\");\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132e2c0-d937-4325-a30e-e715c5304fe0",
   "metadata": {},
   "source": [
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b47c57-ad05-4f10-83bf-c3ff6ff8eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from subgraph callTravelAdvisor:\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "================================= Assistant Message =================================\n",
      "[{\"type\":\"text\",\"text\":\"I'll help you find a warm Caribbean destination and then get specific hotel recommendations for you.\\n\\nLet me first get some destination recommendations for the Caribbean region.\"},{\"type\":\"tool_use\",\"id\":\"toolu_019fN1etkqtCSausSv8XufhL\",\"name\":\"getTravelRecommendations\",\"input\":{}}]\n",
      "\n",
      "\n",
      "\n",
      "Update from subgraph callTravelAdvisor:\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "================================= Tool Message =================================\n",
      "turks and caicos\n",
      "\n",
      "\n",
      "\n",
      "Update from subgraph callTravelAdvisor:\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "================================= Assistant Message =================================\n",
      "[{\"type\":\"text\",\"text\":\"Great! I recommend Turks and Caicos for your Caribbean getaway. This beautiful British Overseas Territory is known for its stunning white-sand beaches, crystal-clear turquoise waters, and perfect warm weather year-round. Grace Bay Beach in Providenciales (often called \\\"Provo\\\") is consistently ranked among the world's best beaches. The islands offer excellent snorkeling, diving, and water sports opportunities, plus a relaxed Caribbean atmosphere.\\n\\nNow, let me connect you with our hotel advisor to get specific accommodation recommendations for Turks and Caicos.\"},{\"type\":\"tool_use\",\"id\":\"toolu_01UHAnBBK9zm2nAEh7brR7TY\",\"name\":\"transferToHotelAdvisor\",\"input\":{}}]\n",
      "\n",
      "\n",
      "\n",
      "Update from subgraph callTravelAdvisor:\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "================================= Tool Message =================================\n",
      "Successfully transferred to hotel advisor\n",
      "\n",
      "\n",
      "\n",
      "Update from subgraph callHotelAdvisor:\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "================================= Assistant Message =================================\n",
      "[{\"type\":\"text\",\"text\":\"Let me get some hotel recommendations for Turks and Caicos:\"},{\"type\":\"tool_use\",\"id\":\"toolu_012GUHBGXxyzwE5dY6nePq9s\",\"name\":\"getHotelRecommendations\",\"input\":{\"location\":\"turks and caicos\"}}]\n",
      "\n",
      "\n",
      "\n",
      "Update from subgraph callHotelAdvisor:\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "================================= Tool Message =================================\n",
      "[\n",
      "  \"Grace Bay Club\",\n",
      "  \"COMO Parrot Cay\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "Update from subgraph callHotelAdvisor:\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "================================= Assistant Message =================================\n",
      "Based on the recommendations, here are two excellent options in Turks and Caicos:\n",
      "\n",
      "1. Grace Bay Club: This luxurious resort is located on the world-famous Grace Bay Beach. It offers all-oceanfront suites, exceptional dining options, and top-notch amenities including multiple pools, a spa, and various water sports activities. The resort is perfect for both couples and families, with adult-only and family-friendly sections.\n",
      "\n",
      "2. COMO Parrot Cay: This exclusive private island resort offers the ultimate luxury experience. Located on its own island, it features pristine beaches, world-class spa treatments, and exceptional dining. The resort is known for its privacy, making it a favorite among celebrities. The rooms and villas offer sophisticated design with private pools and direct beach access.\n",
      "\n",
      "Would you like more specific information about either of these properties or shall I search for additional options?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const stream = await networkGraph.stream([{\n",
    "  role: \"user\",\n",
    "  content: \"i wanna go somewhere warm in the caribbean. pick one destination and give me hotel recommendations\"\n",
    "}], { subgraphs: true })\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  prettyPrintMessages(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d89ee0-0229-4718-9b98-bdd3f59c1014",
   "metadata": {},
   "source": [
    "Voila - `travelAdvisor` picks a destination and then makes a decision to call `hotelAdvisor` for more info!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/multi-agent-network.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87684b48-150e-4e15-b0a5-a9dd7851f8fb",
   "metadata": {},
   "source": [
    "# How to build a multi-agent network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c65639c-9705-49f1-840a-370718852e98",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "  <p class=\"admonition-title\">Prerequisites</p>\n",
    "  <p>\n",
    "    This guide assumes familiarity with the following:\n",
    "    <ul>\n",
    "      <li><a href=\"/langgraphjs/concepts/low_level/#nodes\">Nodes</a></li>\n",
    "      <li><a href=\"/langgraphjs/concepts/low_level/#command\">Command</a></li>\n",
    "      <li><a href=\"/langgraphjs/concepts/multi_agent\">Multi-agent systems</a></li>\n",
    "    </ul>\n",
    "    <p>\n",
    "      This functionality also requires <code>@langchain/langgraph>=0.2.29</code>.\n",
    "    </p>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "In this how-to guide we will demonstrate how to implement a [multi-agent network](../../concepts/multi_agent#network) architecture.\n",
    "\n",
    "Each agent can be represented as a node in the graph that executes agent step(s) and decides what to do next - finish execution or route to another agent (including routing to itself, e.g. running in a loop). A common pattern for routing in multi-agent architectures is handoffs. Handoffs allow you to specify:\n",
    "\n",
    "1. which agent to navigate to next and (e.g. name of the node to go to)\n",
    "2. what information to pass to that agent (e.g. state update)\n",
    "\n",
    "To implement handoffs, agent nodes can return `Command` object that allows you to [combine both control flow and state updates](/langgraphjs/how-tos/command/):\n",
    "\n",
    "```ts\n",
    "const agent = async (state) => {\n",
    "  // the condition for routing/halting can be anything\n",
    "  // e.g. LLM tool call / structured output, etc.\n",
    "  const goto = getNextAgent(...); // \"agent\" / \"another_agent\"\n",
    "  if (goto) {\n",
    "    return new Command({\n",
    "      goto,\n",
    "      update: {\n",
    "        myStateKey: \"my_state_value\",\n",
    "      }\n",
    "    });\n",
    "  }\n",
    "  ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa4444-cd06-4813-b9ca-c9700fe12cb7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core zod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec6e48-85dc-4905-ba50-985e5d4788e6",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53f304-3709-4df7-8714-1ca61e615743",
   "metadata": {},
   "source": [
    "## Travel Recommendations Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd131b-f0c2-4b69-887f-2cbd5afb14a7",
   "metadata": {},
   "source": [
    "In this example we will build a team of travel assistant agents that can communicate with each other via handoffs.\n",
    "\n",
    "We will create 3 agents:\n",
    "\n",
    "* `travel_advisor`: can help with general travel destination recommendations. Can ask `sightseeing_advisor` and `hotel_advisor` for help.\n",
    "* `sightseeing_advisor`: can help with sightseeing recommendations. Can ask `travel_advisor` and `hotel_advisor` for help.\n",
    "* `hotel_advisor`: can help with hotel recommendations. Can ask `sightseeing_advisor` and `hotel_advisor` for help.\n",
    "\n",
    "This is a fully-connected network - every agent can talk to any other agent. \n",
    "\n",
    "To implement the handoffs between the agents we'll be using LLMs with structured output. Each agent's LLM will return an output with both its text response (`response`) as well as which agent to route to next (`goto`). If the agent has enough information to respond to the user, `goto` will contain `finish`.\n",
    "\n",
    "Now, let's define our agent nodes and graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4bdbff-9461-46cc-aee9-8a22d3c3d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import {\n",
    "  Command,\n",
    "  MessagesAnnotation,\n",
    "  StateGraph\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o\",\n",
    "  temperature: 0.1,\n",
    "});\n",
    "\n",
    "const makeAgentNode = (params: {\n",
    "  name: string,\n",
    "  destinations: string[],\n",
    "  systemPrompt: string\n",
    "}) => {\n",
    "  return async (state: typeof MessagesAnnotation.State) => {\n",
    "    const possibleDestinations = [\"__end__\", ...params.destinations] as const;\n",
    "    // define schema for the structured output:\n",
    "    // - model's text response (`response`)\n",
    "    // - name of the node to go to next (or '__end__')\n",
    "    const responseSchema = z.object({\n",
    "      response: z.string().describe(\n",
    "        \"A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user.\"\n",
    "      ),\n",
    "      goto: z.enum(possibleDestinations).describe(\"The next agent to call, or __end__ if the user's query has been resolved. Must be one of the specified values.\"),\n",
    "    });\n",
    "    const messages = [\n",
    "      {\n",
    "        role: \"system\",\n",
    "        content: params.systemPrompt\n",
    "      },\n",
    "      ...state.messages,\n",
    "    ];\n",
    "    const response = await model.withStructuredOutput(responseSchema, {\n",
    "      name: \"router\",\n",
    "    }).invoke(messages);\n",
    "\n",
    "    // handoff to another agent or halt\n",
    "    const aiMessage = {\n",
    "      role: \"assistant\",\n",
    "      content: response.response,\n",
    "      name: params.name,\n",
    "    };\n",
    "    return new Command({\n",
    "      goto: response.goto,\n",
    "      update: { messages: aiMessage }\n",
    "    });\n",
    "  }\n",
    "};\n",
    "\n",
    "const travelAdvisor = makeAgentNode({\n",
    "  name: \"travel_advisor\",\n",
    "  destinations: [\"sightseeing_advisor\", \"hotel_advisor\"],\n",
    "  systemPrompt: [\n",
    "    \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \",\n",
    "    \"If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. \",\n",
    "    \"If you need hotel recommendations, ask 'hotel_advisor' for help. \",\n",
    "    \"If you have enough information to respond to the user, return '__end__'. \",\n",
    "    \"Never mention other agents by name.\"\n",
    "  ].join(\"\"),\n",
    "});\n",
    "\n",
    "const sightseeingAdvisor = makeAgentNode({\n",
    "  name: \"sightseeing_advisor\",\n",
    "  destinations: [\"travel_advisor\", \"hotel_advisor\"],\n",
    "  systemPrompt: [\n",
    "    \"You are a travel expert that can provide specific sightseeing recommendations for a given destination. \",\n",
    "    \"If you need general travel help, go to 'travel_advisor' for help. \",\n",
    "    \"If you need hotel recommendations, go to 'hotel_advisor' for help. \",\n",
    "    \"If you have enough information to respond to the user, return 'finish'. \",\n",
    "    \"Never mention other agents by name.\"\n",
    "  ].join(\"\"),\n",
    "});\n",
    "\n",
    "const hotelAdvisor = makeAgentNode({\n",
    "  name: \"hotel_advisor\",\n",
    "  destinations: [\"travel_advisor\", \"sightseeing_advisor\"],\n",
    "  systemPrompt: [\n",
    "    \"You are a booking expert that provides hotel recommendations for a given destination. \",\n",
    "    \"If you need general travel help, ask 'travel_advisor' for help. \",\n",
    "    \"If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. \",\n",
    "    \"If you have enough information to respond to the user, return 'finish'. \",\n",
    "    \"Never mention other agents by name.\",\n",
    "  ].join(\"\"),\n",
    "});\n",
    "\n",
    "const graph = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"travel_advisor\", travelAdvisor, {\n",
    "    ends: [\"sightseeing_advisor\", \"hotel_advisor\", \"__end__\"],\n",
    "  })\n",
    "  .addNode(\"sightseeing_advisor\", sightseeingAdvisor, {\n",
    "    ends: [\"travel_advisor\", \"hotel_advisor\", \"__end__\"],\n",
    "  })\n",
    "  .addNode(\"hotel_advisor\", hotelAdvisor, {\n",
    "    ends: [\"travel_advisor\", \"sightseeing_advisor\", \"__end__\"],\n",
    "  })\n",
    "  // we'll always start with a general travel advisor\n",
    "  .addEdge(\"__start__\", \"travel_advisor\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77921f6-599d-443f-8b15-56b1adafd3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = await graph.getGraphAsync();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af856e1b-41fc-4041-8cbf-3818a60088e0",
   "metadata": {},
   "source": [
    "First, let's invoke it with a generic input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a0d4df-ff99-40f0-92a8-0b3f2c591040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  travel_advisor: {\n",
      "    messages: {\n",
      "      role: 'assistant',\n",
      "      content: 'The Caribbean is a fantastic choice for warm weather and beautiful beaches. Some popular destinations include Jamaica, the Bahamas, the Dominican Republic, and Barbados. Each offers unique experiences, from vibrant culture and music to stunning natural landscapes.',\n",
      "      name: 'travel_advisor'\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const simpleStream = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"i wanna go somewhere warm in the caribbean\",\n",
    "  }],\n",
    "});\n",
    "\n",
    "for await (const chunk of simpleStream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ea9aa-36ee-40a1-a5fc-b44a079786a9",
   "metadata": {},
   "source": [
    "You can see that in this case only the first agent (`travel_advisor`) ran. Let's now ask for more recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a547d4-0a15-43bd-aeed-c9ba1dfe388f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  travel_advisor: {\n",
      "    messages: {\n",
      "      role: 'assistant',\n",
      "      content: 'I recommend visiting Aruba, a beautiful Caribbean island known for its stunning beaches and warm climate.',\n",
      "      name: 'travel_advisor'\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  sightseeing_advisor: {\n",
      "    messages: {\n",
      "      role: 'assistant',\n",
      "      content: 'Aruba is a fantastic choice for a warm Caribbean getaway. Here are some activities you can enjoy:\\n' +\n",
      "        '\\n' +\n",
      "        \"1. **Eagle Beach**: Relax on one of the world's most beautiful beaches, known for its pristine white sand and clear turquoise waters.\\n\" +\n",
      "        '\\n' +\n",
      "        '2. **Arikok National Park**: Explore this national park that covers nearly 20% of the island, offering hiking trails, caves, and unique wildlife.\\n' +\n",
      "        '\\n' +\n",
      "        '3. **Palm Beach**: Enjoy water sports, beach bars, and vibrant nightlife along this bustling beach area.\\n' +\n",
      "        '\\n' +\n",
      "        '4. **Oranjestad**: Visit the colorful capital city for shopping, dining, and exploring local culture and history.\\n' +\n",
      "        '\\n' +\n",
      "        '5. **Snorkeling and Diving**: Discover the vibrant marine life and coral reefs around the island.\\n' +\n",
      "        '\\n' +\n",
      "        '6. **California Lighthouse**: Take a trip to this iconic lighthouse for panoramic views of the island.\\n' +\n",
      "        '\\n' +\n",
      "        \"Now, let's find some hotel recommendations for your stay in Aruba.\",\n",
      "      name: 'sightseeing_advisor'\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  hotel_advisor: {\n",
      "    messages: {\n",
      "      role: 'assistant',\n",
      "      content: 'For your stay in Aruba, here are some hotel recommendations:\\n' +\n",
      "        '\\n' +\n",
      "        '1. **Renaissance Aruba Resort & Casino**: Located in Oranjestad, this resort offers a private island, luxurious accommodations, and a casino.\\n' +\n",
      "        '\\n' +\n",
      "        '2. **Hilton Aruba Caribbean Resort & Casino**: Situated on Palm Beach, this resort features beautiful gardens, a spa, and multiple dining options.\\n' +\n",
      "        '\\n' +\n",
      "        '3. **The Ritz-Carlton, Aruba**: A luxury beachfront resort offering elegant rooms, a spa, and fine dining.\\n' +\n",
      "        '\\n' +\n",
      "        '4. **Divi Aruba All Inclusive**: Enjoy an all-inclusive experience with multiple restaurants, pools, and activities right on Druif Beach.\\n' +\n",
      "        '\\n' +\n",
      "        '5. **Bucuti & Tara Beach Resort**: An adults-only resort on Eagle Beach, known for its romantic setting and eco-friendly practices.\\n' +\n",
      "        '\\n' +\n",
      "        'These options provide a range of experiences from luxury to all-inclusive, ensuring a comfortable and enjoyable stay in Aruba.',\n",
      "      name: 'hotel_advisor'\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const recommendationStream = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"i wanna go somewhere warm in the caribbean. pick one destination, give me some things to do and hotel recommendations\",\n",
    "  }],\n",
    "});\n",
    "\n",
    "for await (const chunk of recommendationStream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c66f91-39b0-4ed2-91e8-6daf6d124f47",
   "metadata": {},
   "source": [
    "Voila - `travel_advisor` makes a decision to first get some sightseeing recommendations from `sightseeing_advisor`, and then `sightseeing_advisor` in turn calls `hotel_advisor` for more info. Notice that we never explicitly defined the order in which the agents should be executed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9930b9-16b4-4179-9990-7ddf48cb3ed7",
   "metadata": {},
   "source": [
    "## Game NPCs Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7b49c5-070e-4289-88aa-afbfae44cc98",
   "metadata": {},
   "source": [
    "In this example we will create a team of [non-player characters (NPCs)](https://en.wikipedia.org/wiki/Non-player_character) that all run at the same time and share game state (resources). At each step, each NPC will inspect the state and decide whether to halt or continue acting at the next step. If it continues, it will update the shared game state (produce or consume resources).\n",
    "\n",
    "We will create 4 NPC agents:\n",
    "\n",
    "- `villager`: produces wood and food until there is enough, then halts\n",
    "- `guard`: protects gold and consumes food. When there is not enough food, leaves duty and halts\n",
    "- `merchant`: trades wood for gold. When there is not enough wood, halts\n",
    "- `thief`: checks if the guard is on duty and steals all of the gold when the guard leaves, then halts\n",
    "\n",
    "Our NPC agents will be simple node functions (`villager`, `guard`, etc.). At each step of the graph execution, the agent function will inspect the resource values in the state and decide whether it should halt or continue. If it decides to continue, it will update the resource values in the state and loop back to itself to run at the next step.\n",
    "\n",
    "Now, let's define our agent nodes and graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15c38c0-c88a-404b-9687-a9ef9ff20ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Command, StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const GameStateAnnotation = Annotation.Root({\n",
    "  // note that we're defining a reducer (operator.add) here.\n",
    "  // This will allow all agents to write their updates for resources concurrently.\n",
    "  wood: Annotation<number>({\n",
    "    default: () => 0,\n",
    "    reducer: (a, b) => a + b,\n",
    "  }),\n",
    "  food: Annotation<number>({\n",
    "    default: () => 0,\n",
    "    reducer: (a, b) => a + b,\n",
    "  }),\n",
    "  gold: Annotation<number>({\n",
    "    default: () => 0,\n",
    "    reducer: (a, b) => a + b,\n",
    "  }),\n",
    "  guardOnDuty: Annotation<boolean>,\n",
    "});\n",
    "\n",
    "/** Villager NPC that gathers wood and food. */\n",
    "const villager = async (state: typeof GameStateAnnotation.State) => {\n",
    "  const currentResources = state.wood + state.food;\n",
    "  // Continue gathering until we have enough resources\n",
    "  if (currentResources < 15) {\n",
    "    console.log(\"Villager gathering resources.\");\n",
    "    return new Command({\n",
    "      goto: \"villager\",\n",
    "      update: {\n",
    "        wood: 3,\n",
    "        food: 1,\n",
    "      },\n",
    "    });\n",
    "  }\n",
    "  // NOTE: Returning Command({goto: \"__end__\"}) is not necessary for the graph to run correctly\n",
    "  // but it's useful for visualization, to show that the agent actually halts\n",
    "  return new Command({\n",
    "    goto: \"__end__\",\n",
    "  });\n",
    "}\n",
    "\n",
    "/** Guard NPC that protects gold and consumes food. */\n",
    "const guard = async (state: typeof GameStateAnnotation.State) => {\n",
    "  if (!state.guardOnDuty) {\n",
    "    return new Command({\n",
    "      goto: \"__end__\",\n",
    "    });\n",
    "  }\n",
    "  // Guard needs food to keep patrolling\n",
    "  if (state.food > 0) {\n",
    "    console.log(\"Guard patrolling.\");\n",
    "    // Loop back to the 'guard' agent\n",
    "    return new Command({\n",
    "      goto: \"guard\",\n",
    "      update: { food: -1 },\n",
    "    });\n",
    "  }\n",
    "  console.log(\"Guard leaving to get food.\");\n",
    "  return new Command({\n",
    "    goto: \"__end__\",\n",
    "    update: {\n",
    "      guardOnDuty: false,\n",
    "    },\n",
    "  });\n",
    "};\n",
    "\n",
    "/** Merchant NPC that trades wood for gold. */\n",
    "const merchant = async (state: typeof GameStateAnnotation.State) => {\n",
    "  // Trade wood for gold when available\n",
    "  if (state.wood >= 5) {\n",
    "    console.log(\"Merchant trading wood for gold.\");\n",
    "    return new Command({\n",
    "      goto: \"merchant\", \n",
    "      update: {\n",
    "        wood: -5,\n",
    "        gold: 1\n",
    "      }\n",
    "    });\n",
    "  }\n",
    "  return new Command({\n",
    "    goto: \"__end__\"\n",
    "  });\n",
    "};\n",
    "\n",
    "/** Thief NPC that steals gold if the guard leaves to get food. */\n",
    "const thief = async (state: typeof GameStateAnnotation.State) => {\n",
    "  if (!state.guardOnDuty) {\n",
    "    console.log(\"Thief stealing gold.\");\n",
    "    return new Command({\n",
    "      goto: \"__end__\",\n",
    "      update: { gold: -state.gold }\n",
    "    });\n",
    "  }\n",
    "  // keep thief on standby (loop back to the 'thief' agent)\n",
    "  return new Command({\n",
    "    goto: \"thief\"\n",
    "  });\n",
    "};\n",
    "\n",
    "const gameGraph = new StateGraph(GameStateAnnotation)\n",
    "  .addNode(\"villager\", villager, {\n",
    "    ends: [\"villager\", \"__end__\"],\n",
    "  })\n",
    "  .addNode(\"guard\", guard, {\n",
    "    ends: [\"guard\", \"__end__\"],\n",
    "  })\n",
    "  .addNode(\"merchant\", merchant, {\n",
    "    ends: [\"merchant\", \"__end__\"],\n",
    "  })\n",
    "  .addNode(\"thief\", thief, {\n",
    "    ends: [\"thief\", \"__end__\"],\n",
    "  })\n",
    "  .addEdge(\"__start__\", \"villager\")\n",
    "  .addEdge(\"__start__\", \"guard\")\n",
    "  .addEdge(\"__start__\", \"merchant\")\n",
    "  .addEdge(\"__start__\", \"thief\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4cc03e-4e25-44ac-88b1-e415fcbce151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGameGraph = await gameGraph.getGraphAsync();\n",
    "const gameImage = await drawableGameGraph.drawMermaidPng();\n",
    "const gameArrayBuffer = await gameImage.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(gameArrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ea167-c302-41f7-906e-60fd0e5cd004",
   "metadata": {},
   "source": [
    "Let's run it with some initial state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83f50671-9371-46dd-847d-5db824c1141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game state { wood: 10, food: 3, gold: 10, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Villager gathering resources.\n",
      "Guard patrolling.\n",
      "Merchant trading wood for gold.\n",
      "Game state { wood: 8, food: 3, gold: 11, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Villager gathering resources.\n",
      "Guard patrolling.\n",
      "Merchant trading wood for gold.\n",
      "Game state { wood: 6, food: 3, gold: 12, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Villager gathering resources.\n",
      "Guard patrolling.\n",
      "Merchant trading wood for gold.\n",
      "Game state { wood: 4, food: 3, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Villager gathering resources.\n",
      "Guard patrolling.\n",
      "Game state { wood: 7, food: 3, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Villager gathering resources.\n",
      "Guard patrolling.\n",
      "Game state { wood: 10, food: 3, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Villager gathering resources.\n",
      "Guard patrolling.\n",
      "Game state { wood: 13, food: 3, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Guard patrolling.\n",
      "Game state { wood: 13, food: 2, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Guard patrolling.\n",
      "Game state { wood: 13, food: 1, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Guard patrolling.\n",
      "Game state { wood: 13, food: 0, gold: 13, guardOnDuty: true }\n",
      "--------------------------------------------------\n",
      "Guard leaving to get food.\n",
      "Game state { wood: 13, food: 0, gold: 13, guardOnDuty: false }\n",
      "--------------------------------------------------\n",
      "Thief stealing gold.\n",
      "Game state { wood: 13, food: 0, gold: 0, guardOnDuty: false }\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "const gameStream = await gameGraph.stream({\n",
    "  wood: 10,\n",
    "  food: 3,\n",
    "  gold: 10,\n",
    "  guardOnDuty: true,\n",
    "}, {\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (const state of gameStream) {\n",
    "  console.log(\"Game state\", state);\n",
    "  console.log(\"-\".repeat(50));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b396922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/node-retry-policies.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add node retry policies\n",
    "\n",
    "There are many use cases where you may wish for your node to have a custom retry policy. Some examples of when you may wish to do this is if you are calling an API, querying a database, or calling an LLM, etc. \n",
    "\n",
    "In order to configure the retry policy, you have to pass the `retryPolicy` parameter to the `addNode` function. The `retryPolicy` parameter takes in a `RetryPolicy` named tuple object. Below we instantiate a `RetryPolicy` object with the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RetryPolicy } from \"@langchain/langgraph\"\n",
    "\n",
    "const retryPolicy: RetryPolicy = {};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more information on what each of the parameters does, be sure to read the [reference](https://langchain-ai.github.io/langgraphjs/reference/types/langgraph.RetryPolicy.html).\n",
    "\n",
    "## Passing a retry policy to a node\n",
    "\n",
    "Lastly, we can pass `RetryPolicy` objects when we call the `addNode` function. In the example below we pass two different retry policies to each of our nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Database from \"better-sqlite3\"\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\"\n",
    "import { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\"\n",
    "import { AIMessage } from \"@langchain/core/messages\"\n",
    "\n",
    "// Create an in-memory database\n",
    "const db: typeof Database.prototype = new Database(':memory:');\n",
    "\n",
    "const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" });\n",
    "\n",
    "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
    "    const response = await model.invoke(state.messages);\n",
    "    return { messages: [response] };\n",
    "}\n",
    "\n",
    "const queryDatabase = async (state: typeof MessagesAnnotation.State) => {\n",
    "    const queryResult: string = JSON.stringify(db.prepare(\"SELECT * FROM Artist LIMIT 10;\").all());\n",
    "\n",
    "    return { messages: [new AIMessage({content: \"queryResult\"})]};\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph(MessagesAnnotation)\n",
    "    // Define the two nodes we will cycle between\n",
    "    .addNode(\"call_model\", callModel, { retryPolicy: {maxAttempts: 5}})\n",
    "    .addNode(\"query_database\", queryDatabase, { retryPolicy: { retryOn: (e: any): boolean => {\n",
    "        if (e instanceof Database.SqliteError) {\n",
    "          // Retry on \"SQLITE_BUSY\" error\n",
    "          return e.code === 'SQLITE_BUSY';\n",
    "        }\n",
    "        return false; // Don't retry on other errors\n",
    "      }}})\n",
    "    .addEdge(START, \"call_model\")\n",
    "    .addEdge(\"call_model\", \"query_database\")\n",
    "    .addEdge(\"query_database\", END);\n",
    "\n",
    "const graph = workflow.compile();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/pass_private_state.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ed5db3-bda5-49e1-bf75-23e08c9a3af0",
   "metadata": {},
   "source": [
    "# How to pass private state\n",
    "\n",
    "Oftentimes, you may want nodes to be able to pass state to each other that should NOT be part of the main schema of the graph. This is often useful because there may be information that is not needed as input/output (and therefore doesn't really make sense to have in the main schema) but is needed as part of the intermediate working logic.\n",
    "\n",
    "Let's take a look at an example below. In this example, we will create a RAG pipeline that:\n",
    "1. Takes in a user question\n",
    "2. Uses an LLM to generate a search query\n",
    "3. Retrieves documents for that generated query\n",
    "4. Generates a final answer based on those documents\n",
    "\n",
    "We will have a separate node for each step. We will only have the `question` and `answer` on the overall state. However, we will need separate states for the `search_query` and the `documents` - we will pass these as private state keys by defining an `input` annotation on each relevant node.\n",
    "\n",
    "Let's look at an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3114c3ad-0ade-47ba-9488-53d6f7671578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  question: 'How are you?',\n",
      "  answer: 'How are you? rephrased as a query!\\n\\nsome random document\\n\\nHow are you?'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { Annotation, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "\n",
    "// The overall state of the graph\n",
    "const OverallStateAnnotation = Annotation.Root({\n",
    "  question: Annotation<string>,\n",
    "  answer: Annotation<string>,\n",
    "});\n",
    "\n",
    "// This is what the node that generates the query will return\n",
    "const QueryOutputAnnotation = Annotation.Root({\n",
    "  query: Annotation<string>,\n",
    "});\n",
    "\n",
    "// This is what the node that retrieves the documents will return\n",
    "const DocumentOutputAnnotation = Annotation.Root({\n",
    "  docs: Annotation<string[]>,\n",
    "});\n",
    "\n",
    "// This is what the node that retrieves the documents will return\n",
    "const GenerateOutputAnnotation = Annotation.Root({\n",
    "  ...OverallStateAnnotation.spec,\n",
    "  ...DocumentOutputAnnotation.spec\n",
    "});\n",
    "\n",
    "// Node to generate query\n",
    "const generateQuery = async (state: typeof OverallStateAnnotation.State) => {\n",
    "  // Replace this with real logic\n",
    "  return {\n",
    "    query: state.question + \" rephrased as a query!\",\n",
    "  };\n",
    "};\n",
    "\n",
    "// Node to retrieve documents\n",
    "const retrieveDocuments = async (state: typeof QueryOutputAnnotation.State) => {\n",
    "  // Replace this with real logic\n",
    "  return {\n",
    "    docs: [state.query, \"some random document\"],\n",
    "  };\n",
    "};\n",
    "\n",
    "// Node to generate answer\n",
    "const generate = async (state: typeof GenerateOutputAnnotation.State) => {\n",
    "  return {\n",
    "    answer: state.docs.concat([state.question]).join(\"\\n\\n\"),\n",
    "  };\n",
    "};\n",
    "\n",
    "const graph = new StateGraph(OverallStateAnnotation)\n",
    "  .addNode(\"generate_query\", generateQuery)\n",
    "  .addNode(\"retrieve_documents\", retrieveDocuments, { input: QueryOutputAnnotation })\n",
    "  .addNode(\"generate\", generate, { input: GenerateOutputAnnotation })\n",
    "  .addEdge(\"__start__\", \"generate_query\")\n",
    "  .addEdge(\"generate_query\", \"retrieve_documents\")\n",
    "  .addEdge(\"retrieve_documents\", \"generate\")\n",
    "  .compile();\n",
    "\n",
    "await graph.invoke({\n",
    "  question: \"How are you?\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc74ea",
   "metadata": {},
   "source": [
    "Above, the original `question` value in the input has been preserved, but that the `generate_query` node rephrased it, the `retrieve_documents` node added `\"some random document\"`, and finally the `generate` node combined the `docs` in the state with the original question to create an `answer`. The intermediate steps populated by the `input` annotations passed to the individual nodes are not present in the final output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/pass-run-time-values-to-tools.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to pass runtime values to tools\n",
    "\n",
    "This guide shows how to define tools that depend on dynamically defined variables. These values are provided by your program, not by the LLM.\n",
    "\n",
    "Tools can access the [config.configurable](https://langchain-ai.github.io/langgraphjs/reference/interfaces/langgraph.LangGraphRunnableConfig.html) field for values like user IDs that are known when a graph is initially executed, as well as managed values from the [store](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html) for persistence across threads.\n",
    "\n",
    "However, it can be convenient to access intermediate runtime values which are not known ahead of time, but are progressively generated as a graph executes, such as the current graph state. This guide will cover two techniques for this: The `getCurrentTaskInput` utility function, and closures.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the following to run this guide:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "Next, configure your environment to connect to your model provider.\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, set your API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "## The `getCurrentTaskInput` Utility Function\n",
    "\n",
    "The `getCurrentTaskInput` utility function makes it easier to get the current state in areas of your application that might be called indirectly, like tool handlers.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Compatibility</p>\n",
    "    <p>\n",
    "        This functionality was added in <code>@langchain/langgraph>=0.2.53</code>.\n",
    "    </p>\n",
    "    <p>\n",
    "        It also requires <a href=\"https://nodejs.org/api/async_hooks.html\"><code>async_hooks</code></a> support, which is supported in many popular JavaScript environments (such as Node.js, Deno, and Cloudflare Workers), but not all of them (mainly web browsers). If you are deploying to an environment where this is not supported, see the [closures](#closures) section below.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "Let's start off by defining a tool that an LLM can use to update pet preferences for a user. The tool will retrieve the current state of the graph from the current context.\n",
    "\n",
    "### Define the agent state\n",
    "\n",
    "Since we're just tracking messages, we'll use the `MessagesAnnotation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesAnnotation } from \"@langchain/langgraph\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, declare a tool as shown below. The tool receives values in three different ways:\n",
    "\n",
    "1. It will receive a generated list of `pets` from the LLM in its `input`.\n",
    "2. It will pull a `userId` populated from the initial graph invocation.\n",
    "3. It will fetch the input that was passed to the currenty executing task (either a `StateGraph` node handler, or a Functional API `entrypoint` or `task`) via the `getCurrentTaskInput` function.\n",
    "\n",
    "It will then use LangGraph's [cross-thread persistence](https://langchain-ai.github.io/langgraphjs/how-tos/cross-thread-persistence/) to save preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import {\n",
    "  getCurrentTaskInput,\n",
    "  LangGraphRunnableConfig,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const updateFavoritePets = tool(async (input, config: LangGraphRunnableConfig) => {\n",
    "  // Some arguments are populated by the LLM; these are included in the schema below\n",
    "  const { pets } = input;\n",
    "  // Fetch the current input to the task that called this tool.\n",
    "  // This will be identical to the input that was passed to the `ToolNode` that called this tool.\n",
    "  const currentState = getCurrentTaskInput() as typeof MessagesAnnotation.State;\n",
    "  // Other information (such as a UserID) are most easily provided via the config\n",
    "  // This is set when when invoking or streaming the graph\n",
    "  const userId = config.configurable?.userId;\n",
    "  // LangGraph's managed key-value store is also accessible from the config\n",
    "  const store = config.store;\n",
    "  await store.put([userId, \"pets\"], \"names\", pets);\n",
    "  // Store the initial input message from the user as a note.\n",
    "  // Using the same key will override previous values - you could\n",
    "  // use something different if you wanted to store many interactions.\n",
    "  await store.put([userId, \"pets\"], \"context\", { content: currentState.messages[0].content });\n",
    "\n",
    "  return \"update_favorite_pets called.\";\n",
    "},\n",
    "{\n",
    "  // The LLM \"sees\" the following schema:\n",
    "  name: \"update_favorite_pets\",\n",
    "  description: \"add to the list of favorite pets.\",\n",
    "  schema: z.object({\n",
    "    pets: z.array(z.string()),\n",
    "  }),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the tool call schema, which is what is passed to the model for tool-calling, we can see that only `pets` is being passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  type: 'object',\n",
      "  properties: { pets: { type: 'array', items: [Object] } },\n",
      "  required: [ 'pets' ],\n",
      "  additionalProperties: false,\n",
      "  '$schema': 'http://json-schema.org/draft-07/schema#'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "console.log(zodToJsonSchema(updateFavoritePets.schema));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also declare another tool so that our agent can retrieve previously set preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "const getFavoritePets = tool(\n",
    "  async (_, config: LangGraphRunnableConfig) => {\n",
    "    const userId = config.configurable?.userId;\n",
    "    // LangGraph's managed key-value store is also accessible via the config\n",
    "    const store = config.store;\n",
    "    const petNames = await store.get([userId, \"pets\"], \"names\");\n",
    "    const context = await store.get([userId, \"pets\"], \"context\");\n",
    "    return JSON.stringify({\n",
    "      pets: petNames.value,\n",
    "      context: context.value.content,\n",
    "    });\n",
    "  },\n",
    "  {\n",
    "    // The LLM \"sees\" the following schema:\n",
    "    name: \"get_favorite_pets\",\n",
    "    description: \"retrieve the list of favorite pets for the given user.\",\n",
    "    schema: z.object({}),\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the nodes\n",
    "\n",
    "From here there's really nothing special that needs to be done. This approach works with both `StateGraph` and functional agents, and it works just as well with prebuilt agents like `createReactAgent`! We'll demonstrate it by defining a custom ReAct agent using `StateGraph`. This is very similar to the agent that you'd get if you were to instead call [`createReactAgent`](../reference/functions/langgraph_prebuilt.createReactAgent.html)\n",
    "\n",
    "Let's start off by defining the nodes for our graph.\n",
    "\n",
    "1. The agent: responsible for deciding what (if any) actions to take.\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
    "\n",
    "We will also need to define some edges.\n",
    "\n",
    "1. After the agent is called, we should either invoke the tool node or finish.\n",
    "2. After the tool node have been invoked, it should always go back to the agent to decide what to do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  END,\n",
    "  START,\n",
    "  StateGraph,\n",
    "  MemorySaver,\n",
    "  InMemoryStore,\n",
    "} from \"@langchain/langgraph\";\n",
    "import { AIMessage } from \"@langchain/core/messages\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "\n",
    "const tools = [getFavoritePets, updateFavoritePets];\n",
    "\n",
    "const routeMessage = (state: typeof MessagesAnnotation.State) => {\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "  // If no tools are called, we can finish (respond to the user)\n",
    "  if (!lastMessage?.tool_calls?.length) {\n",
    "    return END;\n",
    "  }\n",
    "  // Otherwise if there is, we continue and call the tools\n",
    "  return \"tools\";\n",
    "};\n",
    "\n",
    "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
    "  const { messages } = state;\n",
    "  const modelWithTools = model.bindTools(tools);\n",
    "  const responseMessage = await modelWithTools.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: \"You are a personal assistant. Store any preferences the user tells you about.\"\n",
    "    },\n",
    "    ...messages\n",
    "  ]);\n",
    "  return { messages: [responseMessage] };\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"tools\", new ToolNode(tools))\n",
    "  .addEdge(START, \"agent\")\n",
    "  .addConditionalEdges(\"agent\", routeMessage)\n",
    "  .addEdge(\"tools\", \"agent\");\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "const store = new InMemoryStore();\n",
    "\n",
    "const graph = workflow.compile({ checkpointer: memory, store: store });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAD5ANgDASIAAhEBAxEB/8QAHQABAAMAAgMBAAAAAAAAAAAAAAUGBwQIAQIDCf/EAFQQAAEEAQIDAgUPBwcJCQAAAAEAAgMEBQYRBxIhEzEVFiJBlAgUFzI2UVVWYXSBstHS0yM1UlRxk5VCc4KRkrO0MzRFcqHBwtThGCQmQ0RidoWx/8QAGwEBAQADAQEBAAAAAAAAAAAAAAECAwQFBgf/xAAzEQEAAQIBCAgGAgMAAAAAAAAAAQIRAwQSITFBUVKRFDNhcaGxwdEFExUjYoGS8CIy4f/aAAwDAQACEQMRAD8A/VNERAREQEREBfGzbgpx9pYmjgZ+lI8NH9ZUJfyF3NZCbGYmZ1OODybeTaxrjE4jfs4g4Fpk22JLgWt3HRxOw8V+H2n45O2nxkOStkDmt5FvrmY/0n7kfsGw+Rb4opp04k/qP7oW29zvGnCj/S9D0ln2p41YX4Yoeks+1PFbCn/RFD0Zn2J4q4X4HoejM+xX7Pb4LoPGrC/DFD0ln2p41YX4Yoeks+1PFXC/A9D0Zn2J4q4X4HoejM+xPs9vgaDxqwvwxQ9JZ9qeNWF+GKHpLPtTxVwvwPQ9GZ9ieKuF+B6HozPsT7Pb4Gg8asL8MUPSWfaubVvVrzC6tYisNHe6J4cB/UuF4q4X4HoejM+xcK3oDT1t4kGJr1bA3LbVJvredpPeRJHyuHcPP5ktgztmOU+sJoWFFXK1u7pq3BTyVh+Qx9h7Yq2QewCSN56COfbYHmOwa8AAkhpHNsX2Naq6M3tgmBERYIIiICIiAiIgIiICIiAiIgKM1PmRp3TmUyhaH+sqstgNP8otaSB9O2yk1A68x0uW0VnKdcF1iWlKImgb7v5SWjb9uy24UUziUxVqvCxrcrTOH8A4KpScQ+djeeeUf+bM4l0sh+Vzy5x/apRcbG34crjqt2uSYLMTJoyRsS1wBH+wrkrGuapqmatZIobVur8PoXBzZjO3mUMfE5jDK5rnlz3uDWMa1oLnuc4gBrQSSQAFMrPeOuKo5jQTocjhM1mqzLlaf/w64i/TeyVrmWoQ08xdE4NdswFx2PknqFgiA1j6qLSGm9EX9Q0hkMs+jkKeOsY3wbcgtQSWJGtYZYnQ9pG0tLnNc5gDy0MaS5zQZ3P+qA0NpeaOHJZW1BMacWQmibirkj6deQEskstZETWBAP8AluQjY77bFYTqODXOo+GOu6or6n1VgqV/A3MRbzWDNPM2mw345rcfYtjjfM2NkbS1xja5xc4Dm23UjxBbqDXOptdw28XrqeHJ4uszSNHFtuY2lI2Sr+UN2SPkDHtmc8PZYcCGABrCTsQ3HMcatG4PUmOwFjLulzGRrwW6lWlTntOmgme9kcrTExwLCWO3dvs0AF2wIJ43BnjJjuNGCyOTx1G/QZTyFqly3ac8IkbFPJEx7XSxsBLmxhzmN3MZdyO2cFlHqdcHlBrvSeTu4DLY1lPhji8LNNk8dNWMdqGzK2aHeRo8oFodt528rhuCCbv6mWK7htJZ3T+TxWSxmQx2oMpK83ackUM8c9+xNE+GRwDZWljmndhO2432QbCiIg4eYxVfOYq3j7TS6vZjdE/Y7EAjvB8xHeCOoIBUforKz5rS2PtW3Ndc5DDZc0bAzRuMchA8w5muU097YmOe9waxo3LidgB76rnDiNzdHUZ3tcw3HTXg17eVzRPK+YAjzHaQbhdEacGb748pv5QuxZURFzoIiICIiAiIgIiICIiAiIgIiIKpBM3QcsleztHp2WR0sFsnyabnuLnRyfox7kljvajfkPLszm9dUcOMTrW9DkLeRz9d7YRE0YjUV6hC5u5cCY68zGF3lHyiNyNhvsBtbHND2lrgHNI2IPcVWpOH2Nje52Onv4XmO5jxtt8UX0RbmMfQ0LozqMTTXNp53/v7uuidaA9gjT++/hjWn7PHXL/80pvSfDjF6NvS26N7P2pJY+yc3LagvZCMDcHcMsTPa13T2wAO2432JXsdE2CSfGnPD5BND+EniTY+NWe/fQ/hJ8vD4/CS0b1oRVfxJsfGrPfvofwlU7ePysPFXF6ebqnMeDrOFt35CZYe07WOesxmx7P2vLM/fp37dR53y8Pj8JLRvaoqxq3h7jdZ2K8167nar4WFjRic9dx7SCd/KbXlYHH5XAlePEmx8as9++h/CTxJsfGrPfvofwk+Xh8fhJaN6D9grT+23hjWe3/zTL/80pLTnCrEaXy0WRqZHUtieMOAjyWpsjdhO4IO8U072O7+m7Tseo6rleJNj41Z799D+EvJ4fY+10ydrI5qPckwX7bnQu38zom7McPkc0hMzDjXXyj3sWh88jbj1yJcVQe2fDk9nkLrCSyRv8qCJw6OJ9q8g7NBI9t7W1NaGtDWgAAbADzL1hhjrxMiiY2ONjQ1rGDYNA7gB5gvdYV1xMRTToiAREWpBERAREQEREBERAREQEREBERAREQEREBZ9kNvZ/wPfv4sZHzdP87peff/AHf9dBWfZBpPqgMC7Y7DTGRG/L0/zul5/N+zz/Qg0FERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAWe5Dl/7QOB9rzeLGR2335tvXdLu823d8vd8q0JZ9kAfZ+wR5fJ8WMiC7r0Prql0973/AJf9qDQUREBERAREQEREBERAREQEREBERAREQEREBERARFU7+rMjauWIMHRrWYq8hhlt3Z3RMMg3DmsDWOLuUjYnoAdwNyDttw8OrEm1K2utiKkeHdYfqGD9Lm/DTw7rD9Qwfpc34a39Fr3xzgsu66Aak9XtlsP6omLEz8KpzqPHR2dN+DY8wHGaaWxA5r2v9b78v5EbbDqHg+ZdyPDusP1DB+lzfhrIMt6n+bMeqGxvFybH4bwzTp9ganriUxSzAcsdhxMe/O1h5R+xp83V0WvfHOCzssipHh3WH6hg/S5vw08O6w/UMH6XN+GnRa98c4LLuipHh3WH6hg/S5vw19q+r8rjZYjnaFOGlI9sZt0bD5BC5x5Wl7XMbszcgcwJ233IDQXCTkuJstP7gsuKIi5EEREBERAREQEREBERAREQEREBERAWeaGO+BeT3m/eJ+U+upVoazzQv5gf8+u/4qVd+T9XV3x6rsWBERbEEREBEXHu5GpjRCbdqGqJpWwRGaQM7SRx2axu/e4nuA6lByFXOIx5dBZ9w720pSP2hp2VjVc4ke4HUPzGb6pW7A62jvjzZU64aMiIvGYiIiAiIgIiICIiAiIgIiICIiAiIgLPNC/mB/z67/ipVoazzQv5gf8APrv+KlXfk/V1d8eq7FgXT/i9rXUlfEcQNfadu5mOnp7Lupw37+pX1a7JYJo4ZIYsdHEYpozIHM3mIe4uJB7l3AWeZb1P2gc7bzE9/ACyMu6WS5Xfbn9bSSyM5JJhCH9myVzSQZWtD+p8pWqJnUjPM/4S1lrnjBYm1ZmcCNH168WKioXn168BdRFl1iaIHlm3e8j8oCOVmw2PVZ/pPWeptayaKw2Qj1znaVfQWJyb26XykdWzYtWA8Ps2JpLEL5NuyAADnDmLi4dQtK4j8Aclmc7PNgsXp+1VnxcWNbay2UyMVhnZhwa6yxrnsyAbuC0TcpBB3cd91c6nAHSnitpTE369izZ07i4cVXydS5PRsuhYxrS10kEjHFriwOLCS3fzLG0zIyzEy63z+c4M4HV2UzeFs262eZlYK94V5r0UDoRWdK+u8gSFnI4uY7cFzwCOYquZ6hNqSpiMLk8znLVbDcV/AtSbwvZZYFUw9q1r5myB73MLtmyOJe0Do7qd+y+N4Y6Yw1jTs1DERU36egnr4wQve1tdk3L2o5QdnF3I0kuBO+533J34WV4M6PzWMyNC3iXur5DKjOTmK5PFJ69AaBOyRjw+N2zGjyC0d/Tqd2bIugGwA9731XeJHuB1D8xm+qVYmgNaAO4dFXeJHuB1D8xm+qV14HW0d8ebKnXDRkRF4zEREQEREBERAREQEREBERAREQEREBZ5oX8wP+fXf8VKtDWdytyOms1Pjsbi587SnfPcY6rIxrqrnP53xSGQtZ1fISzZ2/KSOUBnM7uyeYzaqL2mbTu1X91jVZYUUJ4Wz3xMyvpVL8dPC2e+JmV9KpfjrpzPyj+Ue62TaKE8LZ74mZX0ql+Oqxa4x1qfEKnoabB349VXKbr0GONipzPhaSC7m7bl36E8pO+wJ22CZn5R/KPcs0JFCeFs98TMr6VS/HTwtnviZlfSqX46Zn5R/KPcsm1XOJHuB1D8xm+qVyPC2e+JmV9Kpfjr45HH57U+PmqPwcmNrOZzStt2ou0mA69kzs3PDeYgAuJ6AnoSs6LYdcV1VRaJvrifKSItN2iIovD6hrZd3rc708pHXhsWcXPJGbNUSAlokaxzh3te3maXNJY8Bx5SpReKxEREBERAREQEREBERAREQEREBEUARPqiyQe3p4eCWSKSKSKJzMqx0XLuCS4thBe7zMe50YIPZ/5UPU5Czqcujxc0lTHAVrEeYh7OWO5G487mQ9T0LA0GTbbaXyDzAlsti8TSwlMVcfVip1w98vZwsDQXveXveffc57nOc49S5xJJJJXIiiZBEyKJjY42NDWsYNg0DuAHmC90BERAX54av9TJxxy/quoNawah0pX1BI6TNUmG/ZMUVSCWKIV3f93BO7ZWt2AII5tyOm/6HrPaG2U485eZh5o8Pp+tWJ8wksTyyOb394bXiJ6dz29/XYNCREQEREEfl8LBmIQ18k1aZrmPZZqyGKVpa8PADh3tJaN2ndrhuHAgkLg1s1dx11tTNRR72rU7adqjHI+LsWt7RnbkjaJ/KHjckscY9wWl7YxPL0liZPE+ORjZI3gtcx43Dge8EecIPdFW2QS6LijZA10+noYq9WCnBC589U8/IXc3MeeINcwkbbsEbju4EBlkQEREBERAREQEREBERAREQVjLT19V5ezpxklG5Qgi2zlOeN8jnxysd2UI2IYObYueHF3kAAs2lDhZWMbExrGNDGNGwa0bAD3gq/oK8MvpuLJtyj8xDkJprcFl9X1sRC+Rxij5NgdmM5GczuruXmPfsrEgIiICIiDiZbK1MFi7mSyFhlShThfYsWJTsyKNjS5zifMAAT9CqfCbG3G4O7n8pXfUy2pLjsrPXlYGSV43NayvC8fpMgjha7/3h/vriZVp4oaldiGAnSeFtNdkpSPIyNthDm1Wn+VHE7ldKR0L2ti3PLMwaEgIiICIiAiIgKrymvoORrwaeO0zI7Z7eWQOgsySgNI23Y2Nxd1GzA09dzzHa0Lw5oe0tO+xG3Q7H+tB5RQGjrUzsfPjrUmQtW8VN6xlu5Gu2J9shjHCZvL5Lw5r27uaAOYPGzSC0T6AiIgIiICIiAiKFzGttPaftCtk85jsfZI5uxs2mMft7/KTvss6aKq5tTF5W100iq3spaO+NOI9Nj+1UrjHBwy428PcppLP6jwz6lxm8U4tROkrTD2krNz0cD7224Lh3Erb0fG4J5SubO5YuGXEbTuo6kOAra0oam1Nj4pI78XaRxXuaGTsZZJawPNHs/YHoBu4bd4V8X55eoH4W0uCfFLiDkNVZnFwSUo2YvHXPXTOxuRvfzumhcT1GzIx745iDsdwu8nspaO+NOI9Nj+1Oj43BPKTNnctKKreylo7404j02P7U9lLR3xpxHpsf2p0fG4J5SZs7lpVJzuYvauyk2ncBNJVrRHkyubi3Hrcdxggd55z3Fw6RDqfK5WmNt67i4h52XTGlcxBDDHGH5HL15mmVjHDpHVH8p5HfLsWs7hzP3DL1hcLR07i6+OxtZlSlA3ljiZ5tySSSepJJJLjuSSSSSStNVFVE2qi0sdTzhsPS0/iquNx1aOnRqxiKGCIbNY0dw/6rmoixBERAREQEREBERBXYyanEGZu2YlbexjHbuPNjYDDK4bN/Qmf64G/6TYR+gd7Esi1Hx44Y4fiVjKt/iLiaV+nXv1LFY56rHUgkEkAcy0x0gcJgWEM3G4/LA7brVqF+tlKNe7SsRW6dmNs0NiB4fHKxw3a5rh0LSCCCOhBQfdERAREQEREHCzVx2Pw960wAvggklaD77Wkj/8AFUdJVI62ApSAc09mJk88zur5pHNBc9xPUkk/R3dwVn1V7mMx8zm+oVXtNe5zFfNIvqBehgaMKe9diSREWaCIiAiIgg9awtOmshbb+Tt0oJLVado8uGVjCWuaenvbEb9QSD0JV4o2DbpV5yOUyxteQPNuN1Stae47O/MJ/wC7crhhvzPR/mI/qhYY/VUz2z6LscxEReegiIgIipvEfXw0dShr1Gsny9sO7CN/tYmjvlePO0EgADq4kDoNyN2DhV49cYeHF5kWLMagxmnq7Z8pkKuPiceVrrMrYw4+8Nz1PyBVt/GTRrHbeG43fKyGRw/rDVhk5ku3pL12eS9fk9vasEOe75B0AaOp8loAG/QBeV9Zh/A8KKfuVzM9mjzuXhuPszaN+Gm+jy/cT2ZtG/DTfR5fuLDkW36Hk3FVzj2LwwPjd6nfSuvvVb4bUdG8waFzUgyGekZFI0QzR9ZGbbc282zdiN/Ke89wXeqHjDomvEyKLLxxRMaGsYytKGtA6AAcnQLEUT6Hk3FVzj2Lw3H2ZtG/DTfR5fuLy3jLo1x2GaZ9MEo/4VhqJ9Dybiq5x7F4djcFrHB6mc5uKy1S9IwbuihlBkaPfLe8fSFMrqrLWjlkjkILJozzRzRuLJIz77XjZzT8oIWu8L+I02TsMwWYlEl7lJq23bA2QASWuH6bQN9x7YAnvB38rLfhNWT0Ti4U3iNe+Pc0TqaaiIvnRF6q9zGY+ZzfUKr2mvc5ivmkX1ArDqr3MZj5nN9Qqvaa9zmK+aRfUC9HB6me/wBF2OZefYjpWH1Io7FtsbjDFNIY2Pft5LXPDXFoJ2BIadu/Y9ywDhxx91XkeHGi5slgKmZ1hqq3ZixlWvkRFFJDFzvkmnf2AELY2t5dmtkJ8g9S4gdhV150xwO1vpTT+iTWnwEuc0TcuNx4kszivkaVkOD2zERc0EnlMI5RIAWecO6Sb30In5/VEy4+pkMde0vJFrirmq2BZgYbzZIp7FiLtoXssloHZGIPeXFgcOzcOXcDetcXOMOddw64gYS9Ql0brLDVaF5j8Zk3TslrT2QxssM4ZG/vZIxwLWkfKCufd4Daoy9m/q61kMRX15LqGlnq9aEyvx8Ta1d1ZlZ0haJHB0ckvNJyA8zwQ3Zux9NX8DdX8RMfrnKZmxhKWpc7j6OJpU6diaSpUrV7JnPPM6Jr3ue57z0jAGzR16lY/wCQsljjdl7utNR4TT2k4M3Hp63HUvRuzEdfIP5o45DJBVcwh7A2QbOfIwOLXAdy1xYNxc4L6p4mXcjXfj9IPEkrXYvVb+2gzGHZ0P5MMjPaPaeYg9swHcbt7994Y3lY0FxcQNtz3lZxfaIfWnuOzvzCf+7crhhvzPR/mI/qhU/WnuOzvzCf+7crhhvzPR/mI/qhTH6mnvnyhdjmIiLz0EREBdb9a5J2Y17n7L3FwhnFKIH+QyJoBA/pmR39JdkF1v1tjXYbXufrPbytnnF2In+WyRoJP9sSD+ivo/geb8+q+u3rC7JRKLhZjN47T1F13K36uMpsIa6xcmbFGCTsAXOIHUqAHFzQp7taaeP/ANrB99fZVV0UzaqYhrWxZhQ4z2LlPHZx+njDo/IXWUq+V9eAzeXL2Ucr4OTyY3P2APOTs4EtVlj4r6HsSNij1jp+R7yGtY3KQEuJ6AAc/VZxpTgCdLzY6gNO6Ot0qVoSNzVms59+SEPLmtMfIGiQDZvadoe7fl8y5cXErqmPkzeNvhbf27u9U9keM+RpY/VWUj0y2bD6avy1L1h2Q5ZXtZylz4o+zIcQ1wJa5zfeBJ325mpeJGSfktS4zBYM5GDC1Wvv3fXohfG+SIyNbCzlPaODC13VzO8DfdcbI8Lsrb0DxFwbLFMW9R3blmo9z39mxsrGBokPLuCC077A/Sl/QOqcdn9UT4GxijR1JBCLD7r5Gy05mQCEvja1pEgLWtOxLeo861TOPtvyj8v+Cx8I7tjJcK9H27liW1anxFWWWed5e+R5iaS5zj1JJ6klW1Z5pDVGmuHOkcFpfOaswFTL4ihXp2oX5KJha9kTQejy12x7xuAdiOil/Zd0L8ddO/xWD766cPEopopiqqL23oti+NrIyYRseVh3E2Okbcbt3nkPMR9LQWn5CVwcFqnC6pilkwuXoZeOIhsj6Flk4YT3AlpOx/audaxsmcbHiYQTNkZG027d45zyuP0NLnH5Glb70VU3n/X0ZU64dqwQQCDuD5wiAAAADYDzIvylUZqr3MZj5nN9Qqvaa9zmK+aRfUCtOZpuyOIvVGEB88EkQJ8xc0j/AHqoaSuR2MDThB5LNaFkFiB3R8MjWgOY4HqCD/WNiOhC9DA04Ux2rsTCIizQREQEREENrT3HZ35hP/duVww35no/zEf1QqXrSdni5fpN2kuXoJKtWu0+XNK9hDWtHU/KTtsACT0BV5pVzUpV4CeYxRtZv7+w2WGPowqY7Z9F2PuiIvPQREQFTuI+ghrGjFPVeyDL1OYwSP8AaSNPtonnvDSQDuOrSAdiN2uuKLdg4teBXGJhzaYHVe5E6pcfj8jWfSus9vUtNAd+0d4cOh8ppIO3Qr5+sax/9PF/YC7O5fBY3P1xBk6FbIQg7hlmJsgaffG46H5Qq27g7o1538AwN+Rj3tH9QdsvrMP45hTH3aJiezT52LQwYUq4O4giB/1Avsty9hvRvwHF+9k+8nsN6N+A4v3sn3lt+t5Nw1co9y0MNRbl7DejfgOL97J95PYb0b8BxfvZPvJ9cybhq5R7loYQ+rBI4udDG5x7yWglePWNb9Xi/sBbx7DejfgOL97J95eRwc0a07+A4T+2SQ/8Sn1vJuGrlHuWhgnaV6sjIY2Dt5TtHXgYXSyn3msaC5x+QArYuF/DmbEztzmYjEd8tIrVDsTWaRs5ziNwXuHTp3Dcddyrrg9I4TTPOcViaePe8bPfXha17/8AWdtufpKl15WW/FqsoonCwozYnXvn2XRGoREXzyChcxorT+obAsZTB43IzgcoltVI5Hge9u4E7KaRZU11UTembSalW9ivRnxTwn8Pi+6nsV6M+KeE/h8X3VaUW7pGNxzzlbzvVb2K9GfFPCfw+L7qexXoz4p4T+HxfdVpROkY3HPOS871W9ivRnxTwn8Pi+6nsV6M+KeE/h8X3VaUTpGNxzzkvO9D4fR+B07M6XF4XH42VzeUyVKrI3Ee9u0A7fIphEWmqqqub1TeUERFiCIiAiIgIiICIiAiIgIiICIiAiIg/9k="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const graphViz = graph.getGraph();\n",
    "const image = await graphViz.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it!\n",
    "\n",
    "Let's use our graph now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: My favorite pet is a terrier. I saw a cute one on Twitter.\n",
      "Tool call: update_favorite_pets({\"pets\":[\"terrier\"]})\n",
      "update_favorite_pets tool output: update_favorite_pets called.\n",
      "Assistant: I've added \"terrier\" to your list of favorite pets. If you have any more favorites, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "import {\n",
    "  BaseMessage,\n",
    "  isAIMessage,\n",
    "  isHumanMessage,\n",
    "  isToolMessage,\n",
    "  HumanMessage,\n",
    "  ToolMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "\n",
    "let inputs = {\n",
    "  messages: [ new HumanMessage({ content: \"My favorite pet is a terrier. I saw a cute one on Twitter.\" }) ],\n",
    "};\n",
    "\n",
    "let config = {\n",
    "  configurable: {\n",
    "    thread_id: \"1\",\n",
    "    userId: \"a-user\",\n",
    "  },\n",
    "};\n",
    "\n",
    "function printMessages(messages: BaseMessage[]) {\n",
    "  for (const message of messages) {\n",
    "    if (isHumanMessage(message)) {\n",
    "      console.log(`User: ${message.content}`);\n",
    "    } else if (isAIMessage(message)) {\n",
    "      const aiMessage = message as AIMessage;\n",
    "      if (aiMessage.content) {\n",
    "        console.log(`Assistant: ${aiMessage.content}`);\n",
    "      }\n",
    "      if (aiMessage.tool_calls) {\n",
    "        for (const toolCall of aiMessage.tool_calls) {\n",
    "          console.log(`Tool call: ${toolCall.name}(${JSON.stringify(toolCall.args)})`);\n",
    "        }\n",
    "      }\n",
    "    } else if (isToolMessage(message)) {\n",
    "      const toolMessage = message as ToolMessage;\n",
    "      console.log(`${toolMessage.name} tool output: ${toolMessage.content}`);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "let { messages } = await graph.invoke(inputs, config);\n",
    "\n",
    "printMessages(messages);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now verify it can properly fetch the stored preferences and cite where it got the information from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What're my favorite pets and what did I say when I told you about them?\n",
      "Tool call: get_favorite_pets({})\n",
      "get_favorite_pets tool output: {\"pets\":[\"terrier\"],\"context\":\"My favorite pet is a terrier. I saw a cute one on Twitter.\"}\n",
      "Assistant: Your favorite pet is a terrier. You mentioned, \"My favorite pet is a terrier. I saw a cute one on Twitter.\"\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [new HumanMessage({ content: \"What're my favorite pets and what did I say when I told you about them?\" })] };\n",
    "config = {\n",
    "  configurable: {\n",
    "    thread_id: \"2\", // New thread ID, so the conversation history isn't present.\n",
    "    userId: \"a-user\"\n",
    "  }\n",
    "};\n",
    "\n",
    "messages = (await graph.invoke(inputs, config)).messages;\n",
    "\n",
    "printMessages(messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the agent is able to properly cite that the information came from Twitter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closures\n",
    "\n",
    "If you cannot use context variables in your environment, you can use [closures](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Closures) to create tools with access to dynamic content. Here is a high-level example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generateTools(state: typeof MessagesAnnotation.State) {\n",
    "  const updateFavoritePets = tool(\n",
    "    async (input, config: LangGraphRunnableConfig) => {\n",
    "      // Some arguments are populated by the LLM; these are included in the schema below\n",
    "      const { pets } = input;\n",
    "      // Others (such as a UserID) are best provided via the config\n",
    "      // This is set when when invoking or streaming the graph\n",
    "      const userId = config.configurable?.userId;\n",
    "      // LangGraph's managed key-value store is also accessible via the config\n",
    "      const store = config.store;\n",
    "      await store.put([userId, \"pets\"], \"names\", pets )\n",
    "      await store.put([userId, \"pets\"], \"context\", {content: state.messages[0].content})\n",
    "\n",
    "      return \"update_favorite_pets called.\";\n",
    "    },\n",
    "    {\n",
    "      // The LLM \"sees\" the following schema:\n",
    "      name: \"update_favorite_pets\",\n",
    "      description: \"add to the list of favorite pets.\",\n",
    "      schema: z.object({\n",
    "        pets: z.array(z.string()),\n",
    "      }),\n",
    "    }\n",
    "  );\n",
    "  return [updateFavoritePets];\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when laying out your graph, you will need to call the above method whenever you bind or invoke tools. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "const toolNodeWithClosure = async (state: typeof MessagesAnnotation.State) => {\n",
    "  // We fetch the tools any time this node is reached to\n",
    "  // form a closure and let it access the latest messages\n",
    "  const tools = generateTools(state);\n",
    "  const toolNodeWithConfig = new ToolNode(tools);\n",
    "  return toolNodeWithConfig.invoke(state);\n",
    "};"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/persistence-functional.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to add thread-level persistence (functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Functional API](../../concepts/functional_api/)\n",
    "    - [Persistence](../../concepts/persistence/)\n",
    "    - [Memory](../../concepts/memory/)\n",
    "    - [Chat Models](https://js.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "Many AI applications need memory to share context across multiple interactions on the same [thread](../../concepts/persistence#threads) (e.g., multiple turns of a conversation). In LangGraph functional API, this kind of memory can be added to any [entrypoint()](/langgraphjs/reference/functions/langgraph.entrypoint-1.html) workflow using [thread-level persistence](/langgraphjs/concepts/persistence).\n",
    "\n",
    "When creating a LangGraph workflow, you can set it up to persist its results by using a [checkpointer](/langgraphjs/reference/classes/checkpoint.BaseCheckpointSaver.html):\n",
    "\n",
    "\n",
    "1. Create an instance of a checkpointer:\n",
    "\n",
    "    ```ts\n",
    "    import { MemorySaver } from \"@langchain/langgraph\";\n",
    "    \n",
    "    const checkpointer = new MemorySaver();\n",
    "    ```\n",
    "\n",
    "2. Pass `checkpointer` instance to the `entrypoint()` wrapper function:\n",
    "\n",
    "    ```ts\n",
    "    import { entrypoint } from \"@langchain/langgraph\";\n",
    "    const workflow = entrypoint({\n",
    "      name: \"workflow\",\n",
    "      checkpointer,\n",
    "    }, async (inputs) => {\n",
    "      ...\n",
    "    });\n",
    "    ```\n",
    "\n",
    "3. Retrieve `previous` state from the prior execution within the workflow:\n",
    "\n",
    "    ```ts\n",
    "    import { entrypoint, getPreviousState } from \"@langchain/langgraph\";\n",
    "    \n",
    "    const workflow = entrypoint({\n",
    "      name: \"workflow\",\n",
    "      checkpointer,\n",
    "    }, async (inputs) => {\n",
    "      const previous = getPreviousState();\n",
    "      const result = doSomething(previous, inputs);\n",
    "      ...\n",
    "    });\n",
    "    ```\n",
    "\n",
    "4. Optionally choose which values will be returned from the workflow and which will be saved by the checkpointer as `previous`:\n",
    "\n",
    "    ```ts\n",
    "    import { entrypoint, getPreviousState } from \"@langchain/langgraph\";\n",
    "    \n",
    "    const workflow = entrypoint({\n",
    "      name: \"workflow\",\n",
    "      checkpointer,\n",
    "    }, async (inputs) => {\n",
    "      const previous = getPreviousState();\n",
    "      const result = doSomething(previous, inputs);\n",
    "      ...\n",
    "      return entrypoint.final({\n",
    "        value: result,\n",
    "        save: combineState(inputs, result),\n",
    "      });\n",
    "    });\n",
    "    ```\n",
    "\n",
    "This guide shows how you can add thread-level persistence to your workflow.\n",
    "\n",
    "!!! tip \"Note\"\n",
    "\n",
    "    If you need memory that is __shared__ across multiple conversations or users (cross-thread persistence), check out this [how-to guide](../cross-thread-persistence-functional).\n",
    "\n",
    "!!! tip \"Note\"\n",
    "\n",
    "    If you need to add thread-level persistence to a `StateGraph`, check out this [how-to guide](../persistence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf509bc",
   "metadata": {},
   "source": [
    "## Example: simple chatbot with short-term memory\n",
    "\n",
    "We will be using a workflow with a single task that calls a [chat model](https://js.langchain.com/docs/concepts/chat_models/).\n",
    "\n",
    "Let's first define the model we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892b54b9-75f0-4804-9ed0-88b5e5532989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-latest\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a2792-982b-4e47-83eb-0c594725d1c1",
   "metadata": {},
   "source": [
    "Now we can define our task and workflow. To add in persistence, we need to pass in a [Checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) to the [entrypoint()](/langgraphjs/reference/functions/langgraph.entrypoint-1.html) wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87326ea6-34c5-46da-a41f-dda26ef9bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import type { BaseMessage, BaseMessageLike } from \"@langchain/core/messages\";\n",
    "import {\n",
    "  addMessages,\n",
    "  entrypoint,\n",
    "  task,\n",
    "  getPreviousState,\n",
    "  MemorySaver,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const callModel = task(\"callModel\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = model.invoke(messages);\n",
    "  return response;\n",
    "});\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const workflow = entrypoint({\n",
    "  name: \"workflow\",\n",
    "  checkpointer,\n",
    "}, async (inputs: BaseMessageLike[]) => {\n",
    "  const previous = getPreviousState<BaseMessage>() ?? [];\n",
    "  const messages = addMessages(previous, inputs);\n",
    "  const response = await callModel(messages);\n",
    "  return entrypoint.final({\n",
    "    value: response,\n",
    "    save: addMessages(messages, response),\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d8fd9-2e7a-4892-9adc-19762a1e3cce",
   "metadata": {},
   "source": [
    "If we try to use this workflow, the context of the conversation will be persisted across interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654ebcc-2179-41b4-92d1-6666f6f8634f",
   "metadata": {},
   "source": [
    "!!! note Note\n",
    "\n",
    "    If you're using LangGraph Cloud or LangGraph Studio, you __don't need__ to pass checkpointer to the `entrypoint` wrapper, since it's done automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
   "metadata": {},
   "source": [
    "Here's how this works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== ai message ==============================\n",
      "Hi Bob! I'm Claude. Nice to meet you! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "const config = {\n",
    "  configurable: { thread_id: \"1\" },\n",
    "  streamMode: \"values\" as const,\n",
    "};\n",
    "const inputMessage = { role: \"user\", content: \"hi! I'm bob\" };\n",
    "\n",
    "const stream = await workflow.stream(\n",
    "  [inputMessage],\n",
    "  config,\n",
    ");\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(\"=\".repeat(30), `${chunk.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(chunk.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07bf8-68b7-4049-a0f1-eb67a4879a3a",
   "metadata": {},
   "source": [
    "You can always resume previous threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ae8246-11d5-40e1-8567-361e5bef8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== ai message ==============================\n",
      "Your name is Bob - you just told me that in your first message.\n"
     ]
    }
   ],
   "source": [
    "const followupStream = await workflow.stream(\n",
    "  [{ role: \"user\", content: \"what's my name?\" }], \n",
    "  config,\n",
    ");\n",
    "\n",
    "for await (const chunk of followupStream) {\n",
    "  console.log(\"=\".repeat(30), `${chunk.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(chunk.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47bbfc-d9ef-4288-ba4a-ebbc0136fa9d",
   "metadata": {},
   "source": [
    "If we want to start a new conversation, we can pass in a different `thread_id`. Poof! All the memories are gone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273d56a8-f40f-4a51-a27f-7c6bb2bda0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== ai message ==============================\n",
      "I don't know your name as we just started chatting. Would you like to introduce yourself?\n"
     ]
    }
   ],
   "source": [
    "const newStream = await workflow.stream(\n",
    "  [{ role: \"user\", content: \"what's my name?\" }],\n",
    "  {\n",
    "    configurable: {\n",
    "      thread_id: \"2\",\n",
    "    },\n",
    "    streamMode: \"values\",\n",
    "  },\n",
    ");\n",
    "\n",
    "for await (const chunk of newStream) {\n",
    "  console.log(\"=\".repeat(30), `${chunk.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(chunk.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7926a8-4c88-4b16-973c-53d6da3f4a08",
   "metadata": {},
   "source": [
    "!!! tip \"Streaming tokens\"\n",
    "\n",
    "    If you would like to stream LLM tokens from your chatbot, you can use `streamMode: \"messages\"`. Check out this [how-to guide](../stream-tokens) to learn more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/persistence-postgres.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to use Postgres checkpointer for persistence\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/persistence/\">\n",
    "                    Persistence\n",
    "                </a>\n",
    "            </li>       \n",
    "            <li>\n",
    "                <a href=\"https://www.postgresql.org/about/\">\n",
    "                    Postgresql\n",
    "                </a>\n",
    "            </li>        \n",
    "        </ul>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "When creating LangGraph agents, you can set them up so that they persist their state across executions. This allows you to do things like interact with an agent multiple times and have it remember previous interactions.\n",
    "\n",
    "This how-to guide shows how to use Postgres as the backend for persisting checkpoint state using the [`@langchain/langgraph-checkpoint-postgres`](https://github.com/langchain-ai/langgraphjs/tree/main/libs/checkpoint-postgres) library and the [`PostgresSaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint_postgres.PostgresSaver.html) class.\n",
    "\n",
    "For demonstration purposes we will add persistence to the [pre-built create react agent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html). \n",
    "\n",
    "In general, you can add a checkpointer to any custom graph that you build like this:\n",
    "\n",
    "```ts\n",
    "import { StateGraph } from \"@langchain/langgraph\";\n",
    "import { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n",
    "\n",
    "const builder = new StateGraph(...);\n",
    "\n",
    "// ... define the graph\n",
    "\n",
    "const checkpointer = PostgresSaver.fromConnString(...); // postgres checkpointer (see examples below)\n",
    "\n",
    "const graph = builder.compile({ checkpointer });\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fa19c-93a5-4750-a410-f2d810b964ad",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need access to a Postgres instance. This guide will also use OpenAI, so you will need an OpenAI API key.\n",
    "\n",
    "First, install the required packages:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/core @langchain/langgraph-checkpoint-postgres\n",
    "```\n",
    "\n",
    "Then, set your OpenAI API key as `process.env.OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394e26c",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b3204-cca2-414c-800e-7e09032445ae",
   "metadata": {},
   "source": [
    "## Define model and tools for the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5213193-5a7d-43e7-aeba-fe732bb1cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "\n",
    "const getWeather = tool(async (input: { city: \"sf\" | \"nyc\" }) => {\n",
    "  if (input.city === \"nyc\") {\n",
    "    return \"It might be cloudy in nyc\";\n",
    "  } else if (input.city === \"sf\") {\n",
    "    return \"It's always sunny in sf\";\n",
    "  } else {\n",
    "    throw new Error(\"Unknown city\");\n",
    "  }\n",
    "}, {\n",
    "  name: \"get_weather\",\n",
    "  description: \"Use this to get weather information.\",\n",
    "  schema: z.object({\n",
    "    city: z.enum([\"sf\", \"nyc\"])\n",
    "  }),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9342c62-dbb4-40f6-9271-7393f1ca48c4",
   "metadata": {},
   "source": [
    "## With a connection pool\n",
    "\n",
    "Under the hood, `PostgresSaver` uses the [`node-postgres`](https://www.npmjs.com/package/pg) (`pg`) package to connect to your Postgres instance. You can pass in a [connection pool](https://node-postgres.com/apis/pool) that you've instantiated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9d13b1-9d72-48a0-b63a-adc062c06c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"id\": \"ac832b73-242d-4d0b-80d7-5d06a908787e\",\n",
      "      \"content\": \"what's the weather in sf\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"chatcmpl-AGC3tgRXInGLo0qzrD5u3gNqNOegf\",\n",
      "      \"content\": \"\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_I2Ceef2LoxjeaR9m8ZkY7U1R\",\n",
      "            \"type\": \"function\",\n",
      "            \"function\": \"[Object]\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 14,\n",
      "          \"promptTokens\": 57,\n",
      "          \"totalTokens\": 71\n",
      "        },\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "      },\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"name\": \"get_weather\",\n",
      "          \"args\": {\n",
      "            \"city\": \"sf\"\n",
      "          },\n",
      "          \"type\": \"tool_call\",\n",
      "          \"id\": \"call_I2Ceef2LoxjeaR9m8ZkY7U1R\"\n",
      "        }\n",
      "      ],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 57,\n",
      "        \"output_tokens\": 14,\n",
      "        \"total_tokens\": 71\n",
      "      }\n",
      "    },\n",
      "    ToolMessage {\n",
      "      \"id\": \"6533d271-6126-40af-b5d0-23a484853a97\",\n",
      "      \"content\": \"It's always sunny in sf\",\n",
      "      \"name\": \"get_weather\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {},\n",
      "      \"tool_call_id\": \"call_I2Ceef2LoxjeaR9m8ZkY7U1R\"\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"chatcmpl-AGC3ttvB69pQu0atw0lUzTpNePlPn\",\n",
      "      \"content\": \"The weather in San Francisco (SF) is always sunny!\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 13,\n",
      "          \"promptTokens\": 84,\n",
      "          \"totalTokens\": 97\n",
      "        },\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 84,\n",
      "        \"output_tokens\": 13,\n",
      "        \"total_tokens\": 97\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "import pg from \"pg\";\n",
    "\n",
    "const { Pool } = pg;\n",
    "\n",
    "const pool = new Pool({\n",
    "  connectionString: \"postgresql://user:password@localhost:5434/testdb\"\n",
    "});\n",
    "\n",
    "const checkpointer = new PostgresSaver(pool);\n",
    "\n",
    "// NOTE: you need to call .setup() the first time you're using your checkpointer\n",
    "\n",
    "await checkpointer.setup();\n",
    "\n",
    "const graph = createReactAgent({\n",
    "  tools: [getWeather],\n",
    "  llm: new ChatOpenAI({\n",
    "    model: \"gpt-4o-mini\",\n",
    "  }),\n",
    "  checkpointSaver: checkpointer,\n",
    "});\n",
    "const config = { configurable: { thread_id: \"1\" } };\n",
    "\n",
    "await graph.invoke({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what's the weather in sf\"\n",
    "  }],\n",
    "}, config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7a2f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  v: 1,\n",
      "  id: '1ef85bc6-bd28-67c1-8003-5cb7dab561b0',\n",
      "  ts: '2024-10-08T21:29:38.109Z',\n",
      "  pending_sends: [],\n",
      "  versions_seen: {\n",
      "    agent: { tools: 4, '__start__:agent': 2 },\n",
      "    tools: { 'branch:agent:shouldContinue:tools': 3 },\n",
      "    __input__: {},\n",
      "    __start__: { __start__: 1 }\n",
      "  },\n",
      "  channel_versions: {\n",
      "    agent: 5,\n",
      "    tools: 5,\n",
      "    messages: 5,\n",
      "    __start__: 2,\n",
      "    '__start__:agent': 3,\n",
      "    'branch:agent:shouldContinue:tools': 4\n",
      "  },\n",
      "  channel_values: {\n",
      "    agent: 'agent',\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"ac832b73-242d-4d0b-80d7-5d06a908787e\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AGC3tgRXInGLo0qzrD5u3gNqNOegf\",\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"tool_calls\": [\n",
      "            {\n",
      "              \"id\": \"call_I2Ceef2LoxjeaR9m8ZkY7U1R\",\n",
      "              \"type\": \"function\",\n",
      "              \"function\": \"[Object]\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 14,\n",
      "            \"promptTokens\": 57,\n",
      "            \"totalTokens\": 71\n",
      "          },\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "        },\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"get_weather\",\n",
      "            \"args\": {\n",
      "              \"city\": \"sf\"\n",
      "            },\n",
      "            \"type\": \"tool_call\",\n",
      "            \"id\": \"call_I2Ceef2LoxjeaR9m8ZkY7U1R\"\n",
      "          }\n",
      "        ],\n",
      "        \"invalid_tool_calls\": []\n",
      "      },\n",
      "      ToolMessage {\n",
      "        \"id\": \"6533d271-6126-40af-b5d0-23a484853a97\",\n",
      "        \"content\": \"It's always sunny in sf\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_call_id\": \"call_I2Ceef2LoxjeaR9m8ZkY7U1R\"\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AGC3ttvB69pQu0atw0lUzTpNePlPn\",\n",
      "        \"content\": \"The weather in San Francisco (SF) is always sunny!\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 13,\n",
      "            \"promptTokens\": 84,\n",
      "            \"totalTokens\": 97\n",
      "          },\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await checkpointer.get(config);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb52fd-af31-4603-889d-66d783244bce",
   "metadata": {},
   "source": [
    "### With a connection string\n",
    "\n",
    "You can also create a pool internally by passing a connection string to the `.fromConnString` static method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe54e79-9eaf-44e2-b2d9-1e0284b984d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"id\": \"c17b65af-6ac5-411e-ab5c-8003dc53755d\",\n",
      "      \"content\": \"what's the weather in sf\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"chatcmpl-AGC6n8XO05i1Z7f4GnOqpayLPxgoF\",\n",
      "      \"content\": \"\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_n9QCrJ4QbmgFkr5fHEsQHCCO\",\n",
      "            \"type\": \"function\",\n",
      "            \"function\": \"[Object]\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 14,\n",
      "          \"promptTokens\": 57,\n",
      "          \"totalTokens\": 71\n",
      "        },\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "      },\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"name\": \"get_weather\",\n",
      "          \"args\": {\n",
      "            \"city\": \"sf\"\n",
      "          },\n",
      "          \"type\": \"tool_call\",\n",
      "          \"id\": \"call_n9QCrJ4QbmgFkr5fHEsQHCCO\"\n",
      "        }\n",
      "      ],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 57,\n",
      "        \"output_tokens\": 14,\n",
      "        \"total_tokens\": 71\n",
      "      }\n",
      "    },\n",
      "    ToolMessage {\n",
      "      \"id\": \"779c26b0-6b75-454e-98ef-ecca79e50e8c\",\n",
      "      \"content\": \"It's always sunny in sf\",\n",
      "      \"name\": \"get_weather\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {},\n",
      "      \"tool_call_id\": \"call_n9QCrJ4QbmgFkr5fHEsQHCCO\"\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"chatcmpl-AGC6ngqEV0EBZbPwHf2JgTw0n16D8\",\n",
      "      \"content\": \"The weather in San Francisco (SF) is described as always sunny.\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 15,\n",
      "          \"promptTokens\": 84,\n",
      "          \"totalTokens\": 99\n",
      "        },\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"system_fingerprint\": \"fp_74ba47b4ac\"\n",
      "      },\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 84,\n",
      "        \"output_tokens\": 15,\n",
      "        \"total_tokens\": 99\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const checkpointerFromConnString = PostgresSaver.fromConnString(\n",
    "  \"postgresql://user:password@localhost:5434/testdb\"\n",
    ");\n",
    "\n",
    "const graph2 = createReactAgent({\n",
    "  tools: [getWeather],\n",
    "  llm: new ChatOpenAI({\n",
    "    model: \"gpt-4o-mini\",\n",
    "  }),\n",
    "  checkpointSaver: checkpointerFromConnString,\n",
    "});\n",
    "const config2 = { configurable: { thread_id: \"2\" } };\n",
    "\n",
    "await graph2.invoke({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what's the weather in sf\"\n",
    "  }],\n",
    "}, config2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ce743b-5896-443b-9ec0-a655b065895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  v: 1,\n",
      "  id: '1ef85bcd-71b9-6671-8003-6e734c8e9679',\n",
      "  ts: '2024-10-08T21:32:38.103Z',\n",
      "  pending_sends: [],\n",
      "  versions_seen: {\n",
      "    agent: { tools: 4, '__start__:agent': 2 },\n",
      "    tools: { 'branch:agent:shouldContinue:tools': 3 },\n",
      "    __input__: {},\n",
      "    __start__: { __start__: 1 }\n",
      "  },\n",
      "  channel_versions: {\n",
      "    agent: 5,\n",
      "    tools: 5,\n",
      "    messages: 5,\n",
      "    __start__: 2,\n",
      "    '__start__:agent': 3,\n",
      "    'branch:agent:shouldContinue:tools': 4\n",
      "  },\n",
      "  channel_values: {\n",
      "    agent: 'agent',\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"c17b65af-6ac5-411e-ab5c-8003dc53755d\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AGC6n8XO05i1Z7f4GnOqpayLPxgoF\",\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"tool_calls\": [\n",
      "            {\n",
      "              \"id\": \"call_n9QCrJ4QbmgFkr5fHEsQHCCO\",\n",
      "              \"type\": \"function\",\n",
      "              \"function\": \"[Object]\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 14,\n",
      "            \"promptTokens\": 57,\n",
      "            \"totalTokens\": 71\n",
      "          },\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "        },\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"get_weather\",\n",
      "            \"args\": {\n",
      "              \"city\": \"sf\"\n",
      "            },\n",
      "            \"type\": \"tool_call\",\n",
      "            \"id\": \"call_n9QCrJ4QbmgFkr5fHEsQHCCO\"\n",
      "          }\n",
      "        ],\n",
      "        \"invalid_tool_calls\": []\n",
      "      },\n",
      "      ToolMessage {\n",
      "        \"id\": \"779c26b0-6b75-454e-98ef-ecca79e50e8c\",\n",
      "        \"content\": \"It's always sunny in sf\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_call_id\": \"call_n9QCrJ4QbmgFkr5fHEsQHCCO\"\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AGC6ngqEV0EBZbPwHf2JgTw0n16D8\",\n",
      "        \"content\": \"The weather in San Francisco (SF) is described as always sunny.\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 15,\n",
      "            \"promptTokens\": 84,\n",
      "            \"totalTokens\": 99\n",
      "          },\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_74ba47b4ac\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await checkpointerFromConnString.get(config2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58ea01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/persistence.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aad4e28d",
      "metadata": {},
      "source": [
        "# Persistence\n",
        "\n",
        "Many AI applications need memory to share context across multiple interactions in a single conversational \"thread.\"\n",
        "In LangGraph, this type of conversation-level memory can be added to any graph using\n",
        "[Checkpointers](https://langchain-ai.github.io/langgraphjs/reference/interfaces/index.Checkpoint.html).\n",
        "\n",
        "Just compile the graph with a compatible checkpointer. Below is an example using a simple in-memory \"MemorySaver\":\n",
        "\n",
        "```javascript\n",
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "const checkpointer = new MemorySaver();\n",
        "const graph = workflow.compile({ checkpointer });\n",
        "```\n",
        "\n",
        "This guide shows how you can add thread-level persistence to your graph.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note: multi-conversation memory</p>\n",
        "    <p>\n",
        "        If you need memory that is <b>shared</b> across multiple conversations or users (cross-thread persistence), check out this <a href=\"https://langchain-ai.github.io/langgraphjs/how-tos/cross-thread-persistence/\">how-to guide</a>).\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the <code>createReactAgent(model, tools=tool, checkpointer=checkpointer)</code> (<a href=\"https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html\">API doc</a>) constructor. This may be more appropriate if you are used to LangChain's <a href=\"https://js.langchain.com/docs/how_to/agent_executor\">AgentExecutor</a> class.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "## Setup\n",
        "\n",
        "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
        "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
        "best-in-class observability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "10021b8c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Persistence: LangGraphJS\n"
          ]
        }
      ],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "\n",
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
        "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "process.env.LANGCHAIN_PROJECT = \"Persistence: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9e252c",
      "metadata": {},
      "source": [
        "## Define the state\n",
        "\n",
        "The state is the interface for all of the nodes in our graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9fc47087",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const GraphState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdba79f",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will\n",
        "use create a placeholder search engine. However, it is really easy to create\n",
        "your own tools - see documentation\n",
        "[here](https://js.langchain.com/docs/how_to/custom_tools) on how to do\n",
        "that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5f1e5deb",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = tool(async ({}: { query: string }) => {\n",
        "  // This is a placeholder for the actual implementation\n",
        "  return \"Cold, with a low of 13 ℃\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description:\n",
        "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "});\n",
        "\n",
        "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5615fd8",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a simple\n",
        "[ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html).\n",
        "This object will actually run the tools (functions) whenever they are invoked by\n",
        "our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1852d2a4",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a593cc20",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now we will load the\n",
        "[chat model](https://js.langchain.com/docs/concepts/#chat-models).\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form\n",
        "   of messages, so it needs to be able to work well with them.\n",
        "2. It should work with\n",
        "   [tool calling](https://js.langchain.com/docs/how_to/tool_calling/#passing-tools-to-llms),\n",
        "   meaning it can return function arguments in its response.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "77c9701b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({ model: \"gpt-4o\" });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4177b143",
      "metadata": {},
      "source": [
        "After we've done this, we should make sure the model knows that it has these\n",
        "tools available to call. We can do this by calling\n",
        "[bindTools](https://v01.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html#bindTools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b35d9bd2",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb0ae12",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together. We will run it first without a checkpointer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5f85457b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
        "\n",
        "const routeMessage = (state: typeof GraphState.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If no tools are called, we can finish (respond to the user)\n",
        "  if (!lastMessage.tool_calls?.length) {\n",
        "    return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue and call the tools\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "const callModel = async (\n",
        "  state: typeof GraphState.State,\n",
        "  config?: RunnableConfig,\n",
        ") => {\n",
        "  const { messages } = state;\n",
        "  const response = await boundModel.invoke(messages, config);\n",
        "  return { messages: [response] };\n",
        "};\n",
        "\n",
        "const workflow = new StateGraph(GraphState)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(START, \"agent\")\n",
        "  .addConditionalEdges(\"agent\", routeMessage)\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "const graph = workflow.compile();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "41364864",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi I'm Yu, nice to meet you.\n",
            "-----\n",
            "\n",
            "Hi Yu! Nice to meet you too. How can I assist you today?\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs = { messages: [{ role: \"user\", content: \"Hi I'm Yu, nice to meet you.\" }] };\n",
        "for await (\n",
        "  const { messages } of await graph.stream(inputs, {\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ccddfd4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remember my name?\n",
            "-----\n",
            "\n",
            "You haven't shared your name with me yet. What's your name?\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = { messages: [{ role: \"user\", content: \"Remember my name?\" }] };\n",
        "for await (\n",
        "  const { messages } of await graph.stream(inputs, {\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bece060",
      "metadata": {},
      "source": [
        "## Add Memory\n",
        "\n",
        "Let's try it again with a checkpointer. We will use the\n",
        "[MemorySaver](/langgraphjs/reference/classes/index.MemorySaver.html),\n",
        "which will \"save\" checkpoints in-memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "217ac741",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "// Here we only save in-memory\n",
        "const memory = new MemorySaver();\n",
        "const persistentGraph = workflow.compile({ checkpointer: memory });"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "173c17f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi I'm Jo, nice to meet you.\n",
            "-----\n",
            "\n",
            "Hello Jo, nice to meet you too! How can I assist you today?\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let config = { configurable: { thread_id: \"conversation-num-1\" } };\n",
        "inputs = { messages: [{ role: \"user\", content: \"Hi I'm Jo, nice to meet you.\" }] };\n",
        "for await (\n",
        "  const { messages } of await persistentGraph.stream(inputs, {\n",
        "    ...config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1162eb84",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remember my name?\n",
            "-----\n",
            "\n",
            "Yes, I'll remember that your name is Jo. How can I assist you today?\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = { messages: [{ role: \"user\", content: \"Remember my name?\"}] };\n",
        "for await (\n",
        "  const { messages } of await persistentGraph.stream(inputs, {\n",
        "    ...config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73902faf",
      "metadata": {},
      "source": [
        "## New Conversational Thread\n",
        "\n",
        "If we want to start a new conversation, we can pass in a different\n",
        "**`thread_id`**. Poof! All the memories are gone (just kidding, they'll always\n",
        "live in that other thread)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "58cc0612",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ configurable: { thread_id: 'conversation-2' } }\n"
          ]
        }
      ],
      "source": [
        "config = { configurable: { thread_id: \"conversation-2\" } };"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "25aea87b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you forgot?\n",
            "-----\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could you please provide more context or details about what you are referring to? This will help me assist you better.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = { messages: [{ role: \"user\", content: \"you forgot?\" }] };\n",
        "for await (\n",
        "  const { messages } of await persistentGraph.stream(inputs, {\n",
        "    ...config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/react-agent-from-scratch-functional.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a ReAct agent from scratch (Functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Chat Models](https://js.langchain.com/docs/concepts/chat_models)\n",
    "    - [Messages](https://js.langchain.com/docs/concepts/messages)\n",
    "    - [Tool Calling](https://js.langchain.com/docs/concepts/tool_calling/)\n",
    "    - [Entrypoints](../../concepts/functional_api/#entrypoint) and [Tasks](../../concepts/functional_api/#task)\n",
    "\n",
    "This guide demonstrates how to implement a ReAct agent using the LangGraph [Functional API](../../concepts/functional_api).\n",
    "\n",
    "The ReAct agent is a [tool-calling agent](../../concepts/agentic_concepts/#tool-calling-agent) that operates as follows:\n",
    "\n",
    "1. Queries are issued to a chat model;\n",
    "2. If the model generates no [tool calls](../../concepts/agentic_concepts/#tool-calling), we return the model response.\n",
    "3. If the model generates tool calls, we execute the tool calls with available tools, append them as [tool messages](https://js.langchain.com/docs/concepts/messages/) to our message list, and repeat the process.\n",
    "\n",
    "This is a simple and versatile set-up that can be extended with memory, human-in-the-loop capabilities, and other features. See the dedicated [how-to guides](../../how-tos/#prebuilt-react-agent) for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ReAct agent\n",
    "\n",
    "Now that you have installed the required packages and set your environment variables, we can create our agent.\n",
    "\n",
    "### Define model and tools\n",
    "\n",
    "Let's first define the tools and model we will use for our example. Here we will use a single place-holder tool that gets a description of the weather for a location.\n",
    "\n",
    "We will use an [OpenAI](https://js.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://js.langchain.com/docs/integrations/chat/) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o-mini\",\n",
    "});\n",
    "\n",
    "const getWeather = tool(async ({ location }) => {\n",
    "  const lowercaseLocation = location.toLowerCase();\n",
    "  if (lowercaseLocation.includes(\"sf\") || lowercaseLocation.includes(\"san francisco\")) {\n",
    "    return \"It's sunny!\";\n",
    "  } else if (lowercaseLocation.includes(\"boston\")) {\n",
    "    return \"It's rainy!\";\n",
    "  } else {\n",
    "    return `I am not sure what the weather is in ${location}`;\n",
    "  }\n",
    "}, {\n",
    "  name: \"getWeather\",\n",
    "  schema: z.object({\n",
    "    location: z.string().describe(\"location to get the weather for\"),\n",
    "  }),\n",
    "  description: \"Call to get the weather from a specific location.\"\n",
    "});\n",
    "\n",
    "const tools = [getWeather];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "\n",
    "We next define the [tasks](../../concepts/functional_api/#task) we will execute. Here there are two different tasks:\n",
    "\n",
    "1. **Call model**: We want to query our chat model with a list of messages.\n",
    "2. **Call tool**: If our model generates tool calls, we want to execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  type BaseMessageLike,\n",
    "  AIMessage,\n",
    "  ToolMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "import { type ToolCall } from \"@langchain/core/messages/tool\";\n",
    "import { task } from \"@langchain/langgraph\";\n",
    "\n",
    "const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\n",
    "\n",
    "const callModel = task(\"callModel\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await model.bindTools(tools).invoke(messages);\n",
    "  return response;\n",
    "});\n",
    "\n",
    "const callTool = task(\n",
    "  \"callTool\",\n",
    "  async (toolCall: ToolCall): Promise<AIMessage> => {\n",
    "    const tool = toolsByName[toolCall.name];\n",
    "    const observation = await tool.invoke(toolCall.args);\n",
    "    return new ToolMessage({ content: observation, tool_call_id: toolCall.id });\n",
    "    // Can also pass toolCall directly into the tool to return a ToolMessage\n",
    "    // return tool.invoke(toolCall);\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define entrypoint\n",
    "\n",
    "Our [entrypoint](../../concepts/functional_api/#entrypoint) will handle the orchestration of these two tasks. As described above, when our `callModel` task generates tool calls, the `callTool` task will generate responses for each. We append all messages to a single messages list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { entrypoint, addMessages } from \"@langchain/langgraph\";\n",
    "\n",
    "const agent = entrypoint(\n",
    "  \"agent\",\n",
    "  async (messages: BaseMessageLike[]) => {\n",
    "    let currentMessages = messages;\n",
    "    let llmResponse = await callModel(currentMessages);\n",
    "    while (true) {\n",
    "      if (!llmResponse.tool_calls?.length) {\n",
    "        break;\n",
    "      }\n",
    "\n",
    "      // Execute tools\n",
    "      const toolResults = await Promise.all(\n",
    "        llmResponse.tool_calls.map((toolCall) => {\n",
    "          return callTool(toolCall);\n",
    "        })\n",
    "      );\n",
    "      \n",
    "      // Append to message list\n",
    "      currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]);\n",
    "\n",
    "      // Call model again\n",
    "      llmResponse = await callModel(currentMessages);\n",
    "    }\n",
    "\n",
    "    return llmResponse;\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "To use our agent, we invoke it with a messages list. Based on our implementation, these can be LangChain [message](https://js.langchain.com/docs/concepts/messages/) objects or OpenAI-style objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ role: 'user', content: \"What's the weather in san francisco?\" }\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_m5jZoH1HUtH6wA2QvexOHutj\"\n",
      "  }\n",
      "]\n",
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "The weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const prettyPrintMessage = (message: BaseMessage) => {\n",
    "  console.log(\"=\".repeat(30), `${message.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(message.content);\n",
    "  if (isAIMessage(message) && message.tool_calls?.length) {\n",
    "    console.log(JSON.stringify(message.tool_calls, null, 2));\n",
    "  }\n",
    "}\n",
    "\n",
    "// Usage example\n",
    "const userMessage = { role: \"user\", content: \"What's the weather in san francisco?\" };\n",
    "console.log(userMessage);\n",
    "\n",
    "const stream = await agent.stream([userMessage]);\n",
    "\n",
    "for await (const step of stream) {\n",
    "  for (const [taskName, update] of Object.entries(step)) {\n",
    "    const message = update as BaseMessage;\n",
    "    // Only print task updates\n",
    "    if (taskName === \"agent\") continue;\n",
    "    console.log(`\\n${taskName}:`);\n",
    "    prettyPrintMessage(message);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! The graph correctly calls the `getWeather` tool and responds to the user after receiving the information from the tool. Check out the LangSmith trace [here](https://smith.langchain.com/public/8132d3b8-2c91-40fc-b660-b766ca33e9cb/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add thread-level persistence\n",
    "\n",
    "Adding [thread-level persistence](../../concepts/persistence#threads) lets us support conversational experiences with our agent: subsequent invocations will append to the prior messages list, retaining the full conversational context.\n",
    "\n",
    "To add thread-level persistence to our agent:\n",
    "\n",
    "1. Select a [checkpointer](../../concepts/persistence#checkpointer-libraries): here we will use [MemorySaver](/langgraphjs/reference/classes/checkpoint.MemorySaver.html), a simple in-memory checkpointer.\n",
    "2. Update our entrypoint to accept the previous messages state as a second argument. Here, we simply append the message updates to the previous sequence of messages.\n",
    "3. Choose which values will be returned from the workflow and which will be saved by the checkpointer. We will be able to access it as `getPreviousState()` if we return it from `entrypoint.final` (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  MemorySaver,\n",
    "  getPreviousState,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "// highlight-next-line\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const agentWithMemory = entrypoint({\n",
    "  name: \"agentWithMemory\",\n",
    "  // highlight-next-line\n",
    "  checkpointer,\n",
    "}, async (messages: BaseMessageLike[]) => {\n",
    "  const previous = getPreviousState<BaseMessage>() ?? [];\n",
    "  let currentMessages = addMessages(previous, messages);\n",
    "  let llmResponse = await callModel(currentMessages);\n",
    "  while (true) {\n",
    "    if (!llmResponse.tool_calls?.length) {\n",
    "      break;\n",
    "    }\n",
    "\n",
    "    // Execute tools\n",
    "    const toolResults = await Promise.all(\n",
    "      llmResponse.tool_calls.map((toolCall) => {\n",
    "        return callTool(toolCall);\n",
    "      })\n",
    "    );\n",
    "    \n",
    "    // Append to message list\n",
    "    currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]);\n",
    "\n",
    "    // Call model again\n",
    "    llmResponse = await callModel(currentMessages);\n",
    "  }\n",
    "  \n",
    "  // Append final response for storage\n",
    "  currentMessages = addMessages(currentMessages, llmResponse);\n",
    "\n",
    "  // highlight-next-line\n",
    "  return entrypoint.final({\n",
    "    // highlight-next-line\n",
    "    value: llmResponse,\n",
    "    // highlight-next-line\n",
    "    save: currentMessages,\n",
    "    // highlight-next-line\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need to pass in a config when running our application. The config will specify an identifier for the conversational thread.\n",
    "\n",
    "!!! tip\n",
    "\n",
    "    Read more about thread-level persistence in our [concepts page](../../concepts/persistence/) and [how-to guides](../../how-tos/#persistence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const config = { configurable: { thread_id: \"1\" } };"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start a thread the same way as before, this time passing in the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"san francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_4vaZqAxUabthejqKPRMq0ngY\"\n",
      "  }\n",
      "]\n",
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "The weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "const streamWithMemory = await agentWithMemory.stream([{\n",
    "  role: \"user\",\n",
    "  content: \"What's the weather in san francisco?\",\n",
    "}], config);\n",
    "\n",
    "for await (const step of streamWithMemory) {\n",
    "  for (const [taskName, update] of Object.entries(step)) {\n",
    "    const message = update as BaseMessage;\n",
    "    // Only print task updates\n",
    "    if (taskName === \"agentWithMemory\") continue;\n",
    "    console.log(`\\n${taskName}:`);\n",
    "    prettyPrintMessage(message);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask a follow-up conversation, the model uses the prior context to infer that we are asking about the weather:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"boston, ma\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_YDrNfZr5XnuBBq5jlIXaxC5v\"\n",
      "  }\n",
      "]\n",
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's rainy!\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "In comparison, while San Francisco is sunny, Boston, MA is experiencing rain.\n"
     ]
    }
   ],
   "source": [
    "const followupStreamWithMemory = await agentWithMemory.stream([{\n",
    "  role: \"user\",\n",
    "  content: \"How does it compare to Boston, MA?\",\n",
    "}], config);\n",
    "\n",
    "for await (const step of followupStreamWithMemory) {\n",
    "  for (const [taskName, update] of Object.entries(step)) {\n",
    "    const message = update as BaseMessage;\n",
    "    // Only print task updates\n",
    "    if (taskName === \"agentWithMemory\") continue;\n",
    "    console.log(`\\n${taskName}:`);\n",
    "    prettyPrintMessage(message);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [LangSmith trace](https://smith.langchain.com/public/ec803712-ecfc-49b6-8f54-92252d1e5e33/r), we can see that the full conversational context is retained in each model call."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/react-human-in-the-loop.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add human-in-the-loop processes to the prebuilt ReAct agent\n",
    "\n",
    "This tutorial will show how to add human-in-the-loop processes to the prebuilt ReAct agent. Please see [this tutorial](./create-react-agent.ipynb) for how to get started with the prebuilt ReAct agent\n",
    "\n",
    "You can add a a breakpoint before tools are called by passing `interruptBefore: [\"tools\"]` to `createReactAgent`. Note that you need to be using a checkpointer for this to work.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the required packages.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
    "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct Agent with human-in-the-loop: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\"\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"ReAct Agent with human-in-the-loop: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Now we can use the prebuilt `createReactAgent` function to setup our agent with human-in-the-loop interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from '@langchain/core/tools';\n",
    "import { z } from 'zod';\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "  });\n",
    "\n",
    "const getWeather = tool((input) => {\n",
    "    if (['sf', 'san francisco'].includes(input.location.toLowerCase())) {\n",
    "        return 'It\\'s always sunny in sf';\n",
    "    } else if (['nyc', 'new york city'].includes(input.location.toLowerCase())) {\n",
    "        return 'It might be cloudy in nyc';\n",
    "    }\n",
    "    else {\n",
    "        throw new Error(\"Unknown Location\");\n",
    "    }\n",
    "}, {\n",
    "    name: 'get_weather',\n",
    "    description: 'Call to get the current weather in a given location.',\n",
    "    schema: z.object({\n",
    "        location: z.string().describe(\"Location to get the weather for.\"),\n",
    "    })\n",
    "})\n",
    "\n",
    "// Here we only save in-memory\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "const agent = createReactAgent({ llm: model, tools: [getWeather], interruptBefore: [\"tools\"], checkpointSaver: memory });\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the weather in SF california?\n",
      "-----\n",
      "\n",
      "[\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { location: 'SF, California' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_AWgaSjqaYVQN73kL0H4BNn1Q'\n",
      "  }\n",
      "]\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let inputs = { messages: [{ role: \"user\", content: \"what is the weather in SF california?\" }] };\n",
    "let config = { configurable: { thread_id: \"1\" } };\n",
    "\n",
    "let stream = await agent.stream(inputs, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "  const { messages } of stream\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  }\n",
    "  if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our graph stopped at the right place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'tools' ]\n"
     ]
    }
   ],
   "source": [
    "const state = await agent.getState(config)\n",
    "console.log(state.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can either approve or edit the tool call before proceeding to the next node. If we wanted to approve the tool call, we would simply continue streaming the graph with `null` input. If we wanted to edit the tool call we need to update the state to have the correct tool call, and then after the update has been applied we can continue.\n",
    "\n",
    "We can try resuming and we will see an error arise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unknown Location\n",
      " Please fix your mistakes.\n",
      "-----\n",
      "\n",
      "[\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { location: 'San Francisco, California' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_MfIPKpRDXRL4LcHm1BxwcSTk'\n",
      "  }\n",
      "]\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = await agent.stream(null, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "    const { messages } of stream\n",
    "  ) {\n",
    "    let msg = messages[messages?.length - 1];\n",
    "    if (msg?.content) {\n",
    "      console.log(msg.content);\n",
    "    }\n",
    "    if (msg?.tool_calls?.length > 0) {\n",
    "      console.log(msg.tool_calls);\n",
    "    }\n",
    "    console.log(\"-----\\n\");\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error arose because our tool argument of \"SF, California\" is not a location our tool recognizes.\n",
    "\n",
    "Let's show how we would edit the tool call to search for \"San Francisco\" instead of \"SF, California\" - since our tool as written treats \"San Francisco, CA\" as an unknown location. We will update the state and then resume streaming the graph and should see no errors arise. Note that the reducer we use for our `messages` channel will replace a messaege only if a message with the exact same ID is used. For that reason we can do `new AiMessage(...)` and instead have to directly modify the last message from the `messages` channel, making sure not to edit its ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  configurable: {\n",
      "    thread_id: '1',\n",
      "    checkpoint_ns: '',\n",
      "    checkpoint_id: '1ef6638d-bfbd-61d0-8004-2751c8c3f226'\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// First, lets get the current state\n",
    "const currentState = await agent.getState(config);\n",
    "\n",
    "// Let's now get the last message in the state\n",
    "// This is the one with the tool calls that we want to update\n",
    "let lastMessage = currentState.values.messages[currentState.values.messages.length - 1]\n",
    "\n",
    "// Let's now update the args for that tool call\n",
    "lastMessage.tool_calls[0].args = { location: \"San Francisco\" }\n",
    "\n",
    "// Let's now call `updateState` to pass in this message in the `messages` key\n",
    "// This will get treated as any other update to the state\n",
    "// It will get passed to the reducer function for the `messages` key\n",
    "// That reducer function will use the ID of the message to update it\n",
    "// It's important that it has the right ID! Otherwise it would get appended\n",
    "// as a new message\n",
    "await agent.updateState(config, { messages: lastMessage });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's always sunny in sf\n",
      "-----\n",
      "\n",
      "The climate in San Francisco is sunny right now. If you need more specific details, feel free to ask!\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = await agent.stream(null, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "  const { messages } of stream\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  }\n",
    "  if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Our graph updated properly to query the weather in San Francisco and got the correct \"The weather in San Francisco is sunny today!\n",
    "\" response from the tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/react-memory.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add memory to the prebuilt ReAct agent\n",
    "\n",
    "This tutorial will show how to add memory to the prebuilt ReAct agent. Please see [this tutorial](./create-react-agent.ipynb) for how to get started with the prebuilt ReAct agent\n",
    "\n",
    "All we need to do to enable memory is pass in a checkpointer to `createReactAgent`\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the required packages.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
    "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct Agent with memory: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\"\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"ReAct Agent with memory: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Now we can use the prebuilt `createReactAgent` function to setup our agent with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from '@langchain/core/tools';\n",
    "import { z } from 'zod';\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "  });\n",
    "\n",
    "const getWeather = tool((input) => {\n",
    "    if (input.location === 'sf') {\n",
    "        return 'It\\'s always sunny in sf';\n",
    "    } else {\n",
    "        return 'It might be cloudy in nyc';\n",
    "    }\n",
    "}, {\n",
    "    name: 'get_weather',\n",
    "    description: 'Call to get the current weather.',\n",
    "    schema: z.object({\n",
    "        location: z.enum(['sf','nyc']).describe(\"Location to get the weather for.\"),\n",
    "    })\n",
    "})\n",
    "\n",
    "// Here we only save in-memory\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "const agent = createReactAgent({ llm: model, tools: [getWeather], checkpointSaver: memory });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Let's interact with it multiple times to show that it can remember prior information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the weather in NYC?\n",
      "-----\n",
      "\n",
      "[\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { location: 'nyc' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_m0zEI6sidPPH81G6ygMsKYs1'\n",
      "  }\n",
      "]\n",
      "-----\n",
      "\n",
      "It might be cloudy in nyc\n",
      "-----\n",
      "\n",
      "The weather in NYC appears to be cloudy.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let inputs = { messages: [{ role: \"user\", content: \"what is the weather in NYC?\" }] };\n",
    "let config = { configurable: { thread_id: \"1\" } };\n",
    "let stream = await agent.stream(inputs, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "  const { messages } of stream\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we pass the same the same thread ID, the chat history is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's it known for?\n",
      "-----\n",
      "\n",
      "New York City (NYC) is known for many things, including:\n",
      "\n",
      "1. **Landmarks and Attractions:**\n",
      "   - **Statue of Liberty**: An iconic symbol of freedom.\n",
      "   - **Empire State Building**: A famous skyscraper offering panoramic views.\n",
      "   - **Times Square**: Known for its neon lights and bustling atmosphere.\n",
      "   - **Central Park**: A large, urban park offering a natural oasis.\n",
      "\n",
      "2. **Cultural Institutions:**\n",
      "   - **Broadway**: Famous for its theatre productions.\n",
      "   - **Metropolitan Museum of Art (The Met)**: One of the largest and most prestigious art museums.\n",
      "   - **Museum of Modern Art (MoMA) and American Museum of Natural History**: Other significant museums.\n",
      "\n",
      "3. **Economy and Business:**\n",
      "   - **Wall Street**: The financial hub of the world, home to the New York Stock Exchange.\n",
      "   - **Headquarters of major corporations**: NYC hosts the headquarters of many large multinational companies.\n",
      "\n",
      "4. **Diversity and Neighborhoods:**\n",
      "   - **Cultural Melting Pot**: NYC is known for its diverse population with a wide range of ethnicities and cultures.\n",
      "   - **Distinct Neighborhoods**: Each borough and neighborhood (like Brooklyn, The Bronx, Queens, Staten Island, and Manhattan) has its unique character.\n",
      "\n",
      "5. **Food and Cuisine:**\n",
      "   - **Culinary Capital**: Known for diverse food options from street food like hot dogs and pretzels to high-end dining.\n",
      "   - **Cultural Cuisine**: Offers a variety of world cuisines due to its diverse population.\n",
      "\n",
      "6. **Media and Entertainment:**\n",
      "   - **Media Headquarters**: Home to major media companies and news networks.\n",
      "   - **Film and Television**: A popular setting and production location for films and TV shows.\n",
      "   \n",
      "7. **Events and Festivities:**\n",
      "   - **Macy's Thanksgiving Day Parade**: A famous annual parade.\n",
      "   - **New Year's Eve in Times Square**: Known for the ball drop and celebrations.\n",
      "\n",
      "NYC is a dynamic and vibrant city with a rich history and an influence that extends globally in various sectors.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [{ role: \"user\", content: \"What's it known for?\" }] };\n",
    "stream = await agent.stream(inputs, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "    const { messages } of stream\n",
    "  ) {\n",
    "    let msg = messages[messages?.length - 1];\n",
    "    if (msg?.content) {\n",
    "      console.log(msg.content);\n",
    "    } else if (msg?.tool_calls?.length > 0) {\n",
    "      console.log(msg.tool_calls);\n",
    "    } else {\n",
    "      console.log(msg);\n",
    "    }\n",
    "    console.log(\"-----\\n\");\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass it a new thread ID, all the history is lost and their is no memory to speak of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how close is it to boston?\n",
      "-----\n",
      "\n",
      "[\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { location: 'nyc' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_CKgDJqHiadzNLGhB8T8pHQWM'\n",
      "  }\n",
      "]\n",
      "-----\n",
      "\n",
      "It might be cloudy in nyc\n",
      "-----\n",
      "\n",
      "To determine how close \"it\" is to Boston, could you please specify which location you're referring to? For instance, are you asking about the distance from New York City, San Francisco, or another location? This detail will help me provide an accurate answer.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [{ role: \"user\", content: \"how close is it to boston?\" }] };\n",
    "config = { configurable: { thread_id: \"2\" } };\n",
    "stream = await agent.stream(inputs, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "    const { messages } of stream\n",
    "  ) {\n",
    "    let msg = messages[messages?.length - 1];\n",
    "    if (msg?.content) {\n",
    "      console.log(msg.content);\n",
    "    } else if (msg?.tool_calls?.length > 0) {\n",
    "      console.log(msg.tool_calls);\n",
    "    } else {\n",
    "      console.log(msg);\n",
    "    }\n",
    "    console.log(\"-----\\n\");\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/react-return-structured-output.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to return structured output from the prebuilt ReAct agent\n",
    "\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    - [Agent Architectures](../../concepts/agentic_concepts/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "    - [Tools](https://python.langchain.com/docs/concepts/tools/)\n",
    "    - [Structured Output](https://python.langchain.com/docs/concepts/structured_outputs/)\n",
    "\n",
    "\n",
    "To return structured output from the prebuilt ReAct agent you can provide a response_format parameter with the desired output schema to [`createReactAgent`](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const responseFormat = z.object({\n",
    "    // Respond to the user in this format\n",
    "    mySpecialOutput: z.string(),\n",
    "})\n",
    "\n",
    "const graph = createReactAgent({\n",
    "    llm: llm,\n",
    "    tools: tools,\n",
    "    // specify the schema for the structured output using `responseFormat` parameter\n",
    "    responseFormat: responseFormat\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent will return the output in the format specified by the `responseFormat` schema by making an additional LLM call at the end of the conversation, once there are no more tool calls to be made. You can read [this guide](./respond-in-format) to learn about an alternate way - treating the structured output as another tool = to achieve structured output from the agent.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the required packages.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core zod\n",
    "```\n",
    "\n",
    "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
    "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGSMITH_API_KEY = \"ls__...\"\n",
    "process.env.LANGSMITH_TRACING = \"true\";\n",
    "process.env.LANGSMITH_PROJECT = \"ReAct Agent with system prompt: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const weatherTool = tool(\n",
    "  async (input): Promise<string> => {\n",
    "    if (input.city === \"nyc\") {\n",
    "      return \"It might be cloudy in nyc\";\n",
    "    } else if (input.city === \"sf\") {\n",
    "      return \"It's always sunny in sf\";\n",
    "    } else {\n",
    "      throw new Error(\"Unknown city\");\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    name: \"get_weather\",\n",
    "    description: \"Use this to get weather information.\",\n",
    "    schema: z.object({\n",
    "      city: z.enum([\"nyc\", \"sf\"]).describe(\"The city to get weather for\"),\n",
    "    }),\n",
    "  }\n",
    ");\n",
    "\n",
    "const WeatherResponseSchema = z.object({\n",
    "  conditions: z.string().describe(\"Weather conditions\"),\n",
    "});\n",
    "\n",
    "const tools = [weatherTool];\n",
    "\n",
    "const agent = createReactAgent({\n",
    "  llm: new ChatOpenAI({ model: \"gpt-4o\", temperature: 0 }),\n",
    "  tools: tools,\n",
    "  responseFormat: WeatherResponseSchema,\n",
    "}); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Let's now test our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const response = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"What's the weather in NYC?\",\n",
    "    },\n",
    "  ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the agent output contains a `structuredResponse` key with the structured output conforming to the specified `WeatherResponse` schema, in addition to the message history under `messages` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ conditions: 'cloudy' }\n"
     ]
    }
   ],
   "source": [
    "response.structuredResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing system prompt\n",
    "\n",
    "You might need to further customize the second LLM call for the structured output generation and provide a system prompt. To do so, you can pass an object with the keys `prompt`, `schema` to the `responseFormat` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const agent = createReactAgent({\n",
    "  llm: new ChatOpenAI({ model: \"gpt-4o\", temperature: 0 }),\n",
    "  tools: tools,\n",
    "  responseFormat: {\n",
    "    prompt: \"Always return capitalized weather conditions\",\n",
    "    schema: WeatherResponseSchema,\n",
    "  }\n",
    "}); \n",
    "\n",
    "const response = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"What's the weather in NYC?\",\n",
    "    },\n",
    "  ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the structured response now contains a capitalized value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ conditions: 'Cloudy' }\n"
     ]
    }
   ],
   "source": [
    "response.structuredResponse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/react-system-prompt.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add a custom system prompt to the prebuilt ReAct agent\n",
    "\n",
    "This tutorial will show how to add a custom system prompt to the prebuilt ReAct agent. Please see [this tutorial](./create-react-agent.ipynb) for how to get started with the prebuilt ReAct agent\n",
    "\n",
    "You can add a custom system prompt by passing a string to the `stateModifier` param.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Compatibility</p>\n",
    "    <p>\n",
    "        The <a href=\"https://langchain-ai.github.io/langgraphjs/reference/types/langgraph_prebuilt.CreateReactAgentParams.html\"><code>stateModifier</code></a> parameter was added in <code>@langchain/langgraph>=0.2.27</code>.\n",
    "        <br />\n",
    "        If you are on an older version, you will need to use the deprecated <code>messageModifier</code> parameter.\n",
    "        <br />\n",
    "        For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the required packages.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
    "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct Agent with system prompt: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\"\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"ReAct Agent with system prompt: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Now we can use the prebuilt `createReactAgent` function to setup our agent with a system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from '@langchain/core/tools';\n",
    "import { z } from 'zod';\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "  });\n",
    "\n",
    "const getWeather = tool((input) => {\n",
    "    if (input.location === 'sf') {\n",
    "        return 'It\\'s always sunny in sf';\n",
    "    } else {\n",
    "        return 'It might be cloudy in nyc';\n",
    "    }\n",
    "}, {\n",
    "    name: 'get_weather',\n",
    "    description: 'Call to get the current weather.',\n",
    "    schema: z.object({\n",
    "        location: z.enum(['sf','nyc']).describe(\"Location to get the weather for.\"),\n",
    "    })\n",
    "})\n",
    "\n",
    "// We can add our system prompt here\n",
    "const prompt = \"Respond in Italian\"\n",
    "\n",
    "const agent = createReactAgent({ llm: model, tools: [getWeather], stateModifier: prompt });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Let's verify that the agent does indeed respond in Italian!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the weather in NYC?\n",
      "-----\n",
      "\n",
      "[\n",
      "  {\n",
      "    name: 'get_weather',\n",
      "    args: { location: 'nyc' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_PqmKDQrefHQLmGsZSSr4C7Fc'\n",
      "  }\n",
      "]\n",
      "-----\n",
      "\n",
      "It might be cloudy in nyc\n",
      "-----\n",
      "\n",
      "A New York potrebbe essere nuvoloso. Hai altre domande o posso aiutarti in qualcos'altro?\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let inputs = { messages: [{ role: \"user\", content: \"what is the weather in NYC?\" }] };\n",
    "let stream = await agent.stream(inputs, {\n",
    "  streamMode: \"values\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "  const { messages } of stream\n",
    ") {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/recursion-limit.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create and control loops\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"/langgraphjs/concepts/low_level/#graphs\">\n",
    "                    Graphs\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"/langgraphjs/concepts/low_level/#recursion-limit\">\n",
    "                    Recursion Limit\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"/langgraphjs/concepts/low_level/#nodes\">\n",
    "                    Nodes\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "\n",
    "When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/langgraphjs/concepts/low_level/#conditional-edges) that routes to the END node once we reach some termination condition.\n",
    "\n",
    "You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/langgraphjs/concepts/low_level/#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/langgraphjs/concepts/low_level/#recursion-limit). \n",
    "\n",
    "Let's consider a simple graph with a loop to better understand how these mechanisms work.\n",
    "\n",
    "## Summary\n",
    "\n",
    "When creating a loop, you can include a conditional edge that specifies a termination condition:\n",
    "\n",
    "```ts\n",
    "const route = async function (state: typeof StateAnnotation.State) {\n",
    "  if (terminationCondition(state)) {\n",
    "    return \"__END__\";\n",
    "  } else {\n",
    "    return \"a\";\n",
    "  }\n",
    "}\n",
    "\n",
    "const graph = StateGraph(State)\n",
    "  .addNode(a)\n",
    "  .addNode(b)\n",
    "  .addConditionalEdges(\"a\", route)\n",
    "  .addEdge(\"b\", \"a\")\n",
    "  .compile();\n",
    "```\n",
    "\n",
    "To control the recursion limit, specify `\"recursionLimit\"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:\n",
    "\n",
    "```ts\n",
    "import { GraphRecursionError } from \"@langchain/langgraph\";\n",
    "\n",
    "try {\n",
    "  await graph.invoke(inputs, { recursionLimit: 3 });\n",
    "} catch (error) {\n",
    "  if (error instanceof GraphRecursionError) {\n",
    "    console.log(\"Recursion Error\");\n",
    "  } else {\n",
    "    throw error;\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the state with a reducer\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  aggregate: Annotation<string[]>({\n",
    "    reducer: (a, b) => a.concat(b),\n",
    "    default: () => [],\n",
    "  }),\n",
    "});\n",
    "\n",
    "// Define nodes\n",
    "const a = async function (state: typeof StateAnnotation.State) {\n",
    "  console.log(`Node A sees ${state.aggregate}`);\n",
    "  return { aggregate: [\"A\"] };\n",
    "}\n",
    "\n",
    "const b = async function (state: typeof StateAnnotation.State) {\n",
    "  console.log(`Node B sees ${state.aggregate}`);\n",
    "  return { aggregate: [\"B\"] };\n",
    "}\n",
    "\n",
    "// Define edges\n",
    "const route = async function (state: typeof StateAnnotation.State) {\n",
    "  if (state.aggregate.length < 7) {\n",
    "    return \"b\";\n",
    "  } else {\n",
    "    return \"__end__\";\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "// Define the graph\n",
    "const graph = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"a\", a)\n",
    "  .addNode(\"b\", b)\n",
    "  .addEdge(\"__start__\", \"a\")\n",
    "  .addConditionalEdges(\"a\", route)\n",
    "  .addEdge(\"b\", \"a\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = graph.getGraph();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture is similar to a [ReAct agent](/langgraphjs/how-tos/#prebuilt-react-agent) in which node `\"a\"` is a tool-calling model, and node `\"b\"` represents the tools.\n",
    "\n",
    "In our `route` conditional edge, we specify that we should end after the `\"aggregate\"` list in the state passes a threshold length.\n",
    "\n",
    "Invoking the graph, we see that we alternate between nodes `\"a\"` and `\"b\"` before terminating once we reach the termination condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node A sees \n",
      "Node B sees A\n",
      "Node A sees A,B\n",
      "Node B sees A,B,A\n",
      "Node A sees A,B,A,B\n",
      "Node B sees A,B,A,B,A\n",
      "Node A sees A,B,A,B,A,B\n",
      "{\n",
      "  aggregate: [\n",
      "    'A', 'B', 'A',\n",
      "    'B', 'A', 'B',\n",
      "    'A'\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await graph.invoke({ aggregate: [] });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impose a recursion limit\n",
    "\n",
    "In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/langgraphjs/concepts/low_level/#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/langgraphjs/concepts/low_level/#graphs). We can then catch and handle this exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node A sees \n",
      "Node B sees A\n",
      "Node A sees A,B\n",
      "Node B sees A,B,A\n",
      "Recursion Error\n"
     ]
    }
   ],
   "source": [
    "import { GraphRecursionError } from \"@langchain/langgraph\";\n",
    "\n",
    "try {\n",
    "  await graph.invoke({ aggregate: [] }, { recursionLimit: 4 });\n",
    "} catch (error) {\n",
    "  if (error instanceof GraphRecursionError) {\n",
    "    console.log(\"Recursion Error\");\n",
    "  } else {\n",
    "    throw error;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time we terminate after the fourth step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops with branches\n",
    "\n",
    "To better understand how the recursion limit works, let's consider a more complex example. Below we implement a loop, but one step fans out into two nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the state with a reducer\n",
    "const StateAnnotationWithLoops = Annotation.Root({\n",
    "  aggregate: Annotation<string[]>({\n",
    "    reducer: (a, b) => a.concat(b),\n",
    "    default: () => [],\n",
    "  }),\n",
    "});\n",
    "\n",
    "// Define nodes\n",
    "const nodeA = async function (state: typeof StateAnnotationWithLoops.State) {\n",
    "  console.log(`Node A sees ${state.aggregate}`);\n",
    "  return { aggregate: [\"A\"] };\n",
    "}\n",
    "\n",
    "const nodeB = async function (state: typeof StateAnnotationWithLoops.State) {\n",
    "  console.log(`Node B sees ${state.aggregate}`);\n",
    "  return { aggregate: [\"B\"] };\n",
    "}\n",
    "\n",
    "const nodeC = async function (state: typeof StateAnnotationWithLoops.State) {\n",
    "  console.log(`Node C sees ${state.aggregate}`);\n",
    "  return { aggregate: [\"C\"] };\n",
    "}\n",
    "\n",
    "const nodeD = async function (state: typeof StateAnnotationWithLoops.State) {\n",
    "  console.log(`Node D sees ${state.aggregate}`);\n",
    "  return { aggregate: [\"D\"] };\n",
    "}\n",
    "\n",
    "// Define edges\n",
    "const loopRouter = async function (state: typeof StateAnnotationWithLoops.State) {\n",
    "  if (state.aggregate.length < 7) {\n",
    "    return \"b\";\n",
    "  } else {\n",
    "    return \"__end__\";\n",
    "  }\n",
    "}\n",
    "\n",
    "// Define the graph\n",
    "const graphWithLoops = new StateGraph(StateAnnotationWithLoops)\n",
    "  .addNode(\"a\", nodeA)\n",
    "  .addNode(\"b\", nodeB)\n",
    "  .addNode(\"c\", nodeC)\n",
    "  .addNode(\"d\", nodeD)\n",
    "  .addEdge(\"__start__\", \"a\")\n",
    "  .addConditionalEdges(\"a\", loopRouter)\n",
    "  .addEdge(\"b\", \"c\")\n",
    "  .addEdge(\"b\", \"d\")\n",
    "  .addEdge([\"c\", \"d\"], \"a\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraphWithLoops = graphWithLoops.getGraph();\n",
    "const imageWithLoops = await drawableGraphWithLoops.drawMermaidPng();\n",
    "const arrayBufferWithLoops = await imageWithLoops.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBufferWithLoops));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks complex, but can be conceptualized as loop of [supersteps](../../concepts/low_level/#graphs):\n",
    "\n",
    "1. Node A\n",
    "2. Node B\n",
    "3. Nodes C and D\n",
    "4. Node A\n",
    "5. ...\n",
    "\n",
    "We have a loop of four supersteps, where nodes C and D are executed concurrently.\n",
    "\n",
    "Invoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node A sees \n",
      "Node B sees A\n",
      "Node C sees A,B\n",
      "Node D sees A,B\n",
      "Node A sees A,B,C,D\n",
      "Node B sees A,B,C,D,A\n",
      "Node C sees A,B,C,D,A,B\n",
      "Node D sees A,B,C,D,A,B\n",
      "Node A sees A,B,C,D,A,B,C,D\n",
      "{\n",
      "  aggregate: [\n",
      "    'A', 'B', 'C',\n",
      "    'D', 'A', 'B',\n",
      "    'C', 'D', 'A'\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await graphWithLoops.invoke({ aggregate: [] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node A sees \n",
      "Node B sees A\n",
      "Node C sees A,B\n",
      "Node D sees A,B\n",
      "Node A sees A,B,C,D\n",
      "Recursion Error\n"
     ]
    }
   ],
   "source": [
    "import { GraphRecursionError } from \"@langchain/langgraph\";\n",
    "\n",
    "try {\n",
    "  await graphWithLoops.invoke({ aggregate: [] }, { recursionLimit: 4 });\n",
    "} catch (error) {\n",
    "  if (error instanceof GraphRecursionError) {\n",
    "    console.log(\"Recursion Error\");\n",
    "  } else {\n",
    "    throw error;\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/respond-in-format.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c3d2c48",
      "metadata": {},
      "source": [
        "# How to have agent respond in structured format\n",
        "\n",
        "The typical ReAct agent prompts the LLM to respond in 1 of two formats: a\n",
        "function call (~ JSON) to use a tool, or conversational text to respond to the\n",
        "user.\n",
        "\n",
        "If your agent is connected to a structured (or even generative) UI, or if it is\n",
        "communicating with another agent or software process, you may want it to resopnd\n",
        "in a specific structured format.\n",
        "\n",
        "In this example we will build a conversational ReAct agent that responds in a\n",
        "specific format. We will do this by using\n",
        "[tool calling](https://js.langchain.com/docs/modules/model_io/models/chat/function-calling/).\n",
        "This is useful when you want to enforce that an agent's response is in a\n",
        "specific format. In this example, we will ask it respond as if it were a\n",
        "weatherman, returning the temperature and additional info in separate,\n",
        "machine-readable fields.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5860c111",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First we need to install the packages required\n",
        "\n",
        "```bash\n",
        "yarn add langchain @langchain/anthropic @langchain/langgraph @langchain/core\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23523fc0",
      "metadata": {},
      "source": [
        "Next, we need to set API keys for OpenAI (the LLM we will use).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb3ada8f",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e7be8c",
      "metadata": {},
      "source": [
        "Optionally, we can set API key for\n",
        "[LangSmith tracing](https://smith.langchain.com/), which will give us\n",
        "best-in-class observability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bf127e2b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respond in Format: LangGraphJS\n"
          ]
        }
      ],
      "source": [
        "// process.env.LANGCHAIN_API_KEY = \"ls...\";\n",
        "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "process.env.LANGCHAIN_PROJECT = \"Respond in Format: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b214dd10",
      "metadata": {},
      "source": [
        "## Set up the State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4ad79663",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation, messagesStateReducer } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const GraphState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: messagesStateReducer,\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeca531d",
      "metadata": {},
      "source": [
        "## Set up the tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0fe8477",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = tool((_) => {\n",
        "  // This is a placeholder, but don't tell the LLM that...\n",
        "  return \"67 degrees. Cloudy with a chance of rain.\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description: \"Call to surf the web.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "});\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1a6aa07",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a\n",
        "[ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df80654e",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode<typeof GraphState.State>(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f17e92",
      "metadata": {},
      "source": [
        "## Set up the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9c644fb9",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({\n",
        "  temperature: 0,\n",
        "  model: \"gpt-4o\",\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb86967d",
      "metadata": {},
      "source": [
        "After we've done this, we should make sure the model knows that it has these\n",
        "tools available to call. We can do this by binding the LangChain tools to the model class.\n",
        "\n",
        "We also want to define a response schema for the language model and bind it to\n",
        "the model as a tool. The idea is that when the model is ready to respond, it'll call this final\n",
        "tool and populate arguments for it according to the schema we want. Rather than calling\n",
        "a tool, we'll instead return from the graph.\n",
        "\n",
        "Because we only intend to use this final tool to guide the schema of the model's final response,\n",
        "we'll declare it with a mocked out function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e148a48b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "\n",
        "const Response = z.object({\n",
        "  temperature: z.number().describe(\"the temperature\"),\n",
        "  other_notes: z.string().describe(\"any other notes about the weather\"),\n",
        "});\n",
        "\n",
        "const finalResponseTool = tool(async () => \"mocked value\", {\n",
        "  name: \"Response\",\n",
        "  description: \"Always respond to the user using this tool.\",\n",
        "  schema: Response\n",
        "})\n",
        "\n",
        "const boundModel = model.bindTools([\n",
        "  ...tools,\n",
        "  finalResponseTool\n",
        "]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e082c02",
      "metadata": {},
      "source": [
        "## Define the nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "960ef633",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
        "\n",
        "// Define the function that determines whether to continue or not\n",
        "const route = (state: typeof GraphState.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If there is no function call, then we finish\n",
        "  if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) {\n",
        "    return \"__end__\";\n",
        "  }\n",
        "  // Otherwise if there is, we need to check what type of function call it is\n",
        "  if (lastMessage.tool_calls[0].name === \"Response\") {\n",
        "    return \"__end__\";\n",
        "  }\n",
        "  // Otherwise we continue\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel = async (\n",
        "  state: typeof GraphState.State,\n",
        "  config?: RunnableConfig,\n",
        ") => {\n",
        "  const { messages } = state;\n",
        "  const response = await boundModel.invoke(messages, config);\n",
        "  // We return an object, because this will get added to the existing list\n",
        "  return { messages: [response] };\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d09c2d0",
      "metadata": {},
      "source": [
        "## Define the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "51179012",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(GraphState)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(\"__start__\", \"agent\")\n",
        "  .addConditionalEdges(\n",
        "    // First, we define the start node. We use `agent`.\n",
        "    // This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    // Next, we pass in the function that will determine which node is called next.\n",
        "    route,\n",
        "    // We supply a map of possible response values to the conditional edge\n",
        "    // to make it possible to draw a visualization of the graph.\n",
        "    {\n",
        "      __end__: \"__end__\",\n",
        "      tools: \"tools\",\n",
        "    }\n",
        "  )\n",
        "  // We now add a normal edge from `tools` to `agent`.\n",
        "  // This means that after `tools` is called, `agent` node is called next.\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "// Finally, we compile it!\n",
        "// This compiles it into a LangChain Runnable,\n",
        "// meaning you can use it as you would any other runnable\n",
        "const app = workflow.compile();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b24e6c09",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const graph = app.getGraph();\n",
        "const image = await graph.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae844f61",
      "metadata": {},
      "source": [
        "## Use it!\n",
        "\n",
        "We can now use it! This now exposes the\n",
        "[same interface](https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html)\n",
        "as all other LangChain runnables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3ee8225f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: what is the weather in sf\n",
            "\n",
            "---\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- search({\"query\":\"current weather in San Francisco\"})\n",
            "\n",
            "---\n",
            "\n",
            "[tool]: 67 degrees. Cloudy with a chance of rain.\n",
            "\n",
            "---\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- Response({\"temperature\":67,\"other_notes\":\"Cloudy with a chance of rain.\"})\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import { HumanMessage, isAIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const prettyPrint = (message: BaseMessage) => {\n",
        "  let txt = `[${message._getType()}]: ${message.content}`;\n",
        "  if (\n",
        "    isAIMessage(message) && message?.tool_calls?.length\n",
        "  ) {\n",
        "    const tool_calls = message?.tool_calls\n",
        "      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n",
        "      .join(\"\\n\");\n",
        "    txt += ` \\nTools: \\n${tool_calls}`;\n",
        "  }\n",
        "  console.log(txt);\n",
        "};\n",
        "\n",
        "const inputs = {\n",
        "  messages: [new HumanMessage(\"what is the weather in sf\")],\n",
        "};\n",
        "\n",
        "const stream = await app.stream(inputs, { streamMode: \"values\" });\n",
        "\n",
        "for await (const output of stream) {\n",
        "  const { messages } = output;\n",
        "  prettyPrint(messages[messages.length - 1]);\n",
        "  console.log(\"\\n---\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "332a421f",
      "metadata": {},
      "source": [
        "## Partially streaming JSON\n",
        "\n",
        "If we want to stream the structured output as soon as it's available, we can use the [`.streamEvents()`](https://js.langchain.com/docs/how_to/streaming#using-stream-events) method. We'll aggregate emitted `on_chat_model_events` and inspect the name field. As soon as we detect that the model is calling the final output tool, we can start logging the relevant chunks.\n",
        "\n",
        "Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c96cb38d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---\n",
            "{\"\n",
            "---\n",
            "{\"temperature\n",
            "---\n",
            "{\"temperature\":\n",
            "---\n",
            "{\"temperature\":67\n",
            "---\n",
            "{\"temperature\":67,\"\n",
            "---\n",
            "{\"temperature\":67,\"other\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloud\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a chance\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a chance of\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a chance of rain\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a chance of rain.\"\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a chance of rain.\"}\n",
            "---\n",
            "{\"temperature\":67,\"other_notes\":\"Cloudy with a chance of rain.\"}\n",
            "---\n",
            "[\n",
            "  {\n",
            "    name: 'Response',\n",
            "    args: { temperature: 67, other_notes: 'Cloudy with a chance of rain.' },\n",
            "    id: 'call_oOhNx2SdeelXn6tbenokDtkO',\n",
            "    type: 'tool_call'\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { concat } from \"@langchain/core/utils/stream\";\n",
        "\n",
        "const eventStream = await app.streamEvents(inputs, { version: \"v2\" });\n",
        "\n",
        "let aggregatedChunk;\n",
        "for await (const { event, data } of eventStream) {\n",
        "  if (event === \"on_chat_model_stream\") {\n",
        "    const { chunk } = data;\n",
        "    if (aggregatedChunk === undefined) {\n",
        "      aggregatedChunk = chunk;\n",
        "    } else {\n",
        "      aggregatedChunk = concat(aggregatedChunk, chunk);\n",
        "    }\n",
        "    const currentToolCalls = aggregatedChunk.tool_calls;\n",
        "    if (\n",
        "      currentToolCalls.length === 0 ||\n",
        "      currentToolCalls[0].name === \"\" ||\n",
        "      !finalResponseTool.name.startsWith(currentToolCalls[0].name)\n",
        "    ) {\n",
        "      // No tool calls or a different tool call in the message,\n",
        "      // so drop what's currently aggregated and start over\n",
        "      aggregatedChunk = undefined;\n",
        "    } else if (currentToolCalls[0].name === finalResponseTool.name) {\n",
        "      // Now we're sure that this event is part of the final output!\n",
        "      // Log the partially aggregated args.\n",
        "      console.log(aggregatedChunk.tool_call_chunks[0].args);\n",
        "\n",
        "      // You can also log the raw args instead:\n",
        "      // console.log(chunk.tool_call_chunks);\n",
        "\n",
        "      console.log(\"---\");\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// Final aggregated tool call\n",
        "console.log(aggregatedChunk.tool_calls);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/review-tool-calls-functional.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to review tool calls (Functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - Implementing [human-in-the-loop](../../concepts/human_in_the_loop) workflows with [interrupt](../../concepts/human_in_the_loop/#interrupt)\n",
    "    - [How to create a ReAct agent using the Functional API](../../how-tos/react-agent-from-scratch-functional)\n",
    "\n",
    "This guide demonstrates how to implement human-in-the-loop workflows in a ReAct agent using the LangGraph [Functional API](../../concepts/functional_api).\n",
    "\n",
    "We will build off of the agent created in the [How to create a ReAct agent using the Functional API](../../how-tos/react-agent-from-scratch-functional) guide.\n",
    "\n",
    "Specifically, we will demonstrate how to review [tool calls](https://js.langchain.com/docs/concepts/tool_calling/) generated by a [chat model](https://js.langchain.com/docs/concepts/chat_models/) prior to their execution. This can be accomplished through use of the [interrupt](../../concepts/human_in_the_loop/#interrupt) function at key points in our application.\n",
    "\n",
    "**Preview**:\n",
    "\n",
    "We will implement a simple function that reviews tool calls generated from our chat model and call it from inside our application's [entrypoint](../../concepts/functional_api/#entrypoint):\n",
    "\n",
    "```ts\n",
    "function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage {\n",
    "  // Interrupt for human review\n",
    "  const humanReview = interrupt({\n",
    "    question: \"Is this correct?\",\n",
    "    tool_call: toolCall,\n",
    "  });\n",
    "\n",
    "  const { action, data } = humanReview;\n",
    "\n",
    "  if (action === \"continue\") {\n",
    "    return toolCall;\n",
    "  } else if (action === \"update\") {\n",
    "    return {\n",
    "      ...toolCall,\n",
    "      args: data,\n",
    "    };\n",
    "  } else if (action === \"feedback\") {\n",
    "    return new ToolMessage({\n",
    "      content: data,\n",
    "      name: toolCall.name,\n",
    "      tool_call_id: toolCall.id,\n",
    "    });\n",
    "  }\n",
    "  throw new Error(`Unsupported review action: ${action}`);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and tools\n",
    "\n",
    "Let's first define the tools and model we will use for our example. As in the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional), we will use a single place-holder tool that gets a description of the weather for a location.\n",
    "\n",
    "We will use an [OpenAI](https://js.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://js.langchain.com/docs/integrations/chat/) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o-mini\",\n",
    "});\n",
    "\n",
    "const getWeather = tool(async ({ location }) => {\n",
    "  // This is a placeholder for the actual implementation\n",
    "  const lowercaseLocation = location.toLowerCase();\n",
    "  if (lowercaseLocation.includes(\"sf\") || lowercaseLocation.includes(\"san francisco\")) {\n",
    "    return \"It's sunny!\";\n",
    "  } else if (lowercaseLocation.includes(\"boston\")) {\n",
    "    return \"It's rainy!\";\n",
    "  } else {\n",
    "    return `I am not sure what the weather is in ${location}`;\n",
    "  }\n",
    "}, {\n",
    "  name: \"getWeather\",\n",
    "  schema: z.object({\n",
    "    location: z.string().describe(\"Location to get the weather for\"),\n",
    "  }),\n",
    "  description: \"Call to get the weather from a specific location.\",\n",
    "});\n",
    "\n",
    "const tools = [getWeather];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tasks\n",
    "\n",
    "Our [tasks](../../concepts/functional_api/#task) are unchanged from the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional):\n",
    "\n",
    "1. **Call model**: We want to query our chat model with a list of messages.\n",
    "2. **Call tool**: If our model generates tool calls, we want to execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  type BaseMessageLike,\n",
    "  AIMessage,\n",
    "  ToolMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "import { type ToolCall } from \"@langchain/core/messages/tool\";\n",
    "import { task } from \"@langchain/langgraph\";\n",
    "\n",
    "const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\n",
    "\n",
    "const callModel = task(\"callModel\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await model.bindTools(tools).invoke(messages);\n",
    "  return response;\n",
    "});\n",
    "\n",
    "const callTool = task(\n",
    "  \"callTool\",\n",
    "  async (toolCall: ToolCall): Promise<AIMessage> => {\n",
    "    const tool = toolsByName[toolCall.name];\n",
    "    const observation = await tool.invoke(toolCall.args);\n",
    "    return new ToolMessage({ content: observation, tool_call_id: toolCall.id });\n",
    "    // Can also pass toolCall directly into the tool to return a ToolMessage\n",
    "    // return tool.invoke(toolCall);\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define entrypoint\n",
    "\n",
    "To review tool calls before execution, we add a `reviewToolCalls` function that calls [interrupt](../../concepts/human_in_the_loop/#interrupt). When this function is called, execution will be paused until we issue a command to resume it.\n",
    "\n",
    "Given a tool call, our function will `interrupt` for human review. At that point we can either:\n",
    "\n",
    "- Accept the tool call;\n",
    "- Revise the tool call and continue;\n",
    "- Generate a custom tool message (e.g., instructing the model to re-format its tool call).\n",
    "\n",
    "We will demonstrate these three cases in the [usage examples](#usage) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { interrupt } from \"@langchain/langgraph\";\n",
    "\n",
    "function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage {\n",
    "  // Interrupt for human review\n",
    "  const humanReview = interrupt({\n",
    "    question: \"Is this correct?\",\n",
    "    tool_call: toolCall,\n",
    "  });\n",
    "\n",
    "  const { action, data } = humanReview;\n",
    "\n",
    "  if (action === \"continue\") {\n",
    "    return toolCall;\n",
    "  } else if (action === \"update\") {\n",
    "    return {\n",
    "      ...toolCall,\n",
    "      args: data,\n",
    "    };\n",
    "  } else if (action === \"feedback\") {\n",
    "    return new ToolMessage({\n",
    "      content: data,\n",
    "      name: toolCall.name,\n",
    "      tool_call_id: toolCall.id,\n",
    "    });\n",
    "  }\n",
    "  throw new Error(`Unsupported review action: ${action}`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our [entrypoint](../../concepts/functional_api/#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the `ToolMessage` supplied by the human.\n",
    "\n",
    "!!! tip\n",
    "\n",
    "    The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the `interrupt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  MemorySaver,\n",
    "  addMessages,\n",
    "  entrypoint,\n",
    "  getPreviousState,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const agent = entrypoint({\n",
    "  checkpointer,\n",
    "  name: \"agent\",\n",
    "}, async (messages: BaseMessageLike[]) => {\n",
    "  const previous = getPreviousState<BaseMessageLike[]>() ?? [];\n",
    "  let currentMessages = addMessages(previous, messages);\n",
    "  let llmResponse = await callModel(currentMessages);\n",
    "  while (true) {\n",
    "    if (!llmResponse.tool_calls?.length) {\n",
    "      break;\n",
    "    }\n",
    "    // Review tool calls\n",
    "    const toolResults: ToolMessage[] = [];\n",
    "    const toolCalls: ToolCall[] = [];\n",
    "    \n",
    "    for (let i = 0; i < llmResponse.tool_calls.length; i++) {\n",
    "      const review = await reviewToolCall(llmResponse.tool_calls[i]);\n",
    "      if (review instanceof ToolMessage) {\n",
    "        toolResults.push(review);\n",
    "      } else { // is a validated tool call\n",
    "        toolCalls.push(review);\n",
    "        if (review !== llmResponse.tool_calls[i]) {\n",
    "          llmResponse.tool_calls[i] = review;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    // Execute remaining tool calls\n",
    "    const remainingToolResults = await Promise.all(\n",
    "      toolCalls.map((toolCall) => callTool(toolCall))\n",
    "    );\n",
    "    \n",
    "    // Append to message list\n",
    "    currentMessages = addMessages(\n",
    "      currentMessages,\n",
    "      [llmResponse, ...toolResults, ...remainingToolResults]\n",
    "    );\n",
    "\n",
    "    // Call model again\n",
    "    llmResponse = await callModel(currentMessages);\n",
    "  }\n",
    "  // Generate final response\n",
    "  currentMessages = addMessages(currentMessages, llmResponse);\n",
    "  return entrypoint.final({\n",
    "    value: llmResponse,\n",
    "    save: currentMessages\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Let's demonstrate some scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const prettyPrintMessage = (message: BaseMessage) => {\n",
    "  console.log(\"=\".repeat(30), `${message.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(message.content);\n",
    "  if (isAIMessage(message) && message.tool_calls?.length) {\n",
    "    console.log(JSON.stringify(message.tool_calls, null, 2));\n",
    "  }\n",
    "}\n",
    "\n",
    "const printStep = (step: Record<string, any>) => {\n",
    "  if (step.__metadata__?.cached) {\n",
    "    return;\n",
    "  }\n",
    "  for (const [taskName, result] of Object.entries(step)) {\n",
    "    if (taskName === \"agent\") {\n",
    "      continue; // just stream from tasks\n",
    "    }\n",
    "    \n",
    "    console.log(`\\n${taskName}:`);\n",
    "    if (taskName === \"__interrupt__\" || taskName === \"reviewToolCall\") {\n",
    "      console.log(JSON.stringify(result, null, 2));\n",
    "    } else {\n",
    "      prettyPrintMessage(result);\n",
    "    }\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept a tool call\n",
    "\n",
    "To accept a tool call, we just indicate in the data we provide in the `Command` that the tool call should pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ role: 'user', content: \"What's the weather in san francisco?\" }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_pe7ee3A4lOO4Llr2NcfRukyp\"\n",
      "  }\n",
      "]\n",
      "\n",
      "__interrupt__:\n",
      "[\n",
      "  {\n",
      "    \"value\": {\n",
      "      \"question\": \"Is this correct?\",\n",
      "      \"tool_call\": {\n",
      "        \"name\": \"getWeather\",\n",
      "        \"args\": {\n",
      "          \"location\": \"San Francisco\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_pe7ee3A4lOO4Llr2NcfRukyp\"\n",
      "      }\n",
      "    },\n",
      "    \"when\": \"during\",\n",
      "    \"resumable\": true,\n",
      "    \"ns\": [\n",
      "      \"agent:dcee519a-80f5-5950-9e1c-e8bb85ed436f\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const config = {\n",
    "  configurable: {\n",
    "    thread_id: \"1\"\n",
    "  }\n",
    "};\n",
    "\n",
    "const userMessage = {\n",
    "  role: \"user\",\n",
    "  content: \"What's the weather in san francisco?\"\n",
    "};\n",
    "console.log(userMessage);\n",
    "\n",
    "const stream = await agent.stream([userMessage], config);\n",
    "\n",
    "for await (const step of stream) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "The weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "\n",
    "// highlight-next-line\n",
    "const humanInput = new Command({\n",
    "  // highlight-next-line\n",
    "  resume: {\n",
    "    // highlight-next-line\n",
    "    action: \"continue\",\n",
    "    // highlight-next-line\n",
    "  },\n",
    "  // highlight-next-line\n",
    "});\n",
    "\n",
    "const resumedStream = await agent.stream(humanInput, config)\n",
    "\n",
    "for await (const step of resumedStream) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise a tool call\n",
    "\n",
    "To revise a tool call, we can supply updated arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ role: 'user', content: \"What's the weather in san francisco?\" }\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_JEOqaUEvYJ4pzMtVyCQa6H2H\"\n",
      "  }\n",
      "]\n",
      "\n",
      "__interrupt__:\n",
      "[\n",
      "  {\n",
      "    \"value\": {\n",
      "      \"question\": \"Is this correct?\",\n",
      "      \"tool_call\": {\n",
      "        \"name\": \"getWeather\",\n",
      "        \"args\": {\n",
      "          \"location\": \"San Francisco\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_JEOqaUEvYJ4pzMtVyCQa6H2H\"\n",
      "      }\n",
      "    },\n",
      "    \"when\": \"during\",\n",
      "    \"resumable\": true,\n",
      "    \"ns\": [\n",
      "      \"agent:d5c54c67-483a-589a-a1e7-2a8465b3ef13\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const config2 = {\n",
    "  configurable: {\n",
    "    thread_id: \"2\"\n",
    "  }\n",
    "};\n",
    "\n",
    "const userMessage2 = {\n",
    "  role: \"user\",\n",
    "  content: \"What's the weather in san francisco?\"\n",
    "};\n",
    "\n",
    "console.log(userMessage2);\n",
    "\n",
    "const stream2 = await agent.stream([userMessage2], config2);\n",
    "\n",
    "for await (const step of stream2) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "The weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "// highlight-next-line\n",
    "const humanInput2 = new Command({\n",
    "  // highlight-next-line\n",
    "  resume: {\n",
    "    // highlight-next-line\n",
    "    action: \"update\",\n",
    "    // highlight-next-line\n",
    "    data: { location: \"SF, CA\" },\n",
    "    // highlight-next-line\n",
    "  },\n",
    "  // highlight-next-line\n",
    "});\n",
    "\n",
    "const resumedStream2 = await agent.stream(humanInput2, config2)\n",
    "\n",
    "for await (const step of resumedStream2) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith traces for this run are particularly informative:\n",
    "\n",
    "- In the trace [before the interrupt](https://smith.langchain.com/public/abf80a16-3e15-484b-bbbb-23017593bd39/r), we generate a tool call for location `\"San Francisco\"`.\n",
    "- In the trace [after resuming](https://smith.langchain.com/public/233a7e32-a43e-4939-9c04-96fd4254ce65/r), we see that the tool call in the message has been updated to `\"SF, CA\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a custom ToolMessage\n",
    "\n",
    "To Generate a custom `ToolMessage`, we supply the content of the message. In this case we will ask the model to reformat its tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ role: 'user', content: \"What's the weather in san francisco?\" }\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_HNRjJLJo4U78dtk0uJ9YZF6V\"\n",
      "  }\n",
      "]\n",
      "\n",
      "__interrupt__:\n",
      "[\n",
      "  {\n",
      "    \"value\": {\n",
      "      \"question\": \"Is this correct?\",\n",
      "      \"tool_call\": {\n",
      "        \"name\": \"getWeather\",\n",
      "        \"args\": {\n",
      "          \"location\": \"San Francisco\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_HNRjJLJo4U78dtk0uJ9YZF6V\"\n",
      "      }\n",
      "    },\n",
      "    \"when\": \"during\",\n",
      "    \"resumable\": true,\n",
      "    \"ns\": [\n",
      "      \"agent:6f313de8-c19e-5c3e-bdff-f90cdd68d0de\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const config3 = {\n",
    "  configurable: {\n",
    "    thread_id: \"3\"\n",
    "  }\n",
    "};\n",
    "\n",
    "const userMessage3 = {\n",
    "  role: \"user\",\n",
    "  content: \"What's the weather in san francisco?\"\n",
    "};\n",
    "\n",
    "console.log(userMessage3);\n",
    "\n",
    "const stream3 = await agent.stream([userMessage3], config3);\n",
    "\n",
    "for await (const step of stream3) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco, CA\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_5V4Oj4JV2DVfeteM4Aaf2ieD\"\n",
      "  }\n",
      "]\n",
      "\n",
      "__interrupt__:\n",
      "[\n",
      "  {\n",
      "    \"value\": {\n",
      "      \"question\": \"Is this correct?\",\n",
      "      \"tool_call\": {\n",
      "        \"name\": \"getWeather\",\n",
      "        \"args\": {\n",
      "          \"location\": \"San Francisco, CA\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_5V4Oj4JV2DVfeteM4Aaf2ieD\"\n",
      "      }\n",
      "    },\n",
      "    \"when\": \"during\",\n",
      "    \"resumable\": true,\n",
      "    \"ns\": [\n",
      "      \"agent:6f313de8-c19e-5c3e-bdff-f90cdd68d0de\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "// highlight-next-line\n",
    "const humanInput3 = new Command({\n",
    "  // highlight-next-line\n",
    "  resume: {\n",
    "    // highlight-next-line\n",
    "    action: \"feedback\",\n",
    "    // highlight-next-line\n",
    "    data: \"Please format as <City>, <State>.\",\n",
    "    // highlight-next-line\n",
    "  },\n",
    "  // highlight-next-line\n",
    "});\n",
    "\n",
    "const resumedStream3 = await agent.stream(humanInput3, config3)\n",
    "\n",
    "for await (const step of resumedStream3) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is re-formatted, we can accept it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "The weather in San Francisco, CA is sunny!\n"
     ]
    }
   ],
   "source": [
    "// highlight-next-line\n",
    "const continueCommand = new Command({\n",
    "  // highlight-next-line\n",
    "  resume: {\n",
    "    // highlight-next-line\n",
    "    action: \"continue\",\n",
    "    // highlight-next-line\n",
    "  },\n",
    "  // highlight-next-line\n",
    "});\n",
    "\n",
    "const continueStream = await agent.stream(continueCommand, config3)\n",
    "\n",
    "for await (const step of continueStream) {\n",
    "  printStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/review-tool-calls.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Tool Calls\n",
    "\n",
    "!!! tip \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following concepts:\n",
    "\n",
    "    * [Tool calling](https://js.langchain.com/docs/concepts/tool_calling/)\n",
    "    * [Human-in-the-loop](/langgraphjs/concepts/human_in_the_loop)\n",
    "    * [LangGraph Glossary](/langgraphjs/concepts/low_level)      \n",
    "\n",
    "Human-in-the-loop (HIL) interactions are crucial for [agentic systems](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/#human-in-the-loop). A common pattern is to add some human in the loop step after certain tool calls. These tool calls often lead to either a function call or saving of some information. Examples include:\n",
    "\n",
    "- A tool call to execute SQL, which will then be run by the tool\n",
    "- A tool call to generate a summary, which will then be saved to the State of the graph\n",
    "\n",
    "Note that using tool calls is common **whether actually calling tools or not**.\n",
    "\n",
    "There are typically a few different interactions you may want to do here:\n",
    "\n",
    "1. Approve the tool call and continue\n",
    "2. Modify the tool call manually and then continue\n",
    "3. Give natural language feedback, and then pass that back to the agent instead of continuing\n",
    "\n",
    "We can implement these in LangGraph using the [`interrupt()`](/langgraphjs/reference/functions/langgraph.interrupt-1.html) function. `interrupt` allows us to stop graph execution to collect input from a user and continue execution with collected input:\n",
    "\n",
    "```typescript\n",
    "function humanReviewNode(state: typeof GraphAnnotation.State) {\n",
    "  // this is the value we'll be providing via new Command({ resume: <human_review> })\n",
    "  const humanReview = interrupt({\n",
    "    question: \"Is this correct?\",\n",
    "    // Surface tool calls for review\n",
    "    tool_call,\n",
    "  });\n",
    "\n",
    "  const [reviewAction, reviewData] = humanReview;\n",
    "\n",
    "  // Approve the tool call and continue\n",
    "  if (reviewAction === \"continue\") {\n",
    "    return new Command({ goto: \"run_tool\" });\n",
    "  }\n",
    "  \n",
    "  // Modify the tool call manually and then continue\n",
    "  if (reviewAction === \"update\") {\n",
    "    const updatedMsg = getUpdatedMsg(reviewData);\n",
    "    return new Command({ goto: \"run_tool\", update: { messages: [updatedMsg] } });\n",
    "  }\n",
    "  \n",
    "  // Give natural language feedback, and then pass that back to the agent\n",
    "  if (reviewAction === \"feedback\") {\n",
    "    const feedbackMsg = getFeedbackMsg(reviewData);\n",
    "    return new Command({ goto: \"call_llm\", update: { messages: [feedbackMsg] } });\n",
    "  }\n",
    "  \n",
    "  throw new Error(\"Unreachable\");\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for Anthropic (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Usage\n",
    "\n",
    "Let's set up a very simple graph that facilitates this.\n",
    "First, we will have an LLM call that decides what action to take.\n",
    "Then we go to a human node. This node actually doesn't do anything - the idea is that we interrupt before this node and then apply any updates to the state.\n",
    "After that, we check the state and either route back to the LLM or to the correct tool.\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  MessagesAnnotation,\n",
    "  StateGraph,\n",
    "  START,\n",
    "  END,\n",
    "  MemorySaver,\n",
    "  Command,\n",
    "  interrupt\n",
    "} from \"@langchain/langgraph\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { tool } from '@langchain/core/tools';\n",
    "import { z } from 'zod';\n",
    "import { AIMessage, ToolMessage } from '@langchain/core/messages';\n",
    "import { ToolCall } from '@langchain/core/messages/tool';\n",
    "\n",
    "const weatherSearch = tool((input: { city: string }) => {\n",
    "    console.log(\"----\");\n",
    "    console.log(`Searching for: ${input.city}`);\n",
    "    console.log(\"----\");\n",
    "    return \"Sunny!\";\n",
    "}, {\n",
    "    name: 'weather_search',\n",
    "    description: 'Search for the weather',\n",
    "    schema: z.object({\n",
    "        city: z.string()\n",
    "    })\n",
    "});\n",
    "\n",
    "const model = new ChatAnthropic({ \n",
    "    model: \"claude-3-5-sonnet-latest\"\n",
    "}).bindTools([weatherSearch]);\n",
    "\n",
    "const callLLM = async (state: typeof MessagesAnnotation.State) => {\n",
    "    const response = await model.invoke(state.messages);\n",
    "    return { messages: [response] };\n",
    "};\n",
    "\n",
    "const humanReviewNode = async (state: typeof MessagesAnnotation.State): Promise<Command> => {\n",
    "    const lastMessage = state.messages[state.messages.length - 1] as AIMessage;\n",
    "    const toolCall = lastMessage.tool_calls![lastMessage.tool_calls!.length - 1];\n",
    "\n",
    "    const humanReview = interrupt<\n",
    "      {\n",
    "        question: string;\n",
    "        toolCall: ToolCall;\n",
    "      },\n",
    "      {\n",
    "        action: string;\n",
    "        data: any;\n",
    "      }>({\n",
    "        question: \"Is this correct?\",\n",
    "        toolCall: toolCall\n",
    "      });\n",
    "\n",
    "    const reviewAction = humanReview.action;\n",
    "    const reviewData = humanReview.data;\n",
    "\n",
    "    if (reviewAction === \"continue\") {\n",
    "        return new Command({ goto: \"run_tool\" });\n",
    "    }\n",
    "    else if (reviewAction === \"update\") {\n",
    "        const updatedMessage = {\n",
    "            role: \"ai\",\n",
    "            content: lastMessage.content,\n",
    "            tool_calls: [{\n",
    "                id: toolCall.id,\n",
    "                name: toolCall.name,\n",
    "                args: reviewData\n",
    "            }],\n",
    "            id: lastMessage.id\n",
    "        };\n",
    "        return new Command({ goto: \"run_tool\", update: { messages: [updatedMessage] } });\n",
    "    }\n",
    "    else if (reviewAction === \"feedback\") {\n",
    "        const toolMessage = new ToolMessage({\n",
    "          name: toolCall.name,\n",
    "          content: reviewData,\n",
    "          tool_call_id: toolCall.id\n",
    "        })\n",
    "        return new Command({ goto: \"call_llm\", update: { messages: [toolMessage] }});\n",
    "    }\n",
    "    throw new Error(\"Invalid review action\");\n",
    "};\n",
    "\n",
    "const runTool = async (state: typeof MessagesAnnotation.State) => {\n",
    "    const newMessages: ToolMessage[] = [];\n",
    "    const tools = { weather_search: weatherSearch };\n",
    "    const lastMessage = state.messages[state.messages.length - 1] as AIMessage;\n",
    "    const toolCalls = lastMessage.tool_calls!;\n",
    "\n",
    "    for (const toolCall of toolCalls) {\n",
    "        const tool = tools[toolCall.name as keyof typeof tools];\n",
    "        const result = await tool.invoke(toolCall.args);\n",
    "        newMessages.push(new ToolMessage({\n",
    "            name: toolCall.name,\n",
    "            content: result,\n",
    "            tool_call_id: toolCall.id\n",
    "        }));\n",
    "    }\n",
    "    return { messages: newMessages };\n",
    "};\n",
    "\n",
    "const routeAfterLLM = (state: typeof MessagesAnnotation.State): typeof END | \"human_review_node\" => {\n",
    "    const lastMessage = state.messages[state.messages.length - 1] as AIMessage;\n",
    "    if (!lastMessage.tool_calls?.length) {\n",
    "        return END;\n",
    "    }\n",
    "    return \"human_review_node\";\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph(MessagesAnnotation)\n",
    "    .addNode(\"call_llm\", callLLM)\n",
    "    .addNode(\"run_tool\", runTool)\n",
    "    .addNode(\"human_review_node\", humanReviewNode, {\n",
    "      ends: [\"run_tool\", \"call_llm\"]\n",
    "    })\n",
    "    .addEdge(START, \"call_llm\")\n",
    "    .addConditionalEdges(\n",
    "        \"call_llm\",\n",
    "        routeAfterLLM,\n",
    "        [\"human_review_node\", END]\n",
    "    )\n",
    "    .addEdge(\"run_tool\", \"call_llm\");\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "const graph = workflow.compile({ checkpointer: memory });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const drawableGraph = graph.getGraph();\n",
    "const image = await drawableGraph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with no review\n",
    "\n",
    "Let's look at an example when no review is required (because no tools are called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "hi!\n",
      "================================ ai Message (1) =================================\n",
      "Hello! I'm here to help you. I can assist you with checking the weather for different cities. Would you like to know the weather for a specific location? Just let me know which city you're interested in and I'll look that up for you.\n"
     ]
    }
   ],
   "source": [
    "let inputs = { messages: [{ role: \"user\", content: \"hi!\" }] };\n",
    "let config = { configurable: { thread_id: \"1\" }, streamMode: \"values\" as const };\n",
    "\n",
    "let stream = await graph.stream(inputs, config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the state, we can see that it is finished, since there are no next steps to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "let state = await graph.getState(config);\n",
    "console.log(state.next);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of approving tool\n",
    "\n",
    "Let's now look at what it looks like to approve a tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "what's the weather in SF?\n",
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'Let me check the weather in San Francisco for you.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01PTn9oqTP6EdFabfhfvELuy',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco' }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [{ role: \"user\", content: \"what's the weather in SF?\" }] };\n",
    "config = { configurable: { thread_id: \"2\" }, streamMode: \"values\" as const };\n",
    "\n",
    "stream = await graph.stream(inputs, config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now check, we can see that it is waiting on human review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'human_review_node' ]\n"
     ]
    }
   ],
   "source": [
    "state = await graph.getState(config);\n",
    "console.log(state.next);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approve the tool call, we can just continue the thread with no edits. To do so, we need to let `human_review_node` know what value to use for the `human_review` variable we defined inside the node. We can provide this value by invoking the graph with a `new Command({ resume: <human_review> })` input.  Since we're approving the tool call, we'll provide `resume` value of `{ action: \"continue\" }` to navigate to `run_tool` node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'Let me check the weather in San Francisco for you.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01PTn9oqTP6EdFabfhfvELuy',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco' }\n",
      "  }\n",
      "]\n",
      "----\n",
      "Searching for: San Francisco\n",
      "----\n",
      "================================ tool Message (1) =================================\n",
      "Sunny!\n",
      "================================ ai Message (1) =================================\n",
      "It's sunny in San Francisco right now!\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "\n",
    "for await (const event of await graph.stream(\n",
    "  new Command({ resume: { action: \"continue\" } }),\n",
    "  config\n",
    ")) {\n",
    "  const recentMsg = event.messages[event.messages.length - 1];\n",
    "  console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "  console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Tool Call\n",
    "\n",
    "Let's now say we want to edit the tool call. E.g. change some of the parameters (or even the tool called!) but then execute that tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "what's the weather in SF?\n",
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'Let me check the weather in San Francisco for you.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01T7ykQ45XyGpzRB7MkPtSAE',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco' }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [{ role: \"user\", content: \"what's the weather in SF?\" }] };\n",
    "config = { configurable: { thread_id: \"3\" }, streamMode: \"values\" as const };\n",
    "\n",
    "stream = await graph.stream(inputs, config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now check, we can see that it is waiting on human review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'human_review_node' ]\n"
     ]
    }
   ],
   "source": [
    "state = await graph.getState(config);\n",
    "console.log(state.next);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To edit the tool call, we will use `Command` with a different resume value of `{ action: \"update\", data: <tool call args> }`. This will do the following:\n",
    "\n",
    "* combine existing tool call with user-provided tool call arguments and update the existing AI message with the new tool call\n",
    "* navigate to `run_tool` node with the updated AI message and continue execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'Let me check the weather in San Francisco for you.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01T7ykQ45XyGpzRB7MkPtSAE',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco' }\n",
      "  }\n",
      "]\n",
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'Let me check the weather in San Francisco for you.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01T7ykQ45XyGpzRB7MkPtSAE',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco' }\n",
      "  }\n",
      "]\n",
      "----\n",
      "Searching for: San Francisco\n",
      "----\n",
      "================================ tool Message (1) =================================\n",
      "Sunny!\n",
      "================================ ai Message (1) =================================\n",
      "It's sunny in San Francisco right now!\n"
     ]
    }
   ],
   "source": [
    "for await (const event of await graph.stream(\n",
    "  new Command({\n",
    "    resume: {\n",
    "      action: \"update\",\n",
    "      data: { city: \"San Francisco\" }\n",
    "    }\n",
    "  }),\n",
    "  config\n",
    ")) {\n",
    "  const recentMsg = event.messages[event.messages.length - 1];\n",
    "  console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "  console.log(recentMsg.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give feedback to a tool call\n",
    "\n",
    "Sometimes, you may not want to execute a tool call, but you also may not want to ask the user to manually modify the tool call. In that case it may be better to get natural language feedback from the user. You can then insert these feedback as a mock **RESULT** of the tool call.\n",
    "\n",
    "There are multiple ways to do this:\n",
    "\n",
    "1. You could add a new message to the state (representing the \"result\" of a tool call)\n",
    "2. You could add TWO new messages to the state - one representing an \"error\" from the tool call, other HumanMessage representing the feedback\n",
    "\n",
    "Both are similar in that they involve adding messages to the state. The main difference lies in the logic AFTER the `human_node` and how it handles different types of messages.\n",
    "\n",
    "For this example we will just add a single tool call representing the feedback. Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ human Message (1) =================================\n",
      "what's the weather in SF?\n",
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: \"I'll help you check the weather in San Francisco.\"\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_014cwxD65wDwQdNg6xqsticF',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'SF' }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [{ role: \"user\", content: \"what's the weather in SF?\" }] };\n",
    "config = { configurable: { thread_id: \"4\" }, streamMode: \"values\" as const };\n",
    "\n",
    "stream = await graph.stream(inputs, config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now check, we can see that it is waiting on human review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'human_review_node' ]\n"
     ]
    }
   ],
   "source": [
    "state = await graph.getState(config);\n",
    "console.log(state.next);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give feedback about the tool call, we will use `Command` with a different resume value of `{ action: \"feedback\", data: <feedback string> }`. This will do the following:\n",
    "\n",
    "* create a new tool message that combines existing tool call from LLM with the with user-provided feedback as content\n",
    "* navigate to `call_llm` node with the updated tool message and continue execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: \"I'll help you check the weather in San Francisco.\"\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_014cwxD65wDwQdNg6xqsticF',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'SF' }\n",
      "  }\n",
      "]\n",
      "================================ tool Message (1) =================================\n",
      "User requested changes: use <city, country> format for location\n",
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'I apologize for the error. Let me search again with the proper format.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01Jnm7sSZsiwv65YM4KsvfXk',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco, USA' }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for await (const event of await graph.stream(\n",
    "  new Command({\n",
    "    resume: {\n",
    "      action: \"feedback\",\n",
    "      data: \"User requested changes: use <city, country> format for location\"\n",
    "    }\n",
    "  }),\n",
    "  config\n",
    ")) {\n",
    "  const recentMsg = event.messages[event.messages.length - 1];\n",
    "  console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "  console.log(recentMsg.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now get to another breakpoint - because it went back to the model and got an entirely new prediction of what to call. Let's now approve this one and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'human_review_node' ]\n"
     ]
    }
   ],
   "source": [
    "state = await graph.getState(config);\n",
    "console.log(state.next);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ ai Message (1) =================================\n",
      "[\n",
      "  {\n",
      "    type: 'text',\n",
      "    text: 'I apologize for the error. Let me search again with the proper format.'\n",
      "  },\n",
      "  {\n",
      "    type: 'tool_use',\n",
      "    id: 'toolu_01Jnm7sSZsiwv65YM4KsvfXk',\n",
      "    name: 'weather_search',\n",
      "    input: { city: 'San Francisco, USA' }\n",
      "  }\n",
      "]\n",
      "----\n",
      "Searching for: San Francisco, USA\n",
      "----\n",
      "================================ tool Message (1) =================================\n",
      "Sunny!\n",
      "================================ ai Message (1) =================================\n",
      "The weather in San Francisco is currently sunny!\n"
     ]
    }
   ],
   "source": [
    "for await (const event of await graph.stream(\n",
    "  new Command({\n",
    "    resume: {\n",
    "      action: \"continue\",\n",
    "    }\n",
    "  }),\n",
    "  config\n",
    ")) {\n",
    "    const recentMsg = event.messages[event.messages.length - 1];\n",
    "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n",
    "    console.log(recentMsg.content);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/semantic-search.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add semantic search to your agent's memory\n",
    "\n",
    "This guide shows how to enable semantic search in your agent's memory store. This lets your agent search for items in the long-term memory store by semantic similarity.\n",
    "\n",
    "## Dependencies and environment setup\n",
    "\n",
    "First, install this guide's required dependencies.\n",
    "\n",
    "```bash\n",
    "npm install \\\n",
    "  @langchain/langgraph \\\n",
    "  @langchain/openai \\\n",
    "  @langchain/core \\\n",
    "  uuid \\\n",
    "  zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a memory store with semantic search\n",
    "\n",
    "Here we create our memory store with an [index configuration](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.IndexConfig.html).\n",
    "\n",
    "By default, stores are configured without semantic/vector search. You can opt in to indexing items when creating the store by providing an [IndexConfig](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.IndexConfig.html) to the store's constructor.\n",
    "\n",
    "If your store class does not implement this interface, or if you do not pass in an index configuration, semantic search is disabled, and all `index` arguments passed to `put` will have no effect.\n",
    "\n",
    "Now, let's create that store!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings({\n",
    "  model: \"text-embedding-3-small\",\n",
    "});\n",
    "\n",
    "const store = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings,\n",
    "    dims: 1536,\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The anatomy of a memory\n",
    "\n",
    "Before we get into semantic search, let's look at how memories are structured, and how to store them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "let namespace = [\"user_123\", \"memories\"]\n",
    "let memoryKey = \"favorite_food\"\n",
    "let memoryValue = {\"text\": \"I love pizza\"}\n",
    "\n",
    "await store.put(namespace, memoryKey, memoryValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, memories are composed of a namespace, a key, and a value.\n",
    "\n",
    "**Namespaces** are multi-dimensional values (arrays of strings) that allow you to segment memory according to the needs of your application. In this case, we're segmenting memories by user by using a User ID (`\"user_123\"`) as the first dimension of our namespace array.\n",
    "\n",
    "**Keys** are arbitrary strings that identify the memory within the namespace. If you write to the same key in the same namespace multiple times, you'll overwrite the memory that was stored under that key.\n",
    "\n",
    "**Values** are objects that represent the actual memory being stored. These can be any object, so long as its serializable. You can structure these objects according to the needs of your application.\n",
    "\n",
    "## Simple memory retrieval\n",
    "\n",
    "Let's add some more memories to our store and then fetch one of them by it's key to check that it stored properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a tunnel engineer\n"
     ]
    }
   ],
   "source": [
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"italian_food\",\n",
    "  {\"text\": \"I prefer Italian food\"}\n",
    ")\n",
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"spicy_food\",\n",
    "  {\"text\": \"I don't like spicy food\"}\n",
    ")\n",
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"occupation\",\n",
    "  {\"text\": \"I am an airline pilot\"}\n",
    ")\n",
    "\n",
    "// That occupation is too lofty - let's overwrite\n",
    "// it with something more... down-to-earth\n",
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"occupation\",\n",
    "  {\"text\": \"I am a tunnel engineer\"}\n",
    ")\n",
    "\n",
    "// now let's check that our occupation memory was overwritten\n",
    "const occupation = await store.get([\"user_123\", \"memories\"], \"occupation\")\n",
    "console.log(occupation.value.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching memories with natural language\n",
    "\n",
    "Now that we've seen how to store and retrieve memories by namespace and key, let's look at how memories are retrieved using semantic search.\n",
    "\n",
    "Imagine that we had a big pile of memories that we wanted to search, and we didn't know the key that corresponds to the memory that we want to retrieve. Semantic search allows us to search our memory store without keys, by performing a natural language query using text embeddings. We demonstrate this in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I am a tunnel engineer (similarity: 0.3070681445327329)\n",
      "Memory: I prefer Italian food (similarity: 0.1435366180543232)\n",
      "Memory: I love pizza (similarity: 0.10650935500808985)\n"
     ]
    }
   ],
   "source": [
    "const memories = await store.search([\"user_123\", \"memories\"], {\n",
    "  query: \"What is my occupation?\",\n",
    "  limit: 3,\n",
    "});\n",
    "\n",
    "for (const memory of memories) {\n",
    "  console.log(`Memory: ${memory.value.text} (similarity: ${memory.score})`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example: Long-term semantic memory in a ReAct agent\n",
    "\n",
    "Let's look at a simple example of providing an agent with long-term memory.\n",
    "\n",
    "Long-term memory can be thought of in two phases: storage, and recall.\n",
    "\n",
    "In the example below we handle storage by giving the agent a tool that it can use to create new memories.\n",
    "\n",
    "To handle recall we'll add a prompt step that queries the memory store using the text from the user's chat message. We'll then inject the results of that query into the system message.\n",
    "\n",
    "### Simple memory storage tool\n",
    "\n",
    "Let's start off by creating a tool that lets the LLM store new memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n",
    "\n",
    "import { z } from \"zod\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const upsertMemoryTool = tool(async (\n",
    "  { content },\n",
    "  config: LangGraphRunnableConfig\n",
    "): Promise<string> => {\n",
    "  const store = config.store as InMemoryStore;\n",
    "  if (!store) {\n",
    "    throw new Error(\"No store provided to tool.\");\n",
    "  }\n",
    "  await store.put(\n",
    "    [\"user_123\", \"memories\"],\n",
    "    uuidv4(), // give each memory its own unique ID\n",
    "    { text: content }\n",
    "  );\n",
    "  return \"Stored memory.\";\n",
    "}, {\n",
    "  name: \"upsert_memory\",\n",
    "  schema: z.object({\n",
    "    content: z.string().describe(\"The content of the memory to store.\"),\n",
    "  }),\n",
    "  description: \"Upsert long-term memories.\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tool above, we use a UUID as the key so that the memory store can accumulate memories endlessly without worrying about key conflicts. We do this instead of accumulating memories into a single object or array because the memory store indexes items by key. Giving each memory its own key in the store allows each memory to be assigned its own unique embedding vector that can be matched to the search query.\n",
    "\n",
    "### Simple semantic recall mechanism\n",
    "\n",
    "Now that we have a tool for storing memories, let's create a prompt function that we can use with `createReactAgent` to handle the recall mechanism.\n",
    "\n",
    "Note that if we weren't using `createReactAgent` here, you could use this same function as the first node in your graph and it would work just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const addMemories = async (\n",
    "  state: typeof MessagesAnnotation.State,\n",
    "  config: LangGraphRunnableConfig\n",
    ") => {\n",
    "  const store = config.store as InMemoryStore;\n",
    "\n",
    "  if (!store) {\n",
    "    throw new Error(\"No store provided to state modifier.\");\n",
    "  }\n",
    "  \n",
    "  // Search based on user's last message\n",
    "  const items = await store.search(\n",
    "    [\"user_123\", \"memories\"], \n",
    "    { \n",
    "      // Assume it's not a complex message\n",
    "      query: state.messages[state.messages.length - 1].content as string,\n",
    "      limit: 4 \n",
    "    }\n",
    "  );\n",
    "\n",
    "  \n",
    "  const memories = items.length \n",
    "    ? `## Memories of user\\n${\n",
    "      items.map(item => `${item.value.text} (similarity: ${item.score})`).join(\"\\n\")\n",
    "    }`\n",
    "    : \"\";\n",
    "\n",
    "  // Add retrieved memories to system message\n",
    "  return [\n",
    "    { role: \"system\", content: `You are a helpful assistant.\\n${memories}` },\n",
    "    ...state.messages\n",
    "  ];\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Finally, let's put it all together into an agent, using `createReactAgent`. Notice that we're not adding a checkpointer here. The examples below will not be reusing message history. All details not contained in the input messages will be coming from the recall mechanism defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const agent = createReactAgent({\n",
    "  llm: new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n",
    "  tools: [upsertMemoryTool],\n",
    "  prompt: addMemories,\n",
    "  store: store\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our sample agent\n",
    "\n",
    "Now that we've got everything put together, let's test it out!\n",
    "\n",
    "First, let's define a helper function that we can use to print messages in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  BaseMessage,\n",
    "  isSystemMessage,\n",
    "  isAIMessage,\n",
    "  isHumanMessage,\n",
    "  isToolMessage,\n",
    "  AIMessage,\n",
    "  HumanMessage,\n",
    "  ToolMessage,\n",
    "  SystemMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "\n",
    "function printMessages(messages: BaseMessage[]) {\n",
    "  for (const message of messages) {\n",
    "    if (isSystemMessage(message)) {\n",
    "      const systemMessage = message as SystemMessage;\n",
    "      console.log(`System: ${systemMessage.content}`);\n",
    "    } else if (isHumanMessage(message)) {\n",
    "      const humanMessage = message as HumanMessage;\n",
    "      console.log(`User: ${humanMessage.content}`);\n",
    "    } else if (isAIMessage(message)) {\n",
    "      const aiMessage = message as AIMessage;\n",
    "      if (aiMessage.content) {\n",
    "        console.log(`Assistant: ${aiMessage.content}`);\n",
    "      }\n",
    "      if (aiMessage.tool_calls) {\n",
    "        for (const toolCall of aiMessage.tool_calls) {\n",
    "          console.log(`\\t${toolCall.name}(${JSON.stringify(toolCall.args)})`);\n",
    "        }\n",
    "      }\n",
    "    } else if (isToolMessage(message)) {\n",
    "      const toolMessage = message as ToolMessage;\n",
    "      console.log(\n",
    "        `\\t\\t${toolMessage.name} -> ${JSON.stringify(toolMessage.content)}`\n",
    "      );\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we run the agent and print the message, we can see that the agent remembers the food preferences that we added to the store at the very beginning of this demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I'm hungry. What should I eat?\n",
      "Assistant: Since you prefer Italian food and love pizza, how about ordering a pizza? You could choose a classic Margherita or customize it with your favorite toppings, making sure to keep it non-spicy. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "let result = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"I'm hungry. What should I eat?\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "printMessages(result.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing new memories\n",
    "\n",
    "Now that we know that the recall mechanism works, let's see if we can get our example agent to store a new memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Please remember that every Thursday is trash day.\n",
      "\tupsert_memory({\"content\":\"Every Thursday is trash day.\"})\n",
      "\t\tupsert_memory -> \"Stored memory.\"\n",
      "Assistant: I've remembered that every Thursday is trash day!\n"
     ]
    }
   ],
   "source": [
    "result = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"Please remember that every Thursday is trash day.\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "printMessages(result.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that it has stored it, let's see if it remembers.\n",
    "\n",
    "Remember - there's no checkpointer here. Every time we invoke the agent it's an entirely new conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: When am I supposed to take out the garbage?\n",
      "Assistant: You take out the garbage every Thursday, as it's trash day for you.\n"
     ]
    }
   ],
   "source": [
    "result = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"When am I supposed to take out the garbage?\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "printMessages(result.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The example above is quite simple, but hopefully it helps you to imagine how you might interweave storage and recall mechanisms into your agents. In the sections below we touch on a few more topics that might help you as you get into more advanced use cases.\n",
    "\n",
    "### Multi-vector indexing\n",
    "\n",
    "You can store and search different aspects of memories separately to improve recall or to omit certain fields from the semantic indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem 2\n",
      "Item: mem2; Score(0.58961641225287)\n",
      "Memory: Ate alone at home\n",
      "Emotion: felt a bit lonely\n"
     ]
    }
   ],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "// Configure store to embed both memory content and emotional context\n",
    "const multiVectorStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: embeddings,\n",
    "    dims: 1536,\n",
    "    fields: [\"memory\", \"emotional_context\"],\n",
    "  },\n",
    "});\n",
    "\n",
    "// Store memories with different content/emotion pairs\n",
    "await multiVectorStore.put([\"user_123\", \"memories\"], \"mem1\", {\n",
    "  memory: \"Had pizza with friends at Mario's\",\n",
    "  emotional_context: \"felt happy and connected\",\n",
    "  this_isnt_indexed: \"I prefer ravioli though\",\n",
    "});\n",
    "await multiVectorStore.put([\"user_123\", \"memories\"], \"mem2\", {\n",
    "  memory: \"Ate alone at home\",\n",
    "  emotional_context: \"felt a bit lonely\",\n",
    "  this_isnt_indexed: \"I like pie\",\n",
    "});\n",
    "\n",
    "// Search focusing on emotional state - matches mem2\n",
    "const results = await multiVectorStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"times they felt isolated\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "console.log(\"Expect mem 2\");\n",
    "\n",
    "for (const r of results) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "  console.log(`Emotion: ${r.value.emotional_context}`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override fields at storage time\n",
    "You can override which fields to embed when storing a specific memory using `put(..., { index: [...fields] })`, regardless of the store's default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem1\n",
      "Item: mem1; Score(0.3375009515587189)\n",
      "Memory: I love spicy food\n",
      "Expect mem2\n",
      "Item: mem2; Score(0.1920732213417712)\n",
      "Memory: I love spicy food\n"
     ]
    }
   ],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const overrideStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: embeddings,\n",
    "    dims: 1536,\n",
    "    // Default to embed memory field\n",
    "    fields: [\"memory\"],\n",
    "  }\n",
    "});\n",
    "\n",
    "// Store one memory with default indexing\n",
    "await overrideStore.put([\"user_123\", \"memories\"], \"mem1\", {\n",
    "  memory: \"I love spicy food\",\n",
    "  context: \"At a Thai restaurant\",\n",
    "});\n",
    "\n",
    "// Store another memory, overriding which fields to embed\n",
    "await overrideStore.put([\"user_123\", \"memories\"], \"mem2\", {\n",
    "  memory: \"I love spicy food\",\n",
    "  context: \"At a Thai restaurant\",\n",
    "  // Override: only embed the context\n",
    "  index: [\"context\"]\n",
    "});\n",
    "\n",
    "// Search about food - matches mem1 (using default field)\n",
    "console.log(\"Expect mem1\");\n",
    "const results2 = await overrideStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"what food do they like\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "for (const r of results2) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "}\n",
    "\n",
    "// Search about restaurant atmosphere - matches mem2 (using overridden field)\n",
    "console.log(\"Expect mem2\");\n",
    "const results3 = await overrideStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"restaurant environment\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "for (const r of results3) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/stream-multiple.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to configure multiple streaming modes at the same time\n",
    "\n",
    "This guide covers how to configure multiple streaming modes at the same time.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "## Define the graph\n",
    "\n",
    "We'll be using a prebuilt ReAct agent for this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from '@langchain/core/tools';\n",
    "import { z } from 'zod';\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "  });\n",
    "\n",
    "const getWeather = tool((input) => {\n",
    "  if ([\"sf\", \"san francisco\", \"san francisco, ca\"].includes(input.location.toLowerCase())) {\n",
    "    return \"It's 60 degrees and foggy.\";\n",
    "  } else {\n",
    "    return \"It's 90 degrees and sunny.\";\n",
    "  }\n",
    "}, {\n",
    "  name: \"get_weather\",\n",
    "  description: \"Call to get the current weather.\",\n",
    "  schema: z.object({\n",
    "    location: z.string().describe(\"Location to get the weather for.\"),\n",
    "  })\n",
    "})\n",
    "\n",
    "const graph = createReactAgent({ llm: model, tools: [getWeather] });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Multiple\n",
    "\n",
    "To get multiple types of streamed chunks, pass an array of values under the `streamMode` key in the second argument to `.stream()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: debug\n",
      "{\n",
      "  type: 'task',\n",
      "  timestamp: '2024-08-30T20:58:58.404Z',\n",
      "  step: 1,\n",
      "  payload: {\n",
      "    id: '768110dd-6004-59f3-8671-6ca699cccd71',\n",
      "    name: 'agent',\n",
      "    input: { messages: [Array] },\n",
      "    triggers: [ 'start:agent' ],\n",
      "    interrupts: []\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: updates\n",
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-A22zqTwumhtW8TMjQ1FxlzCEMBk0R\",\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"tool_calls\": [\n",
      "            {\n",
      "              \"id\": \"call_HAfilebE1q9E9OQHOlL3JYHP\",\n",
      "              \"type\": \"function\",\n",
      "              \"function\": \"[Object]\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 15,\n",
      "            \"promptTokens\": 59,\n",
      "            \"totalTokens\": 74\n",
      "          },\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"system_fingerprint\": \"fp_157b3831f5\"\n",
      "        },\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"get_weather\",\n",
      "            \"args\": {\n",
      "              \"location\": \"San Francisco\"\n",
      "            },\n",
      "            \"type\": \"tool_call\",\n",
      "            \"id\": \"call_HAfilebE1q9E9OQHOlL3JYHP\"\n",
      "          }\n",
      "        ],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 59,\n",
      "          \"output_tokens\": 15,\n",
      "          \"total_tokens\": 74\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: debug\n",
      "{\n",
      "  type: 'task_result',\n",
      "  timestamp: '2024-08-30T20:58:59.072Z',\n",
      "  step: 1,\n",
      "  payload: {\n",
      "    id: '768110dd-6004-59f3-8671-6ca699cccd71',\n",
      "    name: 'agent',\n",
      "    result: [ [Array] ]\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: debug\n",
      "{\n",
      "  type: 'task',\n",
      "  timestamp: '2024-08-30T20:58:59.074Z',\n",
      "  step: 2,\n",
      "  payload: {\n",
      "    id: '76459c18-5621-5893-9b93-13bc1db3ba6d',\n",
      "    name: 'tools',\n",
      "    input: { messages: [Array] },\n",
      "    triggers: [ 'branch:agent:shouldContinue:tools' ],\n",
      "    interrupts: []\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: updates\n",
      "{\n",
      "  tools: {\n",
      "    messages: [\n",
      "      ToolMessage {\n",
      "        \"content\": \"It's 60 degrees and foggy.\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_call_id\": \"call_HAfilebE1q9E9OQHOlL3JYHP\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: debug\n",
      "{\n",
      "  type: 'task_result',\n",
      "  timestamp: '2024-08-30T20:58:59.076Z',\n",
      "  step: 2,\n",
      "  payload: {\n",
      "    id: '76459c18-5621-5893-9b93-13bc1db3ba6d',\n",
      "    name: 'tools',\n",
      "    result: [ [Array] ]\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: debug\n",
      "{\n",
      "  type: 'task',\n",
      "  timestamp: '2024-08-30T20:58:59.077Z',\n",
      "  step: 3,\n",
      "  payload: {\n",
      "    id: '565d8a53-1057-5d83-bda8-ba3fada24b70',\n",
      "    name: 'agent',\n",
      "    input: { messages: [Array] },\n",
      "    triggers: [ 'tools' ],\n",
      "    interrupts: []\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: updates\n",
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-A22zrdeobsBzkiES0C6Twh3p7I344\",\n",
      "        \"content\": \"The weather in San Francisco right now is 60 degrees and foggy.\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 16,\n",
      "            \"promptTokens\": 90,\n",
      "            \"totalTokens\": 106\n",
      "          },\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_157b3831f5\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 90,\n",
      "          \"output_tokens\": 16,\n",
      "          \"total_tokens\": 106\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n",
      "Receiving new event of type: debug\n",
      "{\n",
      "  type: 'task_result',\n",
      "  timestamp: '2024-08-30T20:58:59.640Z',\n",
      "  step: 3,\n",
      "  payload: {\n",
      "    id: '565d8a53-1057-5d83-bda8-ba3fada24b70',\n",
      "    name: 'agent',\n",
      "    result: [ [Array] ]\n",
      "  }\n",
      "}\n",
      "\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let inputs = { messages: [{ role: \"user\", content: \"what's the weather in sf?\" }] };\n",
    "\n",
    "let stream = await graph.stream(inputs, {\n",
    "  streamMode: [\"updates\", \"debug\"],\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(`Receiving new event of type: ${chunk[0]}`);\n",
    "  console.log(chunk[1]);\n",
    "  console.log(\"\\n====\\n\");\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="how-tos/stream-tokens.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "562ddb82",
      "metadata": {},
      "source": [
        "# How to stream LLM tokens from your graph\n",
        "\n",
        "In this example, we will stream tokens from the language model powering an\n",
        "agent. We will use a ReAct agent as an example.\n",
        "\n",
        "<div class=\"admonition info\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "      If you are using a version of <code>@langchain/core</code> < 0.2.3, when calling chat models or LLMs you need to call <code>await model.stream()</code> within your nodes to get token-by-token streaming events, and aggregate final outputs if needed to update the graph state. In later versions of <code>@langchain/core</code>, this occurs automatically, and you can call <code>await model.invoke()</code>.\n",
        "      <br>\n",
        "      For more on how to upgrade <code>@langchain/core</code>, check out <a href=\"https://js.langchain.com/docs/how_to/installation/#installing-integration-packages\">the instructions here</a>.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "This how-to guide closely follows the others in this directory, showing how to\n",
        "incorporate the functionality into a prototypical agent in LangGraph.\n",
        "\n",
        "<div class=\"admonition info\">\n",
        "    <p class=\"admonition-title\">Streaming Support</p>\n",
        "    <p>\n",
        "        Token streaming is supported by many, but not all chat models. Check to see if your LLM integration supports token streaming <a href=\"https://js.langchain.com/docs/integrations/chat/\">here (doc)</a>. Note that some integrations may support <i>general</i> token streaming but lack support for streaming tool calls.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the <code>createReactAgent({ llm, tools })</code> (<a href=\"/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html\">API doc</a>) constructor. This may be more appropriate if you are used to LangChain's <a href=\"https://js.langchain.com/docs/how_to/agent_executor\">AgentExecutor</a> class.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "## Setup\n",
        "\n",
        "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
        "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
        "best-in-class observability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e76833b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "\n",
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
        "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "// process.env.LANGCHAIN_TRACING = \"true\";\n",
        "// process.env.LANGCHAIN_PROJECT = \"Stream Tokens: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab95dc97",
      "metadata": {},
      "source": [
        "## Define the state\n",
        "\n",
        "The state is the interface for all of the nodes in our graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1648124b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import type { BaseMessageLike } from \"@langchain/core/messages\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  messages: Annotation<BaseMessageLike[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da50fbd8",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "First define the tools you want to use. For this simple example, we'll create a placeholder search engine, but see the documentation [here](https://js.langchain.com/docs/how_to/custom_tools) on how to create your own custom tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8f1ae1c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = tool((_) => {\n",
        "  // This is a placeholder for the actual implementation\n",
        "  return \"Cold, with a low of 3℃\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description:\n",
        "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "});\n",
        "\n",
        "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b27cb3",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a prebuilt\n",
        "[ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html).\n",
        "This object will actually run the tools (functions) whenever they are invoked by\n",
        "our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f02278b1",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd55ee5a",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now load the [chat model](https://js.langchain.com/docs/concepts/#chat-models).\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form\n",
        "   of messages, so it needs to be able to work well with them.\n",
        "2. It should work with\n",
        "   [tool calling](https://js.langchain.com/docs/how_to/tool_calling/#passing-tools-to-llms),\n",
        "   meaning it can return function arguments in its response.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c7210e7",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini\",\n",
        "  temperature: 0,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e59248",
      "metadata": {},
      "source": [
        "After you've done this, we should make sure the model knows that it has these\n",
        "tools available to call. We can do this by calling\n",
        "[bindTools](https://v01.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html#bindTools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b4ff23ee",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe67356",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0ba603bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph, END } from \"@langchain/langgraph\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const routeMessage = (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If no tools are called, we can finish (respond to the user)\n",
        "  if (!lastMessage?.tool_calls?.length) {\n",
        "    return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue and call the tools\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "const callModel = async (\n",
        "  state: typeof StateAnnotation.State,\n",
        ") => {\n",
        "  // For versions of @langchain/core < 0.2.3, you must call `.stream()`\n",
        "  // and aggregate the message from chunks instead of calling `.invoke()`.\n",
        "  const { messages } = state;\n",
        "  const responseMessage = await boundModel.invoke(messages);\n",
        "  return { messages: [responseMessage] };\n",
        "};\n",
        "\n",
        "const workflow = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(\"__start__\", \"agent\")\n",
        "  .addConditionalEdges(\"agent\", routeMessage)\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "const agent = workflow.compile();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a88cf20a",
      "metadata": {},
      "outputs": [
        {
          "data": 
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const runnableGraph = agent.getGraph();\n",
        "const image = await runnableGraph.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00e15e2",
      "metadata": {},
      "source": [
        "## Streaming LLM Tokens\n",
        "\n",
        "You can access the LLM tokens as they are produced by each node with two methods:\n",
        "\n",
        "- The `stream` method along with `streamMode: \"messages\"`\n",
        "- The `streamEvents` method\n",
        "\n",
        "### The stream method\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Compatibility</p>\n",
        "    <p>\n",
        "        This section requires <code>@langchain/langgraph>=0.2.20</code>. For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "For this method, you must be using an LLM that supports streaming as well (e.g. `new ChatOpenAI({ model: \"gpt-4o-mini\" })`) or call `.stream` on the internal LLM call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5af113ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ai MESSAGE TOOL CALL CHUNK: \n",
            "ai MESSAGE TOOL CALL CHUNK: {\"\n",
            "ai MESSAGE TOOL CALL CHUNK: query\n",
            "ai MESSAGE TOOL CALL CHUNK: \":\"\n",
            "ai MESSAGE TOOL CALL CHUNK: current\n",
            "ai MESSAGE TOOL CALL CHUNK:  weather\n",
            "ai MESSAGE TOOL CALL CHUNK:  in\n",
            "ai MESSAGE TOOL CALL CHUNK:  Nepal\n",
            "ai MESSAGE TOOL CALL CHUNK: \"}\n",
            "ai MESSAGE CONTENT: \n",
            "tool MESSAGE CONTENT: Cold, with a low of 3℃\n",
            "ai MESSAGE CONTENT: \n",
            "ai MESSAGE CONTENT: The\n",
            "ai MESSAGE CONTENT:  current\n",
            "ai MESSAGE CONTENT:  weather\n",
            "ai MESSAGE CONTENT:  in\n",
            "ai MESSAGE CONTENT:  Nepal\n",
            "ai MESSAGE CONTENT:  is\n",
            "ai MESSAGE CONTENT:  cold\n",
            "ai MESSAGE CONTENT: ,\n",
            "ai MESSAGE CONTENT:  with\n",
            "ai MESSAGE CONTENT:  a\n",
            "ai MESSAGE CONTENT:  low\n",
            "ai MESSAGE CONTENT:  temperature\n",
            "ai MESSAGE CONTENT:  of\n",
            "ai MESSAGE CONTENT:  \n",
            "ai MESSAGE CONTENT: 3\n",
            "ai MESSAGE CONTENT: ℃\n",
            "ai MESSAGE CONTENT: .\n",
            "ai MESSAGE CONTENT: \n"
          ]
        }
      ],
      "source": [
        "import { isAIMessageChunk } from \"@langchain/core/messages\";\n",
        "\n",
        "const stream = await agent.stream(\n",
        "  { messages: [{ role: \"user\", content: \"What's the current weather in Nepal?\" }] },\n",
        "  { streamMode: \"messages\" },\n",
        ");\n",
        "\n",
        "for await (const [message, _metadata] of stream) {\n",
        "  if (isAIMessageChunk(message) && message.tool_call_chunks?.length) {\n",
        "    console.log(`${message.getType()} MESSAGE TOOL CALL CHUNK: ${message.tool_call_chunks[0].args}`);\n",
        "  } else {\n",
        "    console.log(`${message.getType()} MESSAGE CONTENT: ${message.content}`);\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd29662",
      "metadata": {},
      "source": [
        "### Disabling streaming\n",
        "\n",
        "If you wish to disable streaming for a given node or model call, you can add a `\"nostream\"` tag. Here's an example where we add an initial node with an LLM call that will not be streamed in the final output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "94209d4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOGGED UNSTREAMED MESSAGE I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\n",
            "ai MESSAGE TOOL CALL CHUNK: \n",
            "ai MESSAGE TOOL CALL CHUNK: {\"\n",
            "ai MESSAGE TOOL CALL CHUNK: query\n",
            "ai MESSAGE TOOL CALL CHUNK: \":\"\n",
            "ai MESSAGE TOOL CALL CHUNK: current\n",
            "ai MESSAGE TOOL CALL CHUNK:  weather\n",
            "ai MESSAGE TOOL CALL CHUNK:  in\n",
            "ai MESSAGE TOOL CALL CHUNK:  Nepal\n",
            "ai MESSAGE TOOL CALL CHUNK: \"}\n",
            "ai MESSAGE CONTENT: \n",
            "tool MESSAGE CONTENT: Cold, with a low of 3℃\n",
            "ai MESSAGE CONTENT: \n",
            "ai MESSAGE CONTENT: The\n",
            "ai MESSAGE CONTENT:  current\n",
            "ai MESSAGE CONTENT:  weather\n",
            "ai MESSAGE CONTENT:  in\n",
            "ai MESSAGE CONTENT:  Nepal\n",
            "ai MESSAGE CONTENT:  is\n",
            "ai MESSAGE CONTENT:  cold\n",
            "ai MESSAGE CONTENT: ,\n",
            "ai MESSAGE CONTENT:  with\n",
            "ai MESSAGE CONTENT:  a\n",
            "ai MESSAGE CONTENT:  low\n",
            "ai MESSAGE CONTENT:  temperature\n",
            "ai MESSAGE CONTENT:  of\n",
            "ai MESSAGE CONTENT:  \n",
            "ai MESSAGE CONTENT: 3\n",
            "ai MESSAGE CONTENT: ℃\n",
            "ai MESSAGE CONTENT: .\n",
            "ai MESSAGE CONTENT: \n"
          ]
        }
      ],
      "source": [
        "import { RunnableLambda } from \"@langchain/core/runnables\";\n",
        "\n",
        "const unstreamed = async (_: typeof StateAnnotation.State) => {\n",
        "  const model = new ChatOpenAI({\n",
        "    model: \"gpt-4o-mini\",\n",
        "    temperature: 0,\n",
        "  });\n",
        "  const res = await model.invoke(\"How are you?\");\n",
        "  console.log(\"LOGGED UNSTREAMED MESSAGE\", res.content);\n",
        "  // Don't update the state, this is just to show a call that won't be streamed\n",
        "  return {};\n",
        "}\n",
        "\n",
        "const agentWithNoStream = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"unstreamed\",\n",
        "    // Add a \"nostream\" tag to the entire node\n",
        "    RunnableLambda.from(unstreamed).withConfig({\n",
        "      tags: [\"nostream\"]\n",
        "    })\n",
        "  )\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  // Run the unstreamed node before the agent\n",
        "  .addEdge(\"__start__\", \"unstreamed\")\n",
        "  .addEdge(\"unstreamed\", \"agent\")\n",
        "  .addConditionalEdges(\"agent\", routeMessage)\n",
        "  .addEdge(\"tools\", \"agent\")\n",
        "  .compile();\n",
        "\n",
        "const stream = await agentWithNoStream.stream(\n",
        "  { messages: [{ role: \"user\", content: \"What's the current weather in Nepal?\" }] },\n",
        "  { streamMode: \"messages\" },\n",
        ");\n",
        "\n",
        "for await (const [message, _metadata] of stream) {\n",
        "  if (isAIMessageChunk(message) && message.tool_call_chunks?.length) {\n",
        "    console.log(`${message.getType()} MESSAGE TOOL CALL CHUNK: ${message.tool_call_chunks[0].args}`);\n",
        "  } else {\n",
        "    console.log(`${message.getType()} MESSAGE CONTENT: ${message.content}`);\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f307ce58",
      "metadata": {},
      "source": [
        "If you removed the tag from the `\"unstreamed\"` node, the result of the model call within would also be in the final stream."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8332924",
      "metadata": {},
      "source": [
        "### The streamEvents method\n",
        "\n",
        "You can also use the `streamEvents` method like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ec7c31a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    name: 'search',\n",
            "    args: '',\n",
            "    id: 'call_Qpd6frHt0yUYWynRbZEXF3le',\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: '{\"',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: 'query',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: '\":\"',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: 'current',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: ' weather',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: ' today',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    name: undefined,\n",
            "    args: '\"}',\n",
            "    id: undefined,\n",
            "    index: 0,\n",
            "    type: 'tool_call_chunk'\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "const eventStream = await agent.streamEvents(\n",
        "  { messages: [{ role: \"user\", content: \"What's the weather like today?\" }] },\n",
        "  {\n",
        "    version: \"v2\",\n",
        "  }\n",
        ");\n",
        "\n",
        "for await (const { event, data } of eventStream) {\n",
        "  if (event === \"on_chat_model_stream\" && isAIMessageChunk(data.chunk)) {\n",
        "    if (data.chunk.tool_call_chunks !== undefined && data.chunk.tool_call_chunks.length > 0) {\n",
        "      console.log(data.chunk.tool_call_chunks);\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/stream-updates.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "562ddb82",
      "metadata": {},
      "source": [
        "# How to stream state updates of your graph\n",
        "\n",
        "LangGraph supports multiple streaming modes. The main ones are:\n",
        "\n",
        "- `values`: This streaming mode streams back values of the graph. This is the\n",
        "  **full state of the graph** after each node is called.\n",
        "- `updates`: This streaming mode streams back updates to the graph. This is the\n",
        "  **update to the state of the graph** after each node is called.\n",
        "\n",
        "This guide covers `streamMode=\"updates\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e76833b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk-...\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab95dc97",
      "metadata": {},
      "source": [
        "## Define the state\n",
        "\n",
        "The state is the interface for all of the nodes in our graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1648124b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da50fbd8",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will\n",
        "use create a placeholder search engine. However, it is really easy to create\n",
        "your own tools - see documentation\n",
        "[here](https://js.langchain.com/docs/how_to/custom_tools) on how to do\n",
        "that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8f1ae1c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = tool(async ({ query: _query }: { query: string }) => {\n",
        "  // This is a placeholder for the actual implementation\n",
        "  return \"Cold, with a low of 3℃\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description:\n",
        "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "});\n",
        "\n",
        "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b27cb3",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a simple\n",
        "[ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html).\n",
        "This object will actually run the tools (functions) whenever they are invoked by\n",
        "our LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f02278b1",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd55ee5a",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now we will load the\n",
        "[chat model](https://js.langchain.com/docs/concepts/chat_models/).\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form\n",
        "   of messages, so it needs to be able to work well with them.\n",
        "2. It should work with\n",
        "   [tool calling](https://js.langchain.com/docs/how_to/tool_calling/#passing-tools-to-llms),\n",
        "   meaning it can return function arguments in its response.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c7210e7",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({ model: \"gpt-4o\" });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e59248",
      "metadata": {},
      "source": [
        "After we've done this, we should make sure the model knows that it has these\n",
        "tools available to call. We can do this by calling\n",
        "[bindTools](https://v01.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html#bindTools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b4ff23ee",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe67356",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0ba603bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const routeMessage = (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If no tools are called, we can finish (respond to the user)\n",
        "  if (!lastMessage?.tool_calls?.length) {\n",
        "    return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue and call the tools\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "const callModel = async (\n",
        "  state: typeof StateAnnotation.State,\n",
        ") => {\n",
        "  const { messages } = state;\n",
        "  const responseMessage = await boundModel.invoke(messages);\n",
        "  return { messages: [responseMessage] };\n",
        "};\n",
        "\n",
        "const workflow = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(START, \"agent\")\n",
        "  .addConditionalEdges(\"agent\", routeMessage)\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "const graph = workflow.compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ab3ad3",
      "metadata": {},
      "source": [
        "## Stream updates\n",
        "\n",
        "We can now interact with the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cbcf7c39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: agent\n",
            "{\n",
            "  messages: [\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-9y654VypbD3kE1xM8v4xaAHzZEOXa\",\n",
            "      \"content\": \"\",\n",
            "      \"additional_kwargs\": {\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"id\": \"call_OxlOhnROermwae2LPs9SanmD\",\n",
            "            \"type\": \"function\",\n",
            "            \"function\": \"[Object]\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 17,\n",
            "          \"promptTokens\": 70,\n",
            "          \"totalTokens\": 87\n",
            "        },\n",
            "        \"finish_reason\": \"tool_calls\",\n",
            "        \"system_fingerprint\": \"fp_3aa7262c27\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"query\": \"current weather in San Francisco\"\n",
            "          },\n",
            "          \"type\": \"tool_call\",\n",
            "          \"id\": \"call_OxlOhnROermwae2LPs9SanmD\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 70,\n",
            "        \"output_tokens\": 17,\n",
            "        \"total_tokens\": 87\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "Receiving update from node: tools\n",
            "{\n",
            "  messages: [\n",
            "    ToolMessage {\n",
            "      \"content\": \"Cold, with a low of 3℃\",\n",
            "      \"name\": \"search\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"call_OxlOhnROermwae2LPs9SanmD\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "Receiving update from node: agent\n",
            "{\n",
            "  messages: [\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-9y654dZ0zzZhPYm6lb36FkG1Enr3p\",\n",
            "      \"content\": \"It looks like it's currently quite cold in San Francisco, with a low temperature of around 3°C. Make sure to dress warmly!\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 28,\n",
            "          \"promptTokens\": 103,\n",
            "          \"totalTokens\": 131\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_3aa7262c27\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 103,\n",
            "        \"output_tokens\": 28,\n",
            "        \"total_tokens\": 131\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs = { messages: [{ role: \"user\",  content: \"what's the weather in sf\" }] };\n",
        "\n",
        "for await (\n",
        "  const chunk of await graph.stream(inputs, {\n",
        "    streamMode: \"updates\",\n",
        "  })\n",
        ") {\n",
        "  for (const [node, values] of Object.entries(chunk)) {\n",
        "    console.log(`Receiving update from node: ${node}`);\n",
        "    console.log(values);\n",
        "    console.log(\"\\n====\\n\");\n",
        "  }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/stream-values.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "562ddb82",
      "metadata": {},
      "source": [
        "# How to stream full state of your graph\n",
        "\n",
        "LangGraph supports multiple streaming modes. The main ones are:\n",
        "\n",
        "- `values`: This streaming mode streams back values of the graph. This is the\n",
        "  **full state of the graph** after each node is called.\n",
        "- `updates`: This streaming mode streams back updates to the graph. This is the\n",
        "  **update to the state of the graph** after each node is called.\n",
        "\n",
        "This guide covers `streamMode=\"values\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e76833b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk-...\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab95dc97",
      "metadata": {},
      "source": [
        "## Define the state\n",
        "\n",
        "The state is the interface for all of the nodes in our graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1648124b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da50fbd8",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will\n",
        "use create a placeholder search engine. However, it is really easy to create\n",
        "your own tools - see documentation\n",
        "[here](https://js.langchain.com/docs/how_to/custom_tools) on how to do\n",
        "that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8f1ae1c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = tool(async ({ query: _query }: { query: string }) => {\n",
        "  // This is a placeholder for the actual implementation\n",
        "  return \"Cold, with a low of 3℃\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description:\n",
        "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "});\n",
        "\n",
        "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b27cb3",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a simple\n",
        "[ToolNode](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html).\n",
        "This object will actually run the tools (functions) whenever they are invoked by\n",
        "our LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f02278b1",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd55ee5a",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now we will load the\n",
        "[chat model](https://js.langchain.com/docs/concepts/chat_models/).\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form\n",
        "   of messages, so it needs to be able to work well with them.\n",
        "2. It should work with\n",
        "   [tool calling](https://js.langchain.com/docs/how_to/tool_calling/#passing-tools-to-llms),\n",
        "   meaning it can return function arguments in its response.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c7210e7",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({ model: \"gpt-4o\" });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e59248",
      "metadata": {},
      "source": [
        "After we've done this, we should make sure the model knows that it has these\n",
        "tools available to call. We can do this by calling\n",
        "[bindTools](https://v01.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html#bindTools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b4ff23ee",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe67356",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0ba603bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const routeMessage = (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If no tools are called, we can finish (respond to the user)\n",
        "  if (!lastMessage?.tool_calls?.length) {\n",
        "    return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue and call the tools\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "const callModel = async (\n",
        "  state: typeof StateAnnotation.State,\n",
        ") => {\n",
        "  // For versions of @langchain/core < 0.2.3, you must call `.stream()`\n",
        "  // and aggregate the message from chunks instead of calling `.invoke()`.\n",
        "  const { messages } = state;\n",
        "  const responseMessage = await boundModel.invoke(messages);\n",
        "  return { messages: [responseMessage] };\n",
        "};\n",
        "\n",
        "const workflow = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(START, \"agent\")\n",
        "  .addConditionalEdges(\"agent\", routeMessage)\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "const graph = workflow.compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ab3ad3",
      "metadata": {},
      "source": [
        "## Stream values\n",
        "\n",
        "We can now interact with the agent. Between interactions you can get and update\n",
        "state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cbcf7c39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ [ 'user', \"what's the weather in sf\" ] ]\n",
            "\n",
            "====\n",
            "\n",
            "[\n",
            "  [ 'user', \"what's the weather in sf\" ],\n",
            "  AIMessage {\n",
            "    \"id\": \"chatcmpl-9y660d49eLzT7DZeBk2ZmX8C5f0LU\",\n",
            "    \"content\": \"\",\n",
            "    \"additional_kwargs\": {\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\",\n",
            "          \"type\": \"function\",\n",
            "          \"function\": \"[Object]\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    \"response_metadata\": {\n",
            "      \"tokenUsage\": {\n",
            "        \"completionTokens\": 17,\n",
            "        \"promptTokens\": 70,\n",
            "        \"totalTokens\": 87\n",
            "      },\n",
            "      \"finish_reason\": \"tool_calls\",\n",
            "      \"system_fingerprint\": \"fp_3aa7262c27\"\n",
            "    },\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"name\": \"search\",\n",
            "        \"args\": {\n",
            "          \"query\": \"current weather in San Francisco\"\n",
            "        },\n",
            "        \"type\": \"tool_call\",\n",
            "        \"id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\"\n",
            "      }\n",
            "    ],\n",
            "    \"invalid_tool_calls\": [],\n",
            "    \"usage_metadata\": {\n",
            "      \"input_tokens\": 70,\n",
            "      \"output_tokens\": 17,\n",
            "      \"total_tokens\": 87\n",
            "    }\n",
            "  }\n",
            "]\n",
            "\n",
            "====\n",
            "\n",
            "[\n",
            "  [ 'user', \"what's the weather in sf\" ],\n",
            "  AIMessage {\n",
            "    \"id\": \"chatcmpl-9y660d49eLzT7DZeBk2ZmX8C5f0LU\",\n",
            "    \"content\": \"\",\n",
            "    \"additional_kwargs\": {\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\",\n",
            "          \"type\": \"function\",\n",
            "          \"function\": \"[Object]\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    \"response_metadata\": {\n",
            "      \"tokenUsage\": {\n",
            "        \"completionTokens\": 17,\n",
            "        \"promptTokens\": 70,\n",
            "        \"totalTokens\": 87\n",
            "      },\n",
            "      \"finish_reason\": \"tool_calls\",\n",
            "      \"system_fingerprint\": \"fp_3aa7262c27\"\n",
            "    },\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"name\": \"search\",\n",
            "        \"args\": {\n",
            "          \"query\": \"current weather in San Francisco\"\n",
            "        },\n",
            "        \"type\": \"tool_call\",\n",
            "        \"id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\"\n",
            "      }\n",
            "    ],\n",
            "    \"invalid_tool_calls\": [],\n",
            "    \"usage_metadata\": {\n",
            "      \"input_tokens\": 70,\n",
            "      \"output_tokens\": 17,\n",
            "      \"total_tokens\": 87\n",
            "    }\n",
            "  },\n",
            "  ToolMessage {\n",
            "    \"content\": \"Cold, with a low of 3℃\",\n",
            "    \"name\": \"search\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {},\n",
            "    \"tool_call_id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\"\n",
            "  }\n",
            "]\n",
            "\n",
            "====\n",
            "\n",
            "[\n",
            "  [ 'user', \"what's the weather in sf\" ],\n",
            "  AIMessage {\n",
            "    \"id\": \"chatcmpl-9y660d49eLzT7DZeBk2ZmX8C5f0LU\",\n",
            "    \"content\": \"\",\n",
            "    \"additional_kwargs\": {\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\",\n",
            "          \"type\": \"function\",\n",
            "          \"function\": \"[Object]\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    \"response_metadata\": {\n",
            "      \"tokenUsage\": {\n",
            "        \"completionTokens\": 17,\n",
            "        \"promptTokens\": 70,\n",
            "        \"totalTokens\": 87\n",
            "      },\n",
            "      \"finish_reason\": \"tool_calls\",\n",
            "      \"system_fingerprint\": \"fp_3aa7262c27\"\n",
            "    },\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"name\": \"search\",\n",
            "        \"args\": {\n",
            "          \"query\": \"current weather in San Francisco\"\n",
            "        },\n",
            "        \"type\": \"tool_call\",\n",
            "        \"id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\"\n",
            "      }\n",
            "    ],\n",
            "    \"invalid_tool_calls\": [],\n",
            "    \"usage_metadata\": {\n",
            "      \"input_tokens\": 70,\n",
            "      \"output_tokens\": 17,\n",
            "      \"total_tokens\": 87\n",
            "    }\n",
            "  },\n",
            "  ToolMessage {\n",
            "    \"content\": \"Cold, with a low of 3℃\",\n",
            "    \"name\": \"search\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {},\n",
            "    \"tool_call_id\": \"call_iD5Wk4vPsTckffDKJpEQaMkg\"\n",
            "  },\n",
            "  AIMessage {\n",
            "    \"id\": \"chatcmpl-9y660ZKNXvziVJze0X5aTlZ5IoN35\",\n",
            "    \"content\": \"Currently, in San Francisco, it's cold with a temperature of around 3℃ (37.4°F).\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {\n",
            "      \"tokenUsage\": {\n",
            "        \"completionTokens\": 23,\n",
            "        \"promptTokens\": 103,\n",
            "        \"totalTokens\": 126\n",
            "      },\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"system_fingerprint\": \"fp_3aa7262c27\"\n",
            "    },\n",
            "    \"tool_calls\": [],\n",
            "    \"invalid_tool_calls\": [],\n",
            "    \"usage_metadata\": {\n",
            "      \"input_tokens\": 103,\n",
            "      \"output_tokens\": 23,\n",
            "      \"total_tokens\": 126\n",
            "    }\n",
            "  }\n",
            "]\n",
            "\n",
            "====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs = { messages: [{ role: \"user\", content: \"what's the weather in sf\" }] };\n",
        "\n",
        "for await (\n",
        "  const chunk of await graph.stream(inputs, {\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  console.log(chunk[\"messages\"]);\n",
        "  console.log(\"\\n====\\n\");\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/streaming-content.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c4bd28",
   "metadata": {},
   "source": [
    "# How to stream custom data\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/streaming/\">\n",
    "                    Streaming\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://js.langchain.com/docs/how_to/streaming#using-stream-events\">\n",
    "                    streamEvents API\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://js.langchain.com/docs/concepts/chat_models\">\n",
    "                    Chat Models\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://js.langchain.com/docs/concepts/tools\">\n",
    "                    Tools\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "The most common use case for streaming from inside a node is to stream LLM tokens, but you may also want to stream custom data.\n",
    "\n",
    "For example, if you have a long-running tool call, you can dispatch custom events between the steps and use these custom events to monitor progress. You could also surface these custom events to an end user of your application to show them how the current task is progressing.\n",
    "\n",
    "You can do so in two ways:\n",
    "\n",
    "* using your graph's `.stream` method with `streamMode: \"custom\"`\n",
    "* emitting custom events using [`dispatchCustomEvents`](https://js.langchain.com/docs/how_to/callbacks_custom_events/) with `streamEvents`.\n",
    "\n",
    "Below we'll see how to use both APIs.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install our required packages:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12297071",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29814253-ca9b-4844-a8a5-d6b19fbdbdba",
   "metadata": {},
   "source": [
    "## Stream custom data using .stream\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Compatibility</p>\n",
    "    <p>\n",
    "        This section requires <code>@langchain/langgraph>=0.2.20</code>. For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729644a-b65f-4e69-ad45-f2e88ffb4e9d",
   "metadata": {},
   "source": [
    "### Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9731c40f-5ce7-460d-b2ad-33185529c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  StateGraph,\n",
    "  MessagesAnnotation,\n",
    "  LangGraphRunnableConfig,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const myNode = async (\n",
    "  _state: typeof MessagesAnnotation.State,\n",
    "  config: LangGraphRunnableConfig\n",
    ") => {\n",
    "  const chunks = [\n",
    "    \"Four\",\n",
    "    \"score\",\n",
    "    \"and\",\n",
    "    \"seven\",\n",
    "    \"years\",\n",
    "    \"ago\",\n",
    "    \"our\",\n",
    "    \"fathers\",\n",
    "    \"...\",\n",
    "  ];\n",
    "  for (const chunk of chunks) {\n",
    "    // write the chunk to be streamed using streamMode=custom\n",
    "    // Only populated if one of the passed stream modes is \"custom\".\n",
    "    config.writer?.(chunk);\n",
    "  }\n",
    "  return {\n",
    "    messages: [{\n",
    "      role: \"assistant\",\n",
    "      content: chunks.join(\" \"),\n",
    "    }],\n",
    "  };\n",
    "};\n",
    "\n",
    "const graph = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"model\", myNode)\n",
    "  .addEdge(\"__start__\", \"model\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd69eed-9624-4640-b0af-c9f82b190900",
   "metadata": {},
   "source": [
    "### Stream content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a91b15-82c7-443c-acb6-a7406df15cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four\n",
      "score\n",
      "and\n",
      "seven\n",
      "years\n",
      "ago\n",
      "our\n",
      "fathers\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "const inputs = [{\n",
    "  role: \"user\",\n",
    "  content: \"What are you thinking about?\",\n",
    "}];\n",
    "\n",
    "const stream = await graph.stream(\n",
    "  { messages: inputs },\n",
    "  { streamMode: \"custom\" }\n",
    ");\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9f1f0-c170-40dc-9c22-289483dfbc99",
   "metadata": {},
   "source": [
    "You will likely need to use [multiple streaming modes](https://langchain-ai.github.io/langgraphjs/how-tos/stream-multiple/) as you will\n",
    "want access to both the custom data and the state updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ed22d4-6ce6-4b04-a68b-2ea516e3ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'custom', 'Four' ]\n",
      "[ 'custom', 'score' ]\n",
      "[ 'custom', 'and' ]\n",
      "[ 'custom', 'seven' ]\n",
      "[ 'custom', 'years' ]\n",
      "[ 'custom', 'ago' ]\n",
      "[ 'custom', 'our' ]\n",
      "[ 'custom', 'fathers' ]\n",
      "[ 'custom', '...' ]\n",
      "[ 'updates', { model: { messages: [Array] } } ]\n"
     ]
    }
   ],
   "source": [
    "const streamMultiple = await graph.stream(\n",
    "  { messages: inputs },\n",
    "  { streamMode: [\"custom\", \"updates\"] }\n",
    ");\n",
    "\n",
    "for await (const chunk of streamMultiple) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca976d6a-7c64-4603-8bb4-dee95428c33d",
   "metadata": {},
   "source": [
    "## Stream custom data using .streamEvents\n",
    "\n",
    "If you are already using graph's `.streamEvents` method in your workflow, you can also stream custom data by emitting custom events using `dispatchCustomEvents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390a9fe-2d5f-4e82-a1ea-c7c0186b8559",
   "metadata": {},
   "source": [
    "### Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486a01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { dispatchCustomEvent } from \"@langchain/core/callbacks/dispatch\";\n",
    "\n",
    "const graphNode = async (_state: typeof MessagesAnnotation.State) => {\n",
    "  const chunks = [\n",
    "    \"Four\",\n",
    "    \"score\",\n",
    "    \"and\",\n",
    "    \"seven\",\n",
    "    \"years\",\n",
    "    \"ago\",\n",
    "    \"our\",\n",
    "    \"fathers\",\n",
    "    \"...\",\n",
    "  ];\n",
    "  for (const chunk of chunks) {\n",
    "    await dispatchCustomEvent(\"my_custom_event\", { chunk });\n",
    "  }\n",
    "  return {\n",
    "    messages: [{\n",
    "      role: \"assistant\",\n",
    "      content: chunks.join(\" \"),\n",
    "    }],\n",
    "  };\n",
    "};\n",
    "\n",
    "const graphWithDispatch = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"model\", graphNode)\n",
    "  .addEdge(\"__start__\", \"model\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcded03-6776-405e-afae-005a3212d3e4",
   "metadata": {},
   "source": [
    "### Stream content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce773a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four|\n",
      "score|\n",
      "and|\n",
      "seven|\n",
      "years|\n",
      "ago|\n",
      "our|\n",
      "fathers|\n",
      "...|\n"
     ]
    }
   ],
   "source": [
    "const eventStream = await graphWithDispatch.streamEvents(\n",
    "  {\n",
    "    messages: [{\n",
    "      role: \"user\",\n",
    "      content: \"What are you thinking about?\",\n",
    "    }]\n",
    "  },\n",
    "  {\n",
    "    version: \"v2\",\n",
    "  },\n",
    ");\n",
    "\n",
    "for await (const { event, name, data } of eventStream) {\n",
    "  if (event === \"on_custom_event\" && name === \"my_custom_event\") {\n",
    "    console.log(`${data.chunk}|`);\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/streaming-events-from-within-tools.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23ced4e-dc29-43be-9f94-0c36bb181b8a",
   "metadata": {},
   "source": [
    "# How to stream events from within a tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044eeb8-4074-4f9c-8a62-962488744557",
   "metadata": {},
   "source": [
    "If your LangGraph graph needs to use tools that call LLMs (or any other LangChain `Runnable` objects -- other graphs, LCEL chains, retrievers, etc.), you might want to stream events from the underlying `Runnable`. This guide shows how you can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f60af-43ea-4aa6-847a-df8cc47065f5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core zod\n",
    "```\n",
    "\n",
    "```typescript\n",
    "process.env.ANTHROPIC_API_KEY = 'YOUR_API_KEY'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d02ebb-c2e1-4ef7-b187-810d55139317",
   "metadata": {},
   "source": [
    "## Define graph and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a1760-a063-4d05-8c6f-9d16bc31fa82",
   "metadata": {},
   "source": [
    "We'll use a prebuilt ReAct agent for this guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb38dd9-74d8-456d-9e39-4655f2bf3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-20240620\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "const getItems = tool(\n",
    "  async (input, config) => {\n",
    "    const template = ChatPromptTemplate.fromMessages([\n",
    "      [\n",
    "        \"human\",\n",
    "        \"Can you tell me what kind of items i might find in the following place: '{place}'. \" +\n",
    "          \"List at least 3 such items separating them by a comma. And include a brief description of each item..\",\n",
    "      ],\n",
    "    ]);\n",
    "\n",
    "    const modelWithConfig = model.withConfig({\n",
    "      runName: \"Get Items LLM\",\n",
    "      tags: [\"tool_llm\"],\n",
    "    });\n",
    "\n",
    "    const chain = template.pipe(modelWithConfig);\n",
    "    const result = await chain.invoke(input, config);\n",
    "    return result.content;\n",
    "  },\n",
    "  {\n",
    "    name: \"get_items\",\n",
    "    description: \"Use this tool to look up which items are in the given place.\",\n",
    "    schema: z.object({\n",
    "      place: z.string().describe(\"The place to look up items for. E.g 'shelf'\"),\n",
    "    }),\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17279b8a-049d-483d-af63-8a875098e71f",
   "metadata": {},
   "source": [
    "We're adding a custom tag (`tool_llm`) to our LLM runnable within the tool. This will allow us to filter events that we'll stream from the compiled graph (`agent`) Runnable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7254310e-7016-45f7-9795-6d52a1160086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const agent = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: [getItems],\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d88960-a66b-4699-adee-c12d40b4318a",
   "metadata": {},
   "source": [
    "## Stream events from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4399d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ type: 'ai', content: 'Here' }\n",
      "{ type: 'ai', content: ' are three items you might' }\n",
      "{ type: 'ai', content: ' find on a shelf,' }\n",
      "{ type: 'ai', content: ' along with brief' }\n",
      "{ type: 'ai', content: ' descriptions:\\n\\n1.' }\n",
      "{ type: 'ai', content: ' Books' }\n",
      "{ type: 'ai', content: ': Boun' }\n",
      "{ type: 'ai', content: 'd collections of printe' }\n",
      "{ type: 'ai', content: 'd pages' }\n",
      "{ type: 'ai', content: ' containing' }\n",
      "{ type: 'ai', content: ' various' }\n",
      "{ type: 'ai', content: ' forms' }\n",
      "{ type: 'ai', content: ' of literature, information' }\n",
      "{ type: 'ai', content: ', or reference' }\n",
      "{ type: 'ai', content: ' material.\\n\\n2.' }\n",
      "{ type: 'ai', content: ' Picture' }\n",
      "{ type: 'ai', content: ' frames: Decorative' }\n",
      "{ type: 'ai', content: ' borders' }\n",
      "{ type: 'ai', content: ' used to display an' }\n",
      "{ type: 'ai', content: 'd protect photographs, artwork' }\n",
      "{ type: 'ai', content: ', or other visual memor' }\n",
      "{ type: 'ai', content: 'abilia.\\n\\n3' }\n",
      "{ type: 'ai', content: '. Pot' }\n",
      "{ type: 'ai', content: 'ted plants: Small' }\n",
      "{ type: 'ai', content: ' indoor' }\n",
      "{ type: 'ai', content: ' plants in' }\n",
      "{ type: 'ai', content: ' containers, often used for' }\n",
      "{ type: 'ai', content: ' decoration or to add a' }\n",
      "{ type: 'ai', content: ' touch of nature to indoor' }\n",
      "{ type: 'ai', content: ' spaces.' }\n"
     ]
    }
   ],
   "source": [
    "let finalEvent;\n",
    "\n",
    "for await (const event of agent.streamEvents(\n",
    "  {\n",
    "    messages: [\n",
    "      [\n",
    "        \"human\",\n",
    "        \"what items are on the shelf? You should call the get_items tool.\",\n",
    "      ],\n",
    "    ],\n",
    "  },\n",
    "  {\n",
    "    version: \"v2\",\n",
    "  },\n",
    "  {\n",
    "    includeTags: [\"tool_llm\"],\n",
    "  }\n",
    ")) {\n",
    "  if (\"chunk\" in event.data) {\n",
    "    console.dir({\n",
    "      type: event.data.chunk._getType(),\n",
    "      content: event.data.chunk.content,\n",
    "    })\n",
    "  }\n",
    "  finalEvent = event;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8902e-935b-4724-8b5d-551b7674fd34",
   "metadata": {},
   "source": [
    "Let's inspect the last event to get the final list of messages from the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca382c1f-b1c7-4c8a-bd9b-7a873b891b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  type: 'ai',\n",
      "  content: 'Here are three items you might find on a shelf, along with brief descriptions:\\n' +\n",
      "    '\\n' +\n",
      "    '1. Books: Bound collections of printed pages containing various forms of literature, information, or reference material.\\n' +\n",
      "    '\\n' +\n",
      "    '2. Picture frames: Decorative borders used to display and protect photographs, artwork, or other visual memorabilia.\\n' +\n",
      "    '\\n' +\n",
      "    '3. Potted plants: Small indoor plants in containers, often used for decoration or to add a touch of nature to indoor spaces.',\n",
      "  tool_calls: []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const finalMessage = finalEvent?.data.output;\n",
    "console.dir(\n",
    "  {\n",
    "    type: finalMessage._getType(),\n",
    "    content: finalMessage.content,\n",
    "    tool_calls: finalMessage.tool_calls,\n",
    "  },\n",
    "  { depth: null }\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9457c-5665-4cd5-9a99-d54c84270616",
   "metadata": {},
   "source": [
    "You can see that the content of the `ToolMessage` is the same as the output we streamed above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/streaming-from-final-node.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c4bd28",
   "metadata": {},
   "source": [
    "# How to stream from the final node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964686a6-8fed-4360-84d2-958c48186008",
   "metadata": {},
   "source": [
    "One common pattern for graphs is to stream LLM tokens from inside the final node only. This guide demonstrates how you can do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f994ca-28e7-4379-a1c9-8c1682773b5f",
   "metadata": {},
   "source": [
    "## Define model and tools\n",
    "\n",
    "First, set up a chat model and a tool to call within your graph:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/anthropic @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d51c35c-dbf2-4c01-932d-c5d308ea37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "\n",
    "const getWeather = tool(async ({ city }) => {\n",
    "  if (city === \"nyc\") {\n",
    "    return \"It might be cloudy in nyc\";\n",
    "  } else if (city === \"sf\") {\n",
    "    return \"It's always sunny in sf\";\n",
    "  } else {\n",
    "    throw new Error(\"Unknown city.\");\n",
    "  }\n",
    "}, {\n",
    "  name: \"get_weather\",\n",
    "  schema: z.object({\n",
    "    city: z.enum([\"nyc\", \"sf\"]),\n",
    "  }),\n",
    "  description: \"Use this to get weather information\",\n",
    "});\n",
    "\n",
    "const tools = [getWeather];\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-20240620\",\n",
    "}).bindTools(tools);\n",
    "\n",
    "\n",
    "// We add a tag that we'll be using later to filter outputs\n",
    "const finalModel = new ChatAnthropic({\n",
    "  model: \"claude-3-5-sonnet-20240620\",\n",
    "}).withConfig({\n",
    "  tags: [\"final_node\"],\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef997-5dd6-4108-baf1-c4d6be3e4999",
   "metadata": {},
   "source": [
    "## Define graph\n",
    "\n",
    "Now, lay out your graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efe9fb4-c6c2-4171-becd-d45bbf899209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { AIMessage, HumanMessage, SystemMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const shouldContinue = async (state: typeof MessagesAnnotation.State) => {\n",
    "  const messages = state.messages;\n",
    "  const lastMessage: AIMessage = messages[messages.length - 1];\n",
    "  // If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "  if (lastMessage.tool_calls?.length) {\n",
    "    return \"tools\";\n",
    "  }\n",
    "  // Otherwise, we stop (reply to the user)\n",
    "  return \"final\";\n",
    "};\n",
    "\n",
    "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
    "  const messages = state.messages;\n",
    "  const response = await model.invoke(messages);\n",
    "  // We return a list, because this will get added to the existing list\n",
    "  return { messages: [response] };\n",
    "};\n",
    "\n",
    "const callFinalModel = async (state: typeof MessagesAnnotation.State) => {\n",
    "  const messages = state.messages;\n",
    "  const lastAIMessage = messages[messages.length - 1];\n",
    "  const response = await finalModel.invoke([\n",
    "    new SystemMessage(\"Rewrite this in the voice of Al Roker\"),\n",
    "    new HumanMessage({ content: lastAIMessage.content })\n",
    "  ]);\n",
    "  // MessagesAnnotation allows you to overwrite messages from the agent\n",
    "  // by returning a message with the same id\n",
    "  response.id = lastAIMessage.id;\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "const toolNode = new ToolNode<typeof MessagesAnnotation.State>(tools);\n",
    "\n",
    "const graph = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"tools\", toolNode)\n",
    "  // add a separate final node\n",
    "  .addNode(\"final\", callFinalModel)\n",
    "  .addEdge(\"__start__\", \"agent\")\n",
    "  // Third parameter is optional and only here to draw a diagram of the graph\n",
    "  .addConditionalEdges(\"agent\", shouldContinue, {\n",
    "    tools: \"tools\",\n",
    "    final: \"final\",\n",
    "  })\n",
    "  .addEdge(\"tools\", \"agent\")\n",
    "  .addEdge(\"final\", \"__end__\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b77e74-17e9-4fee-a164-4637013b55ff",
   "metadata": {},
   "outputs": [
    },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const diagram = graph.getGraph();\n",
    "const image = await diagram.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521adaef-dd2f-46d6-8f6a-5cc1d6e0aefc",
   "metadata": {},
   "source": [
    "## Stream outputs from the final node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37c3a5f-5a43-46db-940e-c583df776520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey |\n",
      " there, folks |\n",
      "! Al |\n",
      " Roker here with |\n",
      " your weather update. |\n",
      "\n",
      "\n",
      "Well |\n",
      ", well |\n",
      ", well, it seems |\n",
      " like |\n",
      " the |\n",
      " Big |\n",
      " Apple might |\n",
      " be getting |\n",
      " a little over |\n",
      "cast today. That |\n",
      "'s right |\n",
      ", we |\n",
      "'re |\n",
      " looking |\n",
      " at some |\n",
      " cloud cover moving in over |\n",
      " New |\n",
      " York City. But hey |\n",
      ", don't let that |\n",
      " dampen your spirits! |\n",
      " A |\n",
      " little clou |\n",
      "d never |\n",
      " hurt anybody |\n",
      ", |\n",
      " right?\n",
      "\n",
      "Now |\n",
      ", I |\n",
      "' |\n",
      "d love |\n",
      " to give |\n",
      " you more |\n",
      " details, |\n",
      " but Mother |\n",
      " Nature can |\n",
      " be as |\n",
      " unpredictable as |\n",
      " a game |\n",
      " of chance sometimes |\n",
      ". So |\n",
      ", if |\n",
      " you want |\n",
      " the full |\n",
      " scoop on NYC |\n",
      "'s weather |\n",
      " or |\n",
      " if |\n",
      " you're |\n",
      " curious |\n",
      " about conditions |\n",
      " in any other city across |\n",
      " this |\n",
      " great nation of ours |\n",
      ", just give |\n",
      " me a ho |\n",
      "ller! I'm here |\n",
      " to keep |\n",
      " you in the know, |\n",
      " whether |\n",
      " it's sunshine |\n",
      ", |\n",
      " rain, or anything |\n",
      " in between.\n",
      "\n",
      "Remember |\n",
      ", a clou |\n",
      "dy day is |\n",
      " just |\n",
      " the |\n",
      " sun |\n",
      "'s |\n",
      " way of letting |\n",
      " you know it's still |\n",
      " there, even if you |\n",
      " can't see it. |\n",
      " Stay |\n",
      " weather |\n",
      "-aware |\n",
      ", |\n",
      " an |\n",
      "d don |\n",
      "'t forget your |\n",
      " umbrella... |\n",
      " just in case! |\n"
     ]
    }
   ],
   "source": [
    "const inputs = { messages: [new HumanMessage(\"What's the weather in nyc?\")] };\n",
    "\n",
    "const eventStream = await graph.streamEvents(inputs, { version: \"v2\"});\n",
    "\n",
    "for await (const { event, tags, data } of eventStream) {\n",
    "  if (event === \"on_chat_model_stream\" && tags.includes(\"final_node\")) {\n",
    "    if (data.chunk.content) {\n",
    "      // Empty content in the context of OpenAI or Anthropic usually means\n",
    "      // that the model is asking for a tool to be invoked.\n",
    "      // So we only print non-empty content\n",
    "      console.log(data.chunk.content, \"|\");\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adb47f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/streaming-tokens-without-langchain.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to stream LLM tokens (without LangChain models)\n",
        "\n",
        "In this guide, we will stream tokens from the language model powering an agent without using LangChain chat models. We'll be using the OpenAI client library directly in a ReAct agent as an example.\n",
        "\n",
        "## Setup\n",
        "\n",
        "To get started, install the `openai` and `langgraph` packages separately:\n",
        "\n",
        "```bash\n",
        "$ npm install openai @langchain/langgraph @langchain/core\n",
        "```\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Compatibility</p>\n",
        "    <p>\n",
        "        This guide requires <code>@langchain/core>=0.2.19</code>, and if you are using LangSmith, <code>langsmith>=0.1.39</code>. For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "You'll also need to make sure you have your OpenAI key set as `process.env.OPENAI_API_KEY`.\n",
        "\n",
        "## Defining a model and a tool schema\n",
        "\n",
        "First, initialize the OpenAI SDK and define a tool schema for the model to populate using [OpenAI's format](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openaiClient = new OpenAI({});\n",
        "\n",
        "const toolSchema: OpenAI.ChatCompletionTool = {\n",
        "  type: \"function\",\n",
        "  function: {\n",
        "    name: \"get_items\",\n",
        "    description: \"Use this tool to look up which items are in the given place.\",\n",
        "    parameters: {\n",
        "      type: \"object\",\n",
        "      properties: {\n",
        "        place: {\n",
        "          type: \"string\",\n",
        "        },\n",
        "      },\n",
        "      required: [\"place\"],\n",
        "    }\n",
        "  }\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the model\n",
        "\n",
        "Now, define a method for a LangGraph node that will call the model. It will handle formatting tool calls to and from the model, as well as streaming via [custom callback events](https://js.langchain.com/docs/how_to/callbacks_custom_events).\n",
        "\n",
        "If you are using [LangSmith](https://docs.smith.langchain.com/), you can also wrap the OpenAI client for the same nice tracing you'd get with a LangChain chat model.\n",
        "\n",
        "Here's what that looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { dispatchCustomEvent } from \"@langchain/core/callbacks/dispatch\";\n",
        "import { wrapOpenAI } from \"langsmith/wrappers/openai\";\n",
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  messages: Annotation<OpenAI.ChatCompletionMessageParam[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});\n",
        "\n",
        "// If using LangSmith, use \"wrapOpenAI\" on the whole client or\n",
        "// \"traceable\" to wrap a single method for nicer tracing:\n",
        "// https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code\n",
        "const wrappedClient = wrapOpenAI(openaiClient);\n",
        "\n",
        "const callModel = async (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const stream = await wrappedClient.chat.completions.create({\n",
        "    messages,\n",
        "    model: \"gpt-4o-mini\",\n",
        "    tools: [toolSchema],\n",
        "    stream: true,\n",
        "  });\n",
        "  let responseContent = \"\";\n",
        "  let role: string = \"assistant\";\n",
        "  let toolCallId: string | undefined;\n",
        "  let toolCallName: string | undefined;\n",
        "  let toolCallArgs = \"\";\n",
        "  for await (const chunk of stream) {\n",
        "    const delta = chunk.choices[0].delta;\n",
        "    if (delta.role !== undefined) {\n",
        "      role = delta.role;\n",
        "    }\n",
        "    if (delta.content) {\n",
        "      responseContent += delta.content;\n",
        "      await dispatchCustomEvent(\"streamed_token\", {\n",
        "        content: delta.content,\n",
        "      });\n",
        "    }\n",
        "    if (delta.tool_calls !== undefined && delta.tool_calls.length > 0) {\n",
        "      // note: for simplicity we're only handling a single tool call here\n",
        "      const toolCall = delta.tool_calls[0];\n",
        "      if (toolCall.function?.name !== undefined) {\n",
        "        toolCallName = toolCall.function.name;\n",
        "      }\n",
        "      if (toolCall.id !== undefined) {\n",
        "        toolCallId = toolCall.id;\n",
        "      }\n",
        "      await dispatchCustomEvent(\"streamed_tool_call_chunk\", toolCall);\n",
        "      toolCallArgs += toolCall.function?.arguments ?? \"\";\n",
        "    }\n",
        "  }\n",
        "  let finalToolCalls;\n",
        "  if (toolCallName !== undefined && toolCallId !== undefined) {\n",
        "    finalToolCalls = [{\n",
        "      id: toolCallId,\n",
        "      function: {\n",
        "        name: toolCallName,\n",
        "        arguments: toolCallArgs\n",
        "      },\n",
        "      type: \"function\" as const,\n",
        "    }];\n",
        "  }\n",
        "\n",
        "  const responseMessage = {\n",
        "    role: role as any,\n",
        "    content: responseContent,\n",
        "    tool_calls: finalToolCalls,\n",
        "  };\n",
        "  return { messages: [responseMessage] };\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that you can't call this method outside of a LangGraph node since `dispatchCustomEvent` will fail if it is called outside the proper context.\n",
        "\n",
        "## Define tools and a tool-calling node\n",
        "\n",
        "Next, set up the actual tool function and the node that will call it when the model populates a tool call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "const getItems = async ({ place }: { place: string }) => {\n",
        "  if (place.toLowerCase().includes(\"bed\")) {  // For under the bed\n",
        "    return \"socks, shoes and dust bunnies\";\n",
        "  } else if (place.toLowerCase().includes(\"shelf\")) {  // For 'shelf'\n",
        "    return \"books, pencils and pictures\";\n",
        "  } else {  // if the agent decides to ask about a different place\n",
        "    return \"cat snacks\";\n",
        "  }\n",
        "};\n",
        "\n",
        "const callTools = async (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const mostRecentMessage = messages[messages.length - 1];\n",
        "  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;\n",
        "  if (toolCalls === undefined || toolCalls.length === 0) {\n",
        "    throw new Error(\"No tool calls passed to node.\");\n",
        "  }\n",
        "  const toolNameMap = {\n",
        "    get_items: getItems,\n",
        "  };\n",
        "  const functionName = toolCalls[0].function.name;\n",
        "  const functionArguments = JSON.parse(toolCalls[0].function.arguments);\n",
        "  const response = await toolNameMap[functionName](functionArguments);\n",
        "  const toolMessage = {\n",
        "    tool_call_id: toolCalls[0].id,\n",
        "    role: \"tool\" as const,\n",
        "    name: functionName,\n",
        "    content: response,\n",
        "  }\n",
        "  return { messages: [toolMessage] };\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the graph\n",
        "\n",
        "Finally, it's time to build your graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph } from \"@langchain/langgraph\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "// We can reuse the same `GraphState` from above as it has not changed.\n",
        "const shouldContinue = (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage =\n",
        "    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;\n",
        "  if (lastMessage?.tool_calls !== undefined && lastMessage?.tool_calls.length > 0) {\n",
        "    return \"tools\";\n",
        "  }\n",
        "  return \"__end__\";\n",
        "}\n",
        "\n",
        "const graph = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"model\", callModel)\n",
        "  .addNode(\"tools\", callTools)\n",
        "  .addEdge(\"__start__\", \"model\")\n",
        "  .addConditionalEdges(\"model\", shouldContinue, {\n",
        "    tools: \"tools\",\n",
        "    __end__: \"__end__\",\n",
        "  })\n",
        "  .addEdge(\"tools\", \"model\")\n",
        "  .compile();\n",
        "  "
      ]
    },
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const representation = graph.getGraph();\n",
        "const image = await representation.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming tokens\n",
        "\n",
        "And now we can use the [`.streamEvents`](https://js.langchain.com/docs/how_to/streaming#using-stream-events) method to get the streamed tokens and tool calls from the OpenAI model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "streamed_tool_call_chunk {\n",
            "  index: 0,\n",
            "  id: 'call_v99reml4gZvvUypPgOpLgxM2',\n",
            "  type: 'function',\n",
            "  function: { name: 'get_items', arguments: '' }\n",
            "}\n",
            "streamed_tool_call_chunk { index: 0, function: { arguments: '{\"' } }\n",
            "streamed_tool_call_chunk { index: 0, function: { arguments: 'place' } }\n",
            "streamed_tool_call_chunk { index: 0, function: { arguments: '\":\"' } }\n",
            "streamed_tool_call_chunk { index: 0, function: { arguments: 'bed' } }\n",
            "streamed_tool_call_chunk { index: 0, function: { arguments: 'room' } }\n",
            "streamed_tool_call_chunk { index: 0, function: { arguments: '\"}' } }\n",
            "streamed_token { content: 'In' }\n",
            "streamed_token { content: ' the' }\n",
            "streamed_token { content: ' bedroom' }\n",
            "streamed_token { content: ',' }\n",
            "streamed_token { content: ' you' }\n",
            "streamed_token { content: ' can' }\n",
            "streamed_token { content: ' find' }\n",
            "streamed_token { content: ' socks' }\n",
            "streamed_token { content: ',' }\n",
            "streamed_token { content: ' shoes' }\n",
            "streamed_token { content: ',' }\n",
            "streamed_token { content: ' and' }\n",
            "streamed_token { content: ' dust' }\n",
            "streamed_token { content: ' b' }\n",
            "streamed_token { content: 'unnies' }\n",
            "streamed_token { content: '.' }\n"
          ]
        }
      ],
      "source": [
        "const eventStream = await graph.streamEvents(\n",
        "  { messages: [{ role: \"user\", content: \"what's in the bedroom?\" }] },\n",
        "  { version: \"v2\" },\n",
        ");\n",
        "\n",
        "for await (const { event, name, data } of eventStream) {\n",
        "  if (event === \"on_custom_event\") {\n",
        "    console.log(name, data);\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And if you've set up LangSmith tracing, you'll also see [a trace like this one](https://smith.langchain.com/public/ddb1af36-ebe5-4ba6-9a57-87a296dc801f/r)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
</file>

<file path="how-tos/subgraph-persistence.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176e8dbb-1a0a-49ce-a10e-2417e8ea17a0",
   "metadata": {},
   "source": [
    "# How to add thread-level persistence to subgraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67581a-49fb-4597-a7fc-6774581c2160",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/low_level/#subgraphs\">\n",
    "                    Subgraphs\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/persistence/\">\n",
    "                    Persistence\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "This guide shows how you can add [thread-level](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/) persistence to graphs that use [subgraphs](https://langchain-ai.github.io/langgraphjs/how-tos/subgraph/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83b855-ab23-4de7-9559-702cad9a29c6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install required packages:\n",
    "\n",
    "```bash\n",
    "$ npm install @langchain/langgraph @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60c6cd-bf4e-46af-9761-b872d0fbe3b6",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b9056-fec7-4683-8c22-f56c91f5b13b",
   "metadata": {},
   "source": [
    "## Define the graph with persistence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f1303ef-df37-48e0-8a59-8ff169c52c5b",
   "metadata": {},
   "source": [
    "To add persistence to a graph with subgraphs, all you need to do is pass a [checkpointer](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseCheckpointSaver.html) when **compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74cde2e-c127-4326-8d36-b6acef987f0a",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "      You <b>shouldn't provide</b> a checkpointer when compiling a subgraph. Instead, you must define a **single** checkpointer that you pass to <code>parentGraph.compile()</code>, and LangGraph will automatically propagate the checkpointer to the child subgraphs. If you pass the checkpointer to the <code>subgraph.compile()</code>, it will simply be ignored. This also applies when you <a href=\"../subgraph#add-a-node-function-that-invokes-the-subgraph\">add a node that invokes the subgraph explicitly</a>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1fe22-1ca9-45eb-a35b-71b9c905e8c5",
   "metadata": {},
   "source": [
    "Let's define a simple graph with a single subgraph node to show how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d76f0c0-bd77-4eca-9527-27bcdf85dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// subgraph\n",
    "\n",
    "const SubgraphStateAnnotation = Annotation.Root({\n",
    "  foo: Annotation<string>,\n",
    "  bar: Annotation<string>,\n",
    "});\n",
    "\n",
    "const subgraphNode1 = async (state: typeof SubgraphStateAnnotation.State) => {\n",
    "  return { bar: \"bar\" };\n",
    "};\n",
    "\n",
    "const subgraphNode2 = async (state: typeof SubgraphStateAnnotation.State) => {\n",
    "  // note that this node is using a state key ('bar') that is only available in the subgraph\n",
    "  // and is sending update on the shared state key ('foo')\n",
    "  return { foo: state.foo + state.bar };\n",
    "};\n",
    "\n",
    "const subgraph = new StateGraph(SubgraphStateAnnotation)\n",
    "  .addNode(\"subgraphNode1\", subgraphNode1)\n",
    "  .addNode(\"subgraphNode2\", subgraphNode2)\n",
    "  .addEdge(\"__start__\", \"subgraphNode1\")\n",
    "  .addEdge(\"subgraphNode1\", \"subgraphNode2\")\n",
    "  .compile();\n",
    "  \n",
    "// parent graph\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  foo: Annotation<string>,\n",
    "});\n",
    "\n",
    "const node1 = async (state: typeof StateAnnotation.State) => {\n",
    "  return {\n",
    "    foo: \"hi! \" + state.foo,\n",
    "  };\n",
    "};\n",
    "\n",
    "const builder = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"node1\", node1)\n",
    "  // note that we're adding the compiled subgraph as a node to the parent graph\n",
    "  .addNode(\"node2\", subgraph)\n",
    "  .addEdge(\"__start__\", \"node1\")\n",
    "  .addEdge(\"node1\", \"node2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47084b1f-9fd5-40a9-9d75-89eb5f853d02",
   "metadata": {},
   "source": [
    "We can now compile the graph with an in-memory checkpointer (`MemorySaver`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7657d285-c896-40c9-a569-b4a3b9c230c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MemorySaver } from \"@langchain/langgraph-checkpoint\";\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "// You must only pass checkpointer when compiling the parent graph.\n",
    "// LangGraph will automatically propagate the checkpointer to the child subgraphs.\n",
    "\n",
    "const graph = builder.compile({\n",
    "  checkpointer: checkpointer\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d193e3c-4ec3-4034-beed-8e5550c6542c",
   "metadata": {},
   "source": [
    "## Verify persistence works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69a5f0-b92e-4d4e-9aa9-c4c4ec7de91a",
   "metadata": {},
   "source": [
    "Let's now run the graph and inspect the persisted state for both the parent graph and the subgraph to verify that persistence works. We should expect to see the final execution results for both the parent and subgraph in `state.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13da686e-6ed6-4b83-93e8-1631fcc8c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "const config = { configurable: { thread_id: \"1\" } };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8721f045-2e82-4bf0-9d85-5ba6ecf899d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ node1: { foo: 'hi! foo' } }\n",
      "{ subgraphNode1: { bar: 'bar' } }\n",
      "{ subgraphNode2: { foo: 'hi! foobar' } }\n",
      "{ node2: { foo: 'hi! foobar' } }\n"
     ]
    }
   ],
   "source": [
    "const stream = await graph.stream({\n",
    "  foo: \"foo\"\n",
    "}, {\n",
    "  ...config,\n",
    "  subgraphs: true,\n",
    "});\n",
    "\n",
    "for await (const [_source, chunk] of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b5ce4-becc-4910-8a6d-d6b60d9d6f60",
   "metadata": {},
   "source": [
    "We can now view the parent graph state by calling `graph.get_state()` with the same config that we used to invoke the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e817283-142d-4fda-8cb1-8de34717f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ foo: 'hi! foobar' }\n"
     ]
    }
   ],
   "source": [
    "(await graph.getState(config)).values;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4f30b-941e-4140-8bfa-3b8cc670489c",
   "metadata": {},
   "source": [
    "To view the subgraph state, we need to do two things:\n",
    "\n",
    "1. Find the most recent config value for the subgraph\n",
    "2. Use `graph.getState()` to retrieve that value for the most recent subgraph config.\n",
    "\n",
    "To find the correct config, we can examine the state history from the parent graph and find the state snapshot before we return results from `node2` (the node with subgraph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e896628f-36b2-45eb-b7c5-c64c1098f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "let stateWithSubgraph;\n",
    "\n",
    "const graphHistories = await graph.getStateHistory(config);\n",
    "\n",
    "for await (const state of graphHistories) {\n",
    "  if (state.next[0] === \"node2\") {\n",
    "    stateWithSubgraph = state;\n",
    "    break;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af49977-42b1-40a1-88f1-f07437f8b7f9",
   "metadata": {},
   "source": [
    "The state snapshot will include the list of `tasks` to be executed next. When using subgraphs, the `tasks` will contain the config that we can use to retrieve the subgraph state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e96df3-946d-40f8-8d6d-055ae4177452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  configurable: {\n",
      "    thread_id: '1',\n",
      "    checkpoint_ns: 'node2:25814e09-45f0-5b70-a5b4-23b869d582c2'\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const subgraphConfig = stateWithSubgraph.tasks[0].state;\n",
    "\n",
    "console.log(subgraphConfig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d2401b3-d52b-4895-a5d1-dccf015ba216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ foo: 'hi! foobar', bar: 'bar' }\n"
     ]
    }
   ],
   "source": [
    "(await graph.getState(subgraphConfig)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aded92-99dd-427b-932d-aa78f474c271",
   "metadata": {},
   "source": [
    "If you want to learn more about how to modify the subgraph state for human-in-the-loop workflows, check out this [how-to guide](https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/subgraph-transform-state.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to transform inputs and outputs of a subgraph\n",
    "\n",
    "It's possible that your subgraph state is completely independent from the parent graph state, i.e. there are no overlapping channels (keys) between the two. For example, you might have a supervisor agent that needs to produce a report with a help of multiple ReAct agents. ReAct agent subgraphs might keep track of a list of messages whereas the supervisor only needs user input and final report in its state, and doesn't need to keep track of messages.\n",
    "\n",
    "In such cases you need to transform the inputs to the subgraph before calling it and then transform its outputs before returning. This guide shows how to do that.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/core\n",
    "```\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph and subgraphs\n",
    "\n",
    "Let's define 3 graphs:\n",
    "- a parent graph\n",
    "- a child subgraph that will be called by the parent graph\n",
    "- a grandchild subgraph that will be called by the child graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define grandchild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, START, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const GrandChildAnnotation = Annotation.Root({\n",
    "    myGrandchildKey: Annotation<string>,\n",
    "})\n",
    "\n",
    "const grandchild1 = (state: typeof GrandChildAnnotation.State) => {\n",
    "    // NOTE: child or parent keys will not be accessible here\n",
    "    return {\n",
    "        myGrandchildKey: state.myGrandchildKey + \", how are you\"\n",
    "    }\n",
    "}\n",
    "\n",
    "const grandchild = new StateGraph(GrandChildAnnotation)\n",
    "    .addNode(\"grandchild1\", grandchild1)\n",
    "    .addEdge(START, \"grandchild1\")\n",
    "\n",
    "const grandchildGraph = grandchild.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ myGrandchildKey: 'hi Bob, how are you' }\n"
     ]
    }
   ],
   "source": [
    "await grandchildGraph.invoke({ myGrandchildKey: \"hi Bob\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, START, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const ChildAnnotation = Annotation.Root({\n",
    "    myChildKey: Annotation<string>,\n",
    "});\n",
    "\n",
    "const callGrandchildGraph = async (state: typeof ChildAnnotation.State) => {\n",
    "    // NOTE: parent or grandchild keys won't be accessible here\n",
    "    // we're transforming the state from the child state channels (`myChildKey`)\n",
    "    // to the grandchild state channels (`myGrandchildKey`)\n",
    "    const grandchildGraphInput = { myGrandchildKey: state.myChildKey };\n",
    "    // we're transforming the state from the grandchild state channels (`myGrandchildKey`)\n",
    "    // back to the child state channels (`myChildKey`)\n",
    "    const grandchildGraphOutput = await grandchildGraph.invoke(grandchildGraphInput);\n",
    "    return {\n",
    "        myChildKey: grandchildGraphOutput.myGrandchildKey + \" today?\"\n",
    "    };\n",
    "};\n",
    "\n",
    "const child = new StateGraph(ChildAnnotation)\n",
    "    // NOTE: we're passing a function here instead of just compiled graph (`childGraph`)\n",
    "    .addNode(\"child1\", callGrandchildGraph)\n",
    "    .addEdge(START, \"child1\");\n",
    "\n",
    "const childGraph = child.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ myChildKey: 'hi Bob, how are you today?' }\n"
     ]
    }
   ],
   "source": [
    "await childGraph.invoke({ myChildKey: \"hi Bob\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition info\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "    We're wrapping the <code>grandchildGraph</code> invocation in a separate function (<code>callGrandchildGraph</code>) that transforms the input state before calling the grandchild graph and then transforms the output of grandchild graph back to child graph state. If you just pass <code>grandchildGraph</code> directly to <code>.addNode</code> without the transformations, LangGraph will raise an error as there are no shared state channels (keys) between child and grandchild states.\n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that child and grandchild subgraphs have their own, **independent** state that is not shared with the parent graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, START, END, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const ParentAnnotation = Annotation.Root({\n",
    "    myKey: Annotation<string>,\n",
    "});\n",
    "\n",
    "const parent1 = (state: typeof ParentAnnotation.State) => {\n",
    "    // NOTE: child or grandchild keys won't be accessible here\n",
    "    return { myKey: \"hi \" + state.myKey };\n",
    "};\n",
    "\n",
    "const parent2 = (state: typeof ParentAnnotation.State) => {\n",
    "    return { myKey: state.myKey + \" bye!\" };\n",
    "};\n",
    "\n",
    "const callChildGraph = async (state: typeof ParentAnnotation.State) => {\n",
    "    // we're transforming the state from the parent state channels (`myKey`)\n",
    "    // to the child state channels (`myChildKey`)\n",
    "    const childGraphInput = { myChildKey: state.myKey };\n",
    "    // we're transforming the state from the child state channels (`myChildKey`)\n",
    "    // back to the parent state channels (`myKey`)\n",
    "    const childGraphOutput = await childGraph.invoke(childGraphInput);\n",
    "    return { myKey: childGraphOutput.myChildKey };\n",
    "};\n",
    "\n",
    "const parent = new StateGraph(ParentAnnotation)\n",
    "    .addNode(\"parent1\", parent1)\n",
    "    // NOTE: we're passing a function here instead of just a compiled graph (`childGraph`)\n",
    "    .addNode(\"child\", callChildGraph)\n",
    "    .addNode(\"parent2\", parent2)\n",
    "    .addEdge(START, \"parent1\")\n",
    "    .addEdge(\"parent1\", \"child\")\n",
    "    .addEdge(\"child\", \"parent2\")\n",
    "    .addEdge(\"parent2\", END);\n",
    "\n",
    "const parentGraph = parent.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition info\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "    We're wrapping the <code>childGraph</code> invocation in a separate function (<code>callChildGraph</code>) that transforms the input state before calling the child graph and then transforms the output of the child graph back to parent graph state. If you just pass <code>childGraph</code> directly to <code>.addNode</code> without the transformations, LangGraph will raise an error as there are no shared state channels (keys) between parent and child states.\n",
    "    </p>\n",
    "</div>    \n",
    "\n",
    "Let's run the parent graph and make sure it correctly calls both the child and grandchild subgraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ myKey: 'hi Bob, how are you today? bye!' }\n"
     ]
    }
   ],
   "source": [
    "await parentGraph.invoke({ myKey: \"Bob\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! The parent graph correctly calls both the child and grandchild subgraphs (which we know since the \", how are you\" and \"today?\" are added to our original \"myKey\" state value)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/subgraph.ipynb">
{
 "cells": [
  {
   "attachments":   
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add and use subgraphs\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/low_level/#subgraphs\">\n",
    "                    Subgraphs\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state\">\n",
    "                    State\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "[Subgraphs](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#subgraphs) allow you to build complex systems with multiple components that are themselves graphs. A common use case for using subgraphs is building [multi-agent systems](https://langchain-ai.github.io/langgraphjs/concepts/multi_agent).\n",
    "\n",
    "The main question when adding subgraphs is how the parent graph and subgraph communicate, i.e. how they pass the [state](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state) between each other during the graph execution. There are two scenarios:\n",
    "\n",
    "- parent graph and subgraph **share schema keys**. In this case, you can [add a node with the compiled subgraph](#add-a-node-with-the-compiled-subgraph)\n",
    "- parent graph and subgraph have **different schemas**. In this case, you have to [add a node function that invokes the subgraph](#add-a-node-function-that-invokes-the-subgraph): this is useful when the parent graph and the subgraph have different state schemas and you need to transform state before or after calling the subgraph\n",
    "\n",
    "Below we show to to add subgraphs for each scenario.\n",
    "\n",
    "![Screenshot 2024-07-11 at 1.01.28 PM.png](attachment:71516aef-9c00-4730-a676-a54e90cb6472.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a node with the compiled subgraph\n",
    "\n",
    "A common case is for the parent graph and subgraph to communicate over a shared state key (channel). For example, in [multi-agent](https://langchain-ai.github.io/langgraphjs/concepts/multi_agent) systems, the agents often communicate over a shared [messages](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#why-use-messages) key.\n",
    "\n",
    "If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:\n",
    "\n",
    "1. Define the subgraph workflow (`subgraphBuilder` in the example below) and compile it\n",
    "2. Pass compiled subgraph to the `.addNode` method when defining the parent graph workflow\n",
    "\n",
    "Let's take a look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const SubgraphStateAnnotation = Annotation.Root({\n",
    "  foo: Annotation<string>, // note that this key is shared with the parent graph state\n",
    "  bar: Annotation<string>,\n",
    "});\n",
    "\n",
    "const subgraphNode1 = async (state: typeof SubgraphStateAnnotation.State) => {\n",
    "  return { bar: \"bar\" };\n",
    "};\n",
    "\n",
    "const subgraphNode2 = async (state: typeof SubgraphStateAnnotation.State) => {\n",
    "  // note that this node is using a state key ('bar') that is only available in the subgraph\n",
    "  // and is sending update on the shared state key ('foo')\n",
    "  return { foo: state.foo + state.bar };\n",
    "};\n",
    "\n",
    "const subgraphBuilder = new StateGraph(SubgraphStateAnnotation)\n",
    "  .addNode(\"subgraphNode1\", subgraphNode1)\n",
    "  .addNode(\"subgraphNode2\", subgraphNode2)\n",
    "  .addEdge(\"__start__\", \"subgraphNode1\")\n",
    "  .addEdge(\"subgraphNode1\", \"subgraphNode2\")\n",
    "\n",
    "const subgraph = subgraphBuilder.compile();\n",
    "\n",
    "// Define parent graph\n",
    "const ParentStateAnnotation = Annotation.Root({\n",
    "  foo: Annotation<string>,\n",
    "});\n",
    "\n",
    "const node1 = async (state: typeof ParentStateAnnotation.State) => {\n",
    "  return {\n",
    "    foo: \"hi! \" + state.foo,\n",
    "  };\n",
    "}\n",
    "\n",
    "const builder = new StateGraph(ParentStateAnnotation)\n",
    "  .addNode(\"node1\", node1)\n",
    "  // note that we're adding the compiled subgraph as a node to the parent graph\n",
    "  .addNode(\"node2\", subgraph)\n",
    "  .addEdge(\"__start__\", \"node1\")\n",
    "  .addEdge(\"node1\", \"node2\")\n",
    "\n",
    "const graph = builder.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ node1: { foo: 'hi! foo' } }\n",
      "{ node2: { foo: 'hi! foobar' } }\n"
     ]
    }
   ],
   "source": [
    "const stream = await graph.stream({ foo: \"foo\" });\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the final output from the parent graph includes the results of subgraph invocation (the string `\"bar\"`). \n",
    "\n",
    "If you would like to see streaming output from the subgraph, you can specify `subgraphs: True` when streaming. See more on streaming from subgraphs in this [how-to guide](https://langchain-ai.github.io/langgraphjs/how-tos/streaming-subgraphs/#stream-subgraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [], { node1: { foo: 'hi! foo' } } ]\n",
      "[\n",
      "  [ 'node2:22f27b01-fa9f-5f46-9b5b-166a80d96791' ],\n",
      "  { subgraphNode1: { bar: 'bar' } }\n",
      "]\n",
      "[\n",
      "  [ 'node2:22f27b01-fa9f-5f46-9b5b-166a80d96791' ],\n",
      "  { subgraphNode2: { foo: 'hi! foobar' } }\n",
      "]\n",
      "[ [], { node2: { foo: 'hi! foobar' } } ]\n"
     ]
    }
   ],
   "source": [
    "const streamWithSubgraphs = await graph.stream({ foo: \"foo\" }, { subgraphs: true });\n",
    "\n",
    "for await (const chunk of streamWithSubgraphs) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the chunk output format has changed to include some additional information about the subgraph it came from.\n",
    "\n",
    "## Add a node function that invokes the subgraph\n",
    "\n",
    "For more complex systems you might want to define subgraphs that have a completely different schema from the parent graph (no shared keys). For example, in a multi-agent RAG system, a search agent might only need to keep track of queries and retrieved documents.\n",
    "\n",
    "If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.\n",
    "\n",
    "Below we show how to modify our original example to call a subgraph from inside the node.\n",
    "\n",
    "<div class=\"admonition warning\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "      You <b>cannot</b> invoke more than one subgraph inside the same node if you have checkpointing enabled for the subgraphs. See <a href=\"../subgraph-persistence#define-the-graph-with-persistence\">this page</a> for more information.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const SubgraphAnnotation = Annotation.Root({\n",
    "  bar: Annotation<string>, // note that this key is shared with the parent graph state\n",
    "  baz: Annotation<string>,\n",
    "});\n",
    "\n",
    "const subgraphNodeOne = async (state: typeof SubgraphAnnotation.State) => {\n",
    "  return { baz: \"baz\" };\n",
    "};\n",
    "\n",
    "const subgraphNodeTwo = async (state: typeof SubgraphAnnotation.State) => {\n",
    "  return { bar: state.bar + state.baz }\n",
    "};\n",
    "\n",
    "const subgraphCalledInFunction = new StateGraph(SubgraphAnnotation)\n",
    "  .addNode(\"subgraphNode1\", subgraphNodeOne)\n",
    "  .addNode(\"subgraphNode2\", subgraphNodeTwo)\n",
    "  .addEdge(\"__start__\", \"subgraphNode1\")\n",
    "  .addEdge(\"subgraphNode1\", \"subgraphNode2\")\n",
    "  .compile();\n",
    "\n",
    "// Define parent graph\n",
    "const ParentAnnotation = Annotation.Root({\n",
    "  foo: Annotation<string>,\n",
    "});\n",
    "\n",
    "const nodeOne = async (state: typeof ParentAnnotation.State) => {\n",
    "  return {\n",
    "    foo: \"hi! \" + state.foo,\n",
    "  };\n",
    "}\n",
    "\n",
    "const nodeTwo = async (state: typeof ParentAnnotation.State) => {\n",
    "  const response = await subgraphCalledInFunction.invoke({\n",
    "    bar: state.foo,\n",
    "  });\n",
    "  return { foo: response.bar }\n",
    "}\n",
    "\n",
    "const graphWithFunction = new StateGraph(ParentStateAnnotation)\n",
    "  .addNode(\"node1\", nodeOne)\n",
    "  // note that we're adding the compiled subgraph as a node to the parent graph\n",
    "  .addNode(\"node2\", nodeTwo)\n",
    "  .addEdge(\"__start__\", \"node1\")\n",
    "  .addEdge(\"node1\", \"node2\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [], { node1: { foo: 'hi! foo' } } ]\n",
      "[\n",
      "  [ 'node2:1d2bb11a-3ed1-5c58-9b6f-c7af36a1eeb7' ],\n",
      "  { subgraphNode1: { baz: 'baz' } }\n",
      "]\n",
      "[\n",
      "  [ 'node2:1d2bb11a-3ed1-5c58-9b6f-c7af36a1eeb7' ],\n",
      "  { subgraphNode2: { bar: 'hi! foobaz' } }\n",
      "]\n",
      "[ [], { node2: { foo: 'hi! foobaz' } } ]\n"
     ]
    }
   ],
   "source": [
    "const graphWithFunctionStream = await graphWithFunction.stream({ foo: \"foo\" }, { subgraphs: true });\n",
    "for await (const chunk of graphWithFunctionStream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/subgraphs-manage-state.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to view and update state in subgraphs\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/low_level/#subgraphs\">\n",
    "                    Subgraphs\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/\">\n",
    "                    Human-in-the-loop\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state\">\n",
    "                    State\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "Once you add [persistence](../subgraph-persistence), you can view and update the state of the subgraph at any point in time. This enables human-in-the-loop interaction patterns such as:\n",
    "\n",
    "- You can surface a state during an interrupt to a user to let them accept an action.\n",
    "- You can rewind the subgraph to reproduce or avoid issues.\n",
    "- You can modify the state to let the user better control its actions.\n",
    "\n",
    "This guide shows how you can do this.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to install required packages:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/core @langchain/openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the provider we'll use for this guide):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define subgraph\n",
    "\n",
    "First, let's set up our subgraph. For this, we will create a simple graph that can get the weather for a specific city. We will compile this graph with a [breakpoint](https://langchain-ai.github.io/langgraphjs/how-tos/human_in_the_loop/breakpoints/) before the `weather_node`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { StateGraph, MessagesAnnotation, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const getWeather = tool(async ({ city }) => {\n",
    "  return `It's sunny in ${city}`;\n",
    "}, {\n",
    "  name: \"get_weather\",\n",
    "  description: \"Get the weather for a specific city\",\n",
    "  schema: z.object({\n",
    "    city: z.string().describe(\"A city name\")\n",
    "  })\n",
    "});\n",
    "\n",
    "const rawModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n",
    "const model = rawModel.withStructuredOutput(getWeather);\n",
    "\n",
    "// Extend the base MessagesAnnotation state with another field\n",
    "const SubGraphAnnotation = Annotation.Root({\n",
    "  ...MessagesAnnotation.spec,\n",
    "  city: Annotation<string>,\n",
    "});\n",
    "\n",
    "const modelNode = async (state: typeof SubGraphAnnotation.State) => {\n",
    "  const result = await model.invoke(state.messages);\n",
    "  return { city: result.city };\n",
    "};\n",
    "\n",
    "const weatherNode = async (state: typeof SubGraphAnnotation.State) => {\n",
    "  const result = await getWeather.invoke({ city: state.city });\n",
    "  return {\n",
    "    messages: [\n",
    "      {\n",
    "        role: \"assistant\",\n",
    "        content: result,\n",
    "      }\n",
    "    ]\n",
    "  };\n",
    "};\n",
    "\n",
    "const subgraph = new StateGraph(SubGraphAnnotation)\n",
    "  .addNode(\"modelNode\", modelNode)\n",
    "  .addNode(\"weatherNode\", weatherNode)\n",
    "  .addEdge(\"__start__\", \"modelNode\")\n",
    "  .addEdge(\"modelNode\", \"weatherNode\")\n",
    "  .addEdge(\"weatherNode\", \"__end__\")\n",
    "  .compile({ interruptBefore: [\"weatherNode\"] });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parent graph\n",
    "\n",
    "We can now setup the overall graph. This graph will first route to the subgraph if it needs to get the weather, otherwise it will route to a normal LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "\n",
    "const RouterStateAnnotation = Annotation.Root({\n",
    "  ...MessagesAnnotation.spec,\n",
    "  route: Annotation<\"weather\" | \"other\">,\n",
    "});\n",
    "\n",
    "const routerModel = rawModel.withStructuredOutput(\n",
    "  z.object({\n",
    "    route: z.enum([\"weather\", \"other\"]).describe(\"A step that should execute next to based on the currnet input\")\n",
    "  }),\n",
    "  {\n",
    "    name: \"router\"\n",
    "  }\n",
    ");\n",
    "\n",
    "const routerNode = async (state: typeof RouterStateAnnotation.State) => {\n",
    "  const systemMessage = {\n",
    "    role: \"system\",\n",
    "    content: \"Classify the incoming query as either about weather or not.\",\n",
    "  };\n",
    "  const messages = [systemMessage, ...state.messages]\n",
    "  const { route } = await routerModel.invoke(messages);\n",
    "  return { route };\n",
    "}\n",
    "\n",
    "const normalLLMNode = async (state: typeof RouterStateAnnotation.State) => {\n",
    "  const responseMessage = await rawModel.invoke(state.messages);\n",
    "  return { messages: [responseMessage] };\n",
    "};\n",
    "\n",
    "const routeAfterPrediction = async (state: typeof RouterStateAnnotation.State) => {\n",
    "  if (state.route === \"weather\") {\n",
    "    return \"weatherGraph\";\n",
    "  } else {\n",
    "    return \"normalLLMNode\";\n",
    "  }\n",
    "};\n",
    "\n",
    "const graph = new StateGraph(RouterStateAnnotation)\n",
    "  .addNode(\"routerNode\", routerNode)\n",
    "  .addNode(\"normalLLMNode\", normalLLMNode)\n",
    "  .addNode(\"weatherGraph\", subgraph)\n",
    "  .addEdge(\"__start__\", \"routerNode\")\n",
    "  .addConditionalEdges(\"routerNode\", routeAfterPrediction)\n",
    "  .addEdge(\"normalLLMNode\", \"__end__\")\n",
    "  .addEdge(\"weatherGraph\", \"__end__\")\n",
    "  .compile({ checkpointer: memory });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a diagram of the graph we just created:\n",
    "\n",
    "![](./img/single-nested-subgraph.jpeg)\n",
    "\n",
    "Let's test this out with a normal query to make sure it works as intended!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ routerNode: { route: 'other' } }\n",
      "{\n",
      "  normalLLMNode: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-ABtbbiB5N3Uue85UNrFUjw5KhGaud\",\n",
      "        \"content\": \"Hello! How can I assist you today?\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"completionTokens\": 9,\n",
      "            \"promptTokens\": 9,\n",
      "            \"totalTokens\": 18\n",
      "          },\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_f85bea6784\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 9,\n",
      "          \"output_tokens\": 9,\n",
      "          \"total_tokens\": 18\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config = { configurable: { thread_id: \"1\" } };\n",
    "\n",
    "const inputs = { messages: [{ role: \"user\", content: \"hi!\" }] };\n",
    "\n",
    "const stream = await graph.stream(inputs, { ...config, streamMode: \"updates\" });\n",
    "\n",
    "for await (const update of stream) {\n",
    "  console.log(update);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We didn't ask about the weather, so we got a normal response from the LLM.\n",
    "\n",
    "## Resuming from breakpoints\n",
    "\n",
    "Let's now look at what happens with breakpoints. Let's invoke it with a query that should get routed to the weather subgraph where we have the interrupt node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ routerNode: { route: 'weather' } }\n"
     ]
    }
   ],
   "source": [
    "const config2 = { configurable: { thread_id: \"2\" } };\n",
    "\n",
    "const streamWithBreakpoint = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what's the weather in sf\"\n",
    "  }]\n",
    "}, { ...config2, streamMode: \"updates\" });\n",
    "\n",
    "for await (const update of streamWithBreakpoint) {\n",
    "  console.log(update);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the graph stream doesn't include subgraph events. If we want to stream subgraph events, we can pass `subgraphs: True` in our config and get back subgraph events like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [], { routerNode: { route: 'weather' } } ]\n",
      "[\n",
      "  [ 'weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0' ],\n",
      "  { modelNode: { city: 'San Francisco' } }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const streamWithSubgraphs = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what's the weather in sf\"\n",
    "  }]\n",
    "}, { configurable: { thread_id: \"3\" }, streamMode: \"updates\", subgraphs: true });\n",
    "\n",
    "for await (const update of streamWithSubgraphs) {\n",
    "  console.log(update);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we see the format of the streamed updates has changed. It's now a tuple where the first item is a nested array with information about the subgraph and the second is the actual state update. If we get the state now, we can see that it's paused on `weatherGraph` as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'weatherGraph' ]\n"
     ]
    }
   ],
   "source": [
    "const state = await graph.getState({ configurable: { thread_id: \"3\" } })\n",
    "state.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the pending tasks for our current state, we can see that we have one task named `weatherGraph`, which corresponds to the subgraph task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"ec67e50f-d29c-5dee-8a80-08723a937de0\",\n",
      "    \"name\": \"weatherGraph\",\n",
      "    \"path\": [\n",
      "      \"__pregel_pull\",\n",
      "      \"weatherGraph\"\n",
      "    ],\n",
      "    \"interrupts\": [],\n",
      "    \"state\": {\n",
      "      \"configurable\": {\n",
      "        \"thread_id\": \"3\",\n",
      "        \"checkpoint_ns\": \"weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "JSON.stringify(state.tasks, null, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However since we got the state using the config of the parent graph, we don't have access to the subgraph state. If you look at the `state` value of the task above you will note that it is simply the configuration of the parent graph. If we want to actually populate the subgraph state, we can pass in `subgraphs: True` to the second parameter of `getState` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"ec67e50f-d29c-5dee-8a80-08723a937de0\",\n",
      "    \"name\": \"weatherGraph\",\n",
      "    \"path\": [\n",
      "      \"__pregel_pull\",\n",
      "      \"weatherGraph\"\n",
      "    ],\n",
      "    \"interrupts\": [],\n",
      "    \"state\": {\n",
      "      \"values\": {\n",
      "        \"messages\": [\n",
      "          {\n",
      "            \"lc\": 1,\n",
      "            \"type\": \"constructor\",\n",
      "            \"id\": [\n",
      "              \"langchain_core\",\n",
      "              \"messages\",\n",
      "              \"HumanMessage\"\n",
      "            ],\n",
      "            \"kwargs\": {\n",
      "              \"content\": \"what's the weather in sf\",\n",
      "              \"additional_kwargs\": {},\n",
      "              \"response_metadata\": {},\n",
      "              \"id\": \"094b6752-6bea-4b43-b837-c6b0bb6a4c44\"\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"city\": \"San Francisco\"\n",
      "      },\n",
      "      \"next\": [\n",
      "        \"weatherNode\"\n",
      "      ],\n",
      "      \"tasks\": [\n",
      "        {\n",
      "          \"id\": \"2f2f8b8f-6a99-5225-8ff2-b6c49c3e9caf\",\n",
      "          \"name\": \"weatherNode\",\n",
      "          \"path\": [\n",
      "            \"__pregel_pull\",\n",
      "            \"weatherNode\"\n",
      "          ],\n",
      "          \"interrupts\": []\n",
      "        }\n",
      "      ],\n",
      "      \"metadata\": {\n",
      "        \"source\": \"loop\",\n",
      "        \"writes\": {\n",
      "          \"modelNode\": {\n",
      "            \"city\": \"San Francisco\"\n",
      "          }\n",
      "        },\n",
      "        \"step\": 1,\n",
      "        \"parents\": {\n",
      "          \"\": \"1ef7c6ba-3d36-65e0-8001-adc1f8841274\"\n",
      "        }\n",
      "      },\n",
      "      \"config\": {\n",
      "        \"configurable\": {\n",
      "          \"thread_id\": \"3\",\n",
      "          \"checkpoint_id\": \"1ef7c6ba-4503-6700-8001-61e828d1c772\",\n",
      "          \"checkpoint_ns\": \"weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0\",\n",
      "          \"checkpoint_map\": {\n",
      "            \"\": \"1ef7c6ba-3d36-65e0-8001-adc1f8841274\",\n",
      "            \"weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0\": \"1ef7c6ba-4503-6700-8001-61e828d1c772\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"createdAt\": \"2024-09-27T00:58:43.184Z\",\n",
      "      \"parentConfig\": {\n",
      "        \"configurable\": {\n",
      "          \"thread_id\": \"3\",\n",
      "          \"checkpoint_ns\": \"weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0\",\n",
      "          \"checkpoint_id\": \"1ef7c6ba-3d3b-6400-8000-fe27ae37c785\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const stateWithSubgraphs = await graph.getState({ configurable: { thread_id: \"3\" } }, { subgraphs: true })\n",
    "JSON.stringify(stateWithSubgraphs.tasks, null, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to the subgraph state!\n",
    "\n",
    "To resume execution, we can just invoke the outer graph as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [],\n",
      "  {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"094b6752-6bea-4b43-b837-c6b0bb6a4c44\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ],\n",
      "    route: 'weather'\n",
      "  }\n",
      "]\n",
      "[\n",
      "  [ 'weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0' ],\n",
      "  {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"094b6752-6bea-4b43-b837-c6b0bb6a4c44\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ],\n",
      "    city: 'San Francisco'\n",
      "  }\n",
      "]\n",
      "[\n",
      "  [ 'weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0' ],\n",
      "  {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"094b6752-6bea-4b43-b837-c6b0bb6a4c44\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"55d7a03f-876a-4887-9418-027321e747c7\",\n",
      "        \"content\": \"It's sunny in San Francisco\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ],\n",
      "    city: 'San Francisco'\n",
      "  }\n",
      "]\n",
      "[\n",
      "  [],\n",
      "  {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"094b6752-6bea-4b43-b837-c6b0bb6a4c44\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"55d7a03f-876a-4887-9418-027321e747c7\",\n",
      "        \"content\": \"It's sunny in San Francisco\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ],\n",
      "    route: 'weather'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const resumedStream = await graph.stream(null, {\n",
    "  configurable: { thread_id: \"3\" },\n",
    "  streamMode: \"values\",\n",
    "  subgraphs: true,\n",
    "});\n",
    "\n",
    "for await (const update of resumedStream) {\n",
    "  console.log(update);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming from specific subgraph node\n",
    "\n",
    "In the example above, we were replaying from the outer graph - which automatically replayed the subgraph from whatever state it was in previously (paused before the `weatherNode` in our case), but it is also possible to replay from inside a subgraph. In order to do so, we need to get the configuration from the exact subgraph state that we want to replay from.\n",
    "\n",
    "We can do this by exploring the state history of the subgraph, and selecting the state before `modelNode` - which we can do by filtering on the `.next` parameter.\n",
    "\n",
    "To get the state history of the subgraph, we need to first pass in the parent graph state before the subgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "let parentGraphStateBeforeSubgraph;\n",
    "\n",
    "const histories = await graph.getStateHistory({ configurable: { thread_id: \"3\" } });\n",
    "\n",
    "for await (const historyEntry of histories) {\n",
    "  if (historyEntry.next[0] === \"weatherGraph\") {\n",
    "    parentGraphStateBeforeSubgraph = historyEntry;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"094b6752-6bea-4b43-b837-c6b0bb6a4c44\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  next: [ 'modelNode' ],\n",
      "  tasks: [\n",
      "    {\n",
      "      id: '6d0d44fd-279b-56b0-8160-8f4929f9bfe6',\n",
      "      name: 'modelNode',\n",
      "      path: [Array],\n",
      "      interrupts: [],\n",
      "      state: undefined\n",
      "    }\n",
      "  ],\n",
      "  metadata: {\n",
      "    source: 'loop',\n",
      "    writes: null,\n",
      "    step: 0,\n",
      "    parents: { '': '1ef7c6ba-3d36-65e0-8001-adc1f8841274' }\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '3',\n",
      "      checkpoint_ns: 'weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0',\n",
      "      checkpoint_id: '1ef7c6ba-3d3b-6400-8000-fe27ae37c785',\n",
      "      checkpoint_map: [Object]\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:42.368Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '3',\n",
      "      checkpoint_ns: 'weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0',\n",
      "      checkpoint_id: '1ef7c6ba-3d38-6cf1-ffff-b3912beb00b9'\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "let subgraphStateBeforeModelNode;\n",
    "\n",
    "const subgraphHistories = await graph.getStateHistory(parentGraphStateBeforeSubgraph.tasks[0].state);\n",
    "\n",
    "for await (const subgraphHistoryEntry of subgraphHistories) {\n",
    "  if (subgraphHistoryEntry.next[0] === \"modelNode\") {\n",
    "    subgraphStateBeforeModelNode = subgraphHistoryEntry;\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(subgraphStateBeforeModelNode);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern can be extended no matter how many levels deep.\n",
    "\n",
    "We can confirm that we have gotten the correct state by comparing the `.next` parameter of the `subgraphStateBeforeModelNode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'modelNode' ]\n"
     ]
    }
   ],
   "source": [
    "subgraphStateBeforeModelNode.next;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We have gotten the correct state snaphshot, and we can now resume from the `modelNode` inside of our subgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [ 'weatherGraph:ec67e50f-d29c-5dee-8a80-08723a937de0' ],\n",
      "  { modelNode: { city: 'San Francisco' } }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const resumeSubgraphStream = await graph.stream(null, {\n",
    "  ...subgraphStateBeforeModelNode.config,\n",
    "  streamMode: \"updates\",\n",
    "  subgraphs: true\n",
    "});\n",
    "\n",
    "for await (const value of resumeSubgraphStream) {\n",
    "  console.log(value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it reruns the `modelNode` and breaks right before the `weatherNode` as expected.\n",
    "\n",
    "This subsection has shown how you can replay from any node, no matter how deeply nested it is inside your graph - a powerful tool for testing how deterministic your agent is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying state\n",
    "\n",
    "### Update the state of a subgraph\n",
    "\n",
    "What if we want to modify the state of a subgraph? We can do this similarly to how we [update the state of normal graphs](https://langchain-ai.github.io/langgraphjs/how-tos/time-travel/). We just need to ensure we pass the config of the subgraph to `updateState`. Let's run our graph as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ routerNode: { route: 'weather' } }\n"
     ]
    }
   ],
   "source": [
    "const graphStream = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what's the weather in sf\"\n",
    "  }],\n",
    "}, {\n",
    "  configurable: {\n",
    "    thread_id: \"4\",\n",
    "  }\n",
    "});\n",
    "\n",
    "for await (const update of graphStream) {\n",
    "  console.log(update);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"07ed1a38-13a9-4ec2-bc88-c4f6b713ec85\",\n",
      "        \"content\": \"what's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ],\n",
      "    city: 'San Francisco'\n",
      "  },\n",
      "  next: [ 'weatherNode' ],\n",
      "  tasks: [\n",
      "    {\n",
      "      id: 'eabfbb82-6cf4-5ecd-932e-ed994ea44f23',\n",
      "      name: 'weatherNode',\n",
      "      path: [Array],\n",
      "      interrupts: [],\n",
      "      state: undefined\n",
      "    }\n",
      "  ],\n",
      "  metadata: {\n",
      "    source: 'loop',\n",
      "    writes: { modelNode: [Object] },\n",
      "    step: 1,\n",
      "    parents: { '': '1ef7c6ba-563f-60f0-8001-4fce0e78ef56' }\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '4',\n",
      "      checkpoint_id: '1ef7c6ba-5c71-6f90-8001-04f60f3c8173',\n",
      "      checkpoint_ns: 'weatherGraph:8d8c9278-bd2a-566a-b9e1-e72286634681',\n",
      "      checkpoint_map: [Object]\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:45.641Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '4',\n",
      "      checkpoint_ns: 'weatherGraph:8d8c9278-bd2a-566a-b9e1-e72286634681',\n",
      "      checkpoint_id: '1ef7c6ba-5641-6800-8000-96bcde048fa6'\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const outerGraphState = await graph.getState({\n",
    "  configurable: {\n",
    "    thread_id: \"4\",\n",
    "  }\n",
    "}, { subgraphs: true })\n",
    "\n",
    "console.log(outerGraphState.tasks[0].state);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to update the state of the **inner** graph, we need to pass the config for the **inner** graph, which we can get by accessing calling `state.tasks[0].state.config` - since we interrupted inside the subgraph, the state of the task is just the state of the subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  configurable: {\n",
      "    thread_id: '4',\n",
      "    checkpoint_ns: 'weatherGraph:8d8c9278-bd2a-566a-b9e1-e72286634681',\n",
      "    checkpoint_id: '1ef7c6ba-5de0-62f0-8002-3618a75d1fce',\n",
      "    checkpoint_map: {\n",
      "      '': '1ef7c6ba-563f-60f0-8001-4fce0e78ef56',\n",
      "      'weatherGraph:8d8c9278-bd2a-566a-b9e1-e72286634681': '1ef7c6ba-5de0-62f0-8002-3618a75d1fce'\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import type { StateSnapshot } from \"@langchain/langgraph\";\n",
    "\n",
    "await graph.updateState((outerGraphState.tasks[0].state as StateSnapshot).config, { city: \"la\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now resume streaming the outer graph (which will resume the subgraph!) and check that we updated our search to use LA instead of SF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"weatherGraph:8d8c9278-bd2a-566a-b9e1-e72286634681\"\n",
      "  ],\n",
      "  {\n",
      "    \"weatherNode\": {\n",
      "      \"messages\": [\n",
      "        {\n",
      "          \"role\": \"assistant\",\n",
      "          \"content\": \"It's sunny in la\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "[\n",
      "  [],\n",
      "  {\n",
      "    \"weatherGraph\": {\n",
      "      \"messages\": [\n",
      "        {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"HumanMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"what's the weather in sf\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {},\n",
      "            \"id\": \"07ed1a38-13a9-4ec2-bc88-c4f6b713ec85\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"It's sunny in la\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {},\n",
      "            \"id\": \"94c29f6c-38b3-420f-a9fb-bd85548f0c03\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const resumedStreamWithUpdatedState = await graph.stream(null, {\n",
    "  configurable: {\n",
    "    thread_id: \"4\",\n",
    "  },\n",
    "  streamMode: \"updates\",\n",
    "  subgraphs: true,\n",
    "})\n",
    "\n",
    "for await (const update of resumedStreamWithUpdatedState) {\n",
    "  console.log(JSON.stringify(update, null, 2));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! The AI responded with \"It's sunny in LA!\" as we expected.\n",
    "\n",
    "### Acting as a subgraph node\n",
    "\n",
    "Instead of editing the state before `weatherNode` in the `weatherGraph` subgraph, another way we could update the state is by acting as the `weatherNode` ourselves. We can do this by passing the subgraph config along with a node name passed as a third positional argument, which allows us to update the state as if we are the node we specify.\n",
    "\n",
    "We will set an interrupt before the `weatherNode` and then using the update state function as the `weatherNode`, the graph itself never calls `weatherNode` directly but instead we decide what the output of `weatherNode` should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ routerNode: { route: 'weather' } }\n",
      "interrupted!\n",
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"90e9ff28-5b13-4e10-819a-31999efe303c\",\n",
      "        \"content\": \"What's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ],\n",
      "    route: 'weather'\n",
      "  },\n",
      "  next: [ 'weatherGraph' ],\n",
      "  tasks: [\n",
      "    {\n",
      "      id: 'f421fca8-de9e-5683-87ab-6ea9bb8d6275',\n",
      "      name: 'weatherGraph',\n",
      "      path: [Array],\n",
      "      interrupts: [],\n",
      "      state: [Object]\n",
      "    }\n",
      "  ],\n",
      "  metadata: {\n",
      "    source: 'loop',\n",
      "    writes: { routerNode: [Object] },\n",
      "    step: 1,\n",
      "    parents: {}\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '14',\n",
      "      checkpoint_id: '1ef7c6ba-63ac-68f1-8001-5f7ada5f98e8',\n",
      "      checkpoint_ns: ''\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:46.399Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '14',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-5f20-6020-8000-1ff649773a32'\n",
      "    }\n",
      "  }\n",
      "}\n",
      "[ [], { weatherGraph: { messages: [Array] } } ]\n",
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"90e9ff28-5b13-4e10-819a-31999efe303c\",\n",
      "        \"content\": \"What's the weather in sf\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"af761e6d-9f6a-4467-9a3c-489bed3fbad7\",\n",
      "        \"content\": \"rainy\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ],\n",
      "    route: 'weather'\n",
      "  },\n",
      "  next: [],\n",
      "  tasks: [],\n",
      "  metadata: {\n",
      "    source: 'loop',\n",
      "    writes: { weatherGraph: [Object] },\n",
      "    step: 2,\n",
      "    parents: {}\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '14',\n",
      "      checkpoint_id: '1ef7c6ba-69e6-6cc0-8002-1751fc5bdd8f',\n",
      "      checkpoint_ns: ''\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:47.052Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '14',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-63ac-68f1-8001-5f7ada5f98e8'\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const streamWithAsNode = await graph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"What's the weather in sf\",\n",
    "  }]\n",
    "}, {\n",
    "  configurable: {\n",
    "    thread_id: \"14\",\n",
    "  }\n",
    "});\n",
    "\n",
    "for await (const update of streamWithAsNode) {\n",
    "  console.log(update);\n",
    "}\n",
    "\n",
    "// Graph execution should stop before the weather node\n",
    "console.log(\"interrupted!\");\n",
    "\n",
    "const interruptedState = await graph.getState({\n",
    "  configurable: {\n",
    "    thread_id: \"14\",\n",
    "  }\n",
    "}, { subgraphs: true });\n",
    "\n",
    "console.log(interruptedState);\n",
    "\n",
    "// We update the state by passing in the message we want returned from the weather node\n",
    "// and make sure to pass `\"weatherNode\"` to signify that we want to act as this node.\n",
    "await graph.updateState((interruptedState.tasks[0].state as StateSnapshot).config, {\n",
    "  messages: [{\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"rainy\"\n",
    "  }]\n",
    "}, \"weatherNode\");\n",
    "\n",
    "\n",
    "const resumedStreamWithAsNode = await graph.stream(null, {\n",
    "  configurable: {\n",
    "    thread_id: \"14\",\n",
    "  },\n",
    "  streamMode: \"updates\",\n",
    "  subgraphs: true,\n",
    "});\n",
    "\n",
    "for await (const update of resumedStreamWithAsNode) {\n",
    "  console.log(update);\n",
    "}\n",
    "\n",
    "console.log(await graph.getState({\n",
    "  configurable: {\n",
    "    thread_id: \"14\",\n",
    "  }\n",
    "}, { subgraphs: true }));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! The agent responded with the message we passed in ourselves, and identified the weather in SF as `rainy` instead of `sunny`.\n",
    "\n",
    "### Acting as the entire subgraph\n",
    "\n",
    "Lastly, we could also update the graph just acting as the **entire** subgraph. This is similar to the case above but instead of acting as just the `weatherNode` we are acting as the entire `weatherGraph` subgraph. This is done by passing in the normal graph config as well as the `asNode` argument, where we specify the we are acting as the entire subgraph node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [], { routerNode: { route: 'weather' } } ]\n",
      "[\n",
      "  [ 'weatherGraph:db9c3bb2-5d27-5dae-a724-a8d702b33e86' ],\n",
      "  { modelNode: { city: 'San Francisco' } }\n",
      "]\n",
      "interrupted!\n",
      "[\n",
      "  HumanMessage {\n",
      "    \"id\": \"001282b0-ca2e-443f-b6ee-8cb16c81bf59\",\n",
      "    \"content\": \"what's the weather in sf\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"4b5d49cf-0f87-4ee8-b96f-eaa8716b9e9c\",\n",
      "    \"content\": \"rainy\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": []\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const entireSubgraphExampleStream = await graph.stream({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"what's the weather in sf\"\n",
    "    }\n",
    "  ],\n",
    "}, {\n",
    "  configurable: {\n",
    "    thread_id: \"8\",\n",
    "  },\n",
    "  streamMode: \"updates\",\n",
    "  subgraphs: true,\n",
    "});\n",
    "\n",
    "for await (const update of entireSubgraphExampleStream) {\n",
    "  console.log(update);\n",
    "}\n",
    "\n",
    "// Graph execution should stop before the weather node\n",
    "console.log(\"interrupted!\");\n",
    "\n",
    "// We update the state by passing in the message we want returned from the weather graph.\n",
    "// Note that we don't need to pass in the subgraph config, since we aren't updating the state inside the subgraph\n",
    "await graph.updateState({\n",
    "  configurable: {\n",
    "    thread_id: \"8\",\n",
    "  }\n",
    "}, {\n",
    "  messages: [{ role: \"assistant\", content: \"rainy\" }]\n",
    "}, \"weatherGraph\");\n",
    "\n",
    "const resumedEntireSubgraphExampleStream = await graph.stream(null, {\n",
    "  configurable: {\n",
    "    thread_id: \"8\",\n",
    "  },\n",
    "  streamMode: \"updates\",\n",
    "});\n",
    "\n",
    "for await (const update of resumedEntireSubgraphExampleStream) {\n",
    "  console.log(update);\n",
    "}\n",
    "\n",
    "const currentStateAfterUpdate = await graph.getState({\n",
    "  configurable: {\n",
    "    thread_id: \"8\",\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(currentStateAfterUpdate.values.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the agent responded with \"rainy\" as we expected.\n",
    "\n",
    "## Double nested subgraphs\n",
    "\n",
    "This same functionality continues to work no matter the level of nesting. Here is an example of doing the same things with a double nested subgraph (although any level of nesting will work). We add another router on top of our already defined graphs.\n",
    "\n",
    "First, let's recreate the graph we've been using above. This time we will compile it with no checkpointer, since it itself will be a subgraph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "const parentGraph = new StateGraph(RouterStateAnnotation)\n",
    "  .addNode(\"routerNode\", routerNode)\n",
    "  .addNode(\"normalLLMNode\", normalLLMNode)\n",
    "  .addNode(\"weatherGraph\", subgraph)\n",
    "  .addEdge(\"__start__\", \"routerNode\")\n",
    "  .addConditionalEdges(\"routerNode\", routeAfterPrediction)\n",
    "  .addEdge(\"normalLLMNode\", \"__end__\")\n",
    "  .addEdge(\"weatherGraph\", \"__end__\")\n",
    "  .compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's declare a \"grandparent\" graph that uses this graph as a subgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const GrandfatherStateAnnotation = Annotation.Root({\n",
    "  ...MessagesAnnotation.spec,\n",
    "  toContinue: Annotation<boolean>,\n",
    "});\n",
    "\n",
    "const grandparentRouterNode = async (_state: typeof GrandfatherStateAnnotation.State) => {\n",
    "  // Dummy logic that will always continue\n",
    "  return { toContinue: true };\n",
    "};\n",
    "\n",
    "const grandparentConditionalEdge = async (state: typeof GrandfatherStateAnnotation.State) => {\n",
    "  if (state.toContinue) {\n",
    "    return \"parentGraph\";\n",
    "  } else {\n",
    "    return \"__end__\";\n",
    "  }\n",
    "};\n",
    "\n",
    "const grandparentGraph = new StateGraph(GrandfatherStateAnnotation)\n",
    "  .addNode(\"routerNode\", grandparentRouterNode)\n",
    "  .addNode(\"parentGraph\", parentGraph)\n",
    "  .addEdge(\"__start__\", \"routerNode\")\n",
    "  .addConditionalEdges(\"routerNode\", grandparentConditionalEdge)\n",
    "  .addEdge(\"parentGraph\", \"__end__\")\n",
    "  .compile({ checkpointer });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a diagram showing what this looks like:\n",
    "\n",
    "![](./img/doubly-nested-subgraph.jpeg)\n",
    "\n",
    "If we run until the interrupt, we can now see that there are snapshots of the state of all three graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [], { routerNode: { toContinue: true } } ]\n",
      "[\n",
      "  [ 'parentGraph:095bb8a9-77d3-5a0c-9a23-e1390dcf36bc' ],\n",
      "  { routerNode: { route: 'weather' } }\n",
      "]\n",
      "[\n",
      "  [\n",
      "    'parentGraph:095bb8a9-77d3-5a0c-9a23-e1390dcf36bc',\n",
      "    'weatherGraph:b1da376c-25a5-5aae-82da-4ff579f05d43'\n",
      "  ],\n",
      "  { modelNode: { city: 'San Francisco' } }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const grandparentConfig = {\n",
    "  configurable: { thread_id: \"123\" },\n",
    "};\n",
    "\n",
    "const grandparentGraphStream = await grandparentGraph.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"what's the weather in SF\"\n",
    "  }],\n",
    "}, {\n",
    "  ...grandparentConfig,\n",
    "  streamMode: \"updates\",\n",
    "  subgraphs: true\n",
    "});\n",
    "\n",
    "for await (const update of grandparentGraphStream) {\n",
    "  console.log(update);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grandparent State:\n",
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "      \"content\": \"what's the weather in SF\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    }\n",
      "  ],\n",
      "  toContinue: true\n",
      "}\n",
      "---------------\n",
      "Parent Graph State:\n",
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "      \"content\": \"what's the weather in SF\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    }\n",
      "  ],\n",
      "  route: 'weather'\n",
      "}\n",
      "---------------\n",
      "Subgraph State:\n",
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "      \"content\": \"what's the weather in SF\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    }\n",
      "  ],\n",
      "  city: 'San Francisco'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const grandparentGraphState = await grandparentGraph.getState(\n",
    "  grandparentConfig, \n",
    "  { subgraphs: true }\n",
    ");\n",
    "\n",
    "const parentGraphState = grandparentGraphState.tasks[0].state as StateSnapshot;\n",
    "const subgraphState = parentGraphState.tasks[0].state as StateSnapshot;\n",
    "\n",
    "console.log(\"Grandparent State:\");\n",
    "console.log(grandparentGraphState.values);\n",
    "console.log(\"---------------\");\n",
    "console.log(\"Parent Graph State:\");\n",
    "console.log(parentGraphState.values);\n",
    "console.log(\"---------------\");\n",
    "console.log(\"Subgraph State:\");\n",
    "console.log(subgraphState.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now continue, acting as the node three levels down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [ 'parentGraph:095bb8a9-77d3-5a0c-9a23-e1390dcf36bc' ],\n",
      "  { weatherGraph: { messages: [Array] } }\n",
      "]\n",
      "[ [], { parentGraph: { messages: [Array] } } ]\n",
      "[\n",
      "  HumanMessage {\n",
      "    \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "    \"content\": \"what's the weather in SF\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"1c161973-9a9d-414d-b631-56791d85e2fb\",\n",
      "    \"content\": \"rainy\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": []\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "await grandparentGraph.updateState(subgraphState.config, {\n",
    "  messages: [{\n",
    "    role: \"assistant\",\n",
    "    content: \"rainy\"\n",
    "  }]\n",
    "}, \"weatherNode\");\n",
    "\n",
    "const updatedGrandparentGraphStream = await grandparentGraph.stream(null, {\n",
    "  ...grandparentConfig,\n",
    "  streamMode: \"updates\",\n",
    "  subgraphs: true,\n",
    "});\n",
    "\n",
    "for await (const update of updatedGrandparentGraphStream) {\n",
    "  console.log(update);\n",
    "}\n",
    "\n",
    "console.log((await grandparentGraph.getState(grandparentConfig)).values.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the cases above, we can see that the AI responds with \"rainy\" as we expect.\n",
    "\n",
    "We can explore the state history to see how the state of the grandparent graph was updated at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "        \"content\": \"what's the weather in SF\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      AIMessage {\n",
      "        \"id\": \"1c161973-9a9d-414d-b631-56791d85e2fb\",\n",
      "        \"content\": \"rainy\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {},\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    ],\n",
      "    toContinue: true\n",
      "  },\n",
      "  next: [],\n",
      "  tasks: [],\n",
      "  metadata: {\n",
      "    source: 'loop',\n",
      "    writes: { parentGraph: [Object] },\n",
      "    step: 2,\n",
      "    parents: {}\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-8560-67d0-8002-2e2cedd7de18'\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:49.933Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-7977-62c0-8001-13e89cb2bbab'\n",
      "    }\n",
      "  }\n",
      "}\n",
      "-----\n",
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "        \"content\": \"what's the weather in SF\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ],\n",
      "    toContinue: true\n",
      "  },\n",
      "  next: [ 'parentGraph' ],\n",
      "  tasks: [\n",
      "    {\n",
      "      id: '095bb8a9-77d3-5a0c-9a23-e1390dcf36bc',\n",
      "      name: 'parentGraph',\n",
      "      path: [Array],\n",
      "      interrupts: [],\n",
      "      state: [Object]\n",
      "    }\n",
      "  ],\n",
      "  metadata: {\n",
      "    source: 'loop',\n",
      "    writes: { routerNode: [Object] },\n",
      "    step: 1,\n",
      "    parents: {}\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-7977-62c0-8001-13e89cb2bbab'\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:48.684Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-7972-64a0-8000-243575e3244f'\n",
      "    }\n",
      "  }\n",
      "}\n",
      "-----\n",
      "{\n",
      "  values: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"id\": \"5788e436-a756-4ff5-899a-82117a5c59c7\",\n",
      "        \"content\": \"what's the weather in SF\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  next: [ 'routerNode' ],\n",
      "  tasks: [\n",
      "    {\n",
      "      id: '00ed334c-47b5-5693-92b4-a5b83373e2a0',\n",
      "      name: 'routerNode',\n",
      "      path: [Array],\n",
      "      interrupts: [],\n",
      "      state: undefined\n",
      "    }\n",
      "  ],\n",
      "  metadata: { source: 'loop', writes: null, step: 0, parents: {} },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-7972-64a0-8000-243575e3244f'\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:48.682Z',\n",
      "  parentConfig: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-796f-6d90-ffff-25ed0eb5bb38'\n",
      "    }\n",
      "  }\n",
      "}\n",
      "-----\n",
      "{\n",
      "  values: { messages: [] },\n",
      "  next: [ '__start__' ],\n",
      "  tasks: [\n",
      "    {\n",
      "      id: 'ea62628e-881d-558d-bafc-e8b6a734e8aa',\n",
      "      name: '__start__',\n",
      "      path: [Array],\n",
      "      interrupts: [],\n",
      "      state: undefined\n",
      "    }\n",
      "  ],\n",
      "  metadata: {\n",
      "    source: 'input',\n",
      "    writes: { __start__: [Object] },\n",
      "    step: -1,\n",
      "    parents: {}\n",
      "  },\n",
      "  config: {\n",
      "    configurable: {\n",
      "      thread_id: '123',\n",
      "      checkpoint_ns: '',\n",
      "      checkpoint_id: '1ef7c6ba-796f-6d90-ffff-25ed0eb5bb38'\n",
      "    }\n",
      "  },\n",
      "  createdAt: '2024-09-27T00:58:48.681Z',\n",
      "  parentConfig: undefined\n",
      "}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "const grandparentStateHistories = await grandparentGraph.getStateHistory(grandparentConfig);\n",
    "for await (const state of grandparentStateHistories) {\n",
    "  console.log(state);\n",
    "  console.log(\"-----\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/time-travel.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90616e9e",
      "metadata": {},
      "source": [
        "# How to view and update past graph state\n",
        "\n",
        "!!! tip \"Prerequisites\"\n",
        "\n",
        "    This guide assumes familiarity with the following concepts:\n",
        "\n",
        "    * [Time Travel](/langgraphjs/concepts/time-travel)\n",
        "    * [Breakpoints](/langgraphjs/concepts/breakpoints)\n",
        "    * [LangGraph Glossary](/langgraphjs/concepts/low_level)\n",
        "\n",
        "Once you start [checkpointing](./persistence.ipynb) your graphs, you can easily\n",
        "**get** or **update** the state of the agent at any point in time. This permits\n",
        "a few things:\n",
        "\n",
        "1. You can surface a state during an interrupt to a user to let them accept an\n",
        "   action.\n",
        "2. You can **rewind** the graph to reproduce or avoid issues.\n",
        "3. You can **modify** the state to embed your agent into a larger system, or to\n",
        "   let the user better control its actions.\n",
        "\n",
        "The key methods used for this functionality are:\n",
        "\n",
        "- [getState](/langgraphjs/reference/classes/langgraph_pregel.Pregel.html#getState):\n",
        "  fetch the values from the target config\n",
        "- [updateState](/langgraphjs/reference/classes/langgraph_pregel.Pregel.html#updateState):\n",
        "  apply the given values to the target state\n",
        "\n",
        "**Note:** this requires passing in a checkpointer.\n",
        "\n",
        "<!-- Example:\n",
        "```javascript\n",
        "TODO\n",
        "...\n",
        "``` -->\n",
        "\n",
        "This works for\n",
        "[StateGraph](/langgraphjs/reference/classes/langgraph.StateGraph.html)\n",
        "and all its subclasses, such as\n",
        "[MessageGraph](/langgraphjs/reference/classes/langgraph.MessageGraph.html).\n",
        "\n",
        "Below is an example.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the <code>createReactAgent(model, tools=tool, checkpointer=checkpointer)</code> (<a href=\"/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html\">API doc</a>) constructor. This may be more appropriate if you are used to LangChain's <a href=\"https://js.langchain.com/docs/how_to/agent_executor\">AgentExecutor</a> class.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "## Setup\n",
        "\n",
        "This guide will use OpenAI's GPT-4o model. We will optionally set our API key\n",
        "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
        "best-in-class observability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a7df1d0",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time Travel: LangGraphJS\n"
          ]
        }
      ],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "\n",
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
        "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
        "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "process.env.LANGCHAIN_PROJECT = \"Time Travel: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79ba1c0",
      "metadata": {},
      "source": [
        "## Define the state\n",
        "\n",
        "The state is the interface for all of the nodes in our graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "44968352",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c88187",
      "metadata": {},
      "source": [
        "## Set up the tools\n",
        "\n",
        "We will first define the tools we want to use. For this simple example, we will\n",
        "use create a placeholder search engine. However, it is really easy to create\n",
        "your own tools - see documentation\n",
        "[here](https://js.langchain.com/docs/how_to/custom_tools) on how to do\n",
        "that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b22edfc4",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const searchTool = tool(async (_) => {\n",
        "  // This is a placeholder for the actual implementation\n",
        "  return \"Cold, with a low of 13 ℃\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description:\n",
        "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
        "  schema: z.object({\n",
        "    query: z.string().describe(\"The query to use in your search.\"),\n",
        "  }),\n",
        "});\n",
        "\n",
        "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
        "\n",
        "const tools = [searchTool];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c764430",
      "metadata": {},
      "source": [
        "We can now wrap these tools in a simple\n",
        "[ToolNode](/langgraphjs/reference/classes/prebuilt.ToolNode.html).\n",
        "This object will actually run the tools (functions) whenever they are invoked by\n",
        "our LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0cc63f1f",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const toolNode = new ToolNode(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc409cd5",
      "metadata": {},
      "source": [
        "## Set up the model\n",
        "\n",
        "Now we will load the\n",
        "[chat model](https://js.langchain.com/docs/concepts/chat_models/).\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form\n",
        "   of messages, so it needs to be able to work well with them.\n",
        "2. It should work with\n",
        "   [tool calling](https://js.langchain.com/docs/how_to/tool_calling/#passing-tools-to-llms),\n",
        "   meaning it can return function arguments in its response.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p>\n",
        "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dae9ab9c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({ model: \"gpt-4o\" });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5cfd558",
      "metadata": {},
      "source": [
        "After we've done this, we should make sure the model knows that it has these\n",
        "tools available to call. We can do this by calling\n",
        "[bindTools](https://v01.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html#bindTools).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ca438e74",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "const boundModel = model.bindTools(tools);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2b8a4f",
      "metadata": {},
      "source": [
        "## Define the graph\n",
        "\n",
        "We can now put it all together. Time travel requires a checkpointer to save the\n",
        "state - otherwise you wouldn't have anything go `get` or `update`. We will use\n",
        "the\n",
        "[MemorySaver](/langgraphjs/reference/classes/index.MemorySaver.html),\n",
        "which \"saves\" checkpoints in-memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a29ec2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "const routeMessage = (state: typeof StateAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  // If no tools are called, we can finish (respond to the user)\n",
        "  if (!lastMessage?.tool_calls?.length) {\n",
        "    return END;\n",
        "  }\n",
        "  // Otherwise if there is, we continue and call the tools\n",
        "  return \"tools\";\n",
        "};\n",
        "\n",
        "const callModel = async (\n",
        "  state: typeof StateAnnotation.State,\n",
        "  config?: RunnableConfig,\n",
        "): Promise<Partial<typeof StateAnnotation.State>> => {\n",
        "  const { messages } = state;\n",
        "  const response = await boundModel.invoke(messages, config);\n",
        "  return { messages: [response] };\n",
        "};\n",
        "\n",
        "const workflow = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(START, \"agent\")\n",
        "  .addConditionalEdges(\"agent\", routeMessage)\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "// Here we only save in-memory\n",
        "let memory = new MemorySaver();\n",
        "const graph = workflow.compile({ checkpointer: memory });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6dd42a3",
      "metadata": {},
      "source": [
        "## Interacting with the Agent\n",
        "\n",
        "We can now interact with the agent. Between interactions you can get and update\n",
        "state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0749329a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi I'm Jo.\n",
            "-----\n",
            "\n",
            "Hello, Jo! How can I assist you today?\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "let config = { configurable: { thread_id: \"conversation-num-1\" } };\n",
        "let inputs = { messages: [{ role: \"user\", content: \"Hi I'm Jo.\" }] } as any;\n",
        "for await (\n",
        "  const { messages } of await graph.stream(inputs, {\n",
        "    ...config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221f323d",
      "metadata": {},
      "source": [
        "See LangSmith example run here\n",
        "https://smith.langchain.com/public/b3feb09b-bcd2-4ad5-ad1d-414106148448/r\n",
        "\n",
        "Here you can see the \"agent\" node ran, and then our edge returned `__end__` so\n",
        "the graph stopped execution there.\n",
        "\n",
        "Let's check the current graph state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6ff5468d",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    { role: 'user', content: \"Hi I'm Jo.\" },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-A3FGf3k3QQo9q0QjT6Oc5h1XplkHr\",\n",
            "      \"content\": \"Hello, Jo! How can I assist you today?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 12,\n",
            "          \"promptTokens\": 68,\n",
            "          \"totalTokens\": 80\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_fde2829a40\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": []\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "let checkpoint = await graph.getState(config);\n",
        "checkpoint.values;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "571077e2",
      "metadata": {},
      "source": [
        "The current state is the two messages we've seen above, 1. the HumanMessage we\n",
        "sent in, 2. the AIMessage we got back from the model.\n",
        "\n",
        "The `next` values are empty since the graph has terminated (transitioned to the\n",
        "`__end__`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "22b25946",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "checkpoint.next;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889cd8ce",
      "metadata": {},
      "source": [
        "## Let's get it to execute a tool\n",
        "\n",
        "When we call the graph again, it will create a checkpoint after each internal\n",
        "execution step. Let's get it to run a tool, then look at the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "873b3438",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What's the weather like in SF currently?\n",
            "-----\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    name: 'search',\n",
            "    args: { query: 'current weather in San Francisco' },\n",
            "    type: 'tool_call',\n",
            "    id: 'call_ZtmtDOyEXDCnXDgowlit5dSd'\n",
            "  }\n",
            "]\n",
            "-----\n",
            "\n",
            "Cold, with a low of 13 ℃\n",
            "-----\n",
            "\n",
            "The current weather in San Francisco is cold, with a low of 13°C.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = { messages: [{ role: \"user\", content: \"What's the weather like in SF currently?\" }] } as any;\n",
        "for await (\n",
        "  const { messages } of await graph.stream(inputs, {\n",
        "    ...config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6384c1e3",
      "metadata": {},
      "source": [
        "See the trace of the above execution here:\n",
        "https://smith.langchain.com/public/0ef426fd-0da1-4c02-a50b-64ae1e68338e/r We can\n",
        "see it planned the tool execution (ie the \"agent\" node), then \"should_continue\"\n",
        "edge returned \"continue\" so we proceeded to \"action\" node, which executed the\n",
        "tool, and then \"agent\" node emitted the final response, which made\n",
        "\"should_continue\" edge return \"end\". Let's see how we can have more control over\n",
        "this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3fe0ce",
      "metadata": {},
      "source": [
        "### Pause before tools\n",
        "\n",
        "If you notice below, we now will add `interruptBefore=[\"action\"]` - this means\n",
        "that before any actions are taken we pause. This is a great moment to allow the\n",
        "user to correct and update the state! This is very useful when you want to have\n",
        "a human-in-the-loop to validate (and potentially change) the action to take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "736be42e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What's the weather like in SF currently?\n",
            "-----\n",
            "\n",
            "[\n",
            "  {\n",
            "    name: 'search',\n",
            "    args: { query: 'current weather in San Francisco' },\n",
            "    type: 'tool_call',\n",
            "    id: 'call_OsKnTv2psf879eeJ9vx5GeoY'\n",
            "  }\n",
            "]\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "memory = new MemorySaver();\n",
        "const graphWithInterrupt = workflow.compile({\n",
        "  checkpointer: memory,\n",
        "  interruptBefore: [\"tools\"],\n",
        "});\n",
        "\n",
        "inputs = { messages: [{ role: \"user\", content: \"What's the weather like in SF currently?\" }] } as any;\n",
        "for await (\n",
        "  const { messages } of await graphWithInterrupt.stream(inputs, {\n",
        "    ...config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf27f2b4",
      "metadata": {},
      "source": [
        "## Get State\n",
        "\n",
        "You can fetch the latest graph checkpoint using\n",
        "[`getState(config)`](/langgraphjs/reference/classes/langgraph.Pregel.html#getState)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0f434f69",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 'tools' ]\n"
          ]
        }
      ],
      "source": [
        "let snapshot = await graphWithInterrupt.getState(config);\n",
        "snapshot.next;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f78ad8f",
      "metadata": {},
      "source": [
        "## Resume\n",
        "\n",
        "You can resume by running the graph with a `null` input. The checkpoint is\n",
        "loaded, and with no new inputs, it will execute as if no interrupt had occurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fd4d7eff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cold, with a low of 13 ℃\n",
            "-----\n",
            "\n",
            "Currently, it is cold in San Francisco, with a temperature around 13°C (55°F).\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for await (\n",
        "  const { messages } of await graphWithInterrupt.stream(null, {\n",
        "    ...snapshot.config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2885d91d",
      "metadata": {},
      "source": [
        "## Check full history\n",
        "\n",
        "Let's browse the history of this thread, from newest to oldest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc7acb70",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  values: {\n",
            "    messages: [\n",
            "      [Object],\n",
            "      AIMessage {\n",
            "        \"id\": \"chatcmpl-A3FGhKzOZs0GYZ2yalNOCQZyPgbcp\",\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "          \"tool_calls\": [\n",
            "            {\n",
            "              \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\",\n",
            "              \"type\": \"function\",\n",
            "              \"function\": \"[Object]\"\n",
            "            }\n",
            "          ]\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "          \"tokenUsage\": {\n",
            "            \"completionTokens\": 17,\n",
            "            \"promptTokens\": 72,\n",
            "            \"totalTokens\": 89\n",
            "          },\n",
            "          \"finish_reason\": \"tool_calls\",\n",
            "          \"system_fingerprint\": \"fp_fde2829a40\"\n",
            "        },\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"name\": \"search\",\n",
            "            \"args\": {\n",
            "              \"query\": \"current weather in San Francisco\"\n",
            "            },\n",
            "            \"type\": \"tool_call\",\n",
            "            \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\"\n",
            "          }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": []\n",
            "      },\n",
            "      ToolMessage {\n",
            "        \"content\": \"Cold, with a low of 13 ℃\",\n",
            "        \"name\": \"search\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"tool_call_id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\"\n",
            "      },\n",
            "      AIMessage {\n",
            "        \"id\": \"chatcmpl-A3FGiYripPKtQLnAK1H3hWLSXQfOD\",\n",
            "        \"content\": \"Currently, it is cold in San Francisco, with a temperature around 13°C (55°F).\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {\n",
            "          \"tokenUsage\": {\n",
            "            \"completionTokens\": 21,\n",
            "            \"promptTokens\": 105,\n",
            "            \"totalTokens\": 126\n",
            "          },\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"system_fingerprint\": \"fp_fde2829a40\"\n",
            "        },\n",
            "        \"tool_calls\": [],\n",
            "        \"invalid_tool_calls\": []\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  next: [],\n",
            "  tasks: [],\n",
            "  metadata: { source: 'loop', writes: { agent: [Object] }, step: 3 },\n",
            "  config: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-9c3a-6bd1-8003-d7f030ff72b2'\n",
            "    }\n",
            "  },\n",
            "  createdAt: '2024-09-03T04:17:20.653Z',\n",
            "  parentConfig: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-9516-6200-8002-43d2c6dc603f'\n",
            "    }\n",
            "  }\n",
            "}\n",
            "--\n",
            "{\n",
            "  values: {\n",
            "    messages: [\n",
            "      [Object],\n",
            "      AIMessage {\n",
            "        \"id\": \"chatcmpl-A3FGhKzOZs0GYZ2yalNOCQZyPgbcp\",\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "          \"tool_calls\": [\n",
            "            {\n",
            "              \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\",\n",
            "              \"type\": \"function\",\n",
            "              \"function\": \"[Object]\"\n",
            "            }\n",
            "          ]\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "          \"tokenUsage\": {\n",
            "            \"completionTokens\": 17,\n",
            "            \"promptTokens\": 72,\n",
            "            \"totalTokens\": 89\n",
            "          },\n",
            "          \"finish_reason\": \"tool_calls\",\n",
            "          \"system_fingerprint\": \"fp_fde2829a40\"\n",
            "        },\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"name\": \"search\",\n",
            "            \"args\": {\n",
            "              \"query\": \"current weather in San Francisco\"\n",
            "            },\n",
            "            \"type\": \"tool_call\",\n",
            "            \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\"\n",
            "          }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": []\n",
            "      },\n",
            "      ToolMessage {\n",
            "        \"content\": \"Cold, with a low of 13 ℃\",\n",
            "        \"name\": \"search\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"tool_call_id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  next: [ 'agent' ],\n",
            "  tasks: [\n",
            "    {\n",
            "      id: '612efffa-3b16-530f-8a39-fd01c31e7b8b',\n",
            "      name: 'agent',\n",
            "      interrupts: []\n",
            "    }\n",
            "  ],\n",
            "  metadata: { source: 'loop', writes: { tools: [Object] }, step: 2 },\n",
            "  config: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-9516-6200-8002-43d2c6dc603f'\n",
            "    }\n",
            "  },\n",
            "  createdAt: '2024-09-03T04:17:19.904Z',\n",
            "  parentConfig: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-9455-6410-8001-1c78a97f63e6'\n",
            "    }\n",
            "  }\n",
            "}\n",
            "--\n",
            "{\n",
            "  values: {\n",
            "    messages: [\n",
            "      [Object],\n",
            "      AIMessage {\n",
            "        \"id\": \"chatcmpl-A3FGhKzOZs0GYZ2yalNOCQZyPgbcp\",\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {\n",
            "          \"tool_calls\": [\n",
            "            {\n",
            "              \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\",\n",
            "              \"type\": \"function\",\n",
            "              \"function\": \"[Object]\"\n",
            "            }\n",
            "          ]\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "          \"tokenUsage\": {\n",
            "            \"completionTokens\": 17,\n",
            "            \"promptTokens\": 72,\n",
            "            \"totalTokens\": 89\n",
            "          },\n",
            "          \"finish_reason\": \"tool_calls\",\n",
            "          \"system_fingerprint\": \"fp_fde2829a40\"\n",
            "        },\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"name\": \"search\",\n",
            "            \"args\": {\n",
            "              \"query\": \"current weather in San Francisco\"\n",
            "            },\n",
            "            \"type\": \"tool_call\",\n",
            "            \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\"\n",
            "          }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": []\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  next: [ 'tools' ],\n",
            "  tasks: [\n",
            "    {\n",
            "      id: '767116b0-55b6-5af4-8f74-ce45fb6e31ed',\n",
            "      name: 'tools',\n",
            "      interrupts: []\n",
            "    }\n",
            "  ],\n",
            "  metadata: { source: 'loop', writes: { agent: [Object] }, step: 1 },\n",
            "  config: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-9455-6410-8001-1c78a97f63e6'\n",
            "    }\n",
            "  },\n",
            "  createdAt: '2024-09-03T04:17:19.825Z',\n",
            "  parentConfig: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-8c4b-6261-8000-c51e5807fbcd'\n",
            "    }\n",
            "  }\n",
            "}\n",
            "--\n",
            "{\n",
            "  values: { messages: [ [Object] ] },\n",
            "  next: [ 'agent' ],\n",
            "  tasks: [\n",
            "    {\n",
            "      id: '5b0ed7d1-1bb7-5d78-b4fc-7a8ed40e7291',\n",
            "      name: 'agent',\n",
            "      interrupts: []\n",
            "    }\n",
            "  ],\n",
            "  metadata: { source: 'loop', writes: null, step: 0 },\n",
            "  config: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-8c4b-6261-8000-c51e5807fbcd'\n",
            "    }\n",
            "  },\n",
            "  createdAt: '2024-09-03T04:17:18.982Z',\n",
            "  parentConfig: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-8c4b-6260-ffff-6ec582916c42'\n",
            "    }\n",
            "  }\n",
            "}\n",
            "--\n",
            "{\n",
            "  values: {},\n",
            "  next: [ '__start__' ],\n",
            "  tasks: [\n",
            "    {\n",
            "      id: 'a4250d5c-d025-5da1-b588-cae2b3f4a8c7',\n",
            "      name: '__start__',\n",
            "      interrupts: []\n",
            "    }\n",
            "  ],\n",
            "  metadata: { source: 'input', writes: { messages: [Array] }, step: -1 },\n",
            "  config: {\n",
            "    configurable: {\n",
            "      thread_id: 'conversation-num-1',\n",
            "      checkpoint_ns: '',\n",
            "      checkpoint_id: '1ef69ab6-8c4b-6260-ffff-6ec582916c42'\n",
            "    }\n",
            "  },\n",
            "  createdAt: '2024-09-03T04:17:18.982Z',\n",
            "  parentConfig: undefined\n",
            "}\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "let toReplay;\n",
        "const states = await graphWithInterrupt.getStateHistory(config);\n",
        "for await (const state of states) {\n",
        "  console.log(state);\n",
        "  console.log(\"--\");\n",
        "  if (state.values?.messages?.length === 2) {\n",
        "    toReplay = state;\n",
        "  }\n",
        "}\n",
        "if (!toReplay) {\n",
        "  throw new Error(\"No state to replay\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "342f0154",
      "metadata": {},
      "source": [
        "## Replay a past state\n",
        "\n",
        "To replay from this place we just need to pass its config back to the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c1cefbfa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cold, with a low of 13 ℃\n",
            "-----\n",
            "\n",
            "The current weather in San Francisco is cold, with a low of 13°C.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for await (\n",
        "  const { messages } of await graphWithInterrupt.stream(null, {\n",
        "    ...toReplay.config,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e870c084",
      "metadata": {},
      "source": [
        "## Branch off a past state\n",
        "\n",
        "Using LangGraph's checkpointing, you can do more than just replay past states.\n",
        "You can branch off previous locations to let the agent explore alternate\n",
        "trajectories or to let a user \"version control\" changes in a workflow.\n",
        "\n",
        "#### First, update a previous checkpoint\n",
        "\n",
        "Updating the state will create a **new** snapshot by applying the update to the\n",
        "previous checkpoint. Let's **add a tool message** to simulate calling the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d7656840-3a4a-4a80-af74-214b35cfbadd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    {\n",
            "      role: 'user',\n",
            "      content: \"What's the weather like in SF currently?\"\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-A3FGhKzOZs0GYZ2yalNOCQZyPgbcp\",\n",
            "      \"content\": \"\",\n",
            "      \"additional_kwargs\": {\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\",\n",
            "            \"type\": \"function\",\n",
            "            \"function\": \"[Object]\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 17,\n",
            "          \"promptTokens\": 72,\n",
            "          \"totalTokens\": 89\n",
            "        },\n",
            "        \"finish_reason\": \"tool_calls\",\n",
            "        \"system_fingerprint\": \"fp_fde2829a40\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"query\": \"current weather in San Francisco\"\n",
            "          },\n",
            "          \"type\": \"tool_call\",\n",
            "          \"id\": \"call_OsKnTv2psf879eeJ9vx5GeoY\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    {\n",
            "      role: 'tool',\n",
            "      content: \"It's sunny out, with a high of 38 ℃.\",\n",
            "      tool_call_id: 'call_OsKnTv2psf879eeJ9vx5GeoY'\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "[ 'agent' ]\n"
          ]
        }
      ],
      "source": [
        "const tool_calls =\n",
        "  toReplay.values.messages[toReplay.values.messages.length - 1].tool_calls;\n",
        "const branchConfig = await graphWithInterrupt.updateState(\n",
        "  toReplay.config,\n",
        "  {\n",
        "    messages: [\n",
        "      { role: \"tool\", content: \"It's sunny out, with a high of 38 ℃.\", tool_call_id: tool_calls[0].id },\n",
        "    ],\n",
        "  },\n",
        "  // Updates are applied \"as if\" they were coming from a node. By default,\n",
        "  // the updates will come from the last node to run. In our case, we want to treat\n",
        "  // this update as if it came from the tools node, so that the next node to run will be\n",
        "  // the agent.\n",
        "  \"tools\",\n",
        ");\n",
        "\n",
        "const branchState = await graphWithInterrupt.getState(branchConfig);\n",
        "console.log(branchState.values);\n",
        "console.log(branchState.next);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4689abd9-1008-4d8b-902c-e956a5913e12",
      "metadata": {},
      "source": [
        "#### Now you can run from this branch\n",
        "\n",
        "Just use the updated config (containing the new checkpoint ID). The trajectory\n",
        "will follow the new branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bb95930f-07e5-4e32-8e38-2170d36ab1a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current weather in San Francisco is sunny, with a high of 38°C.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for await (\n",
        "  const { messages } of await graphWithInterrupt.stream(null, {\n",
        "    ...branchConfig,\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "  let msg = messages[messages?.length - 1];\n",
        "  if (msg?.content) {\n",
        "    console.log(msg.content);\n",
        "  } else if (msg?.tool_calls?.length > 0) {\n",
        "    console.log(msg.tool_calls);\n",
        "  } else {\n",
        "    console.log(msg);\n",
        "  }\n",
        "  console.log(\"-----\\n\");\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="how-tos/tool-calling-errors.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to handle tool calling errors\n",
        "\n",
        "LLMs aren't perfect at calling tools. The model may try to call a tool that doesn't exist or fail to return arguments that match the requested schema. Strategies like keeping schemas simple, reducing the number of tools you pass at once, and having good names and descriptions can help mitigate this risk, but aren't foolproof.\n",
        "\n",
        "This guide covers some ways to build error handling into your graphs to mitigate these failure modes.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Compatibility</p>\n",
        "    <p>\n",
        "        This guide requires <code>@langchain/langgraph>=0.0.28</code>, <code>@langchain/anthropic>=0.2.6</code>, and <code>@langchain/core>=0.2.17</code>. For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "## Using the prebuilt `ToolNode`\n",
        "\n",
        "To start, define a mock weather tool that has some hidden restrictions on input queries. The intent here is to simulate a real-world case where a model fails to call a tool correctly:\n",
        "\n",
        "```bash\n",
        "$ npm install @langchain/langgraph @langchain/anthropic @langchain/core\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { z } from \"zod\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "\n",
        "const getWeather = tool(async ({ location }) => {\n",
        "  if (location === \"SAN FRANCISCO\") {\n",
        "    return \"It's 60 degrees and foggy\";\n",
        "  } else if (location.toLowerCase() === \"san francisco\") {\n",
        "    throw new Error(\"Input queries must be all capitals\");\n",
        "  } else {\n",
        "    throw new Error(\"Invalid input.\");\n",
        "  }\n",
        "}, {\n",
        "  name: \"get_weather\",\n",
        "  description: \"Call to get the current weather\",\n",
        "  schema: z.object({\n",
        "    location: z.string(),\n",
        "  }),\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, set up a graph implementation of the [ReAct agent](/langgraphjs/concepts/). This agent takes some query as input, then repeatedly call tools until it has enough information to resolve the query. We'll use the prebuilt [`ToolNode`](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html) to execute called tools, and a small, fast model powered by Anthropic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph, MessagesAnnotation } from \"@langchain/langgraph\";\n",
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const toolNode = new ToolNode([getWeather]);\n",
        "\n",
        "const modelWithTools = new ChatAnthropic({\n",
        "  model: \"claude-3-haiku-20240307\",\n",
        "  temperature: 0,\n",
        "}).bindTools([getWeather]);\n",
        "\n",
        "const shouldContinue = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1];\n",
        "  if (isAIMessage(lastMessage) && lastMessage.tool_calls?.length) {\n",
        "    return \"tools\";\n",
        "  }\n",
        "  return \"__end__\";\n",
        "}\n",
        "\n",
        "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const response = await modelWithTools.invoke(messages);\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "const app = new StateGraph(MessagesAnnotation)\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNode)\n",
        "  .addEdge(\"__start__\", \"agent\")\n",
        "  .addEdge(\"tools\", \"agent\")\n",
        "  .addConditionalEdges(\"agent\", shouldContinue, {\n",
        "    // Explicitly list possible destinations so that\n",
        "    // we can automatically draw the graph below.\n",
        "    tools: \"tools\",\n",
        "    __end__: \"__end__\",\n",
        "  })\n",
        "  .compile();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const graph = app.getGraph();\n",
        "const image = await graph.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you try to call the tool, you can see that the model calls the tool with a bad input, causing the tool to throw an error. The prebuilt `ToolNode` that executes the tool has some built-in error handling that captures the error and passes it back to the model so that it can try again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HUMAN: \"what is the weather in san francisco?\"\n",
            "AI: [\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"Okay, let's check the weather in San Francisco:\"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"tool_use\",\n",
            "    \"id\": \"toolu_015dywEMjSJsjkgP91VDbm52\",\n",
            "    \"name\": \"get_weather\",\n",
            "    \"input\": {\n",
            "      \"location\": \"San Francisco\"\n",
            "    }\n",
            "  }\n",
            "]\n",
            "TOOL: \"Error: Input queries must be all capitals\\n Please fix your mistakes.\"\n",
            "AI: [\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"Apologies, let me try that again with the location in all capital letters:\"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"tool_use\",\n",
            "    \"id\": \"toolu_01Qw6t7p9UGk8aHQh7qtLJZT\",\n",
            "    \"name\": \"get_weather\",\n",
            "    \"input\": {\n",
            "      \"location\": \"SAN FRANCISCO\"\n",
            "    }\n",
            "  }\n",
            "]\n",
            "TOOL: \"It's 60 degrees and foggy\"\n",
            "AI: \"The weather in San Francisco is 60 degrees and foggy.\"\n"
          ]
        }
      ],
      "source": [
        "const response = await app.invoke({\n",
        "  messages: [\n",
        "    { role: \"user\", content: \"what is the weather in san francisco?\"},\n",
        "  ]\n",
        "});\n",
        "\n",
        "for (const message of response.messages) {\n",
        "  // Anthropic returns tool calls in content as well as in `AIMessage.tool_calls`\n",
        "  const content = JSON.stringify(message.content, null, 2);\n",
        "  console.log(`${message._getType().toUpperCase()}: ${content}`);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom strategies\n",
        "\n",
        "This is a fine default in many cases, but there are cases where custom fallbacks may be better.\n",
        "\n",
        "For example, the below tool requires as input a list of elements of a specific length - tricky for a small model! We'll also intentionally avoid pluralizing `topic` to trick the model into thinking it should pass a string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HUMAN: \"Write me an incredible haiku about water.\"\n",
            "AI: [\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"Okay, let's generate a haiku about water using the master haiku generator tool:\"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"tool_use\",\n",
            "    \"id\": \"toolu_01CMvVu3MhPeCk5X7F8GBv8f\",\n",
            "    \"name\": \"master_haiku_generator\",\n",
            "    \"input\": {\n",
            "      \"topic\": [\n",
            "        \"water\"\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "]\n",
            "TOOL: \"Error: Received tool input did not match expected schema\\n Please fix your mistakes.\"\n",
            "AI: [\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"Oops, looks like I need to provide 3 topics for the haiku generator. Let me try again with 3 water-related topics:\"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"tool_use\",\n",
            "    \"id\": \"toolu_0158Nz2scGSWvYor4vmJbSDZ\",\n",
            "    \"name\": \"master_haiku_generator\",\n",
            "    \"input\": {\n",
            "      \"topic\": [\n",
            "        \"ocean\",\n",
            "        \"waves\",\n",
            "        \"rain\"\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "]\n",
            "TOOL: \"Here is a haiku about the ocean, waves, and rain:\\n\\nWaves crash on the shore,\\nRhythmic dance of water's song,\\nRain falls from the sky.\"\n",
            "AI: \"The haiku generator has produced a beautiful and evocative poem about the different aspects of water - the ocean, waves, and rain. I hope you enjoy this creative take on a water-themed haiku!\"\n"
          ]
        }
      ],
      "source": [
        "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
        "\n",
        "const haikuRequestSchema = z.object({\n",
        "  topic: z.array(z.string()).length(3),\n",
        "});\n",
        "\n",
        "const masterHaikuGenerator = tool(async ({ topic }) => {\n",
        "  const model = new ChatAnthropic({\n",
        "    model: \"claude-3-haiku-20240307\",\n",
        "    temperature: 0,\n",
        "  });\n",
        "  const chain = model.pipe(new StringOutputParser());\n",
        "  const topics = topic.join(\", \");\n",
        "  const haiku = await chain.invoke(`Write a haiku about ${topics}`);\n",
        "  return haiku;\n",
        "}, {\n",
        "  name: \"master_haiku_generator\",\n",
        "  description: \"Generates a haiku based on the provided topics.\",\n",
        "  schema: haikuRequestSchema,\n",
        "});\n",
        "\n",
        "const customStrategyToolNode = new ToolNode([masterHaikuGenerator]);\n",
        "\n",
        "const customStrategyModel = new ChatAnthropic({\n",
        "  model: \"claude-3-haiku-20240307\",\n",
        "  temperature: 0,\n",
        "});\n",
        "const customStrategyModelWithTools = customStrategyModel.bindTools([masterHaikuGenerator]);\n",
        "\n",
        "const customStrategyShouldContinue = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1];\n",
        "  if (isAIMessage(lastMessage) && lastMessage.tool_calls?.length) {\n",
        "    return \"tools\";\n",
        "  }\n",
        "  return \"__end__\";\n",
        "}\n",
        "\n",
        "const customStrategyCallModel = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const response = await customStrategyModelWithTools.invoke(messages);\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "const customStrategyApp = new StateGraph(MessagesAnnotation)\n",
        "  .addNode(\"tools\", customStrategyToolNode)\n",
        "  .addNode(\"agent\", customStrategyCallModel)\n",
        "  .addEdge(\"__start__\", \"agent\")\n",
        "  .addEdge(\"tools\", \"agent\")\n",
        "  .addConditionalEdges(\"agent\", customStrategyShouldContinue, {\n",
        "    // Explicitly list possible destinations so that\n",
        "    // we can automatically draw the graph below.\n",
        "    tools: \"tools\",\n",
        "    __end__: \"__end__\",\n",
        "  })\n",
        "  .compile();\n",
        "\n",
        "const response2 = await customStrategyApp.invoke(\n",
        "  {\n",
        "    messages: [{ role: \"user\", content: \"Write me an incredible haiku about water.\" }],\n",
        "  },\n",
        "  { recursionLimit: 10 }\n",
        ");\n",
        "\n",
        "for (const message of response2.messages) {\n",
        "  // Anthropic returns tool calls in content as well as in `AIMessage.tool_calls`\n",
        "  const content = JSON.stringify(message.content, null, 2);\n",
        "  console.log(`${message._getType().toUpperCase()}: ${content}`);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model takes two attempts.\n",
        "\n",
        "A better strategy might be to trim the failed attempt to reduce distraction, then fall back to a more advanced model. Here's an example - note the custom-built tool calling node instead of the prebuilt `ToolNode`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { AIMessage, ToolMessage, RemoveMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const haikuRequestSchema2 = z.object({\n",
        "  topic: z.array(z.string()).length(3),\n",
        "});\n",
        "\n",
        "const masterHaikuGenerator2 = tool(async ({ topic }) => {\n",
        "  const model = new ChatAnthropic({\n",
        "    model: \"claude-3-haiku-20240307\",\n",
        "    temperature: 0,\n",
        "  });\n",
        "  const chain = model.pipe(new StringOutputParser());\n",
        "  const topics = topic.join(\", \");\n",
        "  const haiku = await chain.invoke(`Write a haiku about ${topics}`);\n",
        "  return haiku;\n",
        "}, {\n",
        "  name: \"master_haiku_generator\",\n",
        "  description: \"Generates a haiku based on the provided topics.\",\n",
        "  schema: haikuRequestSchema2,\n",
        "});\n",
        "\n",
        "const callTool2 = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const toolsByName = { master_haiku_generator: masterHaikuGenerator };\n",
        "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
        "  const outputMessages: ToolMessage[] = [];\n",
        "  for (const toolCall of lastMessage.tool_calls) {\n",
        "    try {\n",
        "      const toolResult = await toolsByName[toolCall.name].invoke(toolCall);\n",
        "      outputMessages.push(toolResult);\n",
        "    } catch (error: any) {\n",
        "      // Return the error if the tool call fails\n",
        "      outputMessages.push(\n",
        "        new ToolMessage({\n",
        "          content: error.message,\n",
        "          name: toolCall.name,\n",
        "          tool_call_id: toolCall.id!,\n",
        "          additional_kwargs: { error }\n",
        "        })\n",
        "      );\n",
        "    }\n",
        "  }\n",
        "  return { messages: outputMessages };\n",
        "};\n",
        "\n",
        "const model = new ChatAnthropic({\n",
        "  model: \"claude-3-haiku-20240307\",\n",
        "  temperature: 0,\n",
        "});\n",
        "const modelWithTools2 = model.bindTools([masterHaikuGenerator2]);\n",
        "\n",
        "const betterModel = new ChatAnthropic({\n",
        "  model: \"claude-3-5-sonnet-20240620\",\n",
        "  temperature: 0,\n",
        "});\n",
        "const betterModelWithTools = betterModel.bindTools([masterHaikuGenerator2]);\n",
        "\n",
        "const shouldContinue2 = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1];\n",
        "  if (isAIMessage(lastMessage) && lastMessage.tool_calls?.length) {\n",
        "    return \"tools\";\n",
        "  }\n",
        "  return \"__end__\";\n",
        "}\n",
        "\n",
        "const shouldFallback = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const failedToolMessages = messages.find((message) => {\n",
        "    return message._getType() === \"tool\" && message.additional_kwargs.error !== undefined;\n",
        "  });\n",
        "  if (failedToolMessages) {\n",
        "    return \"remove_failed_tool_call_attempt\";\n",
        "  }\n",
        "  return \"agent\";\n",
        "}\n",
        "\n",
        "const callModel2 = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const response = await modelWithTools2.invoke(messages);\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "const removeFailedToolCallAttempt = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  // Remove all messages from the most recent\n",
        "  // instance of AIMessage onwards.\n",
        "  const lastAIMessageIndex = messages\n",
        "    .map((msg, index) => ({ msg, index }))\n",
        "    .reverse()\n",
        "    .findIndex(({ msg }) => isAIMessage(msg));\n",
        "  const messagesToRemove = messages.slice(lastAIMessageIndex);\n",
        "  return { messages: messagesToRemove.map(m => new RemoveMessage({ id: m.id })) };\n",
        "}\n",
        "\n",
        "const callFallbackModel = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const response = await betterModelWithTools.invoke(messages);\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "const app2 = new StateGraph(MessagesAnnotation)\n",
        "  .addNode(\"tools\", callTool2)\n",
        "  .addNode(\"agent\", callModel2)\n",
        "  .addNode(\"remove_failed_tool_call_attempt\", removeFailedToolCallAttempt)\n",
        "  .addNode(\"fallback_agent\", callFallbackModel)\n",
        "  .addEdge(\"__start__\", \"agent\")\n",
        "  .addConditionalEdges(\"agent\", shouldContinue2, {\n",
        "    // Explicitly list possible destinations so that\n",
        "    // we can automatically draw the graph below.\n",
        "    tools: \"tools\",\n",
        "    __end__: \"__end__\",\n",
        "  })\n",
        "  .addConditionalEdges(\"tools\", shouldFallback, {\n",
        "    remove_failed_tool_call_attempt: \"remove_failed_tool_call_attempt\",\n",
        "    agent: \"agent\",\n",
        "  })\n",
        "  .addEdge(\"remove_failed_tool_call_attempt\", \"fallback_agent\")\n",
        "  .addEdge(\"fallback_agent\", \"tools\")\n",
        "  .compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `tools` node will now return `ToolMessage`s with an `error` field in `additional_kwargs` if a tool call fails. If that happens, it will go to another node that removes the failed tool messages, and has a better model retry the tool call generation. We also add a trimming step via returning the special message modifier `RemoveMessage` to remove previous messages from the state.\n",
        "\n",
        "The diagram below shows this visually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const graph2 = app2.getGraph();\n",
        "const image2 = await graph2.drawMermaidPng();\n",
        "const arrayBuffer2 = await image2.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer2));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try it out. To emphasize the removal steps, let's `stream` the responses from the model so that we can see each executed node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  agent: {\n",
            "    messages: [\n",
            "      AIMessage {\n",
            "        \"id\": \"msg_01HqvhPuubXqerWgYRNFqPrd\",\n",
            "        \"content\": [\n",
            "          {\n",
            "            \"type\": \"text\",\n",
            "            \"text\": \"Okay, let's generate a haiku about water using the master haiku generator tool:\"\n",
            "          },\n",
            "          {\n",
            "            \"type\": \"tool_use\",\n",
            "            \"id\": \"toolu_01QFmyc5vhQBFfzF7hCGTRc1\",\n",
            "            \"name\": \"master_haiku_generator\",\n",
            "            \"input\": {\n",
            "              \"topic\": \"[Array]\"\n",
            "            }\n",
            "          }\n",
            "        ],\n",
            "        \"additional_kwargs\": {\n",
            "          \"id\": \"msg_01HqvhPuubXqerWgYRNFqPrd\",\n",
            "          \"type\": \"message\",\n",
            "          \"role\": \"assistant\",\n",
            "          \"model\": \"claude-3-haiku-20240307\",\n",
            "          \"stop_reason\": \"tool_use\",\n",
            "          \"stop_sequence\": null,\n",
            "          \"usage\": {\n",
            "            \"input_tokens\": 392,\n",
            "            \"output_tokens\": 77\n",
            "          }\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "          \"id\": \"msg_01HqvhPuubXqerWgYRNFqPrd\",\n",
            "          \"model\": \"claude-3-haiku-20240307\",\n",
            "          \"stop_reason\": \"tool_use\",\n",
            "          \"stop_sequence\": null,\n",
            "          \"usage\": {\n",
            "            \"input_tokens\": 392,\n",
            "            \"output_tokens\": 77\n",
            "          },\n",
            "          \"type\": \"message\",\n",
            "          \"role\": \"assistant\"\n",
            "        },\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"name\": \"master_haiku_generator\",\n",
            "            \"args\": {\n",
            "              \"topic\": \"[Array]\"\n",
            "            },\n",
            "            \"id\": \"toolu_01QFmyc5vhQBFfzF7hCGTRc1\",\n",
            "            \"type\": \"tool_call\"\n",
            "          }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "          \"input_tokens\": 392,\n",
            "          \"output_tokens\": 77,\n",
            "          \"total_tokens\": 469\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  tools: {\n",
            "    messages: [\n",
            "      ToolMessage {\n",
            "        \"id\": \"502c7399-4d95-4afd-8a86-ece864d2bc7f\",\n",
            "        \"content\": \"Received tool input did not match expected schema\",\n",
            "        \"name\": \"master_haiku_generator\",\n",
            "        \"additional_kwargs\": {\n",
            "          \"error\": {\n",
            "            \"output\": \"{\\\"topic\\\":[\\\"water\\\"]}\"\n",
            "          }\n",
            "        },\n",
            "        \"response_metadata\": {},\n",
            "        \"tool_call_id\": \"toolu_01QFmyc5vhQBFfzF7hCGTRc1\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  remove_failed_tool_call_attempt: {\n",
            "    messages: [\n",
            "      BaseMessage {\n",
            "        \"id\": \"msg_01HqvhPuubXqerWgYRNFqPrd\",\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      },\n",
            "      BaseMessage {\n",
            "        \"id\": \"502c7399-4d95-4afd-8a86-ece864d2bc7f\",\n",
            "        \"content\": \"\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  fallback_agent: {\n",
            "    messages: [\n",
            "      AIMessage {\n",
            "        \"id\": \"msg_01EQSawL2oxNhph9be99k7Yp\",\n",
            "        \"content\": [\n",
            "          {\n",
            "            \"type\": \"text\",\n",
            "            \"text\": \"Certainly! I'd be happy to help you create an incredible haiku about water. To do this, we'll use the master_haiku_generator function, which requires three topics as input. Since you've specified water as the main theme, I'll add two related concepts to create a more vivid and interesting haiku. Let's use \\\"water,\\\" \\\"flow,\\\" and \\\"reflection\\\" as our three topics.\\n\\nHere's the function call to generate your haiku:\"\n",
            "          },\n",
            "          {\n",
            "            \"type\": \"tool_use\",\n",
            "            \"id\": \"toolu_017hrp13SsgfdJTdhkJDMaQy\",\n",
            "            \"name\": \"master_haiku_generator\",\n",
            "            \"input\": {\n",
            "              \"topic\": \"[Array]\"\n",
            "            }\n",
            "          }\n",
            "        ],\n",
            "        \"additional_kwargs\": {\n",
            "          \"id\": \"msg_01EQSawL2oxNhph9be99k7Yp\",\n",
            "          \"type\": \"message\",\n",
            "          \"role\": \"assistant\",\n",
            "          \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "          \"stop_reason\": \"tool_use\",\n",
            "          \"stop_sequence\": null,\n",
            "          \"usage\": {\n",
            "            \"input_tokens\": 422,\n",
            "            \"output_tokens\": 162\n",
            "          }\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "          \"id\": \"msg_01EQSawL2oxNhph9be99k7Yp\",\n",
            "          \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "          \"stop_reason\": \"tool_use\",\n",
            "          \"stop_sequence\": null,\n",
            "          \"usage\": {\n",
            "            \"input_tokens\": 422,\n",
            "            \"output_tokens\": 162\n",
            "          },\n",
            "          \"type\": \"message\",\n",
            "          \"role\": \"assistant\"\n",
            "        },\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"name\": \"master_haiku_generator\",\n",
            "            \"args\": {\n",
            "              \"topic\": \"[Array]\"\n",
            "            },\n",
            "            \"id\": \"toolu_017hrp13SsgfdJTdhkJDMaQy\",\n",
            "            \"type\": \"tool_call\"\n",
            "          }\n",
            "        ],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "          \"input_tokens\": 422,\n",
            "          \"output_tokens\": 162,\n",
            "          \"total_tokens\": 584\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  tools: {\n",
            "    messages: [\n",
            "      ToolMessage {\n",
            "        \"id\": \"3d24d291-7501-4a65-9286-10dc47239b5b\",\n",
            "        \"content\": \"Here is a haiku about water, flow, and reflection:\\n\\nRippling waters flow,\\nMirroring the sky above,\\nTranquil reflection.\",\n",
            "        \"name\": \"master_haiku_generator\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {},\n",
            "        \"tool_call_id\": \"toolu_017hrp13SsgfdJTdhkJDMaQy\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  agent: {\n",
            "    messages: [\n",
            "      AIMessage {\n",
            "        \"id\": \"msg_01Jy7Vw8DN77sjVWcB4TcJR6\",\n",
            "        \"content\": \"I hope you enjoy this haiku about the beauty and serenity of water. Please let me know if you would like me to generate another one.\",\n",
            "        \"additional_kwargs\": {\n",
            "          \"id\": \"msg_01Jy7Vw8DN77sjVWcB4TcJR6\",\n",
            "          \"type\": \"message\",\n",
            "          \"role\": \"assistant\",\n",
            "          \"model\": \"claude-3-haiku-20240307\",\n",
            "          \"stop_reason\": \"end_turn\",\n",
            "          \"stop_sequence\": null,\n",
            "          \"usage\": {\n",
            "            \"input_tokens\": 601,\n",
            "            \"output_tokens\": 35\n",
            "          }\n",
            "        },\n",
            "        \"response_metadata\": {\n",
            "          \"id\": \"msg_01Jy7Vw8DN77sjVWcB4TcJR6\",\n",
            "          \"model\": \"claude-3-haiku-20240307\",\n",
            "          \"stop_reason\": \"end_turn\",\n",
            "          \"stop_sequence\": null,\n",
            "          \"usage\": {\n",
            "            \"input_tokens\": 601,\n",
            "            \"output_tokens\": 35\n",
            "          },\n",
            "          \"type\": \"message\",\n",
            "          \"role\": \"assistant\"\n",
            "        },\n",
            "        \"tool_calls\": [],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"usage_metadata\": {\n",
            "          \"input_tokens\": 601,\n",
            "          \"output_tokens\": 35,\n",
            "          \"total_tokens\": 636\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const stream = await app2.stream(\n",
        "  { messages: [{ role: \"user\", content: \"Write me an incredible haiku about water.\" }] },\n",
        "  { recursionLimit: 10 },\n",
        ")\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that you get a cleaner response - the more powerful model gets it right on the first try, and the smaller model's failure gets wiped from the graph state. This shorter message history also avoid overpopulating the graph state with attempts.\n",
        "\n",
        "You can also inspect this [LangSmith trace](https://smith.langchain.com/public/c94f95d0-97fc-4d4d-a59a-b5161c2f4a90/r), which shows the failed initial call to the smaller model.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You've now seen how to implement some strategies to handle tool calling errors.\n",
        "\n",
        "Next, check out some of the [other LangGraph how-to guides here](/langgraphjs/how-tos/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
</file>

<file path="how-tos/tool-calling.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to call tools using ToolNode\n",
        "\n",
        "This guide covers how to use LangGraph's prebuilt [`ToolNode`](/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html) for tool calling.\n",
        "\n",
        "`ToolNode` is a LangChain Runnable that takes graph state (with a list of messages) as input and outputs state update with the result of tool calls. It is designed to work well out-of-box with LangGraph's prebuilt ReAct agent, but can also work with any `StateGraph` as long as its state has a `messages` key with an appropriate reducer (see [`MessagesAnnotation`](/langgraphjs/concepts/low_level/#messagesannotation))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "```bash\n",
        "npm install @langchain/langgraph @langchain/anthropic @langchain/core zod\n",
        "```\n",
        "\n",
        "Set env vars:\n",
        "\n",
        "```typescript\n",
        "process.env.ANTHROPIC_API_KEY = 'your-anthropic-api-key';\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { tool } from '@langchain/core/tools';\n",
        "import { z } from 'zod';\n",
        "\n",
        "const getWeather = tool((input) => {\n",
        "  if (['sf', 'san francisco'].includes(input.location.toLowerCase())) {\n",
        "    return 'It\\'s 60 degrees and foggy.';\n",
        "  } else {\n",
        "    return 'It\\'s 90 degrees and sunny.';\n",
        "  }\n",
        "}, {\n",
        "  name: 'get_weather',\n",
        "  description: 'Call to get the current weather.',\n",
        "  schema: z.object({\n",
        "    location: z.string().describe(\"Location to get the weather for.\"),\n",
        "  })\n",
        "})\n",
        "\n",
        "const getCoolestCities = tool(() => {\n",
        "  return 'nyc, sf';\n",
        "}, {\n",
        "  name: 'get_coolest_cities',\n",
        "  description: 'Get a list of coolest cities',\n",
        "  schema: z.object({\n",
        "    noOp: z.string().optional().describe(\"No-op parameter.\"),\n",
        "  })\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ToolNode } from '@langchain/langgraph/prebuilt';\n",
        "\n",
        "const tools = [getWeather, getCoolestCities]\n",
        "const toolNode = new ToolNode(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manually call `ToolNode`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`ToolNode` operates on graph state with a list of messages. It expects the last message in the list to be an `AIMessage` with `tool_calls` parameter. \n",
        "\n",
        "Let's first see how to invoke the tool node manually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    ToolMessage {\n",
            "      \"content\": \"It's 60 degrees and foggy.\",\n",
            "      \"name\": \"get_weather\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"tool_call_id\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { AIMessage } from '@langchain/core/messages';\n",
        "\n",
        "const messageWithSingleToolCall = new AIMessage({\n",
        "  content: \"\",\n",
        "  tool_calls: [\n",
        "    {\n",
        "      name: \"get_weather\",\n",
        "      args: { location: \"sf\" },\n",
        "      id: \"tool_call_id\",\n",
        "      type: \"tool_call\",\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "\n",
        "await toolNode.invoke({ messages: [messageWithSingleToolCall] })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that typically you don't need to create `AIMessage` manually, and it will be automatically generated by any LangChain chat model that supports tool calling.\n",
        "\n",
        "You can also do parallel tool calling using `ToolNode` if you pass multiple tool calls to `AIMessage`'s `tool_calls` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    ToolMessage {\n",
            "      \"content\": \"nyc, sf\",\n",
            "      \"name\": \"get_coolest_cities\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"tool_call_id\"\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"content\": \"It's 60 degrees and foggy.\",\n",
            "      \"name\": \"get_weather\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"tool_call_id_2\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const messageWithMultipleToolCalls = new AIMessage({\n",
        "  content: \"\",\n",
        "  tool_calls: [\n",
        "    {\n",
        "      name: \"get_coolest_cities\",\n",
        "      args: {},\n",
        "      id: \"tool_call_id\",\n",
        "      type: \"tool_call\",\n",
        "    },\n",
        "    {\n",
        "      name: \"get_weather\",\n",
        "      args: { location: \"sf\" },\n",
        "      id: \"tool_call_id_2\",\n",
        "      type: \"tool_call\",\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "\n",
        "await toolNode.invoke({ messages: [messageWithMultipleToolCalls] })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using with chat models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll be using a small chat model from Anthropic in our example. To use chat models with tool calling, we need to first ensure that the model is aware of the available tools. We do this by calling `.bindTools` method on `ChatAnthropic` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "\n",
        "const modelWithTools = new ChatAnthropic({\n",
        "  model: \"claude-3-haiku-20240307\",\n",
        "  temperature: 0\n",
        "}).bindTools(tools);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    name: 'get_weather',\n",
            "    args: { location: 'sf' },\n",
            "    id: 'toolu_01UAjv9Mmj9LRosAsrgKtqeR',\n",
            "    type: 'tool_call'\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "const responseMessage = await modelWithTools.invoke(\"what's the weather in sf?\");\n",
        "\n",
        "responseMessage.tool_calls;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the AI message generated by the chat model already has `tool_calls` populated, so we can just pass it directly to `ToolNode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    ToolMessage {\n",
            "      \"content\": \"It's 60 degrees and foggy.\",\n",
            "      \"name\": \"get_weather\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_01HrJmUek2ninxDiLJrYpDpz\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await toolNode.invoke({ messages: [await modelWithTools.invoke(\"what's the weather in sf?\")] })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReAct Agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's see how to use `ToolNode` inside a LangGraph graph. Let's set up a graph implementation of the [ReAct agent](/langgraphjs/concepts/agentic_concepts/#react-agent). This agent takes some query as input, then repeatedly call tools until it has enough information to resolve the query. We'll be using `ToolNode` and the Anthropic model with tools we just defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import {\n",
        "  StateGraph,\n",
        "  MessagesAnnotation,\n",
        "  END,\n",
        "  START\n",
        "} from \"@langchain/langgraph\";\n",
        "\n",
        "const toolNodeForGraph = new ToolNode(tools)\n",
        "\n",
        "const shouldContinue = (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const lastMessage = messages[messages.length - 1];\n",
        "  if (\"tool_calls\" in lastMessage && Array.isArray(lastMessage.tool_calls) && lastMessage.tool_calls?.length) {\n",
        "      return \"tools\";\n",
        "  }\n",
        "  return END;\n",
        "}\n",
        "\n",
        "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const { messages } = state;\n",
        "  const response = await modelWithTools.invoke(messages);\n",
        "  return { messages: response };\n",
        "}\n",
        "\n",
        "\n",
        "const workflow = new StateGraph(MessagesAnnotation)\n",
        "  // Define the two nodes we will cycle between\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"tools\", toolNodeForGraph)\n",
        "  .addEdge(START, \"agent\")\n",
        "  .addConditionalEdges(\"agent\", shouldContinue, [\"tools\", END])\n",
        "  .addEdge(\"tools\", \"agent\");\n",
        "\n",
        "const app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const drawableGraph = app.getGraph();\n",
        "const image = await drawableGraph.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  type: 'human',\n",
            "  content: \"what's the weather in sf?\",\n",
            "  toolCalls: undefined\n",
            "}\n",
            "{\n",
            "  type: 'ai',\n",
            "  content: [\n",
            "    { type: 'text', text: \"Okay, let's check the weather in SF:\" },\n",
            "    {\n",
            "      type: 'tool_use',\n",
            "      id: 'toolu_01X5yTzVrGZqNz9vf1w2MCna',\n",
            "      name: 'get_weather',\n",
            "      input: { location: 'sf' }\n",
            "    }\n",
            "  ],\n",
            "  toolCalls: [\n",
            "    {\n",
            "      name: 'get_weather',\n",
            "      args: { location: 'sf' },\n",
            "      id: 'toolu_01X5yTzVrGZqNz9vf1w2MCna',\n",
            "      type: 'tool_call'\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  type: 'tool',\n",
            "  content: \"It's 60 degrees and foggy.\",\n",
            "  toolCalls: undefined\n",
            "}\n",
            "{\n",
            "  type: 'ai',\n",
            "  content: 'The current weather in San Francisco is 60 degrees and foggy.',\n",
            "  toolCalls: []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { HumanMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "// example with a single tool call\n",
        "const stream = await app.stream(\n",
        "  {\n",
        "    messages: [{ role: \"user\", content: \"what's the weather in sf?\" }],\n",
        "  },\n",
        "  {\n",
        "    streamMode: \"values\"\n",
        "  }\n",
        ")\n",
        "for await (const chunk of stream) {\n",
        "  const lastMessage = chunk.messages[chunk.messages.length - 1];\n",
        "  const type = lastMessage._getType();\n",
        "  const content = lastMessage.content;\n",
        "  const toolCalls = lastMessage.tool_calls;\n",
        "  console.dir({\n",
        "    type,\n",
        "    content,\n",
        "    toolCalls\n",
        "  }, { depth: null });\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  type: 'human',\n",
            "  content: \"what's the weather in the coolest cities?\",\n",
            "  toolCalls: undefined\n",
            "}\n",
            "{\n",
            "  type: 'ai',\n",
            "  content: [\n",
            "    {\n",
            "      type: 'text',\n",
            "      text: \"Okay, let's find out the weather in the coolest cities:\"\n",
            "    },\n",
            "    {\n",
            "      type: 'tool_use',\n",
            "      id: 'toolu_017RHcsJFeo7w6kDnZ6TAa19',\n",
            "      name: 'get_coolest_cities',\n",
            "      input: { noOp: 'dummy' }\n",
            "    }\n",
            "  ],\n",
            "  toolCalls: [\n",
            "    {\n",
            "      name: 'get_coolest_cities',\n",
            "      args: { noOp: 'dummy' },\n",
            "      id: 'toolu_017RHcsJFeo7w6kDnZ6TAa19',\n",
            "      type: 'tool_call'\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{ type: 'tool', content: 'nyc, sf', toolCalls: undefined }\n",
            "{\n",
            "  type: 'ai',\n",
            "  content: [\n",
            "    {\n",
            "      type: 'text',\n",
            "      text: \"Now let's get the weather for those cities:\"\n",
            "    },\n",
            "    {\n",
            "      type: 'tool_use',\n",
            "      id: 'toolu_01ML1jW5u5aVCFkZhihzLv24',\n",
            "      name: 'get_weather',\n",
            "      input: { location: 'nyc' }\n",
            "    }\n",
            "  ],\n",
            "  toolCalls: [\n",
            "    {\n",
            "      name: 'get_weather',\n",
            "      args: { location: 'nyc' },\n",
            "      id: 'toolu_01ML1jW5u5aVCFkZhihzLv24',\n",
            "      type: 'tool_call'\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  type: 'tool',\n",
            "  content: \"It's 90 degrees and sunny.\",\n",
            "  toolCalls: undefined\n",
            "}\n",
            "{\n",
            "  type: 'ai',\n",
            "  content: [\n",
            "    {\n",
            "      type: 'tool_use',\n",
            "      id: 'toolu_0187eWumoCgxjnCjq4RGHyun',\n",
            "      name: 'get_weather',\n",
            "      input: { location: 'sf' }\n",
            "    }\n",
            "  ],\n",
            "  toolCalls: [\n",
            "    {\n",
            "      name: 'get_weather',\n",
            "      args: { location: 'sf' },\n",
            "      id: 'toolu_0187eWumoCgxjnCjq4RGHyun',\n",
            "      type: 'tool_call'\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  type: 'tool',\n",
            "  content: \"It's 60 degrees and foggy.\",\n",
            "  toolCalls: undefined\n",
            "}\n",
            "{\n",
            "  type: 'ai',\n",
            "  content: 'Based on the weather results, it looks like San Francisco is the coolest of the coolest cities, with a temperature of 60 degrees and foggy conditions. New York City is warmer at 90 degrees and sunny.',\n",
            "  toolCalls: []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "// example with a multiple tool calls in succession\n",
        "const streamWithMultiToolCalls = await app.stream(\n",
        "  {\n",
        "      messages: [{ role: \"user\", content: \"what's the weather in the coolest cities?\" }],\n",
        "  },\n",
        "  {\n",
        "    streamMode: \"values\"\n",
        "  }\n",
        ")\n",
        "for await (const chunk of streamWithMultiToolCalls) {\n",
        "  const lastMessage = chunk.messages[chunk.messages.length - 1];\n",
        "  const type = lastMessage._getType();\n",
        "  const content = lastMessage.content;\n",
        "  const toolCalls = lastMessage.tool_calls;\n",
        "  console.dir({\n",
        "    type,\n",
        "    content,\n",
        "    toolCalls\n",
        "  }, { depth: null });\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`ToolNode` can also handle errors during tool execution. See our guide on handling errors in `ToolNode` [here](/langgraphjs/how-tos/tool-calling-errors/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
</file>

<file path="how-tos/update-state-from-tools.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c58c957-83d8-44ff-8580-a9b3dd39a0a9",
   "metadata": {},
   "source": [
    "# How to update graph state from tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95f30587-8dd2-40be-920d-59539089c09f",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>            \n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/low_level/#command\">\n",
    "                    Command\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return a [`Command`](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#command) object from the tool:\n",
    "\n",
    "```ts\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "\n",
    "const lookupUserInfo = tool(async (input, config) => {\n",
    "  const userInfo = getUserInfo(config);\n",
    "  return new Command({\n",
    "    // update state keys\n",
    "    update: {\n",
    "      user_info: userInfo,\n",
    "      messages: [\n",
    "        new ToolMessage({\n",
    "          content: \"Successfully looked up user information\",\n",
    "          tool_call_id: config.toolCall.id,\n",
    "        }),\n",
    "      ],\n",
    "    },\n",
    "  });\n",
    "}, {\n",
    "  name: \"lookup_user_info\",\n",
    "  description: \"Use this to look up user information to better assist them with their questions.\",\n",
    "  schema: z.object(...)\n",
    "});\n",
    "```\n",
    "    \n",
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Important</p>\n",
    "    <p>\n",
    "      If you want to use tools that return <code>Command</code> instances and update graph state, you can either use prebuilt <a href=\"https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html\"><code>createReactAgent</code></a> / <a href=\"https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html\"><code>ToolNode</code></a> components, or implement your own tool-executing node that identifies <code>Command</code> objects returned by your tools and returns a mixed array of traditional state updates and <code>Commands</code>.\n",
    "      <br />\n",
    "      See [this section](#custom-components) for an example.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "This guide shows how you can do this using LangGraph's prebuilt components ([`createReactAgent`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) and [`ToolNode`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html)).\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Compatibility</p>\n",
    "    <p>\n",
    "        This guide requires <code>@langchain/langgraph>=0.2.33</code> and <code>@langchain/core@0.3.23</code>. For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e52dc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the following to run this guide:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core\n",
    "```\n",
    "\n",
    "Next, configure your environment to connect to your model provider.\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6ff9f-c1e6-499e-a230-9fa231ea7d2f",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9a9c6-fa3f-416c-bac0-3e58d7259908",
   "metadata": {},
   "source": [
    "Let's create a simple ReAct style agent that can look up user information and personalize the response based on the user info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255b9b9-cf67-4cc3-8018-1708f5dfcfd2",
   "metadata": {},
   "source": [
    "## Define tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6b010-aab1-4fe8-8251-907fcae78583",
   "metadata": {},
   "source": [
    "First, let's define the tool that we'll be using to look up user information. We'll use a naive implementation that simply looks user information up using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d070c9f-6e61-4724-85dc-ac4531b9c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "const USER_ID_TO_USER_INFO = {\n",
    "  abc123: {\n",
    "    user_id: \"abc123\",\n",
    "    name: \"Bob Dylan\",\n",
    "    location: \"New York, NY\",\n",
    "  },\n",
    "  zyx987: {\n",
    "    user_id: \"zyx987\",\n",
    "    name: \"Taylor Swift\",\n",
    "    location: \"Beverly Hills, CA\",\n",
    "  },\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d1ecca-ee57-4e97-b8d0-e09de85337d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation, Command, MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  ...MessagesAnnotation.spec,\n",
    "  // user provided\n",
    "  lastName: Annotation<string>,\n",
    "  // updated by the tool\n",
    "  userInfo: Annotation<Record<string, any>>,\n",
    "});\n",
    "\n",
    "const lookupUserInfo = tool(async (_, config) => {\n",
    "  const userId = config.configurable?.user_id;\n",
    "  if (userId === undefined) {\n",
    "    throw new Error(\"Please provide a user id in config.configurable\");\n",
    "  }\n",
    "  if (USER_ID_TO_USER_INFO[userId] === undefined) {\n",
    "    throw new Error(`User \"${userId}\" not found`);\n",
    "  }\n",
    "  // Populated when a tool is called with a tool call from a model as input\n",
    "  const toolCallId = config.toolCall.id;\n",
    "  return new Command({\n",
    "    update: {\n",
    "      // update the state keys\n",
    "      userInfo: USER_ID_TO_USER_INFO[userId],\n",
    "      // update the message history\n",
    "      messages: [\n",
    "        {\n",
    "          role: \"tool\",\n",
    "          content: \"Successfully looked up user information\",\n",
    "          tool_call_id: toolCallId,\n",
    "        },\n",
    "      ],\n",
    "    },\n",
    "  })\n",
    "}, {\n",
    "  name: \"lookup_user_info\",\n",
    "  description: \"Always use this to look up information about the user to better assist them with their questions.\",\n",
    "  schema: z.object({}),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e5f24-5e5e-4a34-baae-467182675bb5",
   "metadata": {},
   "source": [
    "## Define prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb06aea-6654-4245-91f8-af6e8f2b5377",
   "metadata": {},
   "source": [
    "Let's now add personalization: we'll respond differently to the user based on the state values AFTER the state has been updated from the tool. To achieve this, let's define a function that will dynamically construct the system prompt based on the graph state. It will be called ever time the LLM is called and the function output will be passed to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c553d062-d145-4145-84bd-9b798f7c95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "const stateModifier = (state: typeof StateAnnotation.State) => {\n",
    "  const userInfo = state.userInfo;\n",
    "  if (userInfo == null) {\n",
    "    return state.messages;\n",
    "  }\n",
    "  const systemMessage = `User name is ${userInfo.name}. User lives in ${userInfo.location}`;\n",
    "  return [{\n",
    "    role: \"system\",\n",
    "    content: systemMessage,\n",
    "  }, ...state.messages];\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acdd5d-68be-466b-9c21-46cbed91d2bc",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb65028-0359-46c8-b09c-ffc90180f759",
   "metadata": {},
   "source": [
    "Finally, let's combine this into a single graph using the prebuilt `createReactAgent` and the components we declared earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d59db29-fd51-4d29-9854-21763a4855e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o\",\n",
    "});\n",
    "\n",
    "const agent = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: [lookupUserInfo],\n",
    "  stateSchema: StateAnnotation,\n",
    "  stateModifier: stateModifier,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782b8ab-a603-47b8-9a76-77f593402678",
   "metadata": {},
   "source": [
    "## Use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165e153-ab28-4404-adea-796c7bd0701b",
   "metadata": {},
   "source": [
    "Let's now try running our agent. We'll need to provide user ID in the config so that our tool knows what information to look up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de34a58b-1765-4b63-a232-d46790aff884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AdmOZdrZy3aUgNimCIjq8ZW5js6ln\",\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"tool_calls\": [\n",
      "            {\n",
      "              \"id\": \"call_kLXWJYbabxWpj7vykXD6ZMx0\",\n",
      "              \"type\": \"function\",\n",
      "              \"function\": \"[Object]\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"promptTokens\": 59,\n",
      "            \"completionTokens\": 11,\n",
      "            \"totalTokens\": 70\n",
      "          },\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"usage\": {\n",
      "            \"prompt_tokens\": 59,\n",
      "            \"completion_tokens\": 11,\n",
      "            \"total_tokens\": 70,\n",
      "            \"prompt_tokens_details\": {\n",
      "              \"cached_tokens\": 0,\n",
      "              \"audio_tokens\": 0\n",
      "            },\n",
      "            \"completion_tokens_details\": {\n",
      "              \"reasoning_tokens\": 0,\n",
      "              \"audio_tokens\": 0,\n",
      "              \"accepted_prediction_tokens\": 0,\n",
      "              \"rejected_prediction_tokens\": 0\n",
      "            }\n",
      "          },\n",
      "          \"system_fingerprint\": \"fp_f785eb5f47\"\n",
      "        },\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"lookup_user_info\",\n",
      "            \"args\": {},\n",
      "            \"type\": \"tool_call\",\n",
      "            \"id\": \"call_kLXWJYbabxWpj7vykXD6ZMx0\"\n",
      "          }\n",
      "        ],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"output_tokens\": 11,\n",
      "          \"input_tokens\": 59,\n",
      "          \"total_tokens\": 70,\n",
      "          \"input_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"cache_read\": 0\n",
      "          },\n",
      "          \"output_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"reasoning\": 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  tools: {\n",
      "    userInfo: { user_id: 'abc123', name: 'Bob Dylan', location: 'New York, NY' },\n",
      "    messages: [ [Object] ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AdmOZJ0gSQ7VVCUfcadhOeqq4HxWa\",\n",
      "        \"content\": \"Hi Bob! Since you're in New York, NY, there are plenty of exciting things you can do this weekend. Here are a few suggestions:\\n\\n1. **Visit Central Park**: Enjoy a leisurely walk, rent a bike, or have a picnic. The park is beautiful in the fall.\\n\\n2. **Explore Museums**: Check out The Met, MoMA, or The American Museum of Natural History if you're interested in art or history.\\n\\n3. **Broadway Show**: Catch a Broadway show or a musical for an entertaining evening.\\n\\n4. **Visit Times Square**: Experience the vibrant lights and energy of Times Square. There are plenty of shops and restaurants to explore.\\n\\n5. **Brooklyn Bridge Walk**: Walk across the iconic Brooklyn Bridge and enjoy stunning views of Manhattan and Brooklyn.\\n\\n6. **Cultural Festivals or Events**: Check local listings for any cultural festivals or events happening in the city this weekend.\\n\\nIf you have specific interests, let me know, and I can suggest something more tailored to your preferences!\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"promptTokens\": 98,\n",
      "            \"completionTokens\": 209,\n",
      "            \"totalTokens\": 307\n",
      "          },\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"usage\": {\n",
      "            \"prompt_tokens\": 98,\n",
      "            \"completion_tokens\": 209,\n",
      "            \"total_tokens\": 307,\n",
      "            \"prompt_tokens_details\": {\n",
      "              \"cached_tokens\": 0,\n",
      "              \"audio_tokens\": 0\n",
      "            },\n",
      "            \"completion_tokens_details\": {\n",
      "              \"reasoning_tokens\": 0,\n",
      "              \"audio_tokens\": 0,\n",
      "              \"accepted_prediction_tokens\": 0,\n",
      "              \"rejected_prediction_tokens\": 0\n",
      "            }\n",
      "          },\n",
      "          \"system_fingerprint\": \"fp_cc5cf1c6e3\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"output_tokens\": 209,\n",
      "          \"input_tokens\": 98,\n",
      "          \"total_tokens\": 307,\n",
      "          \"input_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"cache_read\": 0\n",
      "          },\n",
      "          \"output_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"reasoning\": 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const stream = await agent.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"hi, what should i do this weekend?\",\n",
    "  }],\n",
    "  \n",
    "}, {\n",
    "  // provide user ID in the config\n",
    "  configurable: { user_id: \"abc123\" }\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2281f-269c-41dd-b6b2-4c743f11ffc9",
   "metadata": {},
   "source": [
    "We can see that the model correctly recommended some New York activities for Bob Dylan! Let's try getting recommendations for Taylor Swift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d71af94-572a-4961-88a7-665e792cf96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AdmQGANyXPTAkMnQ86hGWB5XY5hGL\",\n",
      "        \"content\": \"\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"tool_calls\": [\n",
      "            {\n",
      "              \"id\": \"call_IvyfreezvohjGgUx9DrwfS5O\",\n",
      "              \"type\": \"function\",\n",
      "              \"function\": \"[Object]\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"promptTokens\": 59,\n",
      "            \"completionTokens\": 11,\n",
      "            \"totalTokens\": 70\n",
      "          },\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"usage\": {\n",
      "            \"prompt_tokens\": 59,\n",
      "            \"completion_tokens\": 11,\n",
      "            \"total_tokens\": 70,\n",
      "            \"prompt_tokens_details\": {\n",
      "              \"cached_tokens\": 0,\n",
      "              \"audio_tokens\": 0\n",
      "            },\n",
      "            \"completion_tokens_details\": {\n",
      "              \"reasoning_tokens\": 0,\n",
      "              \"audio_tokens\": 0,\n",
      "              \"accepted_prediction_tokens\": 0,\n",
      "              \"rejected_prediction_tokens\": 0\n",
      "            }\n",
      "          },\n",
      "          \"system_fingerprint\": \"fp_cc5cf1c6e3\"\n",
      "        },\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"lookup_user_info\",\n",
      "            \"args\": {},\n",
      "            \"type\": \"tool_call\",\n",
      "            \"id\": \"call_IvyfreezvohjGgUx9DrwfS5O\"\n",
      "          }\n",
      "        ],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"output_tokens\": 11,\n",
      "          \"input_tokens\": 59,\n",
      "          \"total_tokens\": 70,\n",
      "          \"input_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"cache_read\": 0\n",
      "          },\n",
      "          \"output_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"reasoning\": 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  tools: {\n",
      "    userInfo: {\n",
      "      user_id: 'zyx987',\n",
      "      name: 'Taylor Swift',\n",
      "      location: 'Beverly Hills, CA'\n",
      "    },\n",
      "    messages: [ [Object] ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  agent: {\n",
      "    messages: [\n",
      "      AIMessage {\n",
      "        \"id\": \"chatcmpl-AdmQHMYj613jksQJruNMVP6DfAagd\",\n",
      "        \"content\": \"This weekend, there are plenty of exciting things you can do around Beverly Hills, CA. Here are some options:\\n\\n1. **Explore Rodeo Drive**: Enjoy high-end shopping and dining experiences in this iconic shopping district.\\n   \\n2. **Visit a Museum**: Check out The Getty Center or Los Angeles County Museum of Art (LACMA) for a dose of culture and art.\\n\\n3. **Hiking**: Take a scenic hike in the nearby Santa Monica Mountains or Griffith Park for beautiful views of the city.\\n\\n4. **Spa Day**: Treat yourself to a relaxing spa day at one of Beverly Hills' luxurious spas.\\n\\n5. **Restaurant Tour**: Dine at some of Beverly Hills' finest restaurants, such as Spago or The Penthouse.\\n\\n6. **Take a Scenic Drive**: Drive along Mulholland Drive for stunning views of Los Angeles and the surrounding areas.\\n\\n7. **Catch a Show**: See if there are any live performances or concerts happening at The Hollywood Bowl or other nearby venues.\\n\\nEnjoy your weekend!\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"tokenUsage\": {\n",
      "            \"promptTokens\": 98,\n",
      "            \"completionTokens\": 214,\n",
      "            \"totalTokens\": 312\n",
      "          },\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"usage\": {\n",
      "            \"prompt_tokens\": 98,\n",
      "            \"completion_tokens\": 214,\n",
      "            \"total_tokens\": 312,\n",
      "            \"prompt_tokens_details\": {\n",
      "              \"cached_tokens\": 0,\n",
      "              \"audio_tokens\": 0\n",
      "            },\n",
      "            \"completion_tokens_details\": {\n",
      "              \"reasoning_tokens\": 0,\n",
      "              \"audio_tokens\": 0,\n",
      "              \"accepted_prediction_tokens\": 0,\n",
      "              \"rejected_prediction_tokens\": 0\n",
      "            }\n",
      "          },\n",
      "          \"system_fingerprint\": \"fp_9d50cd990b\"\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": [],\n",
      "        \"usage_metadata\": {\n",
      "          \"output_tokens\": 214,\n",
      "          \"input_tokens\": 98,\n",
      "          \"total_tokens\": 312,\n",
      "          \"input_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"cache_read\": 0\n",
      "          },\n",
      "          \"output_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"reasoning\": 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const taylorStream = await agent.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"hi, what should i do this weekend?\",\n",
    "  }],\n",
    "  \n",
    "}, {\n",
    "  // provide user ID in the config\n",
    "  configurable: { user_id: \"zyx987\" }\n",
    "});\n",
    "\n",
    "for await (const chunk of taylorStream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be85c1",
   "metadata": {},
   "source": [
    "## Custom components\n",
    "\n",
    "If you do not wish to use prebuilt components, you will need to have special logic in your custom tool executor to handle commands. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f8732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    HumanMessage {\n",
      "      \"id\": \"801308df-c702-49f4-99c1-da4116f6bbc8\",\n",
      "      \"content\": \"how are you?\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"8ea07329-a73a-4de4-a4d4-4453fbef32e0\",\n",
      "      \"content\": \"Let me call the greeting tool and find out!\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {},\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"123\",\n",
      "          \"args\": {},\n",
      "          \"name\": \"greeting\"\n",
      "        }\n",
      "      ],\n",
      "      \"invalid_tool_calls\": []\n",
      "    },\n",
      "    AIMessage {\n",
      "      \"id\": \"4ecba93a-77c0-44a6-8dc9-8b27d9615c15\",\n",
      "      \"content\": \"hi there!\",\n",
      "      \"name\": \"Greeter\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {},\n",
      "      \"tool_calls\": [],\n",
      "      \"invalid_tool_calls\": []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import {\n",
    "  MessagesAnnotation,\n",
    "  isCommand,\n",
    "  Command,\n",
    "  StateGraph,\n",
    "} from \"@langchain/langgraph\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { isAIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const myTool = tool(async () => {\n",
    "  return new Command({\n",
    "    update: {\n",
    "      messages: [\n",
    "        {\n",
    "          role: \"assistant\",\n",
    "          content: \"hi there!\",\n",
    "          name: \"Greeter\",\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "  });\n",
    "}, {\n",
    "  name: \"greeting\",\n",
    "  description: \"Updates the current state with a greeting\",\n",
    "  schema: z.object({}),\n",
    "});\n",
    "\n",
    "const toolExecutor = async (state: typeof MessagesAnnotation.State) => {\n",
    "  const message = state.messages.at(-1);\n",
    "  if (!isAIMessage(message) || message.tool_calls === undefined || message.tool_calls.length === 0) {\n",
    "    throw new Error(\"Most recent message must be an AIMessage with a tool call.\")\n",
    "  }\n",
    "  const outputs = await Promise.all(\n",
    "    message.tool_calls.map(async (toolCall) => {\n",
    "      // Using a single tool for simplicity, would need to select tools by toolCall.name\n",
    "      // in practice.\n",
    "      const toolResult = await myTool.invoke(toolCall);\n",
    "      return toolResult;\n",
    "    })\n",
    "  );\n",
    "  // Handle mixed Command and non-Command outputs\n",
    "  const combinedOutputs = outputs.map((output) => {\n",
    "    if (isCommand(output)) {\n",
    "      return output;\n",
    "    }\n",
    "    // Tool invocation result is a ToolMessage, return a normal state update\n",
    "    return { messages: [output] };\n",
    "  });\n",
    "  // Return an array of values instead of an object\n",
    "  return combinedOutputs;\n",
    "};\n",
    "\n",
    "// Simple one node graph\n",
    "const customGraph = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"runTools\", toolExecutor)\n",
    "  .addEdge(\"__start__\", \"runTools\")\n",
    "  .compile();\n",
    "  \n",
    "await customGraph.invoke({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"how are you?\",\n",
    "  }, {\n",
    "    role: \"assistant\",\n",
    "    content: \"Let me call the greeting tool and find out!\",\n",
    "    tool_calls: [{\n",
    "      id: \"123\",\n",
    "      args: {},\n",
    "      name: \"greeting\",\n",
    "    }],\n",
    "  }],\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356863e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="how-tos/use-in-web-environments.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to use LangGraph.js in web environments\n",
        "\n",
        "LangGraph.js uses the [`async_hooks`](https://nodejs.org/api/async_hooks.html)\n",
        "API to more conveniently allow for tracing and callback propagation within\n",
        "nodes. This API is supported in many environments, such as\n",
        "[Node.js](https://nodejs.org/api/async_hooks.html),\n",
        "[Deno](https://deno.land/std@0.177.0/node/internal/async_hooks.ts),\n",
        "[Cloudflare Workers](https://developers.cloudflare.com/workers/runtime-apis/nodejs/asynclocalstorage/),\n",
        "and the\n",
        "[Edge runtime](https://vercel.com/docs/functions/runtimes/edge-runtime#compatible-node.js-modules),\n",
        "but not all, such as within web browsers.\n",
        "\n",
        "To allow usage of LangGraph.js in environments that do not have the\n",
        "`async_hooks` API available, we've added a separate `@langchain/langgraph/web`\n",
        "entrypoint. This entrypoint exports everything that the primary\n",
        "`@langchain/langgraph` exports, but will not initialize or even import\n",
        "`async_hooks`. Here's a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from the browser!\n"
          ]
        }
      ],
      "source": [
        "// Import from \"@langchain/langgraph/web\"\n",
        "import {\n",
        "  END,\n",
        "  START,\n",
        "  StateGraph,\n",
        "  Annotation,\n",
        "} from \"@langchain/langgraph/web\";\n",
        "import { BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const GraphState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});\n",
        "\n",
        "const nodeFn = async (_state: typeof GraphState.State) => {\n",
        "  return { messages: [new HumanMessage(\"Hello from the browser!\")] };\n",
        "};\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(GraphState)\n",
        "  .addNode(\"node\", nodeFn)\n",
        "  .addEdge(START, \"node\")\n",
        "  .addEdge(\"node\", END);\n",
        "\n",
        "const app = workflow.compile({});\n",
        "\n",
        "// Use the Runnable\n",
        "const finalState = await app.invoke(\n",
        "  { messages: [] },\n",
        ");\n",
        "\n",
        "console.log(finalState.messages[finalState.messages.length - 1].content);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other entrypoints, such as `@langchain/langgraph/prebuilt`, can be used in\n",
        "either environment.\n",
        "\n",
        "<div class=\"admonition warning\">\n",
        "  <p class=\"admonition-title\">Caution</p>\n",
        "  <p>\n",
        "    If you are using LangGraph.js on the frontend, make sure you are not exposing any private keys!\n",
        "    For chat models, this means you need to use something like <a href=\"https://js.langchain.com/docs/integrations/chat/web_llm\">WebLLM</a>\n",
        "    that can run client-side without authentication.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "## Passing config\n",
        "\n",
        "The lack of `async_hooks` support in web browsers means that if you are calling\n",
        "a [`Runnable`](https://js.langchain.com/docs/concepts/runnables/) within a\n",
        "node (for example, when calling a chat model), you need to manually pass a\n",
        "`config` object through to properly support tracing,\n",
        "[`.streamEvents()`](https://js.langchain.com/docs/how_to/streaming#using-stream-events)\n",
        "to stream intermediate steps, and other callback related functionality. This\n",
        "config object will passed in as the second argument of each node, and should be\n",
        "used as the second parameter of any `Runnable` method.\n",
        "\n",
        "To illustrate this, let's set up our graph again as before, but with a\n",
        "`Runnable` within our node. First, we'll avoid passing `config` through into the\n",
        "nested function, then try to use `.streamEvents()` to see the intermediate\n",
        "results of the nested function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Received 0 events from the nested function\n"
          ]
        }
      ],
      "source": [
        "// Import from \"@langchain/langgraph/web\"\n",
        "import {\n",
        "  END,\n",
        "  START,\n",
        "  StateGraph,\n",
        "  Annotation,\n",
        "} from \"@langchain/langgraph/web\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "import { RunnableLambda } from \"@langchain/core/runnables\";\n",
        "import { type StreamEvent } from \"@langchain/core/tracers/log_stream\";\n",
        "\n",
        "const GraphState2 = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});\n",
        "\n",
        "const nodeFn2 = async (_state: typeof GraphState2.State) => {\n",
        "  // Note that we do not pass any `config` through here\n",
        "  const nestedFn = RunnableLambda.from(async (input: string) => {\n",
        "    return new HumanMessage(`Hello from ${input}!`);\n",
        "  }).withConfig({ runName: \"nested\" });\n",
        "  const responseMessage = await nestedFn.invoke(\"a nested function\");\n",
        "  return { messages: [responseMessage] };\n",
        "};\n",
        "\n",
        "// Define a new graph\n",
        "const workflow2 = new StateGraph(GraphState2)\n",
        "  .addNode(\"node\", nodeFn2)\n",
        "  .addEdge(START, \"node\")\n",
        "  .addEdge(\"node\", END);\n",
        "\n",
        "const app2 = workflow2.compile({});\n",
        "\n",
        "// Stream intermediate steps from the graph\n",
        "const eventStream2 = app2.streamEvents(\n",
        "  { messages: [] },\n",
        "  { version: \"v2\" },\n",
        "  { includeNames: [\"nested\"] },\n",
        ");\n",
        "\n",
        "const events2: StreamEvent[] = [];\n",
        "for await (const event of eventStream2) {\n",
        "  console.log(event);\n",
        "  events2.push(event);\n",
        "}\n",
        "\n",
        "console.log(`Received ${events2.length} events from the nested function`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we get no events.\n",
        "\n",
        "Next, let's try redeclaring the graph with a node that passes config through\n",
        "correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  event: \"on_chain_start\",\n",
            "  data: { input: { messages: [] } },\n",
            "  name: \"nested\",\n",
            "  tags: [],\n",
            "  run_id: \"22747451-a2fa-447b-b62f-9da19a539b2f\",\n",
            "  metadata: {\n",
            "    langgraph_step: 1,\n",
            "    langgraph_node: \"node\",\n",
            "    langgraph_triggers: [ \"start:node\" ],\n",
            "    langgraph_task_idx: 0,\n",
            "    __pregel_resuming: false,\n",
            "    checkpoint_id: \"1ef62793-f065-6840-fffe-cdfb4cbb1248\",\n",
            "    checkpoint_ns: \"node\"\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: \"on_chain_end\",\n",
            "  data: {\n",
            "    output: HumanMessage {\n",
            "      \"content\": \"Hello from a nested function!\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    }\n",
            "  },\n",
            "  run_id: \"22747451-a2fa-447b-b62f-9da19a539b2f\",\n",
            "  name: \"nested\",\n",
            "  tags: [],\n",
            "  metadata: {\n",
            "    langgraph_step: 1,\n",
            "    langgraph_node: \"node\",\n",
            "    langgraph_triggers: [ \"start:node\" ],\n",
            "    langgraph_task_idx: 0,\n",
            "    __pregel_resuming: false,\n",
            "    checkpoint_id: \"1ef62793-f065-6840-fffe-cdfb4cbb1248\",\n",
            "    checkpoint_ns: \"node\"\n",
            "  }\n",
            "}\n",
            "Received 2 events from the nested function\n"
          ]
        }
      ],
      "source": [
        "// Import from \"@langchain/langgraph/web\"\n",
        "import {\n",
        "  END,\n",
        "  START,\n",
        "  StateGraph,\n",
        "  Annotation,\n",
        "} from \"@langchain/langgraph/web\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "import { type RunnableConfig, RunnableLambda } from \"@langchain/core/runnables\";\n",
        "import { type StreamEvent } from \"@langchain/core/tracers/log_stream\";\n",
        "\n",
        "const GraphState3 = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "});\n",
        "\n",
        "// Note the second argument here.\n",
        "const nodeFn3 = async (_state: typeof GraphState3.State, config?: RunnableConfig) => {\n",
        "  // If you need to nest deeper, remember to pass `_config` when invoking\n",
        "  const nestedFn = RunnableLambda.from(\n",
        "    async (input: string, _config?: RunnableConfig) => {\n",
        "      return new HumanMessage(`Hello from ${input}!`);\n",
        "    },\n",
        "  ).withConfig({ runName: \"nested\" });\n",
        "  const responseMessage = await nestedFn.invoke(\"a nested function\", config);\n",
        "  return { messages: [responseMessage] };\n",
        "};\n",
        "\n",
        "// Define a new graph\n",
        "const workflow3 = new StateGraph(GraphState3)\n",
        "  .addNode(\"node\", nodeFn3)\n",
        "  .addEdge(START, \"node\")\n",
        "  .addEdge(\"node\", END);\n",
        "\n",
        "const app3 = workflow3.compile({});\n",
        "\n",
        "// Stream intermediate steps from the graph\n",
        "const eventStream3 = app3.streamEvents(\n",
        "  { messages: [] },\n",
        "  { version: \"v2\" },\n",
        "  { includeNames: [\"nested\"] },\n",
        ");\n",
        "\n",
        "const events3: StreamEvent[] = [];\n",
        "for await (const event of eventStream3) {\n",
        "  console.log(event);\n",
        "  events3.push(event);\n",
        "}\n",
        "\n",
        "console.log(`Received ${events3.length} events from the nested function`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that we get events from the nested function as expected.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You've now learned about some special considerations around using LangGraph.js\n",
        "in web environments.\n",
        "\n",
        "Next, check out\n",
        "[some how-to guides on core functionality](/langgraphjs/how-tos/#core)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nb_converter": "script",
      "pygments_lexer": "typescript",
      "version": "5.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
</file>

<file path="how-tos/wait-user-input-functional.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to wait for user input (Functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - Implementing [human-in-the-loop](../../concepts/human_in_the_loop) workflows with [interrupt](../../concepts/human_in_the_loop/#interrupt)\n",
    "    - [How to create a ReAct agent using the Functional API](../../how-tos/react-agent-from-scratch-functional)\n",
    "\n",
    "**Human-in-the-loop (HIL)** interactions are crucial for [agentic systems](../../concepts/agentic_concepts/#human-in-the-loop). Waiting for human input is a common HIL interaction pattern, allowing the agent to ask the user clarifying questions and await input before proceeding. \n",
    "\n",
    "We can implement this in LangGraph using the [interrupt()](/langgraphjs/reference/functions/langgraph.interrupt-1.html) function. `interrupt` allows us to stop graph execution to collect input from a user and continue execution with collected input.\n",
    "\n",
    "This guide demonstrates how to implement human-in-the-loop workflows using LangGraph's [Functional API](../../concepts/functional_api). Specifically, we will demonstrate:\n",
    "\n",
    "1. [A simple usage example](#simple-usage)\n",
    "2. [How to use with a ReAct agent](#agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple usage\n",
    "\n",
    "Let's demonstrate a simple usage example. We will create three [tasks](../../concepts/functional_api/#task):\n",
    "\n",
    "1. Append `\"bar\"`.\n",
    "2. Pause for human input. When resuming, append human input.\n",
    "3. Append `\"qux\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { task, interrupt } from \"@langchain/langgraph\";\n",
    "\n",
    "const step1 = task(\"step1\", async (inputQuery: string) => {\n",
    "  return `${inputQuery} bar`;\n",
    "});\n",
    "\n",
    "const humanFeedback = task(\n",
    "  \"humanFeedback\",\n",
    "  async (inputQuery: string) => {\n",
    "    const feedback = interrupt(`Please provide feedback: ${inputQuery}`);\n",
    "    return `${inputQuery} ${feedback}`;\n",
    "  });\n",
    "\n",
    "const step3 = task(\"step3\", async (inputQuery: string) => {\n",
    "  return `${inputQuery} qux`;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compose these tasks in a simple [entrypoint](../../concepts/functional_api/#entrypoint):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MemorySaver, entrypoint } from \"@langchain/langgraph\";\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const graph = entrypoint({\n",
    "  name: \"graph\",\n",
    "  checkpointer,\n",
    "}, async (inputQuery: string) => {\n",
    "  const result1 = await step1(inputQuery);\n",
    "  const result2 = await humanFeedback(result1);\n",
    "  const result3 = await step3(result2);\n",
    "  return result3;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have done to enable human-in-the-loop workflows is call [interrupt()](../../concepts/human_in_the_loop/#interrupt) inside a task.\n",
    "\n",
    "!!! tip\n",
    "\n",
    "    The results of prior tasks - in this case `step1 -- are persisted, so that they are not run again following the `interrupt`.\n",
    "\n",
    "\n",
    "Let's send in a query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ step1: 'foo bar' }\n",
      "{\n",
      "  __interrupt__: [\n",
      "    {\n",
      "      value: 'Please provide feedback: foo bar',\n",
      "      when: 'during',\n",
      "      resumable: true,\n",
      "      ns: [Array]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config = {\n",
    "  configurable: {\n",
    "    thread_id: \"1\"\n",
    "  }\n",
    "};\n",
    "\n",
    "const stream = await graph.stream(\"foo\", config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've paused with an `interrupt` after `step1`. The interrupt provides instructions to resume the run. To resume, we issue a [Command](../../concepts/human_in_the_loop/#the-command-primitive) containing the data expected by the `humanFeedback` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ humanFeedback: 'foo bar baz' }\n",
      "{ step3: 'foo bar baz qux' }\n",
      "{ graph: 'foo bar baz qux' }\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "\n",
    "const resumeStream = await graph.stream(new Command({\n",
    "  resume: \"baz\"\n",
    "}), config);\n",
    "\n",
    "// Continue execution\n",
    "for await (const event of resumeStream) {\n",
    "  if (event.__metadata__?.cached) {\n",
    "    continue;\n",
    "  }\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resuming, the run proceeds through the remaining step and terminates as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "We will build off of the agent created in the [How to create a ReAct agent using the Functional API](../../how-tos/react-agent-from-scratch-functional) guide.\n",
    "\n",
    "Here we will extend the agent by allowing it to reach out to a human for assistance when needed.\n",
    "\n",
    "### Define model and tools\n",
    "\n",
    "Let's first define the tools and model we will use for our example. As in the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional), we will use a single place-holder tool that gets a description of the weather for a location.\n",
    "\n",
    "We will use an [OpenAI](https://js.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://js.langchain.com/docs/integrations/chat/) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o-mini\",\n",
    "});\n",
    "\n",
    "const getWeather = tool(async ({ location }) => {\n",
    "  // This is a placeholder for the actual implementation\n",
    "  const lowercaseLocation = location.toLowerCase();\n",
    "  if (lowercaseLocation.includes(\"sf\") || lowercaseLocation.includes(\"san francisco\")) {\n",
    "    return \"It's sunny!\";\n",
    "  } else if (lowercaseLocation.includes(\"boston\")) {\n",
    "    return \"It's rainy!\";\n",
    "  } else {\n",
    "    return `I am not sure what the weather is in ${location}`;\n",
    "  }\n",
    "}, {\n",
    "  name: \"getWeather\",\n",
    "  schema: z.object({\n",
    "    location: z.string().describe(\"Location to get the weather for\"),\n",
    "  }),\n",
    "  description: \"Call to get the weather from a specific location.\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach out to a human for assistance, we can simply add a tool that calls [interrupt](../../concepts/human_in_the_loop/#interrupt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { interrupt } from \"@langchain/langgraph\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const humanAssistance = tool(async ({ query }) => {\n",
    "  const humanResponse = interrupt({ query });\n",
    "  return humanResponse.data;\n",
    "}, {\n",
    "  name: \"humanAssistance\",\n",
    "  description: \"Request assistance from a human.\",\n",
    "  schema: z.object({\n",
    "    query: z.string().describe(\"Human readable question for the human\")\n",
    "  })\n",
    "});\n",
    "\n",
    "const tools = [getWeather, humanAssistance];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "\n",
    "Our tasks are otherwise unchanged from the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional):\n",
    "\n",
    "1. **Call model**: We want to query our chat model with a list of messages.\n",
    "2. **Call tool**: If our model generates tool calls, we want to execute them.\n",
    "\n",
    "We just have one more tool accessible to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  type BaseMessageLike,\n",
    "  AIMessage,\n",
    "  ToolMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "import { type ToolCall } from \"@langchain/core/messages/tool\";\n",
    "import { task } from \"@langchain/langgraph\";\n",
    "\n",
    "const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\n",
    "\n",
    "const callModel = task(\"callModel\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await model.bindTools(tools).invoke(messages);\n",
    "  return response;\n",
    "});\n",
    "\n",
    "const callTool = task(\n",
    "  \"callTool\",\n",
    "  async (toolCall: ToolCall): Promise<AIMessage> => {\n",
    "    const tool = toolsByName[toolCall.name];\n",
    "    const observation = await tool.invoke(toolCall.args);\n",
    "    return new ToolMessage({ content: observation, tool_call_id: toolCall.id });\n",
    "    // Can also pass toolCall directly into the tool to return a ToolMessage\n",
    "    // return tool.invoke(toolCall);\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define entrypoint\n",
    "\n",
    "Our [entrypoint](../../concepts/functional_api/#entrypoint) is also unchanged from the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { entrypoint, addMessages, MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const agent = entrypoint({\n",
    "  name: \"agent\",\n",
    "  checkpointer: new MemorySaver(),\n",
    "}, async (messages: BaseMessageLike[]) => {\n",
    "  let currentMessages = messages;\n",
    "  let llmResponse = await callModel(currentMessages);\n",
    "  while (true) {\n",
    "    if (!llmResponse.tool_calls?.length) {\n",
    "      break;\n",
    "    }\n",
    "\n",
    "    // Execute tools\n",
    "    const toolResults = await Promise.all(\n",
    "      llmResponse.tool_calls.map((toolCall) => {\n",
    "        return callTool(toolCall);\n",
    "      })\n",
    "    );\n",
    "    \n",
    "    // Append to message list\n",
    "    currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]);\n",
    "\n",
    "    // Call model again\n",
    "    llmResponse = await callModel(currentMessages);\n",
    "  }\n",
    "\n",
    "  return llmResponse;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Let's invoke our model with a question that requires human assistance. Our question will also require an invocation of the `getWeather` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const prettyPrintMessage = (message: BaseMessage) => {\n",
    "  console.log(\"=\".repeat(30), `${message.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(message.content);\n",
    "  if (isAIMessage(message) && message.tool_calls?.length) {\n",
    "    console.log(JSON.stringify(message.tool_calls, null, 2));\n",
    "  }\n",
    "}\n",
    "\n",
    "const prettyPrintStep = (step: Record<string, any>) => {\n",
    "  if (step.__metadata__?.cached) {\n",
    "    return;\n",
    "  }\n",
    "  for (const [taskName, update] of Object.entries(step)) {\n",
    "    const message = update as BaseMessage;\n",
    "    // Only print task updates\n",
    "    if (taskName === \"agent\") continue;\n",
    "    console.log(`\\n${taskName}:`);\n",
    "    if (taskName === \"__interrupt__\") {\n",
    "      console.log(update);\n",
    "    } else {\n",
    "      prettyPrintMessage(message);\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  role: 'user',\n",
      "  content: 'Can you reach out for human assistance: what should I feed my cat? Separately, can you check the weather in San Francisco?'\n",
      "}\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"humanAssistance\",\n",
      "    \"args\": {\n",
      "      \"query\": \"What should I feed my cat?\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_TwrNq6tGI61cDCJEpj175h7J\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_fMzUBvc0SpZpXxM2LQLXfbke\"\n",
      "  }\n",
      "]\n",
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "__interrupt__:\n",
      "[\n",
      "  {\n",
      "    value: { query: 'What should I feed my cat?' },\n",
      "    when: 'during',\n",
      "    resumable: true,\n",
      "    ns: [ 'callTool:2e0c6c40-9541-57ef-a7af-24213a10d5a4' ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const userMessage = {\n",
    "  role: \"user\",\n",
    "  content: [\n",
    "    \"Can you reach out for human assistance: what should I feed my cat?\",\n",
    "    \"Separately, can you check the weather in San Francisco?\"\n",
    "  ].join(\" \"),\n",
    "};\n",
    "console.log(userMessage);\n",
    "\n",
    "const agentStream = await agent.stream([userMessage], {\n",
    "  configurable: {\n",
    "    thread_id: \"1\",\n",
    "  }\n",
    "});\n",
    "\n",
    "let lastStep;\n",
    "\n",
    "for await (const step of agentStream) {\n",
    "  prettyPrintStep(step);\n",
    "  lastStep = step;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we generate two tool calls, and although our run is interrupted, we did not block the execution of the `get_weather` tool.\n",
    "\n",
    "Let's inspect where we're interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"__interrupt__\":[{\"value\":{\"query\":\"What should I feed my cat?\"},\"when\":\"during\",\"resumable\":true,\"ns\":[\"callTool:2e0c6c40-9541-57ef-a7af-24213a10d5a4\"]}]}\n"
     ]
    }
   ],
   "source": [
    "console.log(JSON.stringify(lastStep));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can resume execution by issuing a [Command](../../concepts/human_in_the_loop/#the-command-primitive). Note that the data we supply in the `Command` can be customized to your needs based on the implementation of `humanAssistance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "You should feed your cat a fish.\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "For your cat, it is suggested that you feed it fish. As for the weather in San Francisco, it's currently sunny!\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "\n",
    "const humanResponse = \"You should feed your cat a fish.\";\n",
    "const humanCommand = new Command({\n",
    "  resume: { data: humanResponse },\n",
    "});\n",
    "\n",
    "const resumeStream2 = await agent.stream(humanCommand, config);\n",
    "\n",
    "for await (const step of resumeStream2) {\n",
    "  prettyPrintStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, when we resume we provide the final tool message, allowing the model to generate its response. Check out the LangSmith traces to see a full breakdown of the runs:\n",
    "\n",
    "1. [Trace from initial query](https://smith.langchain.com/public/c007b042-fdd3-49e7-acbe-cadf6969de4b/r)\n",
    "2. [Trace after resuming](https://smith.langchain.com/public/1cea310a-13f5-4de9-ae1c-045b8b33015e/r)\n",
    "\n",
    "**Note:** The `interrupt` function propagates by throwing a special `GraphInterrupt` error. Therefore, you should avoid using `try/catch` blocks around the `interrupt` function - or if you do, ensure that the `GraphInterrupt` error is thrown again within your `catch` block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="how-tos/wait-user-input.ipynb">
{
  "cells": [
    {
      "attachments": {
        "02ae42da-d1a4-4849-984a-6ab0bbf759bd.png": {
          "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAALoCAYAAAD82o3cAAAMP2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBCCSAgJfQmCEgJICWEFkB6EWyEJEAoMQaCiB1dVHDtYgEbuiqi2AGxI3YWwd4XRRSUdbFgV96kgK77yvfO9829//3nzH/OnDu3DADqp7hicQ6qAUCuKF8SGxLAGJucwiB1AwTggAYIgMDl5YlZ0dERANrg+e/27ib0hnbNQab1z/7/app8QR4PACQa4jR+Hi8X4kMA4JU8sSQfAKKMN5+aL5Zh2IC2BCYI8UIZzlDgShlOU+B9cp/4WDbEzQCoqHG5kgwAaG2QZxTwMqAGrQ9iJxFfKAJAnQGxb27uZD7EqRDbQB8xxDJ9ZtoPOhl/00wb0uRyM4awYi5yUwkU5olzuNP+z3L8b8vNkQ7GsIJNLVMSGiubM6zb7ezJ4TKsBnGvKC0yCmItiD8I+XJ/iFFKpjQ0QeGPGvLy2LBmQBdiJz43MBxiQ4iDRTmREUo+LV0YzIEYrhC0UJjPiYdYD+KFgrygOKXPZsnkWGUstC5dwmYp+QtciTyuLNZDaXYCS6n/OlPAUepjtKLM+CSIKRBbFAgTIyGmQeyYlx0XrvQZXZTJjhz0kUhjZflbQBwrEIUEKPSxgnRJcKzSvzQ3b3C+2OZMISdSiQ/kZ8aHKuqDNfO48vzhXLA2gYiVMKgjyBsbMTgXviAwSDF3rFsgSohT6nwQ5wfEKsbiFHFOtNIfNxPkhMh4M4hd8wrilGPxxHy4IBX6eLo4PzpekSdelMUNi1bkgy8DEYANAgEDSGFLA5NBFhC29tb3witFTzDgAgnIAALgoGQGRyTJe0TwGAeKwJ8QCUDe0LgAea8AFED+6xCrODqAdHlvgXxENngKcS4IBznwWiofJRqKlgieQEb4j+hc2Hgw3xzYZP3/nh9kvzMsyEQoGelgRIb6oCcxiBhIDCUGE21xA9wX98Yj4NEfNheciXsOzuO7P+EpoZ3wmHCD0EG4M0lYLPkpyzGgA+oHK2uR9mMtcCuo6YYH4D5QHSrjurgBcMBdYRwW7gcju0GWrcxbVhXGT9p/m8EPd0PpR3Yio+RhZH+yzc8jaXY0tyEVWa1/rI8i17SherOHen6Oz/6h+nx4Dv/ZE1uIHcTOY6exi9gxrB4wsJNYA9aCHZfhodX1RL66BqPFyvPJhjrCf8QbvLOySuY51Tj1OH1R9OULCmXvaMCeLJ4mEWZk5jNY8IsgYHBEPMcRDBcnF1cAZN8XxevrTYz8u4Hotnzn5v0BgM/JgYGBo9+5sJMA7PeAj/+R75wNE346VAG4cIQnlRQoOFx2IMC3hDp80vSBMTAHNnA+LsAdeAN/EATCQBSIB8lgIsw+E65zCZgKZoC5oASUgWVgNVgPNoGtYCfYAw6AenAMnAbnwGXQBm6Ae3D1dIEXoA+8A58RBCEhVISO6CMmiCVij7ggTMQXCUIikFgkGUlFMhARIkVmIPOQMmQFsh7ZglQj+5EjyGnkItKO3EEeIT3Ia+QTiqFqqDZqhFqhI1EmykLD0Xh0ApqBTkGL0PnoEnQtWoXuRuvQ0+hl9Abagb5A+zGAqWK6mCnmgDExNhaFpWDpmASbhZVi5VgVVos1wvt8DevAerGPOBGn4wzcAa7gUDwB5+FT8Fn4Ynw9vhOvw5vxa/gjvA//RqASDAn2BC8ChzCWkEGYSighlBO2Ew4TzsJnqYvwjkgk6hKtiR7wWUwmZhGnExcTNxD3Ek8R24mdxH4SiaRPsif5kKJIXFI+qYS0jrSbdJJ0ldRF+qCiqmKi4qISrJKiIlIpVilX2aVyQuWqyjOVz2QNsiXZixxF5pOnkZeSt5EbyVfIXeTPFE2KNcWHEk/JosylrKXUUs5S7lPeqKqqmql6qsaoClXnqK5V3ad6QfWR6kc1LTU7NbbaeDWp2hK1HWqn1O6ovaFSqVZUf2oKNZ+6hFpNPUN9SP1Ao9McaRwanzabVkGro12lvVQnq1uqs9Qnqhepl6sfVL+i3qtB1rDSYGtwNWZpVGgc0bil0a9J13TWjNLM1VysuUvzoma3FknLSitIi681X2ur1hmtTjpGN6ez6Tz6PPo2+ll6lzZR21qbo52lXaa9R7tVu09HS8dVJ1GnUKdC57hOhy6ma6XL0c3RXap7QPem7qdhRsNYwwTDFg2rHXZ12Hu94Xr+egK9Ur29ejf0Pukz9IP0s/WX69frPzDADewMYgymGmw0OGvQO1x7uPdw3vDS4QeG3zVEDe0MYw2nG241bDHsNzI2CjESG60zOmPUa6xr7G+cZbzK+IRxjwndxNdEaLLK5KTJc4YOg8XIYaxlNDP6TA1NQ02lpltMW00/m1mbJZgVm+01e2BOMWeap5uvMm8y77MwsRhjMcOixuKuJdmSaZlpucbyvOV7K2urJKsFVvVW3dZ61hzrIusa6/s2VBs/myk2VTbXbYm2TNts2w22bXaonZtdpl2F3RV71N7dXmi/wb59BGGE5wjRiKoRtxzUHFgOBQ41Do8cdR0jHIsd6x1fjrQYmTJy+cjzI785uTnlOG1zuues5RzmXOzc6Pzaxc6F51Lhcn0UdVTwqNmjGka9crV3FbhudL3tRncb47bArcntq7uHu8S91r3Hw8Ij1aPS4xZTmxnNXMy84EnwDPCc7XnM86OXu1e+1wGvv7wdvLO9d3l3j7YeLRi9bXSnj5kP12eLT4cvwzfVd7Nvh5+pH9evyu+xv7k/33+7/zOWLSuLtZv1MsApQBJwOOA924s9k30qEAsMCSwNbA3SCkoIWh/0MNgsOCO4JrgvxC1kesipUEJoeOjy0FscIw6PU83pC/MImxnWHK4WHhe+PvxxhF2EJKJxDDombMzKMfcjLSNFkfVRIIoTtTLqQbR19JToozHEmOiYipinsc6xM2LPx9HjJsXtinsXHxC/NP5egk2CNKEpUT1xfGJ14vukwKQVSR1jR46dOfZyskGyMLkhhZSSmLI9pX9c0LjV47rGu40vGX9zgvWEwgkXJxpMzJl4fJL6JO6kg6mE1KTUXalfuFHcKm5/GietMq2Px+at4b3g+/NX8XsEPoIVgmfpPukr0rszfDJWZvRk+mWWZ/YK2cL1wldZoVmbst5nR2XvyB7IScrZm6uSm5p7RKQlyhY1TzaeXDi5XWwvLhF3TPGasnpKnyRcsj0PyZuQ15CvDX/kW6Q20l+kjwp8CyoKPkxNnHqwULNQVNgyzW7aomnPioKLfpuOT+dNb5phOmPujEczWTO3zEJmpc1qmm0+e/7srjkhc3bOpczNnvt7sVPxiuK385LmNc43mj9nfucvIb/UlNBKJCW3Fngv2LQQXyhc2Lpo1KJ1i76V8ksvlTmVlZd9WcxbfOlX51/X/jqwJH1J61L3pRuXEZeJlt1c7rd85wrNFUUrOleOWVm3irGqdNXb1ZNWXyx3Ld+0hrJGuqZjbcTahnUW65at+7I+c/2NioCKvZWGlYsq32/gb7i60X9j7SajTWWbPm0Wbr69JWRLXZVVVflW4taCrU+3JW47/xvzt+rtBtvLtn/dIdrRsTN2Z3O1R3X1LsNdS2vQGmlNz+7xu9v2BO5pqHWo3bJXd2/ZPrBPuu/5/tT9Nw+EH2g6yDxYe8jyUOVh+uHSOqRuWl1ffWZ9R0NyQ/uRsCNNjd6Nh486Ht1xzPRYxXGd40tPUE7MPzFwsuhk/ynxqd7TGac7myY13Tsz9sz15pjm1rPhZy+cCz535jzr/MkLPheOXfS6eOQS81L9ZffLdS1uLYd/d/v9cKt7a90VjysNbZ5tje2j209c9bt6+lrgtXPXOdcv34i80X4z4ebtW+Nvddzm3+6+k3Pn1d2Cu5/vzblPuF/6QONB+UPDh1V/2P6xt8O94/ijwEctj+Me3+vkdb54kvfkS9f8p9Sn5c9MnlV3u3Qf6wnuaXs+7nnXC/GLz70lf2r+WfnS5uWhv/z/aukb29f1SvJq4PXiN/pvdrx1fdvUH93/8F3uu8/vSz/of9j5kfnx/KekT88+T/1C+rL2q+3Xxm/h3+4P5A4MiLkSrvxXAIMNTU8H4PUOAKjJANDh/owyTrH/kxui2LPKEfhPWLFHlJs7ALXw/z2mF/7d3AJg3za4/YL66uMBiKYCEO8J0FGjhtrgXk2+r5QZEe4DNkd+TctNA//GFHvOH/L++Qxkqq7g5/O/AFFLfCfKufu9AAAAVmVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAADkoYABwAAABIAAABEoAIABAAAAAEAAANnoAMABAAAAAEAAALoAAAAAEFTQ0lJAAAAU2NyZWVuc2hvdLB9s7kAAAHWaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA2LjAuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjc0NDwvZXhpZjpQaXhlbFlEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlBpeGVsWERpbWVuc2lvbj44NzE8L2V4aWY6UGl4ZWxYRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpVc2VyQ29tbWVudD5TY3JlZW5zaG90PC9leGlmOlVzZXJDb21tZW50PgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4K3lk4iwAAQABJREFUeAHsnQe8ZVdV//e79/U3b3qftCEkEEMJRIFQBERARWyogL2hIir27h97Q/3Yu37soqKgEFCQoiA1oSVAEtKTmcn09vq7977/77v22XfOe3kTk8zLvDvJb0/OPefssvbav31g1m/W3mv3LSglJyNgBIyAETACRsAIGAEjYASMgBFYVQQaq9q7OzcCRsAIGAEjYASMgBEwAkbACBiBQMDkzB+CETACRsAIGAEjYASMgBEwAkagBxAwOeuBSbAKRsAIGAEjYASMgBEwAkbACBgBkzN/A0bACBgBI2AEjIARMAJGwAgYgR5AwOSsBybBKhgBI2AEjIARMAJGwAgYASNgBEzO/A0YASNgBIyAETACRsAIGAEjYAR6AAGTsx6YBKtgBIyAETACRsAIGAEjYASMgBEwOfM3YASMgBEwAkbACBgBI2AEjIAR6AEETM56YBKsghEwAkbACBgBI2AEjIARMAJGwOTM34ARMAJGwAgYASNgBIyAETACRqAHEDA564FJsApGwAgYASNgBIyAETACRsAIGAGTM38DRsAIGAEjYASMgBEwAkbACBiBHkDA5KwHJsEqGAEjYASMgBEwAkbACBgBI2AETM78DRgBI2AEjIARMAJGwAgYASNgBHoAAZOzHpgEq2AEjIARMAJGwAgYASNgBIyAETA58zdgBIyAETACRsAIGAEjYASMgBHoAQRMznpgEqyCETACRsAIGAEjYASMgBEwAkbA5MzfgBEwAkbACBgBI2AEjIARMAJGoAcQMDnrgUmwCkbACBgBI2AEjIARMAJGwAgYAZMzfwNGwAgYASNgBIyAETACRsAIGIEeQMDkrAcmwSoYASNgBIyAETACRsAIGAEjYARMzvwNGAEjYASMgBEwAkbACBgBI2AEegABk7MemASrYASMgBEwAkbACBgBI2AEjIARMDnzN2AEjIARMAJGwAgYASNgBIyAEegBBEzOemASrIIRMAJGwAgYASNgBIyAETACRsDkzN+AETACRsAIGAEjYASMgBEwAkagBxAwOeuBSbAKRsAIGAEjYASMgBEwAkbACBgBkzN/A0bACBgBI2AEjIARMAJGwAgYgR5AwOSsBybBKhgBI2AEjIARMAJGwAgYASNgBEzO/A0YASNgBIyAETACRsAIGAEjYAR6AAGTsx6YBKtgBIyAETACRsAIGAEjYASMgBEwOfM3YASMgBEwAkbACBgBI2AEjIAR6AEETM56YBKsghEwAkbACBgBI2AEjIARMAJGwOTM34ARMAJGwAgYASNgBIyAETACRqAHEDA564FJsApGwAgYASNgBIyAETACRsAIGAGTM38DRsAIGAEjYASMgBEwAkbACBiBHkDA5KwHJsEqGAEjYASMgBEwAkbACBgBI2AETM78DRgBI2AEjIARMAJGwAgYASNgBHoAAZOzHpgEq2AEjIARMAJGwAgYASNgBIyAETA58zdgBIyAETACRsAIGAEjYASMgBHoAQRMznpgEqyCETACRsAIGAEjYASMgBEwAkbA5MzfgBEwAkbACBgBI2AEjIARMAJGoAcQMDnrgUmwCkbACBgBI2AEjIARMAJGwAgYAZMzfwNGwAgYASNgBIyAETACRsAIGIEeQMDkrAcmwSoYASNgBIyAETACRsAIGAEjYARMzvwNGAEjYASMgBEwAkbACBgBI2AEegABk7MemASrYASMgBEwAkbACBgBI2AEjIARMDnzN2AEjIARMAJGwAgYASNgBIyAEegBBEzOemASrIIRMAJGwAgYASNgBIyAETACRsDkzN+AETACRsAIGAEjYASMgBEwAkagBxAwOeuBSbAKRsAIGAEjYASMgBEwAkbACBgBkzN/A0bACBgBI2AEjIARMAJGwAgYgR5AwOSsBybBKhgBI2AEjIARMAJGwAgYASNgBEzO/A0YASNgBIyAETACRsAIGAEjYAR6AAGTsx6YBKtgBIyAETACRsAIGAEjYASMgBEwOfM3YASMgBEwAkbgIUCg0+mkO++8M3F3MgJGwAgYASNwfxAwObs/KLmOETACRsAIGIEHiMBrX/va9KxnPSu9+MUvTtPT04taT0xM3CtvUQW/GAEjYASMwCMSAZOzR+S0e9BGwAgYASPwUCNw9dVXRxfXX399+tSnPtXt7rrrrkuXX355euELX5gOHTrUzfeDETACRsAIGIF+Q2AEjIARMAJGoFcQ2LNnT/rf//3fIDM333xzkJcDBw6kw4cPp0c96lHp5S9/eXrFK16R+vr6ekXlZfWYmppKd9xxR7fs+PHj3ee/+Zu/iWfKP/OZz6TNmzd3y/xgBIyAETACj2wETM4e2fPv0RsBI7AMAseOHUtr165NjYYXFywDz4pm3X777emjH/1ouuaaa9L73ve+dOutt55WPmW/+Iu/mK688sq4TluxBwrqxAx12u12V6sTJ050n9evX999PlceWKJ5ww03pCuuuKLnSfK5gqn1NAJGwAgUBEzOChK+GwEj8LBHoNVqhZE8NDR02rF+4AMfSC996UvTjh070r/927+lbdu2nbauCx44AnjBIGF4x97xjneER+yBStm/f/8DbXLW60M66+no0aPd17oXbePGjd38c+XhNa95TfrHf/zH9MpXvjL92I/92LmitvU0AkbACJwTCJicnRPTZCWNgBE4UwRYIvfVX/3VQQb+7u/+Lj3zmc9cVuQf/uEfRv6+ffvS//zP/6Sv+qqvWraeMx84Au985zvTN3/zN99nQ5YuvuAFL0i7d+9OO3fuDJJcCPJ73vOehIyrrrrqPmX0QuEnPvGJRWrUCVmdqG3YsGFRvXPhpXgBWZ5pcnYuzJh1NAJG4FxCwOTsXJot62oEjMCDRuB1r3td10tz7bXXnpacEayhpOHh4fLo+wog8A//8A/LSnnOc54ThOxzP/dz0/nnn79sHTJf9KIXxXXaCj1UUA8Aglp1by3LZklr1qxJg4OD8Xwu/axbty7UJeLkkSNH0rno/TuX8LauRsAIPLIQMDl7ZM23R2sEHrEI4IUpieAS9yfhxXFaOQSIUPi2t72tK5DgHt/3fd+Xtm/f3s17uDwsXXq5adOm7tDwypIuuuiiuJ9rP/UAJuytMzk712bQ+hoBI9DLCHi3ey/PjnUzAkZgxRCAGJREJL3TpTpxu+CCC05XzfkPAoHv/u7vTnWS8gu/8AsPS2IGNMU7VmAqhAZvU0nnKinF41fS3r17y6PvRsAIGAEjsAIImJytAIgWYQSMwMMDgfn5+e5AMEDHx8e77344cwT6+/tjKd+ZS+p9CSdPnlykZNk3Vyf/daK6qPI59FLfS3cOqW1VjYARMAI9i4CXNfbs1FgxI2AEVhKB2dnZrjjC5C+X2D9T0mMf+9jy6PtDhABzAmFbiUQkTs4MYzkh+9YuvvjiByT2TNsv7azuIYOElb109W9sy5YtS5ut6jsYcKQBBBIySaj8/2t+FhYWltX54x//eCxhZU4IvvMN3/ANy9ZzphEwAkbACCxGYGX+Vlws029GwAgYgZ5DoO6x2LVr17L6YUiW9PjHP748xp3DkTmfin1CIyMji8pW+2VycjJICWSHc7O47q+Oc3Nz9wpK8eEPfzj93u/9XoJgvOxlL3vIIlai99jY2LLwQRQOHToUF0EzLrzwwkVBNUojyAH72Dj/rH62GISI6Jw/9EM/dJ8E40zboydeMjA/3cHYX/RFX5SazWaozJhKKoE1yvuDuRM5EZmQPgLYoAdyH8gZfYzh93//99Of/MmfxJwXPcDw53/+5+93EBbm8+qrr07//M//nD70oQ8VMek///M/07Of/eyYw26mH4yAETACRmBZBEzOloXFmUbACDzcEDh48GB3SJxhtlz69Kc/3c1+whOe0H2GmD396U+Pd5Y7YnjeF6n4+7//+3Teeeelz/u8z+vKWO4BYvTJT34yvDx1b95dd92VfuZnfiZddtll6VWvetVpidZtt92Wfv3Xfz29+c1vvpf4L//yL0+/9Vu/da/8esYb3vCGCMjxlKc8Jf35n/95HLzNgdBf+ZVf2a2GJ4WAD8973vO6eSv1cOedd3aDSXDANFh87GMfC+/N9ddfv6gb8F8a7RHy+K3f+q2Js+mWJsg4xyJw3tgf/dEfLS2O9zNp/+53vzv92Z/9WSK8P4nv4tu//dsT++qWepM4GqAkjnQo6UwiNULKOGuM+a//wwOy0eVv//Zv05Oe9KTS1WnvEEt0ZjxLE3K/67u+K/32b/92+rIv+7KlxfHOWIlwCiH7q7/6q2Xr8L+D0dHRZcucaQSMgBEwAosRMDlbjIffjIAReJgiABEoaTlDEe/BW97yllIlfdZnfVb3uW64YtDjQTsdOXvTm96UfvqnfzraYrgTVKTT6QRRuuSSS9KLX/ziKOO8ru/5nu8JTwVRIekbbxdeEM5WI6Lff/3Xf4VH7LWvfW1Xl/IAafrGb/zGRZ6OUsYd4gWp+uEf/uHTkrtybABk88YbbwySCNlZmn7yJ38yiObpPENL69/f95e85CX3t2q65557gvQUHQjqwplpdQ8Nnp4rr7wyDrlmnkhvfetb00033ZQuvfTSRX092PaQkV/91V8N4lcXSH+/+Zu/GcToa77ma+pF6alPfWr3vR5in++DfY6QIAKIlO9q69atcZYb7QYGBrptywNtfvAHfzAOSS959Tu6vPrVr06/9mu/lp72tKfVixY9z8zMpG/7tm9bltzWKzKuL/3SL13WM8i3sVziH0D4BwaOSfCRFMsh5DwjYASMwPIImJwtj4tzjYAReJghgFemJAjUe9/73sRhwGVJIEvi6h6Iehh9PFklQQDua6/Qu971rlI1PEGQM4gh3gcS73jl/uIv/qJLrIrXiPzv+I7vCGJWhPzTP/1T+qmf+qlYqlbylh7mjE5f8RVfEQc3s2QTDwZ18Iax1JElf8slSEBJkB+WsNUxKGUQRfB73OMeV7Ie8jv4sxQOUvXoRz86PfGJT1xEDiAfdWL2vd/7vUF28UZNT0+H969435jrpeTswbbHW1UOKi8ggH/BjYPL68c2UKe+b6seKRTyzbVcYlkpcjkwHQ9qSbR/5StfucjThWcKIke//KPBt3zLt8QSz5e+9KXxnZf9bkVGuXOMQd3rSD+//Mu/nIhsyj9IvOIVr4iq/G8DTJf7R40iq37/kR/5kdDh/i6trbf1sxEwAkbgkY6Aydkj/Qvw+I3AIwABvEgf+chHuiN9/etf331e7gGjuL7k7JZbbulWgwTVje1ugR4gQnXCUA4eri9zw4tDsISyHK60p85rXvOaWNJX8sr9/e9/f/qCL/iCeIUk4TEqiX1VhKQvfZH/x3/8x6U4lrdB+JY7FqB4l6jM0rZ6ghz85V/+ZVcf9H0oyRlL8Z7xjGckDqJ+1rOedZ/7kyBA9fPS8CDihSxzxiHjLGcsaenergfbnmAjP/qjP1rEho5/8Ad/ELiw54tlpvT1dV/3dd06POAtK3sY8dCeLoEBqcwLhI95/+///u/Y60gZe+ggTiTq//Vf/3V4CyNDP/Xvjzz2kv3Kr/xKKe7e+QcBvIolvfCFL0yMpXzbLMX84i/+4u6S2eKxLPWXu/O/jf/3//5f2rBhw3LFzjMCRsAIGIH7gYDJ2f0AyVWMgBE4txHA8/RAEssP64m9YSU95jGPKY/3uuOpKgcMYzhDNJYmiFM98EgpZ19U3YtC+2Kk1/cp4ZUoCY8JhncJNkE+Bv373ve+UiXuv/M7vxN7kxZlVnWX5vGO543laBCNr//6r48qD8V5VvSBF5I9bp/92Z/dJQbL6VTyIDc/93M/V17jjr7/8i//koiwSWj3pfheddVV3fpn0v4d73hHVw5ElT1wZa8gS0i58KqVb6BUxotZyNnSQB14BFlaCCGF1LAEFrk/8RM/UZoHaWKJInIIuFES42YZZz3VSSv5yIKc7969u14t4T0uCS/lb/zGb9wLf/53wzfMN1/3gp1umSLYmpgVVH03AkbACDw4BBoPrplbGQEjYATOHQTwhC2XMIzZ3/X93//9i4J3EMyjnoo3gTy8Y8ulu+++O/YilTL23NTblfylxKHk14kZxjTL2UpCNgmjvyzVw6Am4EedmFGnLJ/kuSSCNdSXdZb8euTAkgcJgDSRPudzPifu/LDscaXTj//4jwdpZF/Uclgt1x/LRpfDECLLPrylZb/0S7+0aJnhmbQvHiv0wstZiFnRk+Wvy3mp6kSoTs5YRviv//qv6Uu+5Eu6pIbyr/3ar439hEUu0TNJdXLI8kMIbT3R/5/+6Z/Ws+KZuktT3bOIJ3a5M/34hlkaSUj9ejrd/57+/d//Pd1www31qn42AkbACBiBB4iAydkDBMzVjYAROPcQKN6fojkhwyE5GJNEu2PvTVk2SJ2le2vqofdpu/SAYQI54P2op+L1quctfSZK4tLEEj28JnVPRyFn9VDxBGhYulzvviLmEZwBr0w9sUyvnpCJMV4S3pLiRSzBQ0rZStzrROX+yvuP//iPblUIEqSW5XdLE8SHaIoQnXo6k/b1A5fr0TyRD9Gt9wVxKnvFIIxEoSTVvyWCgJwu1b/BAwcORDWWxJZENM56Yv/kd37nd9azus+Esl/qTa2T+g9+8IPpvpZbdgVVD/WopuhRJ2sEKmEfp5MRMAJGwAg8OARMzh4cbm5lBIzAOYRA2YuEyiwfY3/NUk9BPchHff8WbV70ohdxiwRB+oEf+IGIAIhBzDIz9ufUDVYq4uUiLP3pEsE3lh6U/OQnP7kb6RHiVfYgFTl1MvO6172uu3wO8kbUPPYjlYROP/uzP1teYy8S+7II7FBSfS8dBvZygUNK1ErGs3S5XpHzYO/14w3ur4y6Z4ZlnZBH9lVBHlnaCC4EuYCEPf/5z7+X2DNtXwTWPYnscWO/VSHPYMkyVTygJUGcSfV/BABPCGadNDM/jKEedKR4yOreRYgnCVLFNwixLl5V8vlHBM6GK4ngHnWCVieXHMXw8pe/PCKDlvqnuzPGevRQgrXQV0noQBROSDN7PfnfiJMRMAJGwAjcfwRMzu4/Vq5pBIzAOYoAwRpKWi40OWV1L9RSckYkPIhASezrwfBnuRfnQBXSApmqR3lcei5XaY+xXfeykE9bAjLU9Sth9/HCseeLZZiFsNEnywExwAmkwblWJbGHC+P+m77pmxZ59DDCCVaBJwajvu7dg9wtJazIK8SAZzwsK5mKR+iByKxHOyzeKNqzxBBd2V92unPsqHcm7evRLYkkydzjraoTM/pgaSFkH4Jc5ov5waOEt7T+jZDPGW6QI7ym7JurBx1BHss/SXVih8eXuYfgo0chhvT3xje+Mf4BgmAhpX/mmj6I/kni3LJSxjuBRPjG+Q7wtJ2OONcJPe3Y8wbu6FMS/1DBkQx4pPmHDCcjYASMgBG4/wiYnN1/rFzTCBiBcxSBOuHB+F0u1cnZ0mWL1IfssD/tdIklbHgK8IiVVII31PunDM8Dy8rqe3mQv5RU1I1xIgFCGv+v4CZ4SJBfvCwskVx6qDR7yQgvX87jwvt0ukOGn/vc55bhRJvuy4N8qHsxTxdY4r5E46kpif11y81VKV/ufibt6wQd2cxvPeIhZAcCXIJ0ENaeSJolEdCDeYG0171aEG28WvU9bbQpRKt8s0TmXPqNFNnc8dhBzMrh0xdddFEcVF0nYZx7x3cMmUX/ujwIHGSRw7QhXITUh2T97u/+biLCJcsw695b+it9lb2bdX14Xho9cmm5342AETACRmAxAiZni/HwmxEwAg9DBNgXA3nCIF4aMr4MF0O2pOU8OhAJvANEW2QJGQYvF8Ec8JRglHOW1jOf+cxEGHqi+bGEksQ+oxKGngOqi+fkZS97WQT+ePvb375sZEfa430htdvtuOP9wMNWN7gpwJgmiAjkrW5AQwI5NHkpsYQkQBwgcnhY6nuQoqPqhzOySuTA5fbI1even+f6EtGyJ+v+tCt18BKVRDh4zvxiz999JZbWsdyO5aFn0p5AMSyJXS7xjTGPJSpjqQPpLaSukNHt27cnljnW9/eV+tyZW6JyQtgK+SGfPYB8Z3UMS/1XvepVEc2x7BEkn8R3x/xCpEoqyyj55vGSgeFyCbLG8kS+e/Zt4rnl2y3fMt9OGRPt8ZQxrjrxLMRyOfnOMwJGwAgYgXsj0KezdRbune0cI2AEjMAjDwGWleG9+MIv/MLYM7SSCPB/tQ/kIN9637SrhzKnjGWJHG5NmH+M4aXl9fblmYiNnLmFkb30UOZS53T35XQ4Xd37ykcO58yxVPSB6lDkQlw4DLokyAxLAcsyT/rA08j5cEQ4JIpjSURrZD/YmbRniSln1TGn27ZtC0J2XySEuWJPHLjXiTM6QRxvvPHGxNJbCBTn0SFzab2if7njxdqzZ09EecT79X+dQ8aSSs5Lg4QTHn/p90JAEzx7nAe4XNTL0i8yIOxgvPQfCEodyjhknX9QwNsHGXUyAkbACBiB+4eAydn9w8m1jIAReAQgQCAJvBnfpL1a9WAaj4Chn1NDnJmZCQ9hCbLxQJRnOSf7wfAwPtj2kJOHe2K5KEQe4sjFXj32K5Ylmw/38Xt8RsAIGIHVQsDkbLWQd79GwAj0JALsuWH53oP16vTkoB6mSjFXnOFVD2yy3FDx8BA05dWvfnXsoyp1zrR9keO7ETACRsAIGIGVQsDkbKWQtBwjYASMgBE46wjgReOQZyJJspSRKINEnWTvFQSb6I3sAysBUpYqeKbtl8rzuxEwAkbACBiBM0HA5OxM0HNbI2AEjIARMAJGwAgYASNgBIzACiHgaI0rBKTFGAEjYASMgBEwAkbACBgBI2AEzgQBk7MzQc9tjYARMAJGwAgYASNgBIyAETACK4SAydkKAWkxRsAIGAEjYASMgBEwAkbACBiBM0HA5OxM0HNbI2AEjIARMAJGwAgYASNgBIzACiFgcrZCQFqMETACRsAIGAEjYASMgBEwAkbgTBAwOTsT9NzWCBgBI2AEjIARMAJGwAgYASOwQgiYnK0QkBZjBIyAETACRsAIGAEjYASMgBE4EwRMzs4EPbc1AkbACBgBI2AEjIARMAJGwAisEAImZysEpMUYASNgBIyAETACRsAIGAEjYATOBAGTszNBz22NgBEwAkbACBgBI2AEjIARMAIrhIDJ2QoBaTFGwAgYASNgBIyAETACRsAIGIEzQcDk7EzQc1sjYASMgBEwAkbACBgBI2AEjMAKIWBytkJAWowRMAJGwAgYASNgBIyAETACRuBMEDA5OxP03NYIGAEjYASMgBEwAkbACBgBI7BCCJicrRCQFmMEjIARMAJGwAgYASNgBIyAETgTBEzOzgQ9tzUCRsAIGAEjYASMgBEwAkbACKwQAiZnKwSkxRgBI2AEjIARMAJGwAgYASNgBM4EAZOzM0HPbY2AETACRsAIGAEjYASMgBEwAiuEgMnZCgFpMUbACBgBI2AEjIARMAJGwAgYgTNBwOTsTNBzWyNgBIyAETACRsAIGAEjYASMwAohYHK2QkBajBEwAkbACBgBI2AEjIARMAJG4EwQ6D+Txm5rBIyAETACRqAgsLCwkPr6+hL3s5Hoy8kIGAEjYASMwMMJAXvOHk6z6bEYASNgBFYJgQdEzO6Lu91XWW1shQSeLSJY69qPRsAIGAEjYAQeMgT69Bfb/fyr8CHTwYKNgBEwAkbgHELgvv7aoGxhoZPa7U7qdPLV7rRTR+8qSQud0/2VQ/4pTxjki6vBvdFIzWYjNRpNXdzzdV+Q2at2X+i4zAgYASNgBHoVAS9r7NWZsV5GwAgYgVVEoBAwUaMgVUWVkl/euZ+iVQtpfr6VJiYm0vHjx9PkxEndT6Rjx46licnJNDMzk+ZnZ2PZ4ymKtiASp7cgdbqLjDWbzTQ4MJD6dY2MjKQ1a9akdevXp/Hx8bjWrl2b1oyNpWb/vf8Kg96dkn1KS5O1U1j4yQgYASNgBHoXgXv/zda7ulozI2AEjIAROAsIQMAKmSnP3OvXrEjWiRMn0uHDh9ORQ4fSwUMH0+FDej5yJB09elRlx9P09LQI2Wyam5+XJ619SnM8YnrDI7aAd02y8bLx3NYFUaN/+ms0RNb6B9Lw0FAaHB5O4yJlmzdvThs3bkwbN21K27ZuTdu3b09bt21LGzZuSIODQ13dYwySAeEjlbHEi3+MgBEwAkbACPQgAl7W2IOTYpWMgBEwAquFAARmaYJYHZf367CI18ED+9OBg4fSgf37g5hBziZOngyihsdsuvKOzbdaQbogSH19DXnB+lMznpsiZSxVPLWEsSP5QczUd7tqB1lridRRBnkraUDetCERtRERNbxn69etC5IGYTvvggvSzp070q6du9I2Eba1KsMLtzQV4rk03+9GwAgYASNgBFYbAZOz1Z4B928EjIARWGUE6h6lQs7mRYzwgN15xx1p7759ac+dd6Z9ImQHDxwIknZUZG1ORKwl8sQesH4tMWyICEGG+vWOV4z3hogZZKihPWOx5FA8C162UPNmMXwIIH13L5EzSFnJD8+a3ssd4hZet8rTxhJHiNrWLVvSrvPPS7t3PyrtvvjidMH556ftO3ak0dHR0G3pWE3UVvnjc/dGwAgYASOwCAGTs0Vw+MUIGAEj8MhBYClRKSOHmB0QCfvoRz+a3vH2t6cbPvWpIGYQo8HBwTTIEkMuPePJgpgVDxWkq1F5uiBokJ8gXGrLc7moQoAQZQTJom/qlURfpGir/CjTPZZA6h4eNtWBvHXkbcNTNz83F/mqlIbHRtNjLrkkPeWpT01PveqqdIG8auPaq4a+kEmSiVnA4B8jYASMgBHoIQRMznpoMqyKETACRuBsIVAnQvTJexAi3W+++eb0xje+Mf3nW9+a5kR6IF/lCm8YpIs2urgXj5keu+SrRFksBKj0d4qcwc6CnmVyVpE4ZJBK/fJc3oOMVUsd0Tc8a7rzzIUnj6WQyO6X525wYFD70bakyy9/XJC0xz/hCRFUhKWRdZKG/KJrKOAfI2AEjIARMAKrgIDJ2SqA7i6NgBEwAquJQCE60CtxkiA4MwrwMTU1lebkffrXf/mX9O//9m9pv5YzbtIywWFFTAyPE8sWK68T+kOGkIXXLMgYnrHKW1ZIGPUWkR46LERMz7G8kTylohf1WbLYrVcri6WMqh99q06E7Ffo/rJvrQQXgbS12L8GUVMioAiBQy5/3OPSc5/73PSEK56YxsbWRFldv/pzFPrHCBgBI2AEjMBZRMDk7CyC7a6MgBEwAquJQCE/6IDHCyJDYI+Pf/zj6dprr429ZVMKeX/TTTdFsA9C2BMRkeWLsaes8pjFGCoChcw+yJnKugQNklZdUbf2Aw2jbxJtWbyov4iCJEIWI/Genxb9Rl+SW0hhuS/oHLUW56ghT4SNC3LWbufljgsqa2upJsSRaI8XXXSRljs+JT37856XzteeNLxotGUMpY9FHfvFCBgBI2AEjMBZQsCh9M8S0O7GCBgBI7CaCEA66umYziG76cYb0gc/8MH0vve/PwjZIUVe5CwyvFOcJca+MjxlkBbIVrkXOUViBP+oyvF2Fe9TuZf63CFmJT8Ikd65IytIW/VcJ0lRrvzSjmWUahTBRuJo6470a+Q9arHsUfqjK7o3dXA1Sx3Rkb1pxxVZ8hPXXZcOKfz/gQMH01Oe9rT0BC113KKQ/PRDKvfSX2T6xwgYASNgBIzAWUDA5OwsgOwujIARMAKrhUCd5KDDnJYv3iNv2Yc/+MH0nve8J31UHrO79uxJs7NzaVZLGvFC4QmDoMXSwkrxOlEpHqYgYqpLWTm3jOqlbrnXyU55LvWQBaEqpC30rfost3obvGLIhUY1GiJo6Mm79KAMed27nnkP8qh7i/fwqLUjwMnbFOzk9ttvT3s0/que/vTwog0rRH9JoYtkOxkBI2AEjIAROFsImJydLaTdjxEwAkbgLCNQyAV3/nS0vO/gwYPpfe99b/rHv/v7dJvC5M/Mz8Xyxnnd51vzoSFEC28TB0Kz9DGIGnddhXBREeKDB4s8Lt6po5cusauXQcJIdTmFbEVB9VPvg6zyTrviNYuq9MOl/CITeaUNhA0y1tTF/jOWXc7rIjG2ae2xu+aaa9LePXvTYeHy5S95Sdq5a1fsryt6F7nRyD9GwAgYASNgBB5iBEzOHmKALd4IGAEjsBoIFFJRiAvk65abb0lXv+lN6d3velfsKRsZHUn97cEchl7kJTxYIi8QE8LpT09Px/LGjqI1IqekkM2L6sUz5Kh6phZ7yPBWQYOiXGUk5BY59fySR53yXCdk5IUHTO1DZiWrXp86hehRl+fy3qZfKiuh1xwPGm+fwuormkk6fPRIetvb35aO6Fy3F3/Jlyiy4+VpROeilT6p7mQEjIARMAJG4GwgYHJ2NlB2H0bACBiBs4hAITjFi4RH7PpPXJeufvOb0we0v+yg9lsNjwyngaHhNKkAIBAxPGQk2nKxxHFqajqiNxKNMSIyVp6xQpxUMRMuGooMtSsZkBrKSiTGIpNqJZ2OqBXZhWDxvjSvyEBuKSv30hckjjwwIC8CmuidZZAkljiGJ09jx4t29Oix9H5hc+L4sfRVX/3S9DjtQ1unQ61pG+OJVv4xAkbACBgBI/DQImBy9tDia+lGwAgYgbOKQCEs3Eks5/vIhz4cxAzyMaWAH2vGx9OQ9lZRA4JCHe6lLWSlT3nT01MRIGRAnrNCUCA54QeDNFVkjX6it4osFUIUhAlCVMq5V3Vos1yinL6COKlCeafuIrl6rxMyynnnoo2EpD6NqeTRtkOexlXfSxftGL8eTpw4kT72sY9pzLPpC77wC9Kznv2ctEVHCRSdQ27VD+2cjIARMAJGwAisNAImZyuNqOUZASNgBFYRgS45kQ4QnI985Nr0lquvTh9SABAiFa6VN2hMIfJJLXmNxDy6Z4FBYPCQkUdbzjzDs0ao+VimCNESwSFx71P95VIhMVEmWQuqWwgR+tEP93oqBIg8nosM7kvrlzLqFvJFXhl7kc3+tL5OJnvIyAQ0Jf7i6+vL41isRUrTCozC0QLUbbfa6VnPeU7atHmTDrQWKV2iM/07GQEjYASMgBFYSQRMzlYSTcsyAkbACKwiAhCUQlI4UPr2225Lb3rDGxX04sPpxMRE7B+DmOH9wluWfWtSuJAO3Ut77ix35NyzER1CXUgZBKUsc+S5XNSnThApMCgy9UgeqS6b90J2yp28kqhLoqxOzkp/y9Wjf0hV9K32IVccTFqW6nE/pVqFwKmMaLsgGTfccENqQU61745DqwkUAm71tJze9XI/GwEjYASMgBF4oAgs/pvmgbZ2fSNgBIyAEegJBJaSmf333JPe9h//kd7/vv8Nj9n4uvVpjc4ug2BQN66KNHWJl0YCEeId4gGBmxTJG9UVyxmVR1npq9tO+dTPnqlqj1ZFjsinfrnXwSr5pc9C4qhbUnkufZJfnouehbyV9tSBQEY+LzV58coPSXp1E1iATUVaiVZ5w403poM6+22NDq5+jgjaho0bu9h02/nBCBgBI2AEjMAKImBytoJgWpQRMAJGYDUQKGSFvnm++6670lu1lJEAIFPaP7Vuw8ZYyjg4OBjl3foVOYEKFfLULatkzepctJPyutEWMtMRaRlQlMO2rlgCqXrljgwIU3ivlL+UvBXZhXCpSvRb+uadOoVskV9k1dtQj0TdclGv9Bd6VsSzEDPaR7naxDluak8/etWSxUzSFgiMAvkM6ZKv52PHjqXXv/71ESDleZ//vNiD1tDB1iUtp1cp890IGAEjYASMwANFwOTsgSLm+kbACDwsEcjEAZpSTPNza5hZ/5SOHjmcrvnQh9K73/3udOz48bR2/fogZkOQK6Xik6I+0RUhF2W5XiEalJVnvGcTImfD2ncGMkR1hAgNiNiU5Y2FTEF+yjN98VwIU5FHPs8F6XjWe7Ak7lU5OtT1iILqp8iinMR7qV9/j8IlP+ydk2Kpia7sryPRfX7KY1S53IahP2T0TpHdq0V22Xv3dB1WvW379jyGqt+q6blxq7AuGJ4bSltLI2AEjMAjBwGTs0fOXHukRsAI3AcCp4zVTBDuo2rPFUEsoDtzOsvsYx/5aPpPLWfcs29vGls7HpEZB+XlisObVa94ooIMQW5krENUCI/R0XOXXGWhQXqK94yBQ1Y6InrU68d7praQNEhP3CUDLAueHPwchIi72tMPHqmSynshYqVdea8TMHSNM9Rq8pFTr0v70qZeVuSSx3PRl/dIYAEh1NLGLkaMVYWM9Q4d2P3GN7whzWsf2vNe8IK0ZfPmwO7c+1o0II3fyQgYASNgBHoTAZOz3pwXa2UEjMBZRKBuzLflMZmZOiEWMx/L9zoLMs/DcK8Uwq7VO/Yt2cXMlVmvjGz4U1OPUUY+tKS8Z8M41w1S0VApsqiNQFJVeUGRBklBLOiIZvRNptqVRB4erk/feEt653+9PX3q059Wm0Zas2Y8xoD3qpATyBKEgzZxthnyKIcwiYQUT1fRmz6oOzU51SVfJQ/SEt4zERralaWF9FX6hPBFSPtKB/pWxWpMWraoV/xXyIo2lAOIEu8xXr1zL/mUBsFCdlV3Ub2qHBnRLj8ErPGOLKWiJ3ksZ4TAxl+KVXlU0g+RJhnbzbfckt71znem4cFmevrTPiet0SHejD/mHnk0IAqk2i/w3aBfzo2+o5hKlFOVsnjPevbV5zTmPs8N7U6p1G3ZlVlklHvBhP4Z24yWpg4Or03Do2OpoblCWLcOwp2MgBEwAkagZxAwOeuZqbAiRsAIrDYCGLInj+xLd33yQ2lu4nCEUB8eygE0ZNFm9ToQgqAYS9RVORZ0RRbqhZX9raLwE1X1IB4KKU/FsNEzgQsZysomeP4tdfA7dTo5GmHuJkpwZ6U9hyfTW95xTfrQNddHlMW1GzbEMrwgPCIeJSERIgQx486YMdSDoFW6k0+qG/AcZE0ESOQ1MfCVaMuZaE1d5Ncv2kbfyF76XPVT5IcOkkcQjiCJeqaMfBKjjLpVu0BFz6U993r9aFMrR06RlSVm3buy9YCukDC8erHMU7qUVGYbsvyZWz6TGm+ZSa2Te9OTLt6exkZ1zECfpIZSIrl6iL7QHX256F/CeCRVw9I79XNTdZ7L9Bt5UYm5YQ9ct1ZVR+Pt1uahXq7n6EjEV0cBHD0+mSZPTqRtFz9e1+PSyPiGaOkfI2AEjIAR6E0ETM56c16slREwAquBgEjO8X23ppve++Z0bM8t6YJHXZDO27FJS/fk2wn7N5vJGL/Z0C5GspQloxjkS3TPBrtIkfIXIFcytwkqEYa3jHJIAY4SCAIy2gtt9dkvr0wmVdmDlklUIQos78PgJ9HtBz5xT7r22tvTkeM6ZFpRGUdHR7vkiDqQl0LGCnlY7pwy6gVRQagSvwyda14emBl5ijiUOukQa1IDPSBoykM+nrkYR3VHHksfF1j6WOUxhljuqHvIl4yAV++hk+qBJ6Qvkp5L25xRsnM+ZVwAUchdqRdjpVwZhaBRxjNtFmjHpURbEGcvXpM89NYziV/R2TQ7M5duuvmO1BJ5X/v0C9KFW0bTQMxTJpNlbOGhrOuv9h0mWUQuSJnEg1sk+lIRMxpzHV3jT1SPkESV52+FtuSiJ9roXX9CQ4i+CumDe0vf2bFjk+nW2/YJU3mB+2bTxh27RM7WS66EOBkBI2AEjEBPImBy1pPTYqWMgBFYDQQwasfWjaddF+5MU8fuSTfedijtOzSZdmxZnwYHquARoVg2bjHAw6ru5mEm57LKZI6S8hNGtNpQI/9yj9zqN0uDMPSlWb1k47u0DzIRbSnK7WbbC+mQCNn/XHd3OnRyJo2IlI2Pj3dD34c1L3nUhzwFQatIQUUNYh/XKd+adFJ9lnMG2YIwKtEb9We156qhACFxwHPJR67kB+GC4OhCRnlv6xmiVbxzEYyjKo92lOm9jE8P8dySfFKRhz5RB1yqOkEEVSdQV56UjrLSJuorT5lRBxmQLhKYcNGWnMBd9YJYMQblNVWuSoSk1Jse1X5GS0jv0Hfx1mv3pqdfsjFtWzesNlEcP/QZbbqzmgshX5WmSMo6n2qWMc6qRi71++SpjYpFPuooI+a/wQt98V81FvU9L/2OHptIB46eSGvl2bv4UdvTxq3btKRxqNabH42AETACRqAXETA568VZsU5GwAisCgKYznguhoYH0vr1a9J0ezrtPXAiHZ+cTRvWDKeRYUU8xPBWYilbmPvxTstiPUchVRYlSBi1qBdmfxjUIgXY1hj/NNeFzFwvP5/KOSU/ymmjdOjkbPrIbUfS3oMTChE/mMZ0Jhdh7wsRohWEIvpQG8hJLGmUR4Z8yoPIVIQFmaRMZdBNz1XXyGBv27TK+3U4dYMDrfVM+/AHIkMyeediiSAePsgO2rL3jPzYh6a8QrDq76rWTZA4OqffIDwqCbJVyaQibalTL48+GFstn7xqGDHu6ERyIiGD8tKmqhteQWRUZewvY5x4syan59Nn9h5Pg/oOHnf+eBofXuav0yKfTuiDb6DKCzyKQmiGV033KI9lkmBWfWO5ZS6Xjt2kZqWL7DFLaW5uPh05MZlmZublPR1KWzevT6MjpyJt0oeTETACRsAI9C4Cy/xt0rvKWjMjYASMwEOLgJYUinxAQAb6m2nD2rE0L8/UPQePp8NHT6b160bT2IiW82FoYxVXdi5EppADDPCww7uKZmKibNXPRIW29zKRIQHk6l7ISMhU3Wy2I/CUuU4f07Od9JkDk+m6O46kuY4CgIyPpeGRkdAF+cjpXjLq68/sFYOkRV8iHyT6450EmWm3aSOSo2dIZBkjBG9qejrqsHyyT1EbIWXIQ/9I9Aep0VVIGu2XCxpCfcriyi8houhCfpCrIlulQehq7aJZ9R7eseoZQkUir/QRGeUHDMqz7gU3sqK+9C8+U0gZ5eRDbI9Nt9IN+07qW1lIO9cOppEBQMoygmshpEqBs+a/grd7l7BMXqUEekQ9numoeq+eooz2EWwkZ3a/DdrOzs7HHjOplrZuHE/nbduYhgYH5Elrax9iSzpHD6pZCa9k+GYEjIARMAK9g4DJWe/MhTUxAkZgtRGQ7UrAjfZ8JhoY9utFeE5OzaZDR07GfXzNSBobGsTRIsMY4sUOoExmwvSV9RxR97IVLTs4Ex9qU5+9QsExipVexqxMjP664cwbMru/eokqVc6hE6106z2T6bhIwvg6ReMTMSOYRZajVlUfEAmIE8QI8hneMz0XEhNdFMHxIhxUnz1vI1rOOS+SNo9hr7wim6VzUzMzsdcMHem3eKdCLvIgRGoTHjoBVghWIUrlTpeFRBX55BX9aFfq1supU2SSH8RGdQv1IK/bjspK3b1xTGAtlb4yiZIE6Q3YeP1YlhneP+76Jhgv9cDo8JQ8aPsn07yw2DrGPsFCwKQPYoQb9bNOFX7K6M4NJbkwtAmnWU2vUo8snuMd3ZTCh0YfeoaYHZMXld52bV6Xtm1aJ4/ZYJrVPA/MZxykTLTzjxEwAkbACPQuAiZnvTs31swIGIFVQCCMX1nIGOvz89kg3r5praIS6jDifcfSweNTImcD4VmLYBIylMOrJAs7TOawtbG2swHNEBZkpfdpfxA5p+zw/ARZI4VTQ8Z/SY3KSofQ5ZbBFbr2NZ6jg5MpHTjZSoMii+w1iyiDGO4qC0NeY+COlwyCFORMxnpLAwsvl8pyPbWhnVIhObw2NbBLdm6M4BjX3XmwS9DABlLE+WdoTN8jChAygAcNGbpY/hdLGnmuCFOdpCk7UiFbXRIl3UseFZYSM8pKOffSjjEHehofbUi8R92qTdRnUpVoRyKPK+Y9cnI7HkEk2lfPegmcyAu91Bfk68BxyRRZnVQ0y34Fc4lU4RlKZGjznrZcmoVXM4smaBz68xP1pVOoqJcoiK6DkPFF8MFoVqNqW8/TImc0fPR5W9N52zcqXsugll4qhL7C/qNKJVJ1nIyAETACRqCXETA56+XZsW5GwAicfQRkyYYxK0IFmWhBROQt2bJhbTp2fDrdse9oOnhyOjUH5SWpDHuUDPtZFnYf7fSCHd+AtUUq9+q1ygtbH5IQxcvVUUWyw7KuP0vHvsEkWpY6zaE0umZMy9e0H46qKF8Rj+ItYxyQEchZp/Kc8VzExllualtITSnrF8m5cNN4umznhnR0ckZBME4GqUOlQijmFCCERFv2iIFVDF79sZyRyIex10x3PFDs6yI/UoUfbQs5KmSIO1chX4UQ5YZ0kfEqd8YSe9R0j+WHpbzqiz6oGxdC9EyfJZ/2ZQbIL9jkqmpXKycv9JVM9JuTZ3H/RCeI+8LMhLChNjLwqqot4xOBKlJzKVJyIr9SNzLUPa26mJRMMA+90E8v1OMfDdrzbf2DQX96/O6taYeWMw6JJHP0ARXQhd+cygirV9+MgBEwAkag5xAwOeu5KbFCRsAIrC4C2YCNs6uwmGUAt7Rnh3DpRL5r6f3mI5MRGr7Zz1K9pi7VgwTI24Xxj7GtXP1mDwtuEYzt4GphXYtA0EQDlc0eRniQDLrD4qZl3VoPQFRYJfZAjQ32KQjFoIKXjKSRap8ZxbQOcoUcXZwdBjlDLh4zljUGSSMfolaRNNqRTpEhvajLIS1X/Kzt69NXP/WS9Pbr70ofkwetrX14TRn8YfJr3HOS26cAIWocSysJtQ8BY3yF6ARJUznkDU8eiaWCsJIgwNUzONKOVHThGTzQGzJVEnm8U487fZDIL/jhvQM59Ch5hZApI2TShryCAbgFZirv5iET2VRWQhZ1SJDgee35ay8MpROdVpqYm44yloWGLky8BKmafmijMeAZ5ZVMLX1lXqKS6vbhfNM9k0xmW/qoXflWGC8vlM/NqbLaXrxpLG0cHxUxkyzlS738vYUjT7rSTyggcU5GwAgYASPQswiYnPXs1FgxI2AEVgeBvFgs7Gjs2fB4hNkchnacPSZCtkBERC0ZwwruU/AQTHdIGUQt/CW6x9JEzHnqICsGhGEuLw/WMzkytGlBJmSF3iGGLIUk5RwRlqgvOZKrHW86t0r9N/vTgPQoxAKjPciWjHCMcd65E+CE/FIOGYJQUFaIWxj86g9ZPKubNKhxDYeHMKXLtq3L+Sr42B2HKgIgzQMfLQGVB20KfdWe1C/vTRCCeNOP+iKhR6QYa9AUOo0lf5GPniorRIo8ntGLhG5BeKo65Z0xUiewqPqKemoT6NbalTb1PsABTGgP8oVA0mfBSR3zuigho2DZaAyIKI+lWVWDgrHdMOSpDh4uvgGWioZnDdyUQR6pLzyfoWnGKtyv1BTGImZ8I336lwG8nLGPTwQZotdsiHDPSm/NVeyLI1v5kkgL3SMj4Ee6kxEwAkbACPQ2AiZnvT0/1s4IGIFVQQDDVkm2LWRooYWxm8kO5ItQ+wsKtjCsvV598p5RJxOBbLyH0V8Z8hjMDdUhdbT8DEkEv2jIckdWRE2sEZZsrUPU8ORkw5oDiCEGSB/Q4dQDMsbxzLHHK84Mg4xIRvSr59hjxh2iI9KxoDt5vEM0IBPU585ViAh10TQIjso5222NxgkW/GXxmK1rE0sdRxQB8MO33JNmFAFQQ1C5xoFMEbTJICKdNKp7HFatdsEMuCsviE4tD9kZNTLVN3V40L0kxlUneuBXfw/sVYfxZa9RNR81GYV0FbnlPTBTRxGSX/foWziUesU7xhiYu5Lq/ZMX86r7yIDw6u9Lc32Z5EPGQhb4KPVB7CFPeo/ljrqzZwwQ8JFxb8tTy7xkYpbboVnxnnWEe0mzCuk/W5HALrYUqj38rrQu9X03AkbACBiB3kbA5Ky358faGQEjcLYRkFEbhrP6lW0bP9mJofyw2XWgMoRMHqUhLSsMcqaKQWiyeS2CgUlMa91FEFjKh43e7s/mfZ+WuzX7sqcDI7ylpXAkSFj2kBV5iAiKFpwNMjfQlgmvZYSQM+RikAcpUdUgNpAwXRCtQsyChOmduqGBSEzI5Z18XSTGQCIPvcc0vg1aygmBacnzMyS9LxVBG9T4WS75ybsPp5M6T4v+giSq7byChCCbd7yLgwQJqUhSEBo9R3+qQ34gVfVP3xFkhQcl+o0xqW4si6zLUXlgrrKoV+le2pHHVd5jTPGWf0IXPUa9XCnPd6VLaVtwoj66cpFHOagV7JAP2WouNNLo4LDGr8ArqsFcM29owvzyaYQktWds8LJWu6WyCn/JbuFVY45oJA9Z+aO36LutZaMdBR5BKu078yLpTVWWcDXXpbsExgHaylZWlkWhkxEwAkbACPQ0AiZnPT09Vs4IGIGziwCepMp4VsdhylY/GNXYyljREA88IBCzft37RdQwwnFVsFRNpTLLsyxIGQ0JFDI0OIQEGc8y2iFikDQZzA0IV8iVuY2RrQb80jWGNnu86D/StIx2lJRXixSkI/YrycjHKBdJIA9CxnMY6rXnaKP3CMwhPYpHLYRVP3QFAVs3NpzWV+SMZXOxN0xaXbRhLL3o8RektfKqXXvbwXRMRw2EHhVBYonhpPagIZtxRBRHytQvVxkbdxI6RhkMpSrHk9VQ+1gEqecgS+TpQj/wol2QpDJOZVNeSBvVSNSJ9uVZ9+7+tFyhS0zjtcrjVhJ9McbQVZmhg95JMXaVh2dS7/2KxTEwom9Dc9QJYpa/maKXZkZKZdLGMk/2mKE3886yxeyrFPGWeL4jMIl51SP0udGUZ03tmON+BSPpDOgYAz4Hvj81AuGQpvLAGOijON705GQEjIARMAK9ioDJWa/OjPUyAkZg1RCQTRvplCmbn/BGyP7NHhKRFUhSn5awNbT3q0EQDDXES4IxLYs9KXo+NjjWdJjLGOvZyM8EgroY9hFAI6znTDb6tXQRA17mdyZlGPjqa0HLGQn+EAREYjHYIWQs84MIRb/KL2ShELTYo6R6vKMO79nrk/PwgpFilBAZPRP9ceu6MZEzeQeVF3vkNBj6hB7tWjucXnDZLpU10ifuPJQOnGDHGZBIx0retELLgwOh9gclrxAkPXQxCkxpo4RuJGSgQyFCjCsOslZe14NW2iCryueOvBi/nkt/9WWQ5EU/uude87hLXyGtkrnIk4hsjaXUK3f66uZLJ87Ji12B8/omhprZywqZRy88ZPpu+jVfGSd5HPUc5+IhX3/65HmDlPFtoB97+qIPkfbgX+qjoedOS3Ou5Y/tvixTyACcWtCIMelHA0XPjsgiY+bTdDICRsAIGIHeRsDkrLfnx9oZASNwVhHAw3LKqKdrDOYwxHFdydANux1LVw8RrRGPmcoI4oEnLLehmMAakJ4+nT/WlCdK/3cr2YQ4RwaErKn8MLBl4CND4qMO9XgIQ135A5XXaWEOD1WY8JjeYbRTs5CxrKfMcgmKZXEY5rrQijzE0q4kSAVRArmTcsj37HnZMDqcdm9Zn9boTLeKJgQ56IhoduStoc1GhW9/8eU70651I+k9N9+Tbt5/PBMEOqzqTOsx+lT/g0N5iWTooXfyQy8BEjqjY+V9C/Ki8vCUkU99XQEed6UgPDyUfD0GoarKqcX7vJaBcg9ipnueldw3zcnHk9hNvEtGIXndfD0UnUMX3tUuyJOeI9gJk9uZS9qVqOMWRKc07cwz84Z3Nb4L1Q0MlI+XFTK7AAEnX8xMPjXhoDb4zdThgr4jviWIWhtc5WqDoCkT5fkvxgCiWS+ekIZAroxXvPvHCBgBI2AEehoBk7Oenh4rZwSMwNlFAEs4zO9utxAkDPUwdWUFd6MoYhDL68FyP6xjPDqQqBEREAztubn5NKf9VxH8QcZxBBbBZMaLpmARCAxDWkSmqTz6RUZ4N+hMasS+LRnrVF+QUd43JyKFivwRqQqyUTRVQZde6LmQB4qzX0xEpapLvQgSQhvJCa9bVcboOTZg54Y16cKv/4IAAEAASURBVJLt6+TlydEbw/snAV0skCE9x0USrtA5aMPCYsPIULrhnmNpcmZOpJBetFdNxGiGuhWJGdZh1XiL9BOkJgak8kjg2yUcgKB2jCWe4iXmofueK+RS1QuyRvtaom6Qp3p59dytltlNyC7eMsqqaehWqz8U4hb6MTb01MV4WmoYZ7spzH1jkL13wl44sjyVP9Awvoe8F41e8J5VxLIi6QSEaS7kvWUxJG1LZJEndI4xtbTnLKJaQvj4fpCcu49yCDU98cLwuvrVB+FnI2AEjIAR6DkETM56bkqskBEwAquPAOZvThjNXJjqGLyZPGF947HgXlWMm7xkWr4XBzHrPUxxlrlJQEvkirqxPFDGNMQmfFTIqFK/hFM3vEeytMOg1oFoeKqSovK1CaUeS9fC7I6u61QErZEWBKeSGfrqGWO/HDZdjgfAazaP10Z9MUREI3/tmuF0oQJ/nL9pTSZjEACMfGQjSyS0oYAXRBlsyxM4PqwDkHesS+uHm2mjlkF+Ys+RdEAHdjNmCCYBTGiHbNqMjgx3jwBgnxZEJ0iR7tQLllFwAWfySHou5dyDKFJe1S3kKMqqvK5XKxgOOHSlhUhk1lO3fEk+dahZ5gaM6Yc+mYN63zGzGntbwVI4rLwxypxqHmFPSv3az8d8kJjvCCAj4cjmCkIvnFgqK8YufPIcUT+WaKoOODab+g70rFYqybJPjebUkzoITNXEyQgYASNgBHocAZOzHp8gq2cEjMDZRQD7GeoTxjZ3jN4w1LNlyz4y/BR4PcK8Jlu2bzNIFav5MgmTAHmgKg+RniEhGP4Y6ZCJkKZ2ETJfL5j4YWiLuDW1jw2LP+8FkwaSOY/XDCJTGfjIR69GdQelIDk80A93pbKcrhAz8hgb/SM/gnags4x8vUT+dnnNdm9dJ9I1lGZm5zQ2WlV6Q0ogBCJsDdWniAAgcpqlR28eT+Na6ghZ+9jdR9L+EzNpanZeBJDRzYd+QVjlVRuVnDgKQLpG35LDfrEyLuSiY7nrMacy3moMkckzY6ZMKf8CgwiR+il4k5/nNarFcyF2UUftIVqR0EWpS9Z4oQ+wUqp7G0Nn5dXlg23fvPqX96xfRyg0OPagwrC/fzCTdNpU/aAnCfKOR7YVxzdEVmDATyZ0ef8f5+2x3DG0LD/IimdQ5hvTSyglOSqDozkZASNgBIxAbyNgctbb82PtjIAROOsIyJiXkR42c2Xshwph8cvYrhm4UBMMXgz7ICsKj88ywZYM8vCUqZBlgXjSwgsmQYgMGxoyRHvuktGRYLrgFV/IgljgQJ+WtcnAn2/PyfskgqfyYsQXUhFLFhGIYF0diJPuyApPTPSSbXRcY4Ucsj8KrxmkkU4hGxCXAXl6LtZyxvM3ro33WIYZhBJ9leStiSAV0JimCJbaQhRaOnuL8O5bxofTsy7clLbIg/bJe06kzxw4kY4omuO8glfQDh3pE6/dyMhILAPFuxbUJICpAazulpIpVCgkDAwKqUIuwJb65U798gwmpNJD3CFFVb+8R91KFnVZOlja8V6ei3cydFB92nWJlp4hccx5C3I7Le/ZyICiNyooiv5w9lnWPWMKbjFjyEA7ltKK7LfbiuSBLH0/A2oDfi2CgLBkNOpyk0Yope+l6MY44grd9c1qigtWjMHJCBgBI2AEehcBk7PenRtrZgSMwCoggFEc9i5hyWXc8idbv7rpETtebCvXwbBf0C4gvGZyL4XRrciNCzq3DFMZIoXx3C8SE3u35EEKWRLSYJ9QZU1jOOOJw+imPoEgYhmhjPI+Eb3OnKSJ0MSFTJGZIJCSFgmFSZTpFoZ4ZOhHckl49jqNXE4dzi3D41VII3U4BmDzmrF08bYNacvakSCD7KmLMat93iNFN1pypz5jaZ/G0ZGekMy+hoipyOToSF96wvYkgjaQtoxpmeO+E+nuIxNpLsggNKyVpqUEaiNjVCStn2iXkhEeKWSXManzIEKhBFoq8VyVd+spO/BWWSbXalfVAY/yzNijfSWDMmTRFqSinCpFvp4XeSurOoUk63VRCroF2VR78AWzBXk9tUks9Q/p21JtROORI1AI/c+LePPOnjLGL66rhF5opLqSAeb8AWf2H1ZFUR4/RXC8qF1uqjewBHPyKoIddfxjBIyAETACvYiAyVkvzop1MgJGYNUQiHOiRFKClMngDZMYaxr7VgmbNwx3jHolitocCNyWkU2eCBhetAE9t1jOp7J2G++SKkouziOIl/7LpE4eDyI9IrhfHhPoEwa6qE4QoPaMAnZovxmyyA81ojy6X/RTCEW2/rMhjrcFTTvigpBIljfS9xzLDecUuEOyRB/UhJH2pcvO25Qu3DSehhVpsC0GIC4QhCAT0CyTZZ30EWdtiSh0NN4ghKrM2FstGvWlHRrFuPZXXTA+lD6+fzhdv/9kOnxyOs1pmd+AvDkzjFXh5VkCOFqF2yeKZZAsyS9EDTKFzoVkdYkW4C9J9WWI1C8kqrQJckM7XYFLJTf6KPKqdpTTQ2lb3usyS15RI+ZH4+nuOxT+rVmRNHkPG1ryCaAsRyQ1+BiEUxNCv6B9efpOCBYTAWMI/SEswTjUVf34JjVfvGtmQkbMm74x9Ge8OaF19Vzyg/hXH3FVyzcjYASMgBHoPQRMznpvTqyRETACq4gAxi4HRmPekjCIs9Er8iJjGns6yrKFrBcZywTtUAHkqCMSNailgXiBks6gmpfHBPKWG8pgDgMaMiRDWVXEbaplbhKlYkhZ1as8LvKlyLBf0P4j6gdZCa0krjLEMdHDDOcdQ5xyntFPCa9LeEzUHtMcPSFDs/NzXfJDcAnGyIHTV1y4WeebjUoOREGkUu0bBLDQeKhzigBA+HLExY6W2rH8ES9bs60jAhQAhKWKLb3jEQOPERGTbfKkffrQRNpzfCYdF1mZm89EsTMzEyRqSJEuRxXNMQJhBH4VuYJAqu+YG42hzE0ZowoZak6MW1epU3QuBItKPAdJyy9RNx/KXVpVoqq6SKeEK4gZ8sGCd/qrEvMfSxulO6SKcrBuyHPWPz2XFrTkUzH28/zoFvMpolZE5DEyZvrSH74RiLue+1j2qmeGysW8tJkT3ZmnPnlFFyWERlP9hAzEkOFkBIyAETACvYyAyVkvz451MwJG4KwjkO18GcFh2ar7YvMGC5JVjK0rg7hE3OvTPrM4hFoNZSLLUJbRHGQiLON4p4QQ/PyhDqmQBt6yzZzbYkhTJvNbe9dEWuThYh9b9qCgV5VoVBEYlv3lNrUyySgePlTvyICPZZFqNyfyNC+ZdbI3KCL1qG3r02N0EQgE8gexa+rgbLxA4TlDryACkiWZombSSwSrH+8gz3i9WlquxxluA2mAfvTcGOiXJ24gbRjOyxxvOzKVbjk6lfaemE4TePDYRyV9YpmlyMyQCBpEjcOaITvso4NQBRHirvEWchQjVp06cQML3iOvlOlOqnu9SrkqRtnSH3IjAAg4KxViVr2UL6RMYLwH5pV+7K2jV+54z/pnFAofAiuPGAl5cV4ZukbrTMjAMuaTfySAWJVyjQEC3NBS2oUgY/LODmnZ68Cc+skfKjrHaBi3nhk23sTIO/X10L2TETACRsAI9CACJmc9OClWyQgYgdVDADt9sSGb34pGQQrwWEBSIC0ylgcgMHiOZHSXZXM5/Hlu1a/6EBzIAMLxKnFB6rJHTN4T2dYcak2odNpiTseSRu05g0RxBpoK8n4ziQ2t9F4IGj1BBDKHFDGTfJbMdRMdSAayIGcQofLOff3YcHryRVvTxrUKc8/SRBn0QUClY5zDJnnhpWGMKuMPhKIpTxAyG/IYip0FiWz3i4j0qw8iFKqv/gERMHnPmiJpY7pv1T60bVrqeOOhgXSHSNoRLduEoE11ZkIviCOEdFhLHcGOsQSRYvzVgAJLmIdSl2TpObxX3KuyQtSoR1r6Hm3BCnwpj5+KzJBHGQmsq0TNrhzVKe8UhzzaqX+eIWBg3ZwfSB15z/oVKKUxIM+a8iGbeGnznjJwZK7lZUVglSinawhdo9EPd897zkTO+Obm9X3M8R3iWUMXDYDm+TvQgxqEhlV+keu7ETACRsAI9CYCJme9OS/WyggYgVVFQJYsxj2GLVZutWxRt8gOQ14WMEY13h2MZIxnyAumMWSmIys5IiEyDmUHeaMY01myg9wFgZBRHbxJASNCvuSo3xbnmmnpX0dL4jDkw/DG6JcXJtpDGtQ+vEl6jv8zr/LIJ9FHRS3iHaMdYjajZYQ8QzSROyhSSOj8Ky7YrGeWZLIPSuPCA6Z7H2MLD2EmfYxbOWrPLx404QQx0EDw8DW0XLF/UARNZ6C15uQ5mxsI7xlnwPUPKYy8rjUjg2nXmqF02/rRdIOWOt55TF407UWbE5FpT02lWe2HG5GuY+xFw4umsaEvJJU+8RYGXQIT3uNXP6pTCFrJqt+XJW3IqDBDTtSR/Ixibn2Kmql8scDoL7LAoCqLOpJJu4ZktTWW+cmZ1K8xt0Xo6U6wSnGNRwS4T9h12wpf8ORzYg6px7EM/QM64FxetTntTwvBkGEu+oWVIQF5+YlHvfONZE9cfT+eSpyMgBEwAkagBxEwOevBSbFKRsAIrB4CGObhFZH1q6cwdGUqY+OG4Rv2L+/6E3XjEDAKRVhkSIc3REQmjGTJygsARba0x6uJJwmLW1YzhEsPce8oKIaYnbJznxjRHQUCCWImsoNnCjICMYlDrGvwYN/TOymeK/3J452Ue1I0fsmYm53tLh9U51G2fs1IulyBQDaPj4T3T4wskzORTGWIWIqEydvD+OKCsOmZpO6CqMUz5IyxyWvW1hLH1ny/PGgKIa/3+Xkt0wyPWn8cQD2gpZNDQ9NpjfaibR8bSLdsGEk3H5lOd4mkTWrsCx3JkH4tkZphRXMcGxlVfXmdhB973NrqvxAaCCGKFFLGvARpDXZCUUaoS3D1Dm8pXk50B1/yYu5pR5uqfalLvZi3Kp93EtKpEzpwV0JekQH+eM9a09pnNzGjPXgKGKOxhF7qJ9qz709/IPF0TT9gB86hN5kqHxRuzMHU1KQCQGrcuWcVhfZ6zylUVJM8lPIllFLfjYARMAJGoFcRMDnr1ZmxXkbACKwKAhjCsqxl62bvhahGGMdYueRhSWMw94cRLTKmgB99hB4MD0UhXTqXLIxpjGxIgMiCjO8mMjgLTV20WwoQoT8DWhc40K8TnBWlj2AirTQXpKFDIBB5SYKYqV2QPnRDLmSpQgcDP56Vxz6xpeQtqkGaRGggZjPySCGThMwRLTVkr9kVF2xJg3pmaWY5DDvOSYNMckyAXIgszQxPmpbX4VEEB44QyMxVN/1RicYqD5n6G9B+KDxGjSEt65sblBdtTtdsLPHjAGrI1pCWU45NTKct62bTpZtm0+0iZ588NJlu0zU9J8+bzk/Dm4buYyJpEXYfkis9oVOQHqgHIwripLHhYSPxG7jxDnbklecl98BRdZj/aBe1a/X1XogZknIPuVKWXD0v0w+6sXeNM8oaM3OKVMl8Ixvii2rqV/Nf9qLxHvMgfNEbkVxteSL5Nk8lCWF+qtT9FiQ8hqd8RJDPnNcPIi9tfDcCRsAIGIHeQsDkrLfmw9oYASOwighgZJcrP2VlIB2Ej8cLVmxhiEq/lqNFgcogAZ2IqihSEksA5QHBsBZtIBoiBC2W/VW2dZA/SlWnX+H0w4qmvghaR4RkYUah1SUv5EI4VBbEIquklmqiKwx5LHElSApWfCxD5FkJgxxiGMsZRXDYz0UbAnlgwV+k5YxPvXhH2rx2VIRAy+vwlEG6IIC6IAmZkClfY9bGOMnPJI19c5mv5v5DDclmvBxQDZlpyPvTEKnoiFC1RcZas4Pyos1FGH/2oAVJ03LHUem2VsFBNo6Pph3S5baNY+nWw5Np7/HpdEJYzImocUEuIWgEDYlIkJIPSWMJKaQsgocwbvUdWOg55kFl3AMzgCEpLzDLb4Fp9RhYg1ORAd542pCLnJJi5JITOeTTTynUPWSoDT6ulu79It3z8gw2WPap/XoLHeEsvPGSxRJFzUt8E2DNuIRlzLvmUdwu5DE/TZH6Zp8ymEbVxHHG90XKuqAKT1mbxVpFNf8YASNgBIxADyJgctaDk2KVjIARWF0EMOHDjIc0seIQA1f/QUSy5VsZ43qPZX4iXpjFELEgPjLSOaQZo7tJVD2a65lQ/PJfxTMESuZ1tGvJIMcADwInQtaa1Plj2n+FMZ89HmFu567VD2qQIBaFsJXneIccoZOMdfoknDukZkYEKJYASi5lW9eNpScqdP7jz99U85pVJAzPoHSMvXIiZOHJibuWKoo4QNTyXjShI92zVuoXTOhf44cANgly0eqPaIz9Wr5JJMeGgmNARuabWmKpe1ORHNmPNjg8n0ZGh9MGLa88X2etPWrzVLrt6GS6XSTtriOTImnZ8zet+9jIcCZp2o/GclH2x4E9Y2VJIcQS1JiHIFR6Do9azIuU1D0SupdL7YKs8V7KlScQomohfLRUjVwnxh7F8UM+ekS5nsudQvRqiWA2pmZSc1g4SG6fvjEOKCchH/3Bk/lp61vg24uxSGYOMiMiFyT6lGTqxxygWAwrjy3XgPDn8jxP0ZV/jIARMAJGoEcRMDnr0YmxWkbACKwWAtmwxusQT2F8i+Toni/pJUOZcOcsqcNLRph6DHgM6a4hLw8YxnCElZfxnUPNE80Q4iDHUhjmIm8SRxnL2hYki3D37WnCyufIhxj6XIUsFAMbw5tnrmyKV+/UrYz9OJdNBv+sAoBM64ooiHrHGzMqUvOEC7amJ1+wLW3UnjPOZYNsQbzCe6b3CNuOJ03LGOMMM0gZ79wJalGNIXTSoIomgZzUgGxASJpa9glBY4liHDsgItWG6ImY9cuLxr6ypsLBtyFtqjMgsjY83JJew+lRW9amu49NpRsPTqTPHDiW7lF0x0kdMXBU7SanZxRYRCSNoCEDCjQiT1yE35d+HPTMOIMoVRgFuao+qzpmAjHjy71K5SnOqyNP44g5qOrGnFT1u896j3a6l35DXDV/Qb70zfRPaT/dqAKfaJyxVFYBPuCznba8pTGb2p8ncs4y11i+yTjIjzp44SCgEH39qbxl9EPvjIvuxEprzJAMJyNgBIyAETgXEDA5OxdmyToaASNw1hDAqI6zpdQjpi7GNkRNVCVseJacxfLEBYJWcK4X3iU8ZZArPecGMrZZciZvWBjsinpIvurhXctGuu7sMyJ8vkgOwfPjLDOFRl/QeVhtkbPYb1YtQwSATILC/M8eHjKrFGV6hkDiucpGupYzymM2IWJG9ENpmIOKSMlLd21MT3v09nTh1rXKlQ7SI5YxQrqqkP4sZ4wljXqHlPGcyRVeMz2rL0DB40f/7KHKNEC/MARhA1aNjuo2tN9KMiJSYRtZhNYfEAkRSdNyxgERq3mRNPaotUTSIoCKMNggkrZeJO1i6XnHjnXp+n3H0w17j6Z7jk/IE6gw/PIGTkwpsMiasTjAemhQATNE/jQg9SHiKz0CV5ErxkcKHSFbSvlXU8OLxsB7IVsgHSSLu64I1KJ7pGrs4aXTM+WlXdyrat326KG8hoj4vJZpDiisfkeBUGgn56y6xtOHp4y+MjHPeIa2gS/fJeSe74cViwt5nWMQtfgukaMrkty8+Q9vPOk3XL/dGlHNP0bACBgBI9BbCJic9dZ8WBsjYARWGQGMZEzZyiSu7tprBNnQJVoShnJfxNjHpM/GLp4PiAg2OwnjGTNbHE5Ea7rrKUNM8AKMdYKAaOlaeKMw8BVcpD0rw1yEDAOcPsv5ZiETtlESRKPqLLqsnmNpJORMbVkWeXJ6Oggae7JYKkdIdvaXfd5lu9LF29YpGAlkTARL+Sw1hHwFKVN+g31NLEPUM2SqofPc4l0kS0wnkzJYJ33rXT+BRomiCGmFoImdSVZF/lryJkJGJaNPERxZ2sfesVZLfWvJH2OfF5HsVxnL+njngOrRZitdun0gnb9xTXr8zvXpOhG0T+85kvYogMjM7EwmodrTNqK9aOxJ4x7jkHzGtsAFprrAhncSyx7ricAdENwImqI7o+qSLTBXOXhHHm1L+wp/8mM+qjpRV2WxT033mFd5B+d1TMLCSWE+Im8hkRv1Bw8q81SJkgT0BR9hJXw78rAFxZPQFtjIsxaML5haNY7acHjkYh74PiHnOadoqFcnI2AEjIAR6CkETM56ajqsjBEwAquJACZrNoxPGdhhxsqmDYMeAxf7FlO6KsBU7yMzcxOZvyoIAiCyJfIlK195Ku+XAU7buPQjAc1wp7GsUWQMsiNyluRFKt45ljjm6pKq+jyTctf5jXwITkksd6OEsPmTOi8sljPKGxXeFuVv0uHPn3/5+emynRvS+Ii8TJAxkSOI2bDCtHMR/3BO7VmGSICShojA0JDCuKufUYW+H9VSQkhgS0SkDUGQ7nHGG4qFWlkfApEssOdORDbIidrHPjZh0tDY6Lclj1mb5Y660IGIhE0dVN2elfdMz7zjXWtCWOUpGhhsp0ulN3vSnnTBpnTj/hPpE3cdSrcdOJEmJifjDLcpEdJhSNrQsMLw61Bt7WeLKJYaawd9lCCuECsIG3PRJVXoSAWVFW8Ur2AcbXhRgsCVwCA8U5/UhSDelE1eVUYfnJUHSWbZYp/muk9h9fGmQb6ozDfCckwIGXohL7xqatPpiOmrHBwg9V2iWfVLl/QXqftQ6aCSU19QqeS7ETACRsAI9BoCJme9NiPWxwgYgVVFAAMaCzcO9S0GrixkloRhKAft4CGSHjCaVS8IGka6rOsgS1EkY1yFQaxUJ/ZDhXwaYCxnQdm2zwEgWOPGuWfhRVHdIA/0JXIAAQiSyHv0JXOb/JJUDvXA6zMlYjYpsoIXKrx+yt+gvWVPedSO9Dm7t6R1OgSaw7IH5A3jkGcI2d1HJtLtB/akPYeOpwPHT+q8MdrSVV72OCwPz4a1a9KObZvTpdqrdslFO9O2jWtDF/GLrBPjU5KmWgaoZxGzIIzCiWWBYNNRlMFYQinvFe940VjuyL60Bt40kbaOypoEEplXWP4B9uBlDxqEdVDjGxWJXDs2krZtWKMlj+vSHYdOpE/Km3ar9qWxzJFlnDODM2loakCBRobTkJZQDnIItuSyJJNDrUNP6UQkRU1UzFNgrOcIeKI7hIxZAvfQXe+kmLlqrJGx3A911U8gon6CpOk7otmCyNmCDusGYMhWH5EXSWDEt6NKeGf79Ld0S8cu6KPIOKo+S16RoarxE5E/41ntSluEVInvgTkMAlgyfTcCRsAIGIGeRMDkrCenxUoZASOwGgiEES2zmzv7e6BP+UUZlfEbemE4Z5NbRjLL5HI99qaFB4lKygoCEvuxRAQwzlnapyJC5IeBzgvv6rCBIS4DfF7kA89JECoVFGJF1EMxhryKTW3Ij4V5MrwJH9/QHb2DmGmPGV6kWaIzIkNl60VkrrhoS3rGxdvTVkVDZL/XkAgapOx2eZ0+dffB9Kk796db9h5J+4+eTCcUUZA9YPUEsRkaHkib142nC3dtS1c8Znd6/jOfmC69cGcaV5TFeekdhKFqFDu41L+GJmKBF1BEBa8ZB1rrHb0CF/RviwA2NXaRtAZeMnm5+hQgoylixj6r/jkIqzxp4CMvG/vIBvo7absI4xYRxnF5yO5QVEdI3Kz0XtA1IxwgJP2MVXvRIJcjInUEHBnAYwgBVN941fpECCHC8Q3oHXLUVlk/d40nyFk1rkzP9MJgVc4V7ShnTGoXY6veIXVBtvTOPZZXwmZF0GiI55RvBfKGBuisV8nR3FJFdemByKBt7WXMZJ9/FFB+6NzViB5DLe5dnfiHBb61+KHEyQgYASNgBHoVAZOzXp0Z62UEjMCqIFDxpWzZ4vmpp6WvMo7zrqRuK7XLZAiDPAiaCEkY/9jxWpa22HsBWcPolhQ8KgTQ0DLBMOBFPsqyNaRDB0nxK4M8CFnkIFfveuZOyPwJec0gZnjfIItrRZyeeMHm9LmX7tBywDVaTqi9ZPKY7T8xk24SGfvgTXenj9+6Nx08PhWBS4ZEXjYoCEd/c6zqgSWMbXmjID6ttGf/kXTn3oPpI5++Nd2270B6yfOfkZ755MeK+AxmMiklRQekUNY5xsMrxEQEA7IRyzgVJASiAQnCUwXhamoMRHJs9Ys2zilP56W1xVAgaW3tS2vjcQpPmogjdeVFukdes/fdvCddf8c96eiJ6SAthJsf1BhHhrSnS+Odl94TEwoeMjkRHrRBkbEhedIGdCcYSVPRHtE2lomCpfQKTxceKIGbCVGeBXhT7AXsopMf+Dy6ewQZq96RyTwGQSvvkSeSKZI+IO9Zn5a80g2RF/lG+Ko4zjuOY9AznApiBn6QbeoiPMhZ9KL3WlIV6X4qo6kGeHC7hPFUkZ+MgBEwAkagxxAwOeuxCbE6RsAIrB4C2Lwyg3XpCW+DDFqsYH6jjB+KdMOXAWmIpWeynjGAMZ5zCP7KA6LKECYVhoEdy8sQgeGfLW71RsCHodRRIBAOZ2bZHiHvqRvGtO4QGoz+JuH21b5Nf9wrb0sIVzmEbFL7rTjPLHSRTqPav/WE8zen5zx2Z3rs9vXShWWDDUU7nE7vvO6O9D+fuC3tO3oirVszlC7etSXt3LIhbd20Lm1ZvyaNidSJ3cU1I2J2+MREuuegwtkfPpIOaOnjxORUuvpd12h4A2nXjk3pysftTtPso4JISD8w5CGWNUpnSEN40Bi/XvA4LkjHhggYpLQtnCARDQX/aCiiI94zgoE0IaptLXUUYWvhSdPSxz7tQ+Pw7uOT0+na2/enN19zc+A2pj1x42PDadOa0bRz43jaoQAiTNq+IyfT7SJxB09MpjkFEJmaEgWSHhyCjReNZY+ctzYkojYgMlciUQaO0rV4MBlW+KkYjNqTGCuXlK++AY0rinRXdpA8ysp8hTxVx0Oo6JxDaxSshMid+hbaImVgxNJWSBqHTUMkSRyvAIh03cKlpj4aipAZ/0AQCvBTXSrL2ukOOaNdfM8hyj9GwAgYASPQowiYnPXoxFgtI2AEzj4C2awN01sGLW9Yvyw3U17NsMVgx8sxoOIFeZQaivIAgSJhOIeBTH2RjQj6EcQkE7IIOy+Z2NAsiaQLlt3NT8sgl3eHPU7IRwyacBGmPwxtyIzK0QmyRlCNIHl6nmaPlZbxETqf9qQhLRG8cvc2EbMdaffmccmUDmq7/+R0+st3XZ9uu+do2qCDqJ/92ZelJ116frpwx+a0bp2WCGoJ5JiCbrD0kX1pDRE8FJ2dXwjP3FG1v/H2vemjn7olvfuD16W59lw6dHxS49W5XRqauECkBdYzMhIxSQ5QltLCSWOQfvEOBmChMbUhJ6qP5wyvoB5E0vCYiZSJlEBWWuxJ41nkjX1oAwqockhnht2tiI3D8pA96dG70hUX70yXbNsYESlHtHxySBeEb0LLNPeKoN2iMd+w53C6/eDxdPik8BKR5c+UFG8KLzxu/eqTJZx41YgkCcaMhHtGNl6YwPzOnCjxG3V4zxOmOrmNZjUvPWVsakdUSCBJwjSIoM6SI3one8/akFvNOWQKefwjAG240I/lpsrOsmP5be5fOcxwtFFhlIciKN1VnFpORsAIGAEj0KsImJz16sxYLyNgBFYVgdhHVtegrBPDIMf4hmgEAcPuxcOBd6x4KDDFK/tcZn3sQwvjWDVFdvrDQyRCgnGuNnhy2jPaH8aSPcmPfVV4VWpGf6iiMmxyCAyJX+oQ/GI6ljHm9uSv1d6qx5+/IT3/sp3pPHmPImS++oVEHpuai6WOL7rq8nTlZRemR+/cksbkaRpiP5b2ZXE4dOzHYk+WyAlLIFlWNzzSTOvSWNoq79puBQR5xpWPTV/8vKeIBOI52yrSIKIKa0BJ3eG3mSiKgMDBIB9gJuwgcB0Rl+DAjERl1I3jCCBmIiTtVt67FfuxtCeNs9LaLHPEq6Yz5lQ5rVs7lj5f5PKpj92dHq1z0DhQezDYofoRNuDD8r9BEcy1Y0PpIgUPgbDuOXwi3bjvaLpl/7F0p7yARybn0vwsJ47NBjGDnIEB5CyOD9B44l0DYZ5jQKrdTdKFlMdb3cN9xthUX89EagySp3pMfYPAL5BpfQjNWNrIuDRHkHS8hTRlP5rYLZ8f0MUSWOlFPwRW4VuMM/Wq/gN7FOk+oJcu9R2gU+RkBIyAETACPYuAyVnPTo0VMwJG4GwjgP0ahjcPxaANoxcDt0oqi2VqvFZ2bxi+Mr4JLoEVHd4hLGtVkOmsNWgiISIMEUZe7RHJ0r+mzhfDqzNHVMRpkTN5hSIqIQY37VWxGPvZgyLvkvoQtxGpEVGRAT8rEheBP6I/GfDqdePoYLpSwT+e+eht6bwNY1oWx7I56QYxlDds1+YN6cueqaWLWr64Y/P6CI2/oLIF1QsfXbCAPGaM/wWRpIaCb+AhhFyykI4AIOvkdduxdVO0Qb+gjnQTOMhdxkClJ2PoU9tM3CAV6gXiIsIVDAQPI56iuEROCIQhkoIXKQKaqF5fS/LQASLDEkeRFuSOS5/P/qyh6Gs0SIsiLOJlE8FhaahckepbpFGkblBjHxnupHXSfev6sXSBiNqTjk2mPfKo3SmydvvBE+meo5PpuOYDwgsCcfi25i4ChxSypvFFNEdhGvOqeiXRJj6fyMgYBg6StiAMmJ8S/ANyT0h8zrZrDmjZangoVS6PIWfeBSODzmmcZZkr8wjW9MFSUJY3xnln8iLGclLVpQ/mKFN5vokiq9IndPOPETACRsAI9CICJme9OCvWyQgYgVVBANMVgz8IUdfCjlwVKCMeKyMY85j/ZCgH6VBhhF2HhEGEgpuogQzwtuqxN0hWeSZdInDUwbBnfxHRBzG0CYjRvaRH7k9KQfj0KjEiZJjeqiu5kBCWuBUPG+XbFYmR4B8Qsws3KaCHMmkLYULXQe03Wzu+Ju0eHRIJEMkRgZkSMWyIuPRJdlMkBk8RfUg93dAZvTSmIAZ5+d88hEcBOxoE0xBBQrZ+4r4Q+6D0TJ8iFyz9jDPfwFBym+AjIpZv8iGp34g9qb5iL5p06sOTBglSvQ4eP3nNgsyIIEFKGx2RSe7qe1h7xRgHhKwjsPFUNjk/TG0HBtUebFUe2IoAC440LNm7RIa2rxuNw7iPnpxMdx+ekBftZLrr8Mm099hE2j8xnU5ouSnBRIhbieesIdLGfINd9q6BqzCRjowuEljQCSlwqfDXRJQ68Y2pOIKjzGj+R6UjE6UkzWNcbc41k+zYm0fdKA6qlckaUT81rvJtFpkhRD8xJ2oEKSx/SpnvRsAIGAEj0JsImJz15rxYKyNgBFYJAQzk8DkEGcMaxpzWVazqeJWpyx3Dt6pBOSHfcVtADk4tZdS7KmfDWsWqFwQEf4a8I0QblIMojGz2kYWHBLJSyWZJXl76p75EKCBKC2rDnjdISvQf8hcUIn9U55htTldp2d4WHTY9o0iA85I1tyBPkuhF7OVqTEnOidhX1SdPmViG/oMMiFzogmg1ReB45+rXM563iPCo9yHt7RrWfrQRnR02ovD1a7SMkAAaeNQUoSPLkcyG9lAhL5MT9aEy9lkFEE2IqGgbHjpwo57GE7gIC9pF4BD2rDF+ARD7sNSe5Y7IhGgxdoJkTItcTsnbNT2rfXcKSDKvvOyBrIgVJIauwFTtI+AKGMrrSBh7CJIeRLD6006dmzauqJObRHI3H51IB0XQjkzO6miB+TSrOhGMBX07c2qvMVTENIfjZ9yQtUyFGBuUjW+B7wpvW3giqdOdX3WtYCuhIAPSqJiPgb6BtDALXiKVfFRKkFnqQVLxMvINsD5Sb/HOU76oraS6oQFrIp2MgBEwAkbgnEDA5OycmCYraQSMwNlAQLa0yAm/0DMMWj13DW28XcqK7GzsylbXcjwIlfw+EA0SRrOM9jjzLJrj+YhsGegEsRiUCAV5UGTGFudcKckpFESNfUYQrnJ+FQY9JC3XUkWRE8ohcLHfLZTJKmlVW3rszrXpSedvTOuG+9NR7SubUiTAQ9Pzab/2Ux2b7qRJ7amanNP5XyIkDekRA2JQ+i/s/tBGL5K7AGmRx4ulgOMjIzr0eTCNaj/ahrWjadP60bRR3rdtm9em7VoiOah8oh72R7RDlg4qDL/2r4WnScQulgaKmPRBBOURyxiCkR4pZ/yMCbIRe8+kgk5f5uy2DmwWciJC15YHbE5eJoKfRCAPkbFpHTh9SEsS92nf2METUwqlP5kmRNRmRL5mVfeYznubmNYhzhokh0+jSwyYWWCuNW8EfAnvo/rrk7dqjcazdriZtmrZ5nnyrB2cmEk37D+uCJdTIkR5njNKIofCiaQpF8nO5DTGLQIGUYPj90FE1TcEEB0gltz5bgYYt8hZEETVZW8fSzZJeOTaLfns1GV4WSFZ/BcfRJ+IczPNBX8Ft6yXHpjOnGJu81v5lkqR70bACBgBI9CbCJic9ea8WCsjYARWAQHM22L4YuIWMlbu2OUyg5VUpuf+QRn7KgwqJwMfw58gF+H1UuXwmsiaDk8Hy94WVF/rHfF44GXjz4A8TEFKICIQL/LVthjTxeSGmOFhKsQsvDEy7iFrpCGFkMep9PE9x9LR6YPpqLwuk3MsD8RrIwKgekQ3HNUyxHEZ9QN9HHxcjRFzXvXm5GE6gRdKZGFGxBG+ybXQOSyiVYWdl5esX88sfRzVgdQc7ExAkDU60Hq7Qtefv31j2r1zU9q2SXvZFPUR4jakM8QG5WUb0LliDbVrQMjADd7AesHonhc8ZvSXycqsdMCzxXLFKZGwvfuPppvv3Jdu3XMw7VVY/BMTOs9NyzrxgBGMhDG2hF0sQxSpmRdmLXknwTI8aSqHjw1Xuo8N6VBtCJsUAWf1in8xHdESx/0nlCMySTCUlnBp6hDrjWsb6aQCr0RfqC25/XjBIF56Zu7ohys8j6pDxMvwQOpZIAbhpC+eGWyEzUd/4d4nffKSUsSJ8Es258HlfWWaS/6Erv+fvTd79uy67vvWneeh7+15RGMeCAIgQIAEB4miJFOkTJco2bEc2UlKrgzlPMT5C/KYh7ykKqk8JClXhkrZLrscxyXJFElRJEGCBEiMxNiNRs/zdOf53nw+a59z+4Kihhejfw97X5zfOWePa3/3KXJ9e629dvMV8lwsdTC0BDOhtPeUxz544LtATvrJPZFZWH8qAhWBikBFoFMRqOSsU1emylURqAh87AhID9DHSRAq2RcKbbIxClKhpkQLU76h5Wsh8VIxtm2rDKNS84bSbvuGAKk7qxyvb0gmIB0ozmkZgTBsYc3SarZtPdKaQrljpkWHeyr/EBHvjq87XxnPQkXtiRM3FqN3ZoW+KCdvjfs9o31xbBxiMTKC5YuDmbFupaULgtKf7/0xj0Xt52evxZvnrsRZoheu0rfzGsNNsh9i5X4qzzmb4ZwzD6J2slOE3L+fs82mBoZiCSvgpWtzcebSjfjpe2djYmSYcPzDsW9qJI5gXTu8dzIO7JmM6akJxhxIC1u6QuJGaPj6QnAlgsyZa5153mSs8/R37vKNOHOR+7VbMTu3GLc9pwy8+rGwjWEhPEAExl3DY7Gbu6TXQ7XfOGMExrnEyoOxJ8eYO2ef6c44x9lsV28tskK2H4h794zFowen4h5kXcXSuAgW7uOTgN3EAnd2bjUuErBDfLsgoYOsaW8/ckoEseAps0FKDLRiZEcJ9hpX7nFjDdYhcxKoTcr7XChSH++5V4yPwr2DMuB1cO3BDXXLICjmkSTPSdb4FjclmXwXYq/xMbGinth5bl26UiKb35TfYrGkISd1szc/wFJiaU0VgYpARaAi0KEIVHLWoQtTxaoIVAQ+fgRUYnU1k5e1JCx12qITp0Coz9xT/U2lepP9ULpCuk+qkIvMRiGnWhKpsi8pFXSyPHjYwCF204vCn3ueIENGasz9ZrShtCjU3E0SFhXzJGw7nkspyj6Kv0E4bi0WIiCxGiUgxkO7+uM4xOzA+BCECdfE0UH2iw3FAFascULQL0PeTly8FT89dTHeOnMlbs6vxNFjR+Lo4UOxm0OoYY2xtrwE6XEPF1YlLEhzuAhevn47rl65HqcvXcUaF/HIvYc58HkiFjgOQKvTOaIenjhzIU6c687IiKNDvTEECRtr5BgbHUnyJkl0L5tormIdW8A6Noc1zDPJdEdcghTNINMMfWohPIQ75aN7OYNtYCKJ5hgBPXZBuga5XyWAx09PnI93zl9H3u549P6jsYf9Y4NYo4a5BrXacQ7bGmULuDteYQ6nTp+NM5C4JMuA/snD03EcwrdstEYurYjTg0sxMbscH8ysxaJLj7S6kG5JyulTuSSTWxDUXohsnovG4ruWadVkvdIVkjquYb9y2Iukym/GenxzfX53rjr1+ckyCZzEuKebcWiUVjbHwz1WQmf/2bfyKNrOr4Z+/Gv78rst71mx/lQEKgIVgYpAhyJQyVmHLkwVqyJQEbi7COQByirKDQ9LaXguwTlKZlq62E9ksIrutLQVFTkbNcqxURgzMAYdSMxUxE3F3ZC7PI06GbUPZTwJGPe0kliRfv4qYmY/krFibSl996PQ7+7vik9ODea5X8OEjh+BjI2ME+xifCzvtwhw8crp8/HdV07Eu+euxwEI2W/9ypPx+CPHow9SdvPKxbh8/kIsYalax83Q/U0eTn1k7554+P5jcQnr08/fej/ePnsVojdKOPtj8cixAzEzMx8nz12O9yBJ5yE+p6/OxDtnZ9gbtsAc15NIjUPOJifGCOEPUYRYSmrn5hfi1u1Z9ozNZiATicuI43Gm2hHC3h/EsnV873jcgwVuZFhXQSx/tF2GML72wfn4zuun4vSNhTh4+J544J6DMcmZbMtzN2Pu9kysQ66WupejHyvaOPO/55798emnHovrc8/Fa2++E2+99VacefPDWITsfPaBQ+yjm+TMs9UYhiRK/iaHFmJ2dSbOLWDhbNbGdTIpZxIx1nFzk+MQkEmC1t9Y0dKyJjGjbhI5vgE5U9M8ram6uPoNSBK7OdncYwbS5ZR1LJYv3SfpAGK55qHV7MNLUkYnWvAk9iU6qF3zl53L2NpnJS1l5an+VgQqAhWBikCnIlDJWaeuTJWrIlARuCsItPuPtrVnpcAy1r6rZGvbcp+YPEudGQ0ZBR1LCETJ/V0to/Opu1GgDRihUp5WNlpJ0jYhPV2QC4mZe6WShNk3VU0q2ea1RC0tMaVo+7eQOVpADiVqJgxVsX+4l8AWfXmw9BguiONT4zExPRVDuDfeZj/aN197J7736vsEClmPp55+Kv7O1/92PPupT8apd1+LV158Ic58cCpuQ8AuX7ieliwjJg5hVdq3fzoeeeLR+I3PPROf/MRj8e//9Hvx+odXID7vxZ4D++PoPUdias9UfOKh+bhw+Va89O6Z+OHPT8fJC6vsgVvFCqZlbDVuEQVxdHQsg4fobjgLKVuEwLlfTLI5gFXtCIdnf5bok48f3RN7kd9DsvsHBrmzjw1iN7+8Fu/iRvnvXz1F4JOIX/uN34xPP/FArC/cjHdeezWuXboWM5xbdvnS9VjA4mcwkmFdIQ/tjWeefy5+7W99NZ799Kfjz77/Qnzr29+NF96/FIusx28/83AcnN6FfKsxD2ncxMJ5HGJ2Y3kWC19Zl3Ztdq6J34QukRIm3Rx1YzUQSPkuWEuJFOuJ7QtZWDLMYZK9HjYLbuJOuUGkSTaH4VqLayLfk0cO5Pr6RfC9SPJ7IaUbuD661n4nkjT/2m+EV1L5DnxyIP+ycpZlbv2pCFQEKgIVgQ5FoOe/I3WobFWsikBFoCLwsSKgcj1z5WzMXDoZKwu3cePrStKUQqD1zs4txQXOwLqKyxu+hCjgKN6YNAzBblL51oKmDpyHE6cGrqKscqxyXQicirW5GQAEC9YGxGEdrX+NIBbuJds+y0xlnnY7CRrN/kLSeqMSn+SM52nc+B6eGo49I1iKJsdifNdk7No9Hbv27o4ZRP9/X3gj/sU3fxIzC8vxpS/9SvzTf/rfxBc+/7l4782X4n/+7/8HrGa34unnPx/P/+oXY2VpLq5fu4qs7GVjH9YN5n8G98G+ruX4j/7g78fDjzweZ89fjtffeo8IjYPxueefxDo3DjwcUk04+gO4FuJhGW+xp839agbLSEsRAGhhApR0C1zAjVFiI2S9uDqODA/F1544Fk/fuz927xqPYa1+U1MxyTzGd+/O59dOXor/849/GNcW1uM//U/+IP7xH/5h3GDt/t2/+Jdx8sSZ+Oyvfime/dznYmVxLuZmbrJvbokxiGCJNe895F1evBmf+fwX48u/+bU8UPuV134eb2KF64cEHtk3TYCT0WLBUkasfqduLsYc+9JYkOKyyEqIu5Yz17dEgjSCY9kP2FpMC0GSJ91Zf9csvwPyes3XSjYAUJCz/AcCmb9gJNnP7nn0HwUg+MiTJM/vg31q63w3Y3hZ3rNrmP11Q0lucyz8LwcIAKNr59DeozGx794YGp1Aar4/+66pIlARqAhUBDoOgUrOOm5JqkAVgYrA3UIgydnVs3H74olYnp+FLGmwkBwVRdaAFOdvLsQ19ix1EVjDCIapQHNToZZoZMhz6FmXlo7MUxGmHLfAwtHKu2Uq9phqsJhsEIyC8PC6v6mA+ycByLFVyKV7f3nS0mSyT4aClA3EJ3ZDZkaHIWYTMYkVaA/EbA6XuH/15z+Lf/NnP83z1v72b38t/vAP/3E8+dST8dpL34///X/8n+LmtZvxO3/wD+Pv/eF/FY899VyMTI7Hm6+8FtfZ02VofXT9nMcNCM6ta+fjt3/vH8TjTzyVLomvvfEmgT92x3H2e40ShKOXCIcDRHFchZC9efICh12XOW5gKfJcMLHyUOw15r68vAzexWomMZsmZP+XHjsSB/dNxuj4RExAyiZ374WU4dY4tStefud0/OtvvRiXbq/Ef/1P/kl84xu/F2++9M345//s/4kb1+bjK3/n6/Ef/5f/bXzy2eczauSZM2fj4sWrYESETK1T/F05dxWythD3PvhAPP/F3+Dctv44depkvIE1TpPoYQjaXsitZ8t1w7/euXw7zzxzPbxaa5Uku03tWqVFq1k318Tk+rTWNMc3bH6SNNbP/Yc9WDq73MRnA+pqWTXSp/XMy3Ho07Po8juhLIkhH+oY8zq2C3dRyFkfiyQxlMT1MFcDvwxNH4vxfcchZ5N2nrIoU00VgYpARaAi0FkI8L/4NVUEKgIVgYqACKROjFKrYqxezdafzMuXLEcxljhRmEq4dSBRKu9aVgzYkMqyym/2YyMq0V92BMkrhK1xR3OvGZayPFMsSZn9FSLWWsxaZV/5fjG1ZSrq7WVo9xEIzyBh/vu1mnhQNFEXF+n3Z++zx+zld9Ji9oUvPB+/93u/G59+7jOxABF98XvfiRPvncaCEzG9n0Os9x+IsYld7E+bwlTTxwHMXbHMvHXP7IGpbK0txvuQsffe+EEcOzQd3/jd3437H3wsvvXCK3GDw5sHhtnjNjkBOdwVk5OTMYgr4qDnn2EtM/y+Z4g9c3xvfP6BffHp+/ZiYRsmn7PRKDdoxhBBPnwfpJ9h+5nGWrZ7Kkanp2MGQvudH76Ba+RG/M7v/E58+Te+DBFZjBe++714750TuAX2xd5DB7EW7omJXdMxyjy6+gY4jHuLOWDZ4t7LHDbXVuKDN16Nk2++GrsgsF/5ylfi61//enRDKn/09un48fsXmDpy40I5BukZZQ79EvJMECPxgFB+JLnuJGtlwA7KtYS6qq6Xrq6+pGWVx7Y3M32WtG1BqrIua+Z3wGeSZVpo/UcAQ+v7SRnpsnyrVOAbMNnOsbbT9gA82KimikBFoCJQEehoBIovTkeLWIWrCFQEKgIfHwKqr3mh5BqYgVtaUXxQfVYFTgsVCnS6KVobJZojzCjHwiKj82pcHdWHdVkzwPkmertuj1ritIqgY5f9ZlheJHVpiXF/W6OUt+Trr5q9gUDyYGfFhBj0McYg1pc+SQ7nkLk/a5Cw9m9dvB5//OKbceX2XNz/wL3x+7//+xCzz3L2WF+cOfl+vP7yK0RLXKaXXixlL8ehY8didGQ8fvBn345r164z70JC8AqE+EWMDDFboiu++sKfxtHjD8YXvvjFWCKy4//9z/63+PHP3mFv2m72bI0yx3XOOBtCRqxDyDQE4Xro4GR89qFD7OsajzGClaywz+vJ+w7ED98+F69+cBmLZLH6eCaaLoaDWOEGJ8ZjaGws+jn8+gff+km8c+oCe9+eiT/4g39EiP6xeOVHfxxvvfZWhtsfGLgVb73+Rjz8yo+xmo3Gj77/vTh35lxxF+Uwa3afxTDrNTG4FYszN+LDd1+JKxc/iGPHjsff//u/H6dPfxgv/OBH8eLbH8Zxzm177OBEEsWxYcgZMi0Cgu6nk4A9PjoQs8sbMYt7YRLzhiS1a5YujqLqodqMyxeV30quW/v/wH5brjkuk90QuS76de35EpJQbXL8Aqfocb4cFjM+KAPLlK+UG/Xy22Hd83uhUbGg2ppLYuflW+FvPNVUEagIVAQqAp2KQPt/DZ0qX5WrIlARqAh8bAjIw9K24UMqtvmQyi46MEpueW+V4BRMZVhCJQtLDRj1W0UbJT33gqEQGwJiC+KUhwBnF/zwXzd1PFPM4CBl3NK/FpckajnAL/9xqDyWmAd4RuyFJMwR7l4i6GHTEhz3bg1grZolcMbL75yJV949Ffv37Y/f+92/HZ957nNELtwVszO3IC4n49LZi6j/6wTrWIs///Z34xIugKNENvzpj1+OmzMzzE3XvbIHbx3ymfuYiAh57oN3iep4Oh5/9tfZb/a5+Pkbr8fPfn4inv/MJ2L08AE4KmH+mbtniAnRg1jZvvzJY/HZR49ysHM5nFo3PV1EJyYmYhniep59bVq5YLLU4Xww60HqJLy3ZxbihZ++GfsPH4uv/K2vxP3330/QktPx5s9ejquXbyLhFnVm4wd//kLMs6fOwCEv/ejHcZGw/8lSqOFM4FJEg+yFzHKm2pXzcfqdV2PvHiJOPvo4+9f+EW6WK8z9J/FHP3krjv7Wc+wv7OXAateQA7+Zx14Crtw/NRT7sEr+/PpCLBMpchETVx5ILUFjzlTNdfR70cImOVU+La2em5Z7yKjrqkuwerWwaUkFA5N1so9848fvim/DbWh5sLjfDW0kaHTMkJzF5gi8+1eYWVNsR141VQQqAhWBikBHI1DJWUcvTxWuIlAR+LgRSJUWRbeotqrSKMRNUIadskjA0qVRYpEqsS6LaL+2hci00RzTjVHlWYWact3SMmIj72lpoSj3l+niSL2/CTFTDuVThffqw43xySPT8fZVAl/AOrohMUl42G80wD6mE5duxlvs+TKi5NPPPBFf++pXk/Bo/TFYxsz1K7G0tIz8yh1x7uK1OHvx+8WFzv1UEJiStA3iFsjce7HujI2xX2xxMWZvXybS4kxM43r4ld/6avyz//V/iZMnzxOGfjiGcU9cJgrhoocxI/XnHj0Sjx0/EANY1YYhY8Nj44SP780Ih08/NRg32X/3L7/9s1jBOrUqxmJLhMJuw+azJ+/V109wTtlm/OZXvxzPPvtsirWyshxnT5+LZfa0SVQ96PvilRtx6Y+/neW6ASajaciJboJrTLSfyI1DzGNzdTFuXr0IPuwl3OyNz3/x1+LD02fi9IenILRn4+RTD8R905z7xtr5KewmHObTewjrT4j/AayTF7Cc9cws4epZ1sNBd/Kglmi7Vt0QTS1Zrp8ui0n0JVzML49TkGxB1CRZm5q6+GYk2brJJplj36B0zm/SPXr5DSFX7nl0YBnwL6QcwyL+aqoIVAQqAhWBzkZg5/9/dLakVbqKQEWgIvAfGAFVV9TcHEXlWe9HiIwGAABAAElEQVRE87Z4SJcwM/MdpZoM9xxt+4yluo3yLfmSpCWtgFrwKGnbVq4pMYBHH5cubFrNck+SijpKeavIO07Zn/ZXKNTK5wVB3A9RGIOIbSKrVqcMx87eK/dvXfLQ6FtzWM12xxeefzb27juQ1hsPwF5dW4bUEOwDOZQ6R+vC5TKIzriJm2NDzFrlX3Iif+glsuDYOIcuQ+g22Lu1sb4cwwTyeOihh+K+Bx+Kd0+eg+RdhWRtEiyEQ6TpfwQydIhzyvbunYox9o7tYm/b1P7929c99x6Lhx84FlMG4eBMrxkO1V4h2IUUTejnIJAvvvJ2HDl6bzzxxJOxZ89eFyddJxfn5t3kxQRcEyfiYd/LWKx01ZQ1SSstsKfS36DHDYximevDaRMcXE4Jq5EmP/3MM/HUp56KOc47e/fcNSxtrBPraOh7D79+cPc4rp0DnNU2yLzpQ+vpL0ktMfKui+OGofaVASDN8xtKt0RlZww/ulxTZNVS6/fIbTtJ+jGvlfbIYh/StXLOXZlh+9s2cu1KvR0dtYX1XhGoCFQEKgIdhUDRQjpKpCpMRaAiUBG4Owiouha7BA+Nki9BM23xrtLc/o8menFGHFTPNrmnSldCiZgBHYyskaRsu5zG9qUyrbbNZWj6PEQYZTsVdIolZ16pTDf17P+XpeyOAkRJt7thrDjKuIKS34M1rciqtWcLEgJxevA4oe8fJkLh6vYYymiUwG7cCgsZ3DGSA2TCiqMsOQHGwjw1PtIXwxAbCU6fEQbZq2aVkZGx+Bzh6ZfgQzduEt2Q68MLV9Na+Kn79hPNkSAjWMwmpgjUsYtz13CdHKLN4BhWNAKHPMgB17/yzKOyXNwbF2JuYSVJ1yZHDSzg0njj1mw8+9yzceTI0Va0xLnX+dLGP+dd5OWhXbCsLe5lafuxKk6O9RHRso/xcVlkb5vYSXWWlhchsPsTqzH2u2mx09o2t4obKg8DtO0zYAl70IaIjNnXwyY81t/0keF4T0ybu2squcuz0ejPlNYw7sqbRwxADnOvY0piyHz3t63FKtEsVyCnbbJum/xUPU+vuN22i9bcKfMbdji/g5oqAhWBikBFoLMR+MX/H+lsaat0FYGKQEXgPyACqrN5yTK0YOSfmWnrSAW3qMSUq2Sbs1PfbZ4lCbm/TIsFDVSK3SvU9qhCrkuaASCSiKnSM6ZEzbSzy2QMmfuX/2SIdUjW9AiRGSEZ2lu2GFtrne5vhwlq8TyE55mnH4eMjGC9WYUMqvSv4/Y4HNO79xP8YxApusOjtgrNcLwWESVSwzesO8RstCf27SFMP8RmdHwI18RJ9oQNJxEZYG/Y/Q88GIOcp7UIQzt19lK88e6ZJCGPHdsfe3ZPJiEb5Nwy94MZDdFjBtzT1T/IuWgH9sUTj95L0IuN+IBDrG/PLUNm1mMFt8ilpaUYQf777n+ACJC7GkA4JwyXwOm9e4hOyfEG5DLtQpyzRjuHprrlXHsm+mL/niH2uQ0whxHOTttPu7IOjtVP1JPjx4/Fpx5/MA7sJdojZQYugcfGOCTYSJi6jPZD0nL57nT/lz5J1HK9/XbywyjEOi2mYLuOldB5W2T4/Pz3Ae9MwXbruF2u4bKpW6nfVLGi+X01/eTIO7+eZu5k+UnXVBGoCFQEKgKdj0Ddc9b5a1QlrAhUBD4mBNRfcx+QxEbNOAmJvyq5hTQVmlb2+GxbOCRW/GnMsJ0EBp2avorS3JUBHnihX+utY33ZXFERxyKjayMkSasKzVMJd7QcXa18x7N1VPC970yOgxdgHGZf1NVFzgyD480THGQPzR3rgSP74tD990b39CG1/rTc9Kjsb6xi/RmPw8ceiMNH98fczXlIB/vJaDvLYdVLtJXImBIXCOvu0a545Ph4HD86xj6ztejH+rVrz0HC5I9kPV02d2MVO3z0WAz2zHO22KU4jXvjrtHBJDnjhMV3v1k/IfIlVUmIsiWi0XZsYjQOHdgTe4nkeH12OWaJCKkr4DKRINcgLkfvORb7sGpJAgsekREZH33qifjxd17EujTLHi2CdsDSOAIt3RHF1XnkeXWsz57xnnjykak4tH8kZubWYnzPAaJTfoIanh8GYU6S3BX3HDkU3/jql2Ji8UYszs3FIoePT0DG9o0NJinrJ1iJQThWjdTIGXAmx9qZ2vlJrtqk9Uz8k0mydlpNexlzk/mb7XdlYJEM9pEfQ/kiXHbz/M48tiG/RYtYUyhcfjQ7v4wsbwZtPqVWhHqvCFQEKgIVgQ5FoP3/3Q4Vr4pVEagIVAQ+XgS2ICTqu0mkVIZ9lhSZlRf/s4mmm2H0W4032RE1UfzLXzbJxirNGmSSVKXqXvpSl19Hy5YMOIYK+k7S1brDtXl5Z7z2PWWkncm2krBje8YgHsPs1VqNy/PLSajc5zSJtemBQ3uyvJuoFVptlFOLVDf70vYfuScefeITMYaL3iiE48BEbxyc7IkpwhIO4O7ofqpB5nBgojueeZQw+E/vjaldAxkc5PADn4q9B+7FpVO3xkQLl8reePjhh3AVHI3zBBfZRLYn7j8QE1ioBggS4tllfRAz98rl5Ms0EiODf0xPTcQXP/1wErfz127F1ZscCI5L3yD75+6//wFcJ0dckqzPb4wQVOSTz3w+jt13mND8A+wJYw8ec9gHCRsjouQg8uP1SHASCWwf8u+O5z69F9dELHYjRFw88ljsP/owuBTi6jxWlhewkHXHc48dz8Ooz99aANflmOKg6gMTwxnExOiLUq5FXC5XIbQmhvlrk0RtjXUpLcpvkjeZmRPTFGYpnWkdy4uc/Cb4mNp/LCgdlO8p+T8fRftd5FpkX6w0XfmpZlTRv1a6WqEiUBGoCFQE7iYC1XJ2N9GvY1cEKgIdh0AqwGqzJK1g28puyUmdufw0JVRVldYmlgEcIBy2lq+pZHuuWXDGVRckxwLd44zuuCHRwk3OvUMlPLpKtI0+0rtCkEHf3k22473ULFm+L+EiuYew7tOQk0UsTRdmVyB+Wuk2sDhxTtbqSgysLUVX/yCHMLvnrByOvIYr3djEnvj0F78cZ955Ly59cDJGhzbjIEEulpFvdpnAGgwzToTCB+6diE9+YhrXxKE4dWomesb3xCPPfDX2HX44ZWpl1K3z3nuOxLfefi1ee+907k97/jECfezivLBh3BkJ79/dS9TCHXNyjibd+fbs3hVf+9Vn4uU3T8VrJy/GwT3TuFFOcSD2WIztvYcw/p6bltVz3AHe77n/k/GZL/1azF2/HrcuXWQ/WTduiz2Ji3vFrD89NRgP3TcRTz25h0Oue+Lnb9+I8cNPxT2PfZbgHsMxh3UsLZlYFFcXr7HJ7HIMri7ETfYGvn/xFlEiV2ISy95uXEB1w+yG8a0RXXKRaJRa9RqRimC/8PuLFjRdGQs57uczkSznMmvew6LKMvsBudZpycOqmv8AQBaMradXEgcxxMVR2HRxlOeaGlqX2OraWvr1F0rXgpY1609FoCJQEagIdCIClZx14qpUmSoCFYG7goDKtW55Eqii9ZKjXkuSY5UXHiRHeUm+iLoIGbGeSnYGBLHmtvWDsmysBa2c5yVzUzH3gGZd3OxrZ7j+HJ/8nck6ypa5lvFu8le9fR5Ctk7e7rGBGMf17uYC7o0Qr43V1dxb5l6mno2VGFybj9VeXAI3sD71YMGBtA2y1+uxp78Qs3/3fPzJ//V/xMr8DPvJeuOeKQ6AhpQZjXAP+7P27h+OPt4vXVqI64vd8fBzX4n7Hv405M79X4U0qv9nREL6ff+Dc3Hywwvx0OHp2LtrLEawmA1w6HRPWs2KjUn5fXI6NM29cgPIc4hIjnumxuNnb30Y75+9Ep99/Dgy7UprVre7/Zw0DcTeaJoDA6Pxa1//uzF743K89K0/iS3cIHdP9MckFj7nYFTG/QdGYv+hUebdFSfevRVbowfjgae/GscffJY9bRJWXVOxLC5zXtr81ehevA2B5uw3zks7d2M+hhhvYsjojgRBIfiLh0KvrIOzLqrNerV3pvKXJuesE6Q4aUFkYfPb0Za2mYQdi+amAWWoaeVMPpd1L0u/bT9rKyR2xf3Ur4RycLGD7IJGfkM1VQQqAhWBikBnI1DJWWevT5WuIlAR+BgRUHX1wGSTz2loSAUZxVY915SVeFHZlZBkBvnUky+wgaxU4dcDobVW5B8WoV4CX+haZrTE9YYMtFYT+9tW7BtFv1WmW4tHW15UboUpSYX89vwKJGEzHj00FVfmVuLVszfijQu3cPnD2sS+rVWCXAysD0UfZGJ45XasQwo2jLAY/WktGsMK9qu//Z/FEKHhX/n+H2FB+yCu31iJiTHP3DLyYhfh+Am7z/6qnpGpePJX/0E884VvxDhWt2RJza+EY27mevzgu9+Ot99+Kw6xd+zLT94fQxAuXRr7sNzp9pgoi01DsnIQ+wBDMZLEfePXn41rHAFw6uJ1LHBn42n6GLx6OhaGJ6NrNwSJfW4t35AUHzryUHz9H/4XMb1/V7z8rX8b87NYwm4xB6yKuqsa1v/ClcVYhySP7n0gvvwb/3kcf+hZiN0QboySM8LpL1+N3rkL0b08w5pvxg2sZa9/eCnm5xfi2aO74pG9k0mo3C/Xg/xzq+uxYNTNRhAtZO06MZ1fmvJTYo3Tekq7fKed30K2lVuR7/EDErJ8JauQR8g8pjXHs6XjdYMnX1W2KWtRvhA/R9+tm1bgHCgz609FoCJQEagIdCgClZx16MJUsSoCFYG7g4CKrymJ2S+IoLJbSBpECkU3LWaoyLoldm/hV9atwlzoWkuo3FfWi6WmW9dG2lh3I8kCSraRHUgq5EWRztftn7YPM7bd4nxpyJuPmVC+5yAXy1h5DmGdOjg5FD/9cCNePXcrPnF4TyxzULTXIOSohwONeyEyfViH3Ie12Y08Xbtio3+M0PjT8elf+Z3Ys/9InHjrx3Hhg3dieZZgGFsruO31x9jU7jh25OE48sCzcfzh52Jq+mAG8UiiIJngvLOluZtx6/x78affezFOnTkfD+ybiONEixziDLQBiF8P+8Yy7HxLFJp7zpU+ZCq6Nno+24P3HokjtH397dPx0jvn4j72zQ0SjbJv6IO0bo5MHYSgjRcCDLh9tDl07JH44ld+P/YfPh6n3vpJXL/wQawszMSi69Q7HJPT+3GBfAr5n497HngGN8uxtB6ur87G1tKV6J7nWplrIiNuEsp/MX787hnI7WrsJQLl5ChnuwGcl8cVzHLw9RLWSXmUBtSWmHlv16xdpp13MUuXU/arpfXMQvLy+8DK6f85JzdLC2xpmXTMOowlUZP0b58/l+0TvlJ5x6/Yal38pR/Zjnr1sSJQEagIVATuPgKVnN39NagSVAQqAp2EQGO1UPG9Q5hkELzxX5I2Xy2XTKD0lrDmuA3i4piKcNuScvtIRR0lG16W1pCyx8yoe1pBGNB+/gapVfx3VlUUFfTbnAe2iHudFqTjHPT82MGZ+O57uPh9eAUCMpj7vCQ8yuccejBR9UAOuzx8eYCAG5vTRMyY5Pyxg/HJ534rjj7wZNy4fDrmbs9AXpYJgNGDFW4qdu87FtNcw+wdc2xl2mBP29ryfKzMXImrZz+If/fNb8ULP3mDgBx98cyDhwhXjzsjERr72NfVi8tlIbVlFknKeBSBfJZ0IGAvJHJ691R85XNPxMzsQrwEQdo/NRZfwaWwBwukJHoDEjW06xD9TuIqyT42rEgDWNOO3vdE7DlwPO5/9Jm4efV8LC1iQVvbSqvd6C5cLA89ENNEaCQz1hauQ95uxto8rowQ1licj01I5hZzOnf5RvzpS+/GqfPX4tE9o7GXYCvK5X4zraCufyHFOiTmDMqk/ga/ztH2a1pa/TB8T+sZWLDPTBNiT48ErtRLl1n63WR9tSx2EdjFYDIeieD+Qbwgc13bLym/zUaOdP1Evty39jeQrVapCFQEKgIVgbuHQCVndw/7OnJFoCLQYQhINtB9U832Rxe7korqncaHzMNiYYEsJ5OVOcOKZ60n7V6f7IMO0wLCz7pKt0SM/zKP+rok7ozUKMHIfizLvu/8tJYYCVGr3FsqOdOC47UG4TowORrP3XcgPrg6E6+cuREDhnxvD8imvha7/jxjbC26V7Ep9c/G5vJsbCyNx+bSbg5WnmSP2SH2aB1nMlr8IAhQMX8lDhuQmlVC1m9BLHSXXF+ei9vXLsbpE+/Ea6+9Hv/6my9GH2FEPvPgPfHk/YdiiGiRQ0RY7CP8fTfRGO1vO4kH75njs4kXzz6TVH7x2cfjOgdPn4YgfeeV9wnk0RuPzi3GUdwMJ+dnY2nmZgxOHcAqh7vhoJZBCCiRIwf6h7GQfSqtY4lkdq38kBoI6erc1VhbvBnrXgu3YhMCJyEzIIiBNs4x3rdfejv+5Mdvx/Rgbzx9ZCqmiGapxcxDt90/6Fl1s8sGA3GldswpJ/HLf3J9JeRNci0ztL4fRXNJrNo9dUJiSe5DlLBxbWm23fA74Q8rm+TujgttTjQxzSGomnvX7gzZjFxvFYGKQEWgItCJCFRy1omrUmWqCFQE7goCjVqbY0uevH5pkkwkO6OUOmnJ0KKBdWzLA8ckHzI5E8/lQGpJlCH1IGMQF/dASQRSEW9JCaUq6y0Jcwh16naonWVUYr9Wae9wS+z1ujK7GPNYz4Y5JHnvrpH4+lP3xp+8eTZe+uAy+WvxLCH2984sxN7d0zFFsI0hCRrErbd3OaJ3nkAZN2Jr8FKsDo5F78AIBGkAbqZFCsJjOEDj/yc5c8/cSizMzsatW9fiJhESPzx9Nl5+/d34KQE8Bnu74mtP3xeP38v5Z7hZDk9Mcig1ljMOnG5oGLMCmsRXrPLVnDYz6/VSf4P9YM898XDMzy3Ev/vBa/HP//z1+BTE6akHj8TRQ9dj9/Q5zlnbHeOTe9nTNkFo/FGsfMyLIwISIzFv5N5cX8FYtsQZc/NY3RbzvoVLYiGZazG7sMSh1/Nxe2Y+fvDq+/ETokX2b67G8/ftjUkw1VrWl3gVF9VV8L9B4BXD6JdIne1KtfP5q++SVNdUkpeujeAgWXfRJewC5J+Hgyc1Zg+c7/4W0Mg1gE3zhZQvrvxmFX58898EHMvvtKaKQEWgIlAR6GwEKjnr7PWp0lUEKgIfMwKp9qIgq8amxaEZP3VmCr2nigv5MsJiT5raUI8Nlb8jbZMZiUEqz+yJwuKiRWSVPtZRyDMYCO8fdUH7qIK/860lbQ5j/k5jyAr72C7eXuDQ5tU4ODUaE5zf9dDh3lTy/+yd8/Hj9y/Gu4SDf+DgdDx6dG88yDVOsJCBIaxBuDvmmV9YnCQg3UZTROnXOrTF/LawdiUpwEKk3OtYmJYIMnLuwrV4+9TFeOvUhTh75Uass39qnGiGv/nUcSxmh7G+TcXorklC4I/jegjJwx2wENniqleAdDY7k/Y558caMH4/YfcPHT4Qv/75T8Ugcv2Lb/4oXj15Id44dSn2TU/E48f3xMP3HIp7cz8aAU+wthl0pFv3QwOPSEjocJPw+JsQ2E3kN0rmhnv/mI8HgDuf2fmlOEFUyDecz4eX49bMXBwaH4zPP3wwjuDOOEjo/X4sf31Y7npYR1gVLo3rcQtCJ5FK66LfDbhZJjH865Lz9INahSB6htuWrpISdgOM0Ieh8JV9C5KWxKoh/FK1dcrcm5dkV+KVqPlt2oAr88S5ED0/03Z/2l8nVy2vCFQEKgIVgbuHQCVndw/7OnJFoCLQYQhIr7RCpKJdtOK/IGFxbURxVuv1BSVY4uK+K6MQ8pqEho541qqh2yJnYOGG5n6w3j4CSRg6PZurxKNY2wcZrcvbThL2FwQgoy1v69t2g+siByVfw+Xv4a2pHGN0pC8eO3YgjuyeiFdPX4kXT16KH71zJl4+cTEDawxCNEYhZ7tGhzmAuh9iRRh+SN1QYyVSXl3uFhsSMs++ttscxHyLPWDziwQJgewsEs0QQ1kcmR6NTz1+GLJ0IPZMs3eNa3zXVIzt2hVDHBItYUrigvwFNhr9kpS5YgMu3RDXLjAdwCXy4OGD8eWBvnjk+L744U/fjh9i1Tp9/nKcOHMphodO4MZY5rIbl84xCNowh0VPMq8R5tQPMZHsrOICuExgjwVdQJF/bmmFg6W5IGbOYwVyuUb57vGB+M1HDsSjByCWHlQNaZW8umevBxLey3qJy9XZpbjJYd+5dgiO2DnOL5/Zjsm6twxy6LrzU4iiZAxgMi97osh6fDtejqelkRulfCt8W2mB9c2PqU0+Z7+lntn5nZC/87iGtnq9VwQqAhWBikBnIVDJWWetR5WmIlARuIsIpHK9tSNaY7qVoeSq7zZyqffmC3dDqSdRUvmnkgRgU0Kmzk01y/I5dWeCPxDGfovyjNKosoyirVvbzpT97cz4S55tlWHovTsIROYmJENy5vlmRi7UwjOG1WcEwtWNi+DuXaNxdWY5CcVliNztpeW4ShTHKzduIW+JQNiv6x6XxMxuFV0ZlwkZn6HctRYy31GI0h4sdAd2HYTcDXKm2Egc2rcrDu6dimEsckMTE0R/5NDp8dG0frmHzP5MhYA0LyXrzi+4aLFMyGhgG0nvAC6YU7hjDuFWaPuDe3bF5eu34/rt2bh4fTauEHJ/bn6O4CHsHaONtqR+CFufJI+52KPyr0PQ3Je3biCOBvrBvq7YTRTIvXtHYnp0IKY5ZPoQJG96bJCOSlRGzzQTF9e5B2voGvdLuIjOLxHCnpTycv9lZ9S1KwxsmbxvtmDwbJ9a8pSRx7y7nobyt+PsW5dZ66bNtDxlZ81P1rF8Z79NWVoP7aqttLNhfa4IVAQqAhWBjkKgkrOOWo4qTEWgInC3EfiI/sqLFCKj3bFHjEj5jaYMcckS3lGGtYSZtEykFQPlfYtg6OrJd0iOCjgaMpkZjQ/lu9WWVd4ldn8dMVOWbfmoXywrJUfvPQOCXLw5n+Rr/1Sx8vTh3ifBOAbhOHxwd6xC3G5j7blyez5uQeZmCcG/wn40ZWPrVJGfvlTkaZZumxKTovNDeCB87mkbp7/d41i0dk8SJh/rFC5/A0RjHDT4B/vLBkfGCNCBG6BBQLbbFzwkIe7P4y3/y0mVAaxwZ47UcMbpMgrZHKSsB1fHBx7sZS77Yml+PmZwP7x49UZchaDdXpAssQcMC9g661XWopCyDLriuPTIbCBs0hzn0x2jzGcSa9v06BDWtkEsZWV9DbLRw5Uk251eYGQAji6I1BKuh6evzbGXD8JNn/abIP0C2Ta7JWXtnkFn/ovJsPq6vNrPJgE/XNv8Lvg1zxJJq3I72ka6y7pIlCR2olrkyHk3z9xsnGXlJ3PqT0WgIlARqAh0KAKVnHXowlSxKgIVgbuDQLcBPUhpZUilVzJVlOZUwHeYHwzeoE1G7VvLU+rNrSbMSyrL6baosm89fiUNRetO8qAi7Yjt4ddFES9KdsrBT9G9Vc5bwSSMTVRH7w0hUME/d3Mhzt+cIyDIqFp+khn3X/XgltfVRErcvTfivhSC/qiTQSiQz3manZIzmPucunHpk6AUQiGpQQgtghAu8zHPpZUu92Oxr6yfg6KTlKUbY0NsnLfz4HKAO3NkJF352tLEtmDd1i936sB/e7tK1En3rnmg9fD4RIztXs49aevs23L/2CbupRKoTYmwhIo+JZ4ejl3iaZbxHCrXAnlybr47WK6XdfgTA9eRfstxARKx9bS83eSg73O35iG7kDWaJUECF/HMtXFNfCe165PPmcNPU7d93UDm7aidyCGp78ZyRm9FLleFLuX0iOgi5T8GFOhKxhYVnIJIi6LPOU8eXFMtijVVBCoCFYGKQGcjUMlZZ69Pla4iUBH4uBFQaU5lFk22aLppQWKDT3lNBVdVl/ePKLulsoq5lrTWbU0l20OoUamzn3W0a4lDu/+ndFcIiVMthKB0rHp9p8TCUq7G3ZI625hsp9ve5dtzceraTDx6ZBo5cOuTJJC0Xhkt0WAf3bk3DuLRkKZiaWG0VpNX+09yQB2tXszJ2ekeKKnQomR/9m8wkSRwuPzZr/uzDOSRxA6ZMrAFIsjpNmQWzog+rONbcWGkcAdpIxvrFDiZp4yMS41cAPE1SIhz8b6xPgrxMsAHFxazvENSJWjZQKzYs5WkTcLmHBk5XUvLE6/Mk+ccI0VhfcBNUqb74yrBT/CHlOpl+wUiYn54Yy6us/fuL0vK2ab2OS1nZOaKNOtiHdcuiZX7yzKDH2QS301C/yuvckuiJdC2L2tmZZ7pyzL/cWA7PyfDu20psKzJKo3qb0WgIlARqAh0JAKVnHXkslShKgIVgbuBQCqvKuWNQquJgrN+1YvvKO8qwc0fdCTF3PKcKwhLcW8sVib5mARE45LKtwRGq9M6BzpLJgp5sBK9cW2nnc9kbqv4VoGkJKmjjvnF4rXdMi0qM0Rr/ODK7bgxs8S+sIG0Ghlyn8o5B132PES5B1LlXbIlgbRvk7Vyj1JDwLS6ZXmWKi8PtLGORwRk0BOJSJOXFZJoScwkQZ4rJoiQM1zxsjl9dkvq3H/nfGUOEkJuJWHtklAtE+LfYur2YPUzpYupmbgjJhnsZ4wkLaybJMUDmSWBugnawMWjr7SqUeZzegRmqW2pk2ynEOY83gBCpiVulT15XcjPTKhn/1TmPw/8fvvCjVggUIop1xeRTHb1l6WWpCXS4JV1XRuf6dugHxkohDHsrgs/2vbMPP8hQIg2eZDcGwAmIzsqUyalzFb5ltNWXK4NiC7+qQra1K23ikBFoCJQEehUBCo569SVqXJVBCoCHzsCqrlbubEMPTZfyFCTTuU99XIUZkgBSq56bg8BJ/ohOKlUqyxzuT+qq8uDpSRrVEKZpiT/tDb1cqjzpmQhA1I0qjydWcO+JVFtkgRmSvLEkzo2dZKEUGap7ZIY8axbnK6NZ6/NxkmsZ8f2TKDAG/iCkPpafvp0nWM3HE0kFI7XY9h8SKTEQaJlp10ctGwYfd0guwf6cy4pRxZaBXmdq/KSxCJTI7qzsmSL4Coba7gbEglRsiXhMTCI++C6jW6JS2QXGAK6swfXsu9qC6vXFkRqVXLGOD3rnLc2QLRHDpguo5Ot/JRpWXP+1CrYbbLHLsd2klwN4ZEMa0Ez8EbmJc6UKzPEq4TX14mQ/WqSSPq2HosLhlirwE/i5JlkFwg+8s6FW6UvmouHRLmI0pBzhXBd7YfUYpIvmdG4P9LWccRQ2VoXyEIGxYSojG5pVG66SuKMlFvI6B5B5yj8jq0V0i8p521RLkyxQGp7rKH0AaWmikBFoCLQ4QhUctbhC1TFqwhUBD5eBIqq65gNYULZLa53SYPkWhRJXiRpkgKf0a+1FqEMG7BBUsNL8qxiOYPotIpykoXkA0WjbsiYCr6h4z+S1LgZMJta4NjeyLdu2b1FNw0BkLhJzrTsnLx8O569b3+MQrS61iFfWO5U/ntaAmAb/5NU2Ce/KSN9J0lk0OKaiOwNyWzHTxl4UdpGJLPKC5kSActKAA2sUBC0NYiWZKsPsreFRa8XHDZ71rFMFYui4kCfUh4tkZKP1aWlJET9WIokiz0QleLmiGw2cHAxIsnDEqiUlZckxdwsZy7d4u7isSDePDuMjDIeATgkPVQhkafbqThxrWuJg+D6TFDHuMZRAqevz+X5ZmVNs0V+D7Y2JZ6MmSKVrAZj67YZWstyQBqUZ9cic1JWSHSzR9BGaxBd60ugU1bXx49rrYxjr1o4UyYn6Phi5Mi85rU9uLVrqghUBCoCFYFORKCSs05clSpTRaAicNcQUI81FSpWlOl0H1TRTWVexVelHjKBFSUJDMSnjysVfjuQ4DSKsoq0zxKW7Nc+JFatYs67ynyjpqeinhX5SbW7FQhlO5MDU1IIGiTCfu3TwnQnjFhk79V7F2/GB1dn4rHDHDDdV9zlSsAMlfxiQTLqoGSlu8f2SIAFS8OhvGV7OErSta6wnxQhCxkzJbJiylSKmsyUJd3vcOHUYrZCJMWl+YXcJ9Y/gtwDG4wL4VrTWtfOLSeBfEiDZW2ZsPgZzAM5PQogLXzsa8spU7VpxVi8SGwlZmAtDvluHSttgKQPXK0lKzmL+WLXXOIiuRWnghUumRzune8Qtn4wOH9jPt67dCtWmFdvEj9X705qZSpTKmNaKplUzPZuXmt5ZMK57lo+07JnIfMRH8mYtsAtfj3kW2x0D7U3iVi6nOagYJZjtBIwlv2QvFOUbTKj/lQEKgIVgYpAxyJQyVnHLk0VrCJQEbhrCKDIJhdBrU2FmrvkKRX7VPyLoivBMk93Mc8UkwDocmhScdatUSuUyrHv3WlRgytgBUnlulTMOqr41inj8aI2bUvyTKnIOzbv8hCTfWsR2uSugm/gDPT5VPAvE0nwRycuxbHd4zHImWQbWIC6IEIbfRIPLGi24/KupWrTPVzMUqpomPu0MlmmWyRREpGgNVIV2Rq5UpAUpsiZwvmokNlecrMWqysrsbwwB8FYIRur1Npg2TOGm2POkf5s5tS0Uq2w32vx1m0ydDVkTsPD7FPDyrZFaH5dGE3KYAOxaoZPYpZ5llvU9upzIWKiDL8p7eyAMTaNoqnro+QMuTN6IiStWM2KS+MsJPMEhPcMlrMcTiJoon6uc2IiYdpe3TKG+Tvmt5OgZXvXj+TYzkXO55R0bcw1hvhD/TNzdUWLnkTMV35IIpfEbGcezzks7bSoatlt62ej+lMRqAhUBCoCHYlAJWcduSxVqIpAReBuIlB07FThGyW8SJN6Po+6k/VmeHqIQlrGLO/CkoISrJVMzT219yYfApJWDspTDU+mZ1lJRcW+o2ynVm0RgthXkjGUazT1bNDtM8mIkDsJWksMLVvk/K2fnbwcn7n/YIwQ1XAY1rala6OEA4K0gQVKV8FN5pLEcZO+GWwL6xlcgzxIEc/pPtgo+sqZ00qt3ydyWuEdVLLQzrup075atM4ZZMtzhJ/HvXFoZDT3uxmUxPGdazam4hoBRFYWsbTNzBKWf5Cz07C+uUeNg7C3+iEwyJ3jbI9N2wSdjG3mWsRLmWRiEkXJjm0SfwmUJKeQltxzJmmEIG0asAVS6r7AdfEy4AtyvX3xFhbJWwQCWYXwtDNDFNb1zhyEpCnLxWM8ksM2M+SBJ/rbFj9rNGJlPYk87peSRCPS0FIX2j723G1tEqjEwCYG+RBUCTVzQII7/dOzRUKR3x0lQiDprKkiUBGoCFQEOhuBSs46e32qdBWBisBdQCAtYIy7rX6r6fqfyi4Kdy/EZoBzvPr6CWyhlYSKKvlGbyxkLStm3XRvhPeoJGu5KESLvnho+09egtscjctsc6Bm4g3Z67Kh+STPRMu2qX0rE2QFYmGeNSQOBv64NjMfP/ngYuwdG45jAwQiyb1TEg8saZCQntxP5X40ZKFvpKMfFH1JIB1ldMUNnnsMkGHvXDkwjwkIN2VW6U+ZKWxkTKrgK8VaFvvAzLD8y7PLscy+rZX5xdx/1o3F0T7EzS615m1wKPbK0qKDxODYaIb/z5dkVeVp+1d5Epb82c4uD+Q5L7BJq5HPrJMLZrO0fIKTWJif0Q+xnhn8Q2tfEjXaWj67vBYvfXg1Tl+9nSRHa5SpkB/vRQxzN5mLewLFzPnriuh4mbIi79xTdGRqv42UVfko6aHcUPrryo5MVrY/mxu5cmvLc92cnvN25coIrQWtvFmiGyTWuF76a2QugtTfikBFoCJQEehEBCo568RVqTJVBCoCdw8BdWMV5lR6fdQygZKdfyrhPdGPJWwLy5LnanVD0FSYVbBTPyZf65MEridd9gohyL1Bzsp+m75tUhpx0x+xTeZTTb4CE7DzxiKnFLyqwEOgkvjxvMmYKt6o8DIBZLNMObbi5fcvxX17JmP3+FBMEJJ+Y20z1nqRG81+ax0lH+K1uYkFjX4kl84hx3AcLvvwngTCh+3Ei2YoZTGVyZS780P4EgXSc88gX1jIBkeHY2lulmuea44pY1Ekv0S/lLwisy57zMGyseldMTA0XEiwdQ0IInHcTo7fvFBWUi5gkl/f8zDqRkQnkvMwX5z4S4fVhgBJxrTQra2vEsSEqI2s7xZ4acF6/9LNeOfsdQKBrCQe9t1GP8z9bGXKWZbEjPJCBKlnZZKkrTFnlQyxbXEjJ/sjLyXL/gqhbweEVud6ZL8EMTHZXtJrHWEno/TZ9uu3mwWlbjaqPxWBikBFoCLQsQhUctaxS1MFqwhUBO4mAui0STwkaeq5qLwpjqqzB0mjwUcPe7lSwadC6sXZhHoZVUNXtJIP88Iy5JlnEhYjPFI7SQ35kCgtOfKHtB45CmRB170cUc6h5QUiVCRAFi1l9GFPCteNS+Im5brCJQ+BTOQz91vzS/Ha6atxhL1nT2JBywOZISG67m0QWt/ojZ6DJllxrkZ+LCQJQsW4WwQTwXQEsVCeEumwRUJREaaZS77xLBJKJ2mAeEG+enDH6+0ngAbh8AeHR4prJQc7Z/ALx7c+8/Cu3Ib3HxgeisERiNngQL73SHZ1gTRCYZtoIwT5g/zliZGFN1+ck3vIsD7lxXyAtpAmKxUi6NyVJS1m4gIx21zjnTz36t1cWI43IGY35gntT89pgXJsL5Lr0JKs7QOzyS+ld2QpETazSflp2vtie2fgp5Xnw/F96IaYhI29i8qovEm0wIgQnPks3HZTvgU6aOqIQdma17ThtaaKQEWgIlAR6HwEKjnr/DWqElYEKgIfKwLt7h0GVbv2UgHmpuFD5XjN/U+b/SjSuAryDmXI8nWsPtnEeqnwaxCDlEE2VOi7cB/UXa30xs12kJskgmjlKuCSE1Vta7VJQmeGJUlB6MMaWlj8z59CxrhTT6Kmkm6eZ7D9/My12L9rNA5OjcWeXeNY/AgMgvVsY409aJCebgNhdHslzaA3iFgTtjGJGsQmrXUKwdgOmcr/TiEZL1mC88tnXnPuEDP2jUmQBoYHY218pMgLyVrFfdEDqqEPTgLZIWb9A1lveHwsBkbGMrpjL5EaPTS7C1kTmO1xU5Iyrh04bpt4LiHxQQySlYSM9XH98mICrlG6LjJ/9+KV/WWFnGlBk7Sa/+b5G/HG6euxwp63XEfn6NWkJKLbL7kqlJc1s49cY8rbFrmOLlSbZGRNUjatfRK0JGf5PbQjlO+lx+iaHH6mdVTotrSi5YLkKz3xzke1PcL2QztKvVcEKgIVgYpApyJQyVmnrkyVqyJQEbhLCKCwpwJ/Z3hU4WRmrXUkzRsowyr2WwbSkDahrLuny/DnBtLY2DCQQx8ueYSybyISoupvExGV527OIDPynqnwr0aLVrGnZo6XQ5Nv4A4U/aKr80xWRmqkZlqTrEcz9x51N5Y4o/9JJm4tLMYrH16Oo7vH4ovDAxkcJANsQM7WtUgh8wYydrO/bItxttxnpggSmIbk2Jdl9i09U+oiX9LFnD9ZmZyFuVoCt5h/nmmGBWp9bQjCBTmTjDDH3gH2TUHOGITaXMjRT/1+rWbDo5A0LWeDSe60vnl+nH1Lqspa5Fs2Jbu5K7Pjc2dMz0trg35kBQudE32UuTEv55YWM2Rk3TYgYWXf2Xpcn1uOnxBY5ercQs46LVk7iJkwZQJnpUmSBEa5QL7nooAG8zUbXpWpXTvbt3vUSonwIJ/fhdZVGvGWfyKfQVwod59gt3sBlUWw7Zj1Ugbn5YOt2mS+V00VgYpARaAi0NkIVHLW2etTpasIVAQ+bgTQltV3U5FtdNu0Qajwmrip7Mc6J08RYR6KRsAF9khByLQUSUikYBqeLNMaZkAIPRE1bkiesqcdlpNUnBlUpdqxtLBpG9u2vrQD0yZlaYTLvVw5Brl2yliZ7JvH4kJZss5cuR3ff+dcHN27Kx4+MoBXXLEW9UCONtz3xf6zTeTvco9bDxfWGKSmXyw4XZIXrIV01YbVTxHS4uOTg2dO8+xcoQY5H6xhEMC+gSEIB1hAglI2sSJqY0ZCxKom6F0QxD6sZANDQxC0QYjcEMSMiJhExtSqliHq26HoWxJTXEcZ27FSlJIPEywWM4mcc6Gc/3JNitumgUDYV6bFjLkZeGPDw7IJl28wEInwtbnFeBFi9t6Fa1hLOWtNotWkEjRFkNmvR69ik0kGZnKwNjXt7DNzc+0huk259sp2rc3ahFRugFMvxwa0sErCej3HzUT0yFwdMFGmLoJ9GLkz2Z/lzjM73ykLzwmQFWqqCFQEKgIVgU5FoJKzTl2ZKldFoCJwdxBA32716qJnq9ibVywt6rdaL8zU+hI6NWrdwPoiOdtEK07LRaN5SwBWUPx1y+vNfVtJr1LnLn3pnlgMLBKiVKdTmedJTV4hknmVd5X0tIg0JMB397KpjGuF64EAbaTSLjUkIaOWMQ9XPnnhVrzw7vk4gIvjJLKs9xAYREK2CvnphsxonZOQ0afkI0ml1hj68Gw2hYMHMJbuhQzoBASHm1L9hZRZltMf4/T0DcTQ6FiSxsRjcIiw/quJbQY3oU6vOA1gPYOU9RERU4tZN3I5FwmlZMw9d4kBeRmsxUKJi+ugPJCrLSMuQnKUL/dwFTDBg3fWUgKkdSz33knQkGOdQCCbhutnvvOE/X/73I347tvnCJ1Pf46bozJOmxrSldYwsWhScTFN9EsOMmRriRT1Ehar+5DNKM13ZNOKl6TXbyJpW8pb1oTvjjKkzXnZxnWS693hYrxkX/bPAGDSLYH0u5B411QRqAhUBCoCHY1AJWcdvTxVuIpAReDjRmAriVDqtCjMqrxaZFDmuaszS8QyIAQh4LsxnXn2VDcHOCcpSGWZeirHpIz+R5ueXvI4W2wLq5D6clo7GnKjdatYXSwoDVXEU3F3eLNkXk2f9qsMSdhokocsI1/KRbsN+pBitfvXlFkyIkGbXV6Jl94/H4fYe/brT9ybURDXsRZ1G7I9955BaJRrQ7JAL3JQCSP9ugfNA6ndB9XVi3slc+nCpbOVNeevqE5Q8fNW2njotJnd/VjGeochqr3Rj1XM/WarEB+AclpJ4Dw/roty9+n16spoqH0Tc7CPTSIodnXRxkGcqy6jIOh4dAQpYx9bEjP6VNbMtblAuI7WoS8DhXA3EIiWsrSaLRGpETwGwPO1yzfjhfcvxtlrt5P8QmGzL/sz5d0xNZGWN7NzbZPUNsRtW26/I+shqG1bspfrnOsNVnYg1qwgtsyCJWvfJeFPye+QK/8hAIMmU4RQOre+Ipn4JxkFm1wbv2eKujwQvVRxlJoqAhWBikBFoEMRqOSsQxemilURqAjcHQSSA6hAqyl7odGi4zeKsgq050bpxlgOnS76tgqxlbCKNOSoDeIhqVMh74aASITcA2WfGt+KrtxozHnboT1bh9fMwXKkEI1aX8hfypZdKWReuiH6X9mbpuULmbIPfpBBYnV9ZjF++M7ZuH//rrgfYtPPIElOdOdzT5mCsedMkpbKPeV24z46sdnA5bEb8iMJ0BpGA4Yo7nVl7GJfKo0Y3nI62MTyJrFaXVqKNS4tVeKglUzrWMqHZUxy1A1GzsgDvXu0iCGn58tloi+GtmMqQFk06BkYQ9ksy0vrk8yFelROsmIjn3WtpGyDsfLcN10aCUySYfMhasp99tpsvH72Wpy6fDuHTLrE8HbX47gmZKdDH/K5jbRp7s4DqlNOM+nXKSRdRA7dPrf7aJ6tJp7bsjM/SVXWgzAXF1mkoGnrPpv/mODHmuQzp8hnyDy52r2T6Yar8Cmzg9RUEagIVAQqAp2KQCVnnboyVa6KQEXgriCQViL14UbxLup3qs1FuSXDYA3W6+3pU+dOZRp1WL0alzryJRP20TCw9LhTf4YUqCi7d6q4o6FwN107oqlR/Zs3+yg5efC0g5G0FTkAVKIkLWnKy03ykPlt3VT2tRep0HfHEkTkjTOX48/eHGd/V2/cf3g3XAxihqzryF0IF3OgfwNVOI9uGhv3RJLSJUmQTECgMmJgkqYu9mRphcKilWUcN8A4Rmgkk6iMK7G6uBDL8wuxNDsXa4uLSY76OIpgaHIiw+tLoFY5eHoF4qbVTtLm1cc5ZwNDBAUZGMDlUWsakS+J6NjjAeBY/AA/SYvEUeKbFjKxSDnEPIFn9tTTSsalZU0ytgFBXFvxTDP3mmEVRP45wuZ/770L8TJ7zTyGoBdZcgW270IOKOKSP5JPxshcMstDjm+uOd6zbj6Z2dTjse3Lp3btdT/MaJEbunVurzLyOdf10iedak1rLWXbje2oSVnGs99aEtRGzra83isCFYGKQEWg8xCo5Kzz1qRKVBGoCNxFBCRDWqjU7VOX3n4oRKU1qSUJSkagRYvKtNtA+c3Dn9GU060QJbxX9zPKDEXv+WgqyekuicJsfrd98EyDbSLms/2pwmd0QGQgJ/O8mbQQSdIUMwkeY2lAUSFP2WiswS33V6m5U1/lX5K1srIe33vrXExxKPTEyEDs55DqDIJBiP20nNmX8nHP8bO5hNLDtxFXTCRpWKx0K9QKduvSlbh+4ZKCpdy6cWY0SwJbrBD4Y3l+LlYhO6vUVfYkt5DDfqIxDuLi6ETWqLeyhPsn/Sc2YNc3Mhj9nI/WBzkb4NyzQfas9Y+NJKHrI6pjH4FD+nWxpK9cNNrmsigj1zaRzGeIDuTGICBGiUxihkujAUHcp7a0tMyxA1fjB2+fj3M350BLDPkenJPg0SWC+7NtHXMe7ndztfwSEi9qmAzSYbN0BbUf/pKoW9ash+ucz6633wb54iPZ7cGNtGuDeUGAKUrMFCGtf2V6vPlOf34MfgAppBIWC6Y5Wh3zKxWTmioCFYGKQEWgoxGo5Kyjl6cKVxGoCNwtBNIYpR6+M+1QiLWASWjQgVP5Te8zrUj8VwJpcCdwwwYKel9a0FDT3SSkAk2d7fOyGuKUw6CUJylDUU8ln7ZUhRBBBrxzZfTEHUq25VlmHvUkcUnQ6GPDcOywCg+6tm0hCw4ecWN2HoJ2OnaNDcVvjo9SDxIJoVpF1iRlkgbZHe5/7j1zClAHhcF1U8pBEy5JyQZWsuvnzseb338xFmcXcy+bVdzn1qurpBVT5pwBfZFHmUTVsPXL8/Mpu3LnPjDGTPdEm91wXNq5ILTrSese0R8hdSMTozF96EAcfPihGJ2ayjHKMLRIhgYaZNhvOTsMyxmHS7svzaiMm1jM3GtmEA4tVaeu3Ipvvnk6LkHMJEA9uW6KVubrhHOJXRcuJXOdJJIm12Fnat9da4mq0zDZMvcd+gIGYpjWNzunrlP1GAAPwu7ut+9Ckv2WbOd5epsbyO73khN2HZynHZZUkPb7Id8xlJO/mioCFYGKQEWgsxGo5Kyz16dKVxGoCHzMCBQlWhqihoxim3RHBZ0cLrO1dKyzh6qb/U6GiVfl1eVMClUsI7ifQT6MFaEL3Rr99OCCZ7TGNZTrVMZVtGknmUqF3DvJvtJa44uKOmOZ1yaJWkvcck9RU5AWF54t0zqWBA0ZNrC+2I99dOvSx0O+kXf2+u3485+fjvHRoXjuwcNJMrshLev0kXu8uPf1Q76420HhKLQuZhx6pIz6Pbgf7rv3npi9eSve+slrce3mbCy5p0vZGEd5fHYmyqfLZD9Eq0/yBtnq87w3kF6hzSpkTauRvGMDIrsBds5TcuUcxHCYhxFcIvfun4rhXRMFLxs0ROQjd9vqTuo+s1VImPPDlVG5teJJDlex2J27eju++drpOHHhRqzRl4eFO233djndpEiJo0A4D/EQaB5I+b3wKO7bqTCklFuimhYvZDRlK+uCgwQq+3FAix2wIVRWuWMRVSa+CfxM5d2St3TTpJGzt7kdyEuzIz5Gv2TdS9271u3HWVNFoCJQEagIdDQClZx19PJU4SoCFYGPG4FUlIu6nFpxKt2pMSsJGrHKvheKcRcRBSUxRTFHaUYr3pLBUN6FuWkTJlR0d6wmWYs66sco5I0mXQga9dPKonbNxSjUphuJmQQgiUfJI3tbGkoyJS2ROFiPxknQsl4hhRI0625SJwlj0iSMYpzV9ubpK4zVHRNDA/Hk/QfJw5Jkr8jREtJe5OMwAP74v4x0nUO6jCDJfPiz3vju6Xj4uafTWnbiZ2/G3MwsZGszFrlu4UZ5fXElFsBsGZIlsUjy6DDgl1Y0e4KQrHEV108CEFI2AKEYgNgM4rY4TOTGMQKSDAz1xW5cMe97/JG476lPxsDoSMEeOcU+CZLPziStTJCYdGWUmGEdXFnGcraC1Uy3wfU4fflW/H8/fT9ePnEh5okemUE/XACSPGnbymUG+eW9fBkJrJnmW981dFwylMM3U85Jy19TVjJLzRyEeVpXLCWqGfaebpKoU+DSlvJSx9oS13UKulmTJNx22lj5lMAGipM9a7hLTCyoqSJQEagIVAQ6FYFKzjp1ZapcFYGKwF1BYFudVpFNDbeI4Z6xzMofLDup6EpruLDmqFD7n8p8RuvLcq02dqKroO25IERWVJFviVe3ddGiVcwz8IcucBASlXG1cuvZyve8p6aepfmuYFnWEDmftezgFGcHsoks76adRC4Jmlo77VYIjHHy4vX4/ttn2H82FEcP7Er5N9Zwn9tBIv0/i3WYpRYgOy8SFcaQJARSMDQxHvd84pG00Fw+cTrmb8/GMuH7JyBk02ODsYDlah6itsQer6VVZYGMUVYCpRQ8JGCSMq1qk4N9MUJY/SHk7yN/EHkGCQSye990HH7wvjjy0P0xMrkLyQrO0trcHwZIWo9yBNwDjcy4wTx1ZVxjbEP4a9HswqJ0msO5f/Te+XjtwyuxiBUtkwCKD0kilujxKpq5LmW5yoI09ZJcU9e7pLpN7VNaInO1snPktJPShebWppsk1pJr3VfXJY+bBEDxW+D7WQcrZpptlatHskcf+Y8Fdua8s9s7z+IAKgy0/WW3otV7RaAiUBGoCHQgApWcdeCiVJEqAhWBu4eABCmT9+ZxW61NnTd/GnKm2ou6bESGxmLRtrEPrSWqxpIPgjimIr1tvZA9NUp8ki+1alJav1oZfEcJT4JmYfPsY/usEp/WFe5JGKlTFHTFV5EvpCItOtme2UiwIIDOVSV/ZnE5fvDGh3S6Fd/47GNxcO9k9DOf9ZWkIzmcHWn1YccWd8go5YkVyn+SSoNX0N+uA/tjiIAdk1jSbrAPbfbyNULnL+OWyB4qZFnFkraMBW0J4qH7oMFJ1nS3ZBTJxiCHUA9CxAZwdxwZYG8Z8klAuiwbGYmR6cnYe/yemD56JAbGxpCm4CsOIii+XmKR57IxjgE/dGdcw2qmO6PkTKvZGUjpnzHv7791Ji7fmsdlkrGafWaiJ8n2j4mlDIk5YyiLBHd7mRLX4napDK4ZrTLlevpkHdI2cXMNXH/rcm+jcWbbJPsWMwfLsMA6Vo4HQda11e/NYDPuwct3O89BxaH5YhEm8RCZFKyVyso1VQQqAhWBikAnIlDJWSeuSpWpIlARuHsI6Lan7osERc1Vq/Upb0X/RTlW1XZ/2iasqxsyke55KMxZU2KAYm1eWi4gZ6lAawGxoQq5CnZL0BrF3SHatK1GNwp85tMmiRwvrULfEjw199YChzovlyoy5jiQDvpZJ1MJkwgxpgElch7kX59bjO+8cjImiH74xcePxz0HpjnXGIKWQDAX6rjXrkeG1WeURvbasZms4NSQGOpgoyKq4lgcevSRGCNIx7WJMxnJcX1hMQnaCHUSYho6vu57uueZJxnqxX1Rq5nzTPdH50re4PhIjO/bH9NHDsXYrikCZfQV4iLAztdbol/k3MRatgYB22qJWe4vK3vNVnBdvMDh0n/0sxPx43cvxNXbLTEDDQHRQsjNWbnnLNfCNfNJnC0z03VjDm15PpOX34rFXNZJ0bIB7WzTpvaZe3bnvKlnVfEoHYhTsbS6P8/vao3JJsljfXSfLRKVbw4o7yQ7tT/+zC7/WHCnuD5VBCoCFYGK9DLmrQAAQABJREFUQOchUMlZ561JlagiUBG4iwio7JpQdfOeCrmqM9YxCQrablF4Gx1bruI5Ugb8kHi1lpGMFlgic2TextZaWjFUtJuOHcTeciTvpnxv3Bqzr1aBpyz3fqm5I0f2AjEw+ZyyUebutkzJCh2AGXDTMqNlyL1XkghbOgUtaqm8U/82Z3z9qxd+jsvhWnzt2Yfj+KE9WJkIcW9Fzozubfrc2uqLPrK2IBPdW+CFGUmxsh/mt9lVrGgTWNGGd+2CUF2PmxfOx9zVG7G6sJD79TSj9WIS6+7dCo6gpr0EInswSn+SmN7BAUjZeIxhhZs4sA8XxgmIMP+3xXy0xDliJufHnxZK+5VEem6Z4fydb3um2abWM+Z2HmL2b158K35AMJS5pVXGYh5OwI4lZhKyfGfiefcmCJCknKePCtHsCUwhgEFyVXpp2vMC5uWLaio1NyNE2keOQzvXupnNnYrIkmSNnBLIpREnv0OHh6RJ1GxpY2Qt4yMbT1ZrUXVqNVUEKgIVgYpA5yNQyVnnr1GVsCJQEfg4EUDhNaXlQv1b5bdkFf1WjZcr9WMVd7Vh69OgcBcV7qZctVnrBgq6bnYG5tDNLD3nymahVN6zCxVt+vFKt0PuuhyqcJvngIVINUOq0JOXSr31mvZaz1Jc8uxXSiEVM8LizvyWWJR9aDlCjjcLQfvWqx9A0Nbja59+JB44sifJ0zpszDn0Ys7SCufYvb30SIAO54hNLectuSmp9NnLnrGxfbtjcGIsVo4uxvIskRxnZjiMej7PNdPNkK5yUuLkQdMDw5xnBilzD9sgroueZdaLu2OxRBZMs02DmFEQt8RcSybPujEahXHTkPk8az3zHLMVQv6fvnIj/u2L78SP3zmXwT8kP2LkmilEefYO0WmYWCFqYNmQL+e3ja3fAMK4Vu3M8+6aNPXy4Rd/6MuUFlDwtI3zz7G2O/I7Kt+S4q37LXJRfVs2v5jmC7GH/M9+ZZGKZoZflnPJ+ZhVU0WgIlARqAh0LAKVnHXs0lTBKgIVgY8bgVZBdlyVfZXbVlmWDLR7e1CXi2govO75KaRBjdkeKG2V51KLbC1MEDNLLURhNswEDn1Zw/xUzPONH7VvFPXMZ+BtcmCRdYrWnXfbNdJYknlJyFTqVfopz3faaF2TRGhxae9Ff6eMphIBOohrtxfiO6+eiuuzS/H3vvB4PP3gkexXq98WZMx6krotolX2YMnq4QTkLsK7uxfLHvIgax5aMXu6sSoOQbw4TNrQ9xtr+zkbjciJjSVLcqWQXYTU7+3pS4LWy8HSPUbDpM+WVJR5QjIlKY4kGOLZzHXdEPyQsCRnRmf0wlLGgQKxzFg/O3ku/uil9+LNDy/H3KIWs+K+mJN3TQBBN8ZCzBpyBV4mf3P/GO9ilZY2JpilYmslUrse7XvJ/et/bdcmR5DjlpFKrgR7KyNPFnLGSBRwkScuLl3io9kR3me4/ZSaV2tuV/K5popARaAiUBHoWAQqOevYpamCVQQqAh83AqncqmijzeYzP1osUvVulVzvWsNUy1uNuLEWyakkUl26+alsZ1+pGlO1UeC5u/cng3c0Eyw1StvMaqwq9p8qu+OYmvdG7U5LWingt1XuJU2MK3mQ+rV9pyxWtg/kLySxtEvrmbJaBjlxL9js4lK88cElgnL0E8BjnTD7h2KUaI5KknvXcGdMlzrqeqabJK2b8PrOX/lMOvmBBPUK0bVvmFeeDbeFJawE7fAMsjI/2yqbZ3IV/Eo/VCwPDX8pdIQs+pWA5llfRmRsCRn3Tfeb6fq4sRYXOXft9ZMX4ntvfhivfXA5FggMkmPZRWMNs+t0XXR8rkwNeFnW4GtWS8ysk3Iig/hb1jTJ/i3/SEJeClLuNj8DgfCiPCb7LjhQzT/bmPxueM4gIdwJ3rg9VrGJlu9ry0glXpbmPwTYD0lDXTOGrzVVBCoCFYGKQGciUMlZZ65LlaoiUBG4Wwg0yrCqskqt5KJRb4uizGsq0BCNLLF+tlE5TzUYHRhNWEW4vVSmVfh5707F2S5LXZ7ItqeSthV/FX7yJTqSg79wb9u3bdv3HKtI4kHKBtuQOLRh9B1FQtGFtcoojxKxtExBgJRDAmC+1rUZgnh8//WTcYWQ+LNEdHz83gNxcM+u6MeqtW4ofC1VBgeB4BgcxXs3lsQMGuLYyiYh8y5MDu6+sEbWnCuBRRoqVCaZ7oVUkZDZdLu9RCx7oDlo8JgBWZjHBm6LBgDRbdSw+ekqyPMSBPP81Vvxg7dOxws//zBOXbgZK8jZqzUuZSuEKjFhMP+6JdqSa8vzvX1Oce4QMzF0jRK/sra22J6LU/U9++GhTb5zteRcQpwWWfJs28q1iRygm+HzPXjbfyWwaSHPjfVQOFwr+ijSe9+Gt+Bt/411sRWh3isCFYGKQEWgcxGo5Kxz16ZKVhGoCNwFBAyLX0hAqrkouLrQFbJjTtF4vZOL0ix5QP1N447Ks6lYlHiRX3BTcdYVz5eiRjfkwPr0Y2qV+EIKyGg6U2FPRX7Hu/VNbRvLfZaIta6XKQp5GkxaImAdCYPy6taY0RqTiBWLWbo7NvVbt8dFLFCvnbwYN2YW44ufvDe++MR9cf/B6RjEorbV2xc6a0r03OvVg1tiz2YvljSDo0DSJLANEXLclEmylSTLefNsHd4ThWKmbGqS01gZrV9QAmfHghBtGQFTYqaVDMLlmWDpHsn6rULWbs8uxnvnLsd3Xz8VP3nnbNyeX8p17UMesXV+bZKMtrhrtSuJOtTL9bHcTN+58lli1uTbtsU261BVzCXHH0m8b1vCbEMdDwZPi2V2T/2mz7Qy2pg6G8zV/v3WipMq1kvsomLQrpn4aJ3VkpjfH8+F53LnHwYUtyW3dltTRaAiUBGoCHQmApWcdea6VKkqAhWBu4gAem2SgZZI+ZZ6tsQCjVeilTxCdz4JWhIQ6vCsYiwFU6lWQVcZTzdClWZd5lCqtZ4l+YFYZMd2rvK9c868q3Br5UqrF/1Yp1XGU7Fv2tlM8mOyvgq6Sn7eyVOx31Dp17rlc1Ovhz4ldEksvJO/6VwgPUkiaCO5U6k/fflmXLk1GyfOX4/f/swj8dwjR2NkmPGo2+U+tCQFWtAkaebxDB78JElxvs7HsIPpNZjT4UewTc6vTe2z8yApi+58SXCZg2RMC5nvRmjc4Ny0LcmZliaI2YeXbsQLWMu+/cr7cQ65aZp4d0u8xKxJSarEhbxCfkRAMlMws6ZlSpZULgt4Qq50ZyQ/yS/5BmvJowyUnefEzfa+g4X4+Gx/zlXi1hLpfG7m2u5r82Nw715ihQRye5NEy3lsdRlIBdz9xwOHVEgun01ill+UGflMpt9vTRWBikBFoCLQ0QhUctbRy1OFqwhUBD5+BFRqi7JbFF1UXEkAL2l5gEyUfVGSH0gBanhaSFS2pUip/0rCVOhVnMlHP1apNs+TxZLj5SCFFDjHVtn3uU3bbnNkOMY2kWoqJBngWdmSYLUNJUKSAd9TriKWZMBx0u1PJV7lP+cA4TCghxSkbWex7XkvVbtjBRL02gcXOLB5Nt46czmef/RY3Htwb4yNDkUfwTvEQ0KRhFTS1u07ckNOdHFsrVVJUHxP2QopTDLoeKQc0UElwjKrxJ5cZWEMCaD7zLwyUiP39fXVWMIN880PL8a3X8Na9u45XDEJpU9zLWOmJCvcffNZeRIjfwsLEq6yPjYguSesXd8kWuQlhhY2/eaji0zKuZWHUo7M22TOZ9s4D+pb1zXMqTZ95R40hWoE8ZZ9+tGAxTp76JS9i++qXA6GnBZD+pPwtXv/7MdkJ94aGfOl/lQEKgIVgYpARyJQyVlHLksVqiJQEbh7CKDIfkSJRXVO5VbyZUrVuBALlW1ySz6/jXlDulGsaUQbtFwigWItFUg3SA9w7ia8e9qw7JPUKu48tjq1inoq9uQlScjxirVG61fKiWxpRfPd1CritE25aGN/SU/IU+KsbyYVzLeehHOTKBPb+9DIS1Jkme3oR8V/BQvVmSu34tb8Ita0a/Glx47Gw0cPxsG90zE8OsyB0cwqt5UxX6xkaS107ljkWtfOMkFdLMtM05JH3ylvkhWICy+JLG0zRL75EDNlEsskbZI3ojPO37odMzdvx9zcfFy9eCOuXL8ZNzlUO61hyJ9z5i40W631yKHpR6JVyLJkKVHirt2LRB2fipS/cKddVqFT8WyPP8h1tIDy1grWts/1zFY7+sr5kqlwbRnPPXwj9Ep39E+RJNdj8zbXAQa5BUtCxo1a/pbLXpzGNklsSrh9ZIx8rz8VgYpARaAi0HEIVHLWcUtSBaoIVATuKgIqvirKybtUeFF7UaDLg79qypIOiBaXrmUShVSILVM7VlE2LzV76mE5CkLpZ3COrF8sJ9mr/dhEZd47yXszYuZrMUqlX0LQEjRlbOvxLBGRuORZaMrbvNuvfSVRyAbI1bRNBsRz9tTOkbckagJgHu0lZiZrJmHi2VD0V27Mxu3rt+IqYm0tzseuqckYHhuPfs4l6x8YyIO5DRCSrWQWELQkTDmguXZa5pEykZH9g5Fl4q61Mi+f890Dpjm/bGUplucWYhESNjc7E/Mzs7GyjFVpbZUDssucW1LpnjbJslbMbp61GOZskpXxJK6OiCi5Coxf8hr5xKjBPqXlOechPlmF3AYj8fK5rVdqZLWS5yN10grYtM265tuf/SifuGAJKxZbK2IllI05N62dPJZ13G7NGjsH8aIry3l1/LxTloSW95oqAhWBikBFoHMRqOSsc9emSlYRqAjcDQRUbiUBGbFPfbkhM6kMIxDvqT+r8KtEowWT05COsrdIhTg1YpX0JAAq95AUXNJyn5R9FPpBtaz9kZkW1zT6z374VeEnpatio9hLFvLdfK7sB4Ve0lCoFHnZqPSv1SZJnko/yYOkiWuYfbunLV0myd9k/kmbslPKka8lOY7UErt+SNfo8EBMEkJ/a2EhbixBljhYemySw6NHRmIQK1r/ICRtGJfHlqjleWjMRUwyMZIiOG7znkKLCfgasEQisqEbI/vz1iFeayvLsUIURg+xXpidg5wtQNZWWRMJUVcM4l6pbC2syk4nBSQHgkU7usFKTElquRer2R0SlmW0bUmZcNiXV2LgXfm8W5lnU8JGXpuyTMxbgZo6rmkSZol705aOHYQ+/D5YnWwjBh5nRln5L79N8clvk/7KyDzkOFaiLMsbecjnNeu3ctV7RaAiUBGoCHQmApWcdea6VKkqAhWBu4VA49qmxlv0aQiEG5d8V/VG8W34Vir2PSjXKvCpsKMXq0Cnwp/9qECbpdJPgV55zgvi10aMaHTuoqD/glJvXW0828o3z9neuwp9U99hWwXf5zvUgJdUzNtWpcz+0sUuSQtlyo+g5nWTJ83pZg9a3nXJvNOcliU55/7uvpgkKsjUCKH1teZwzeFiuDAzx4HT/TEwxKHToyMxNDaCJW0w8wy3X4KoEHY/IyN+RNokEGmpcx+ZY0vIlldjVVLGnjL3la0sLWfgDwODmPoHB5AXjMF8YZ4zzJyPRKVNrkXzmuvQ5Oc+wHa9nXObLw5t24Y4ZXP63NlvC0veHZM27dU2F3/Lm+G3n9u21ttZnutoG/eP5QqU/5s28Ikfk1L6PeWa8C7ZLzzT3NKZYfhzWsjO51ryJWw5kpVqqghUBCoCFYFORaCSs05dmSpXRaAicHcQQLHVcrVNiNR4VZRVctWivZpUrGDs00L9tTyVZyxkJWIj7yrzqbTTH9Yf33twr4uBXoJkEF1wE4sPeRIBh7GLVonnMZ+3g32omNNXKvveVby5doiT4+eY9qXiTt9tkuQZsdF2BrjIg6QpVsnPeVGeMsgWHQtTizYcCZqSaTHKgCj5Zk5Wy71kPX0iANmiX/N1n/PssYWV1SRqzrl/AGdDQu/39mPZ0qLGPc9E68by5hAkg3usb+L+SVsjMGolKy6Mq7GxSsj8xABMGUdS3OvePcckX5kdX5JWzpKTyDA/5ttLXcta61g3E84w/+JBsg/nSo18J2N7XTKDehkUxBfHsi+fm/a+J2beScqT9Ru5LE8a2dTLSu0P8m2ntj/m4J6zXO+mUAKm9UuAyzxc+6Ztk+9wYmQUzgwW4niKRjMfxaamikBFoCJQEehsBP5/9t48yrbsru/b994aX1W9V2/seVQPUkugAQlNrVlBGJYxJkwGs+KshGC8Ev+RP8xahAwLr0XiFWNj7BCHJKwQDIYABmwwgxCap9YAEq2Wultq9fxe9xtrHu6U7+e3z7l16g3d73VX1T1V9d3v3Xv22WcPv/05V3r72789WJzV+/3YOhMwgR0mEFPFNJrNfhBFQqyUI1wZw/iWUTCD+bgyZQxpooGxPiGaNMiOjT94rg+5CXlQLREhkdI6IM+QdhPss8FDDKLzgJ+8gyG00rN8kD2KR/2qL0woBvIRj9orXxrwZ8GhNOVHpDFoJzDgx+sW2+hjJ/bpQ38Z7rNhRsQlYKKMFIHkZhZxxeAekUa7mN2S9ytEmaqPVBkcwoR2aJOMCu1iu/uk6Y9NTUcM0RrPcr+pCwspk9dGZXtJJNvImESc4qQWxRQrguzCCUgYlxAcx5sZriO9FzgV7DAf+cgzhA79ghPPo06VL69wj7gajWuZh7r1rLSjzAODQSjqrN6XTylHKOtA4HHEATZIweerfh/NSe2sAsPIrTej31gw4xchmxCY/GcBagpeKl5ttq/nlC3bI1KK66jSXyZgAiZgArUkYHFWy9dio0zABIZHgMH4YEgbsXLcnQe7McrVgFhqgIE/w2ztqNdjK3qEjQbvTEHTWcgaLLOZRvhMVA+lNcBWZTqjOaWpcZXrp/Xzy/ISSQCVHpSoM/c+alf+LHJUN3l0XwqtnIukIg8J1XoUH4g0HmlQX/aMdIRZiD51IbxXeqhoiDdiDP0pwTPkS9ihNDwwCLAxecziAGdUD4INg5UvT1dUTHlySlzQFNn2wgjucwuU0qdID6Gjslnw5DpyTnKTV9/xV7bpSo6eNswgNi6xOCG72NQkh1w+pj0qgT4R4ml0mnZUnSpi45ay7bJV3ifh4r5QHluCH2WLvkY9ipesKIuRkQ9FWISI8a71jiJOR/j9yMM4elCbqagPmEc9iP/wxkYbiFcOn6ZUrjd+YdSjJPLzpyhcNkdWBxMwARMwgV1AwOJsF7wkm2gCJrBzBNhGvadzosrBNQKGcTCeiBjgaqCMk6YVwgsPFCJNo2hG88WHS3UsHB4LNhjhD6JBdSHQmjOTIWrWtU6KaXx5OlouGYNy1RMiQldG3uUUyFIQlG1kDx6ZlE0D/pjuiDDLSdk7RlyD+/CUFfE4Yyvs0eYTCIU8us/xEA1qIYSXauIZgf6qDBucIIJG8JwpT3SddEXy4clFmooMhEuUR+yV+fOVmskTzGmniJNGvSE46H9+FLYEH72L8CDpQbSvvCN4zmTX+GhTm4hQgEYVUDqys9zAg90b4yUVz8u2SltDeBfCKWxTFWVV5RWByrPBOyrsxf4yT7RN87K/gTdLJsV6wbKfGDEibiMIM/Gc1Pq5CbbRJxReMV0bDf1gxL6naZ8lD3JQTZ6umFss7Y92+HIwARMwARPYVQQsznbV67KxJmAC209Ag1z9jWFtDG7zoJrzpkhjgB2bWmgwTWCgzAP2+ECEdTV4xoNWip0YSCuDjhBTJn1C5CkzzWhQPjorgSah0Flpa4ojO/RRJ481YFc5RZWW20JMEUpBEB40xIHSqT7ykqEUaKToeRTTlQgmIFDIi/ePqqMPFGODDlUUZ7JFVF8E6kOUUZEEGGVHtSPiAW36MabdGln/xS6PcTi3isT0O5UJ0VYIN6WqnEpiKLboQz9KTqqclrKtOWfcqHh4iWI3SS2Qi1pkByIzhCYmKU/2lPFummlqrJUmZddidz2bXLQVOy1G8yqgmmJzEJ5xJ3vLHRyxM+cgVxTIVzjoFnEcIoiGaR8jB6Eapwuqi3wKub96X+KMOAyPmLhxNhwes9YBfbTWTB3Tb4mt8/HaBTBKR9vxzmlTKezgSP343oKt0nWrQAalFPYVKVwcTMAETMAEak7A4qzmL8jmmYAJ7CyBPvMRGVDTLF/66IiyYrwbQ2CeKLkhh0fekCLGwOTVmJhdCyVXdIOYyYIGoRWBkXMxoA5BpFsG+qPT4/KYjGrLeA3I14tyzFWLMbYqloiiXDSB+0iBuGZSaoAvm9QO4lClo0xcJCSiXJE/DoBWGRXJQXWGUCvvuVIf+fkgrvDUYDP3hcCADSJnFBE0MZZGJIL4sKNliA/lQyBJsRTiTIJGaaVAobqwXh2gRw0EodKyiNEj4vkr0rjDE8iGKmyrAcvMjn5LmOhZCyEigcT9uNqdHhuTcGyl5VVKRINRZ+SFC0HlSi+a1GUk5ZzZrngeqXJscYUHlyIt94M+K7++omwhXBHw0cfyW4WacpXiaWtqMxi99NgMpSHvXkeMu30dSB71y17ZFbzU016Pg8qLEPNkM8vgww+AEEIuR/N30d/Bi95ILftXze24CZiACZhAvQhYnNXrfdgaEzCBIRMYiAiG88VoNjZXKD0RGjSzOwZCgIH/SAzXERgaqDMI1x/EUmz0ofLZO8MAXh2jPg2+qSK8YbmYEhWRoAmRgUeOikjlFGuVGdGUNqZDktrWroUsaGtq6iUBwcGUSARUC4EkOxAynA3GWjiEZQgsMqtq6iu34Y+yCC8exDombFNcVUdWhIzS6QOHN8fujwgMpY9o6uCMtrBnx0Q8Z/2Y3pg3ywihof5QVQgN2Y4N4VFTzaTrVlf1if7qhp0ao9W4RX6IYXCWMCOfXI+NmG4q8+iDGFMOAZnFmm7FQs7IdGBKW/iPjav+1aLT1Jztjiu9Ux+wDXFcBlCwsyT1ZFuz/eVzruHp0xXtHB402d8od43kOVWPqV6Y6L4lD+OohPeIzl+jvyMSZ7yfThvFL1uVvrS6rKMCtCNlbAyifoRNvEe8kaqf96B6uSdgZ6+DWC36zm8KNvEHDspAESDDVp+NXkYV/jIBEzABE6gpAYuzmr4Ym2UCJjAcAnlNmIayEmOlUIsRt8xhgMuAtytB0JAgaoxriC79xLbzSAvtCsK3AnkYvutZFwGgATJVMsiOkAfbEdUzkhlfUwdCMAbSKoC3i3F6R3WNSCA1NNAfl1eIaYfllD4G342u1iGFdFMFiCmVw6uGnFN1qk8iTWUQNAzqER+cEYaAGyXOM13j0GfVJPdfHuarbuwKgSBhQWA9WUvnmx2emUjHD+mAaXnNGmyRr+exM6LUUUwfDJUSFug+ekTNmBv2lMKINDrcGJEd3JBVmcjXUxp2c97ZqPoFbHWB3oC4yKe8imIofcAHdvjw4fSq1x5Jt7Zm0xOPPZTmzp/WMzGMNXLyVtFf/UF0tmRvbGoie6knW0pE1NSnwZRFPdCd7tWsBBxdYgpiC8+h6uDdIXZhSd/yGkW1orh+AhKRpIt5h/eaf0Pk57WNqp6exNq63iNGhMyKTuW+YTvp5Mc+mMRVabxb2iYBZvk9q5xu2OExcmBP1EdGBxMwARMwgToTsDir89uxbSZgAsMhoDGtxr/xiUFt1lkxIC5GwDH4ZZBdTieMYTTKgUEwiohAHYozVZKBfvhSQqAxnM4DZ+rLA2kG72U5RtoxrI5rX16jbret5/KiSaCxNqorLwrthwdJ91FS90wTpCVEAiKppamXiKMswJRLeSiXr4iJLGpIQrDRD9qhFoTX2NikvEnZQ3Zo5oY0e+R23U+k7oWH09GjS3EsAJ4jPE4tTd3j7LJYe6a2S6ErPRehjyePhtQAbYTRXIsQHrHyRvnCAyn7RhCVeNGigL4RNaqGL8RIXMs8EjLjs0fTddM3p87U3en6O96Y2ivPSRStpHZHh1ivXkiLC2fT4uL5tK4Drgms/8JWxHB4HJWGII2NTZQeYjZLoMivboSgy33lHcsCzKBT6kR0UbdIKJKjXq1FpAr6gcDL3e6lVR0toIaUjictKlF+RSOuuor3HP+hgIK8I7yMqoA6+Ci1CLSd/0NBCDw94PcXvxPlQMg5mIAJmIAJ1JuAxVm934+tMwET2GECebpcZSCbx8sxVtbQNzwhDNZDUGmQnLfQrw6P5d2QxyLG13wjKhhIM1DWwBkBMhgjS3T1NdhmMJ4H93SWITQFGNqrHnSIHvKnrbVJWYyQVggrjfjJh5hg04wylMPwcofA6EDYobzUqXgTO1VEvp80MTmTxicm0+TEQV2n0vj4TBodmw5x1mQHSwm+yckjync9raXRY2NpZvQZHQEwF+KvhfdMIo21VTElUPbEdEjWc6nv2FOah9DgHh4R46YMSotecC0/4hz9RugicMKTRK5CaAogkiSmC6ZxHXJ9PE2lO9Ji40Sanb1eu2DeKpwdeczWNC10Pq2uzqXVtQtpael8Wl46p89cWllekFhbC5FJu2Gj7CTEzo7ooeI+NmYhHraKod4hPdStbCO5E7t5hrHKh6X6GeiqabBigT8xggp0OxyunW2nXeqhVbyzUaHieOzwwDGFk3ecf6M5H/njZSpf9pQpQlyfbAsN059I9pcJmIAJmEDNCVic1fwF2TwTMIGdJoCnBOHAmFZDfsa+KKoQAHlgzKA3aVoj64YYBCNCYuAeN3yhRnSNgTT3GoQziNYIucV5aGyCQSqPVG9MX6QmPUf4xFQ6xUNyYIN2cSwH7p1mO4SYRv8hRhAttI3nalAnFcsmBu1s+9GVmAlvU7imaFvbzY9PhBgbHZlIE1OH0qFD16Xp6aNpZua41mwdTeMSYq2RqUIM9bQmqp3WZMfyqjx4qu+G6+6WEJtIvXMPq6l2rKnicG3WkLEGLcSNPEKxi6Ku0VnZqU5CJwL9434jcF8EReO5Xka8B668FIXs7dNTKZ4QSsG5l9qtfhqV7ROjt6WRxeOps6A8EkqNBhuXSGxqvdf0gUaajemNOmNu9XSan382nT/3bJq7cCotLZzWGdmLEm3zqdPRAeFqLksghJfeWxheWK/fAO+xpfWA2Mk7pDtMQUQ8dpWZ7ubyEm/xjlqpo+JMb6Rupqfi0Vxf18Yf0Qf6R/3Ynd21ePT61KX6mY4Z7zIaojHZpR8jWGATf6gH21QN90EXO/g98nEwARMwAROoNQGLs1q/HhtnAiYwDAIhhBjVMrqOQTeD7xztSBC05e1orK9LvEhoaHDM9EFlLkzVgFiDdsQJ9QympUkmSUpkYSXhFEVUJvJQOTUgqIpQij0G8SGsynF1kYU2GHyzMwXFKUp5pjwW1cnX1UzrrbyOiXVRI00JsQNT6cCB2XT46E3p8BGmKd4sQXZCfZnWwF+aUwdiU35lTbWvrGZPDWIDIUqf9DDWnY1LuI0fT/Pnn9LZYqtpUrsjks5OjXk6YGaQN7MIRRI44TToJXwJdJI4t6pfdxEtvZgwGmz8Qa+VJ6bqBRwEjoQLZ5qNytN38ytTq30ktefaYXsDlxXNc8SBFn/h5GpIZMKp0ZhNM7Nicew1ytKWSHs8Pf/8Y+nUs4+mBYTa0qI8akuyRR4rrCrsLI82iKmqDU2NxO7CdmzhTUcvYtMWxbBf7Y2NjoVYlXyL30X0TznbrBmkT+JcNKH6ZLTqigBXRZhymZmoDR7JpPU1ed60SUxHgjPaDY56wMPBN+8tmx+J/jIBEzABE6gtAYuz2r4aG2YCJjAcAnlQq7FyDJi5CyeYEtoa6K9rINyT16zJDomKh/jSgBhRwiA8RBV6iME6hfVhYM16Lkbw7BnCFEQ8cjGiJw9BjUSZfENCxGK0XubJKfHNLow5IFwQZLpKsJUeOtpll0POTmPd2fTssXTzzfekG295dbruxlfKkzSd2jKiqw/em45EGcIgps0hFHQTDEL86bmewYGztXptedK0scWhm29Mjz/9uKY2npM3CmHIGjc8fywyK8WZhIH6G1xIRTwUgbiqLHsa4hKREobwTAqkfA5bFEbsRimhwYHd8QfhKNGyrr5O3XJfShKb6TRTRTUFVEVYcyXVEuKEPvFCmnoJvBO23+8qruPQJJ7G08zhu9Oho3ene+99Zzp96pH0jUc/mx5/7EFNg1xW+QCRLS/eHZuJUGMZoj1sViLd7Mn7JYrRP3xYrBNkc5AoE7ZkwRUGqpI4uoD+K4iy8ikePwoEn4LqjnTy0CfV12u3U3tN0zW1O2Vb7zq8ipqm2qOhFqWoj3qKqC4OJmACJmAC9SVgcVbfd2PLTMAEhkAgxING1siG8GRJlbCWZ2F5PS1qu/Nbjx9Kr7rzRnmeWJulaXwhRgpDlQ+HRwyEJQqohb8Ms4mjExAqZShjMZhXYm6b8pRjNL0Rp/xA/JFePieqPwiPXDVlZYSm8jU1NbExe1vqHbhBXqWjqS9B1miOybM3IbGVhVEIoNJzQ3Oqhyl62EQ8CyDdaJxPP2gNZ9S6PGkHDhxOt37Lm9PCEw+mU0tPp1sPU3eelolN/CXQL/7EodSRXD6Ip5Fn8MUjPjSF4CWajZFd2lJF8exFUzaJFHh2NJ1y4shtafyGe9KqhEmnd1alZK8Ka1lfCDEqKj1e1InIRODR15h6qcReT6ISW1sz6cj1r0mzx+9K3/amxTSy9GhKc9+UJ/G0Cq6X3Yq8ASqDjzazrYqqD5g9OPNM8XitkUtfGKGQ3xsdhnkuxIUQv5ccLfLTX0V5AUUFeM7m5hbTwrn5tKL/WLCwspYOaVMWIQleuYByU66sy1cTMAETMIHaErA4q+2rsWEmYAI7T4BRscSFvmOQrUE3Xq4FTWGcW1pJ05Nj6Z7brkt33XIiHZqeyOINIxnQ68PgNzb8UCwGwpGeB8WIk3Igz7PQVkqqtkWcEGkqe3G4NIXMSkUFKNBqe2QmLY2dSIut69LSyLHUbh5MncakxNhY4nzr2DQDD0sIMJXAu6Pi1LCxoyNV6pnUDeJBWeMesRNxeXDw0qxq/dltd96Rzow30+lv9NNTmgp48+HRNC6PUrV/YSO9ig5kkRYGRx9hpzu6MRAdJOT2yYe3iEyl7s27Pqa0Jm/WWlObl1x3R5q44ZWKT6buyrL6KLupiz6qPN/0hz7TVq6dtVoSaPRNLy0EEUXCQSbP2sgBbSyiTVJaJ9Lo4WNp/Kb70sHuqXSwczJNd86lke6q6o23mlmFnblubFa10Q5xQvUem+J3Qp8Vz/Yorpv4FQUQnuS0HMkiM0QW70R5wouo+Mrqenry2bPpiWfOpVUJtNGVdU1hnZS4ze+U9xr9Lhvi6mACJmACJlBLAhZntXwtNsoETGBoBDSKZSDP+JWhd1tCZG5hOYTBDUcPpduuOxzCbFRT4kLAMJzWoB8RRxiULQfCSkPuaTitZ6qTQbX+cGiwvgZiQS4ZiucQVUWuMiWuZZXxOFKoVXd4yiaPpt7ksbQmYbY2cl1aaRxOq90DaU1TELvFdLcstBj0FwN/hImiIcKojziX+FK9xU2ZRrv0hAddeRRXtUkI3T52462RNv/0I+n02rl0WApnWoKtJbsQE9FjuJZxsYuC1Ed6XPmiP0WQEbERC9c4fFpPwjBaV9vC1VF/x4/enMaOqf3JQ6kjzyYiKE9XRIyVzVCC8vwlh3hFPGSfhJJu4EjABjqlJLitaNpnR4K3O3IotSaOpNHG9Wmscyo1Vk+m5vIZGaGDrlW+KK0KVLfqQDhFdfGdfwM5Wublmt8xzTG9tKwje9uwUyFUI5HyqTyHiiKSCZzhfWBsNPHb7Opdn5QHbV1Tbnk3E9oAhWK5z2X5XM7fJmACJmAC9SRgcVbP92KrTMAEhkCA4StiIYSDxtYdTfebW1yVZ2ItHTk4lU4cmYlnJ8/MxcHIDODJm4fhGmwzEGZQrgF1DIW5r/QjcsZge+MBZVSJijBYH9RERdmWXFPUndvLFTIFrjkyJlFyMKWJw6mvM706E3enNU1fXNN6o44G6H0dJs0fxEY0i2n6w7i+9JIhzHgWV7UZ5isfz1l/Foddq0zepr94rvLcI/YWF5fTkcMH0y13vTKdnZlNpx77iuY8nlPba2lspJ/G5CIarezeSH0DxnSev2ov0uTpCcEbGCRCsBb7VIYruxuuyyPW1Q6Ta+OH0tiJO9LBm14hr2AzLS4tRz0hkosyKq5+FMJZ8fxu1D7qRhDQxzQFSzURYTC1UHds+IFdrGlbVf52dybNj82midYNaaJ5OI21v5bSwrOa7nhB/eXMNFWSX6iihXqKDtJGrp88aj3aJZFfSp/dYbA5slSek8LLAVDkJFsuQ9Z4b7qqdBoT40Mzk3r33XT2/JI8aOsSaZrbqOmtlOVvrLSjcQcTMAETMIHaErA4q+2rsWEmYALDIMDgnq3gOxI25+dW0/zcUjo4Nan1VWNpSet75le0CyCbQ8RQmpFuOepGAOV4DLg3kjeyKLdy5W4VlzyWz4PnwbiZUXcRspBR5mKsT7HW6IS8ODOpOXVTGtHaqMaRe9N6fzzpdC1tuy/RJDFBiMF7tJMbC5ETT/IXqXykC+Ia94VIQCpIlkg/sBZKT0iXseSNuDw9FBqR0FnWro6jWuR07MT1Wot3NJ0++Uw689wTabwzn6b7a+mAMuqYZR0mzTRRiQR9wTn3XfWRSL266G9uT9fYnVBMmT7YZXdKnaO2ljSd9NCN6dj1t6emzmRb0zb069oUgzrxsEV5lWXKInbDm6o7xMPDSf1Mb2TTEprN9fPq1ESIwIbmOoY0VBp15LVyub6edupc1bq9zuQ9aXzqjtSceyR1n/ty6s49mzor89qgQ540heKnEPFsAYmlTZEcbcdLCqOzpaX9XAmDsvk2+Jd5SCJecgxhqoYntBZyVV7Eef2HhenpMQn38ciXv4qKfDEBEzABE6glAYuzWr4WG2UCJjAUAhrpNke1oYTEzVmJsjPnljRgb+gw5hF5Ino642tFnhjWazEdDtGSh9DkYRjNkD62YdQtgoCvBuuZlC/nVJoGz3g6comNa8gICR6cKA12sUBcFHWUI2uEGmeJTepg5QO3fXtqHX+1zvY6oHZHtAsg7SBKyuG8WohG5X+S96jc+ZCn2VL6oLjy0A6OnvCm4cRR/tg8HkGmT5zHFaYrpwog/cgbhdkARHa12Y1yTdtxSEDdfPudqXvr7Wnu3Jl0/tTT6fT882miu5gmdQ7ZjBw541JBo+ooZ6KF+Al+dFZVqtrw1inS1k6KqxKbK92WvGVTaVJb/x+54fY0Km/h2lo3byOvfPDvc1A2XY4dFDnbDRa5TniXHjhsRyiyIUhD9ceW+jBTYTY4RGxGB4EX745NSPL7w0zifLCtpzPOWofuS2OHXpEaZ76a2o9/Ki2fekzl8y6NtE5/NgfVRR38ATrtxN+CRTzhOSFbG1EqKivDDhL5omzBj9mtbG4SYlqPVpZXtFnIaDo4PanfoXjTNwcTMAETMIFaE7A4q/XrsXEmYAI7SYBB7fLCUnr0kcfT1x7SIFvTBg+Mj6VVnXWVB+UaLGtAHAJIVwSR/habO2TRgr2MlUMY6BnDYV1iWZjG5LF7IGl8yhDjbiUwYGfgnsfaklBRgNKYMp6mjl6XTtz7/nTg5ten5oETOtB4XMN3tSuzmjIE7xBFcqMM9vVAf6NmPcfuPJ2RwT8CAoHAcwXa58N98eGe+stM8KE6ylEXKhNxM6rdAbkN8SJVtCavDVMCZ48c1VlqR7TJhjbu0EYdSzo37MKiDnjW1vS99ZXUX1qTsGLzfwlWNYMRebfHEfVX3sGDB9OkDsi+bkZTGA/I/6Y2OlJQKxJmsbW9SoQHTgKRzT7whXXxiGGnXgCCjK30w0km43gnTJWkDwi0sBcxExbwnbua3zX9RJTBSN8YyF8yqc4mAlotdvujaZ0dMI9+i3bvPJ56Bx9MZx76cFo6ezK2uY8fiNoNuCqXG0GgUT8PVHeumkyRrogCDRX5iZNVX3EpHpGS82BnzoKZMEFgs2nLwtxcWlm8kA7fcTrdGlMvo5C/TMAETMAEakrA4qymL8ZmmYAJDIOAPCnaWGJ09sY0e8NtOhvsUHgbshjTsFh/Y5dDOYkQCiFQGOpzo+FxDNwVY2A8GEUzatYDxETssEe+omsxZZB4Lq5sUjsSGQz7QxQUj/AUHbj+nnTgpm9JE0fvlDA7rrokiIrplQzI8QRFYNSPBEGcqBY8YKX9iCv+5I0MFSNPmJ/FVi6TuxPCMPJjnrxKKBuFqENRuowIwGtVbvyByoBB4NCmIL2+RFZL3qXWWGqNTUpgHUq9YzeoTc6H00FtHAyt2pUxrg1NJ6UCzklrxjq1Ed2OSqipP0rra0ohf8JotcO5bkEKW4oFZHlqH7zpp7xqItCTVwyxhoCN47+EmfvwUMoDFrs10jfli7j6AbO83k7pKhcCDSvpI/0XY1oPUUmH5cFszNyaJm6dScenjqeZU19Jqycf1c77OsRa74q8vCL4UZpfQenxog+xk6SekE6IehUnN0kUgy1fwZj0sFF5lJ4fEZF3VXdxSLn6GP0e5Xd9XJ7KCUo5mIAJmIAJ1JiAxVmNX45NMwET2DkCDJo5s+zwiZvTG9713emO+94gr81kDNIZ6BMQNgyS2WgjUnST0xjo52F1KYDKETP5GDhnIcfYuRx+b6RRhnQG5FF/tJXLLWmN23xH55JN35JGDt0uzxEDbmXAAsbiGplnK3JZ2qU+TM6ijKvEWyFQCoWgPBIlbKOvP6WXCVtDV4YRiJqoJNqizghciIcOoC8SHtEnJUj0RO/iPtfb7raV2NR0x2YaH0V0jeuTp9ht1Jk7zkYd1EX9/EEQdjlkWWeqdVSPnuQ2VD46FSnZLmQosVweOvpT1ENfw2ZyiA/42Mo+hEtT/dT5adQd3BRDgCFw2MVxQygrXymy9AwzI5SRYCIxOn0ijR88mma0i2Tjtlel2Ylump0qNuUo2qCdYFaWLdKpD3EZ/VA8bCJRkaie/Pob6+vEJvLFFzl5xMMNcUYa4rsrz+Wt99yXDkzPRq6yfZ47mIAJmIAJ1IuAxVm93oetMQETGAKBGNCrXa5HtKkFnzoEPD9PPr+aHnx8Lp3W5iRdCbMYz5daQ0bGurFCaIS4YDCuD54YBEysj8MjpLrC+0OaHqmmGPCHMFN5npMw8KQx+OcTaTyjJZVRXaUXDSERnqqsykJcxvxNTflrSBCFiIhSEkQStGthLd4y2s9BEjH1WbdGE6SGbaFBihz5ksVMfk5KKUSwkWfYxjX8WRJXeBKjL3i9ZE/uPwVzHeoGLzxuOcxaDrQc1D7r/sChZB1UnfuLcKQOytF2fq4Met4KD16uGm9bR2vkENKHb31VuveWg+mVN09r6mdYVjTiiwmYgAmYgAlcnoDF2eW5ONUETGCfEWBgTwiRkiNx/0JfGpprmH51gbyEav4yrZpepnE+2fPnl9PDT8ynU6dXpBhGwrMXIkNSRBoCNVMIMYSI0nQfBzCXcQkHplgiJPgaiC3FY3qm0nmGWKNsiDcpGgRbeM0iX54eSD7EU0fPOCyafoQYwlNTChumZZKOV46I8pIvHFe6hpCjIkQRz/WVc+R4JGhmI7Xk8kU6N2VeHmJshOKB4tTDtMXIpzawkadhtyIIVjnJ9Fw5VZfMlq7KXrSYTUpRCihAFxGqTfhTQ/XgUQ3eKhPajvppLxsVXrii62i1qJsdM0+fW9S0xtXU0edVtx2W5xDjc99pSlkH4eL7wYMrRApT42m1nitkV2M5F++s/K1fMa8fmIAJmIAJDI2AxdnQ0LthEzCBOhIYDFyLwewL2XhVg+KigsvlLdNCENJeKAnEUj89e241fe3x8+m5C+uF8EAsZNFE/m4hupiuiLrCm4WnKK4hsEiW0NKUNgbyTHEMQaY8lEeQIdYQW+E94yoh0mU9mJ6zHi7XlfN2WSOmivo9bYJRtEP+tso19cnigiMGFKRQEGQhBKKu3NNYi4c4UFrOp2vBOcqXQKKOnCWYXOldlM0V9cmM8C7Sp7aMZCt+qLGGjb7ieURJIcxQWQip8oDrSKM+2Y3I62ujkugw24yQH16KIGC1Ei5715SFfnaUn4mR4UCLShFAudjZ+dX0zWdTuu7wgXR8dlIeNCqjDepCKOb7/B2PrurrWvNfVaXOZAImYAImMHQCFmdDfwU2wARMYL8TYIAeG1AIBFv2n5TH7Etfn0vPnVmUIGimUe0YiUhhA5Ce1EBscIEAIk0CJNaWhWAq4sWzEGCqE1EUZ4YVogqZ0dEW9R3WcmlHv7Y22sgeNoSYxJw0DGUjTVc1rDw6ZDnS5RWT0GEr+8U4S2stjYxm79xAaEg5IB6UPT66aOqf4giX4hlpFwfyE2IjxELc5ZTKN8YRuKiusgws8NctLa2mxWUdwLy2prVqXW0A0k0t+q8Pm5MQmKKIOOI8s4ae4xlrrrdieiLesCYbaEjQtbT1/yjLxaRLG9qREg6IOdgQtFRNSbHqLm+yIntaOng7+w2zQEP4ntR7/NSDp9Lr7r4u3XLiQBqP7f5VZyHMcm3+NgETMAETMAHtzmwIJmACJmACwyUQHhSZgMiak7B49OnlhMdlbb0j4TMiMSDRJOGR13ohoBBqSiFdH9amRZqUCtolvGQSECHeol7FdY37EFqdtLLa1rFkq2l9XSJGAo1peFFOVyqJurlSJ+101/Ucwca2+T2dVdZLz0p0NMYXtMHHSNSf158VokPepAgIMgJiK8cib1QscYIYUiv63rgW2Sq5y5LUnZPpS9xwVSgZLi8upufOLqQL4tfpsttjW6JMG5HoWIQWG4ko5I1HEGA5jjhr6MyyOGxa8RE9IO+4RPGYtsqfiP08EGEU0GRH9amrRWoxrTH6KfsRxDIOa1SFvGhFn5TAezx1diU9PLmYDkyMyIs2Hrtchs1lh8Iyf5mACZiACex3AhZn+/0X4P6bgAnUgABD+oa8Zh0dfr2Wzix2Nd0Qb5fOANNipqa8MUxJxGMTm1voGnGppU0iSrWEgGK6XZ4EGeVUdQTEwLq8ZGsSZotLy2lF546tLM2n9vqy5gN25G1aSGvL8taprRauLqkMpkUi/Na1a+Ls2Fg6OH4wjWlr/Il+J7Wf0iHHqxJo8jIREDmIxjjwWHX0ZXtIlBBSkQXD6GpcIgVxoryEOEqAa9zlr5gCWdSlDPpbPs2ClPvQf6qXJ2ucofbc+TS9tCTRKjmlHSnDM6bDsdklkkAZtrcPrVWIOw5vjq389bwjobY2PpPWp2fSVDoQYq2pPja1aK0vUdZRwRByvAcOqpb3DLNiy32Vp0pmm9JcTD+Nd6k1aHq3z0jQzhxo6jNegYBVDiZgAiZgAiZgz5l/AyZgAiYwVALhAZKsYEA/v9TWGrPV7HqRVV2mGLKNoLxZTJZDkJVnb2XvWZ4Oid6JjyqhvrzZB2umFFSGNERcBwGmrfkXdND2/PnT6cKZp9PK/KmU2kvKtJ5Wl06nxbnTKtONqZRMA+x02hKJKS131tLxw0fTtxy9IZ2YOpw6i500+/R4Gj2ts7Nidw3aVjXhMZJ40U0WThI9iCseqishZLBWC7SY5kjPsC0LJvqH0XjU1I9QPCgg5SEVgYUYg1dMv1R6CC1EIUJWB2JrzdzNS+vp4Bpr5zKvqBuxSVnS1AjnxMka/dUHvhKf0abiC42x9MzBW9Ny747woE2Mj2h6o/KoDNw5gJqppSrJijSqjQ/9w1aecTi3jBI/fbTWbXQ02/3k8wvpennOZiZxx9F87nvc+MsETMAETGDfE7DnbN//BAzABExgWAQY3EfQAF1j+nR+sZ2ev8CG86wxk4cGMaZdGxta+xSCQpmz+MpXJAt1oC82pjXqhkVbEnW9OMdMz1V5WwIBz9zi8nJaOP9ceu7RT6aT33hAuxEup8NHj6QxecVmxkbSkRt1WLGEBR4fRBUeI8XSqs4Zu3dqKr3x6ES6faqZFrBR67B6EnVZYEnBqdmuymriYxhFWbxniBTsz2vfEDeIl/ysI+PxBhLi/DjEj+qNJLxSsJHtePM4aHogxtRGTMXETupDeEUtqmdU3r6DYoZAUuCb8tBiTR1iCqVISg/voNbO9ZbOpNYzT6VR8svWvzx8Nj00fiCtTs3orDWJMfZDUVfQdNTDWjVuFIv6qTHeA/dqoJgdGtzxgOLdw5x5idozOhbh6KGJNDlGa7QXF3+ZgAmYgAmYgNec+TdgAiZgAkMjgDDRh8H8wuKqpr2tpBV5fPAQaQ6exIcG+UzLa2kiXbGZRaxrytoHlRDlY0MQ4vyRzkEwIFRCLEjYsFaNzT9WltfS3Jln0plHPp6+8eWPaE1VI931ilek+771Nem22+9M0zM6j0s7YOAxG9VatzGJJAQW0xQRU5MScIdYgKXPQa3hamp7f8RONBStKqq+oDVon6mC2IRwRIB0ZRzxltJj/RfTILFVz/oSMF2tfUOljhTt5n5JQsFJ6dmTyDRFrSXTnEH6h8eMdqgvRJzuQ5SqQWxn/ViIRNmDSGKaJvkRgNjWGBmVd6uT5p54Ip35zd9O7Qe/kEa7Kyp3No11VuU90/RS2o+26F8WU3gIVaMEHr4zpfFREu8yPGtK6UlhUrdeYRREsOFFO6WpjUdnxtKt1x+iqIMJmIAJmIAJDAjYczZA4YgJmIAJ7DQBhvQIl0YcMn3qzHJa0y6KoxrUx/oriZdue10bdrTTqAQJAaGCCycEC6Ih/sSjONtLY3+JBNQO+fiLF4c1Y720PPd8OvvwR9NXP/+naWJ6PN35invS69/0pnTfq1+dJg5MSIBpM4zRUXnRxtPExHgalxgbQfhIyIzJy9PU1MC+BE9X9pKG2MDDFuWUjjpRk7pX+/ogqAhsMx92S3yNaDpjrAFTOp40AuIPkce2IrHGLuqiD3mXSfgQ1uX5i81IZFesb1P+EH5FO7S3se6LnksIihtij8DOlFJn6seopim2tK199lytCFpj9mjqTU6n535xIbW+/ldQy64uGAdHtaUrZ6XhQcunUWvKJo7B+KN+yBtH1zkAG4sRm0xrRDBrr8gQpwjMZ84wRXQy3XbD7AaDoo/Y6WACJmACJrB/CVic7d93756bgAkMnQDipRG7JZ7WWrNzC2up0x/TgF9CgIE+uyBqyl1Xgq3QEWExT8NDhn6hCqmGPK1RIiOqRFDkDUTIyPS/9YXn0xlNYzz12OfT1MxEuuc1r0mve93r0z333JNmDx3MJKQ02KGwoXbbEgvUOaZzucYQDqo3zvKSkMKLJhUlK/hDDyREECyVgBcLSRJeKuXlLmkLeYRWCM/Sdu5VZ6QrCx4tamRKZVeewyZeRFpQR0fGEHZ5SqRyYoLqV9/Vv1hzBjM9xxS8dHDBa4aopI5Rts6XJaNqY1T1spHJ4sJCWpxfTKvK3jh0KHWPHk69b6hiiSg+Ifgy5IHwi84ggNU/ttYHTmwwoitr6rJIFZ94B5kBAlbOS4k1HUMg0XZhQVv+r6ynA+NZIKoSBxMwARMwARPwtEb/BkzABExgmASYuse2+Qiz5TUN5EekEuRtiR0QJR7QBR2JjzEi6AFdQuigwiISGiQEQdmPEG8SBog8hF1f2+Avnnoknf36Z9L66ly66Y4701133Z1uvOkmecRGtHPjgsqzhfyIdnNcl1dMnqXCc9YJkSKRKC9XTxuK9NsSNXjREFQygAOoMaSvdpRJH7Wuusrpjn3l66kfCCmEjuSM2kIwqTQeq1BYKld0Jo4IoD7ljXVmBQOmIwKAHRVZg8dmIiDJ6+sQaFpLpimMDanEfLSA7gGmvHnXS93BDDGo5jhTe3F9VdNJV9LS8mp4KFe1i2WH5/o01e+GpiQSGjEvUe8m2osEfemGv6TRTvl+uCUoHU8dXsvmqN4jglfeS9rtidWcNi05O7ecJo7NZJa8WwcTMAETMIF9T8Ces33/EzAAEzCBYRCIaX5qmMOhnzg1n06fXwmvCiuhEFVNbfjFlOoAAEAASURBVDiBNyZ2CJSI6erTkhcrtIDKcQ1hoC/0TaxFUx6m9eG1QgShmxB/nTVtef/kV9LC0w+lqWMn0vTUgdSSSnjumSfTyWf0XNMmEWQIwjWJsxunDqa7jh1Jk9oMo43gkpepL68Xm4p0EV6INwkOvFSIKMr1I57FWY7TePZUIV7Yxh5PHCFP2YwYsoYexzU8VbIX5xNCKPQKnVOIjTyU0lVbdLjcYh/vVLm2LZeAlYSkhBFr9zgSQKYVsJRDDNdVB2vFnjk3n1bGJ9LIodk0dmBSni1tv68uxIYl2gGkiRiVkosppLI9hKP6r0RZont94z9kR814BzEVMj/n5cR6M5XDe5fnQnZiGijTOS9IjD9/dildd2RaZ8bBJfeTvjqYgAmYgAnsXwIWZ/v33bvnJmACQyJQCjON39Nqu5ueOLmYzs2va7CvdVxKbOAF0llbrIlq4c3qybPTW5NgmRhIgtAbMZ7Pg3q8UBEQNSFeEDBM/pNIWDqfVhbntFtjN42dPZfOrK6lC9/8hqZQyruj5/ikiLfV9rjK/tCxY+kG7eA4rc1BFmVLbPIhtRTb+OOxUsVtXVlbxpb5IcFkBu2hMXiGe4rDmuUgi5Ada8qp+tFo0kDyJkV2WYDIUZAheMtQOkwWDFHFA+WnbYRmeN70NJ7rAWnkjz4rK/WGCFQ6dq8Xz6SgJCKxPdfdZy3aeXGZnEprb3lbOvj+D8jDpcOhJejCPsrrLDcVCPvCUpUl0BZt0l0+3GYz8iYlcSNLEIl4+BpqCyQx/VIFyLu43E6nL0iQKw8brxRVU72DCZiACZjAPiZgcbaPX767bgImsPMEGMwTGKAjRE6fX04XFtc1nVAShaVRCiFCUEwa1Leao9IVK5oSJ2HF8qSoAJGSo9w2wkOV69RtjuhBeHV0xTO2uroiIcicuka6sLouUdZLbWVF04VAi2tKhyQG5SJL3ZXldF72tZWuvxEQEAiXsF0pCC88faUtZEJoUSfSJTxQWXbpLv8p85R1UDdCK7eRv1VtbI3PNacoohDTFYsE6ma9GS1l+UTujfykIuRKzcozclCOCKhHJb7GhWT+wME0/5rXpqPHj6eGdqmkP3QyT7tE3uo2V5/jgoADrVEozxBWes77bMQatByPdXDymo1I8BGY4tjT864+HGtwQVMbF5bX04S8kHgTQ/RFTn+ZgAmYgAnsVwIWZ/v1zbvfJmACQyHAGD8khSIIsqdPL6QFrXVC9PAM0YBw6eFxQYAw3VBetJiqpwdRPirA/HyvDIqHpNCluBb19LVeqtNejWmBUUKiAsESwkxZs+BBYEnIsMZqopXaWm+1pEOZ26qL57QSW8eHTVGLbJRt4U3LHiRaxSw+ZUA7Id5GtO6MdESn/HCK5b7RUerPpZBTpPNUwi6+MxOKkIuyeL7iRl9lfhLw0g1cd8oMEZLw6mVvG5Ug6MhN/eqqPh31sac1d+35udS8/jrxllCS1XHGW4/3Qm5avzjISnUakVa2G68hsoqbhDBTJMfkfcQbF1Mj4xm1aRMYiWS8Z2fOLaZjhybjXV/cgu9NwARMwAT2HwGLs/33zt1jEzCBIRKIQXrR/pq8J08/r50C19n1UB4WJAMaAoFVHHI8Iq8Kg/tOe03es/U4h4ziWS5IICiCkEB4xLTDqIOnElooFIV1bXzR0SHS7FoYgk/pTYmDntQTAify0a7ycpgz96z7Yu1YKBylq4UomzVQFjl5ep+yUJasVFAJsf5NlcmSEEV4rRCdOAWjZcpEPHu0yuKqPdJDjOl5mBQNq0BxX5ajHkLcK2MWbGVqfhYZyKe/0Q9FEJexm6IKxoYh8i6itJiCGE5LMW0yvVQ1hzUh0rhHbtJXtYFhuuM+JKAiOUkHiMdmIrJG2/YLvMQxUxwlgsNGec9UYkkezOe1KcjdUuYjFXi53my1v03ABEzABPYXAf6bn4MJmIAJmMAOEmBIz05+85rWdn5BW+UzTa5sP0b+iAdFJBBGtBEHB0Nz217XujQiylzm5//EKYLoICA6CCFSqEJ/uvKCIUAGhZSepwVWKooyStc1PhIR+JAQG9RYjes2NgNhVh8eLjQcefggZCKur/IfmOy9ynZSNgesVhk1gC3aAzK3kR9Gn3KNfKsyCUnqw7OYr9X84kUW6otP9TsLTtqgLf4MbNZ9BPHHwxhnp4k3wgkPZN4QhNxItFwyF6CRbH8uv9G3skrWktHeiAQxa/N62mAk9JcMZQdLerGy3k9ndIQCXlEHEzABEzABE4BA+W+naZiACZiACewEgRihayMQbc5x6sxiWtH6rhBUeX6cLGDQLy+LBv9Mx+OsM7xnbGaxvt5W3uypCY2mrNxn0YHAQEJk0UCuHJNA4OwwlUcwEBBTylh8SayEsslChWfIEQqH50pXxFUE4nGf2yVP2Z4azy1KkPAPC1485AyBfrBJyLrS8MhlqYO9PGUblHx4c7l5CFbivaJ0PNMVcUVKrjE/U0IE6mOLkLxNSFSa8xWZ0VG5dBFRKWwqJVFk0w0c8XLRNl41cmFk2f+wF1YR1CpZVCWXgYdRid0QYuqXvGYt7VLJ5ijkxbKwI/rSjAPHz86tS6RpBaAq57mDCZiACZjA/iZgcba/3797bwImsIMEstcrD+6X1zSlUevN1tlxI8SJJFEeuRdTBCVu9IgyTGscGRnT9vC92NyDgX7UglpQJERDKIQsFDZkR843OTERW8qvRsYsEkoxEXe5IQnALG84dHpEKg0hhkALIRNisZRVedMPdmBEyOhSCAs950YBgZfTc39zKnXqXvXT1xBDkY+1ZwiiLMI2/mHKYjPyUqdsCPFIn3WfbUOw4UXLfaUdnlEmRGJ5r8SG2mbXej0KsRcbmkQ+1at1drBmTVv0mCmgxUYepWwKe+kgf7kGt8yMOiMoua3dMHk8qgO9R3WOXOxAqYc5D31SPG5aaVmnX5/VMQrrEumDOnJN/jYBEzABE9iHBDb+DdyHnXeXTcAETGCnCcSgXo2urLXTc9qpEWcW54RloZKtwUvGwJ+8rFXiHC+mNuLBacvjhhuGgTw6J+pTPmmBEATxf+q6QcAReBKHR1NPThqIgBAwWSUUT3SkmeocVfssSB6JZ7SE6Mpyi2mLnBJG2yHa1Baiijzk5Cu8ftgfCVFabePdwhOGcYroQxxRla2MSNRBMRBQMgRUfhT9oyiPIo9qREqRFl4+xbJUywVIJ4CiwBE2k44tUYduQpixJkyJCNRgLo9Xc2wsCpK3fBeULevNPUZw5l5EhXrKwd08G5W4a+nd9fWSKZ8tzQ3zvvt6tqZnz51fTGu8V0JZeb7ztwmYgAmYwD4jwL85DiZgAiZgAjtEgEE5g//FpXY6O7+mwbiG8QzUab8YmMf/MeN6UQICpsmW+px5pvVQbOzBQB/dVIoUBv2luMPrkysqKtMldmHUmjO8RoWMoLUrhGJaoirHDtrYCIWtSgxRWDzIluYbvFN8oi8bJuhWf0iWaMP2subIEvfSQTxXQjznqlw8zz1iIxEJH2XY2B5/Q3DmPFFbCDtkWp6CmdnyfMPLl2VSIMaScBFSu9pjGqICYpJNOuJPPMh9x6YwV2lRi/KQN+pSWuxIKQ9neCEL0V22Q0ns0lcENl9hKeCTzy2m5ZX1SIPPRv6cz98mYAImYAL7h4DF2f551+6pCZjAkAmUgmZFXpKFlbW0pimNMYjXID0G/fpCmMTYXW6c8LZopI4sYe3S6Nh4CDvEVh7AF6IuxEEhT/QgRIP6ilAg9CTo2JACL1Qp4mjvcoH08kNp/pEIgRhWSfpRPyqLXDK21GFKKMqF/OA2ErIF5Fe/dCPtER8eIofwvoWYw2o9CwGlKx487FU0xBiCjJr5IOKy0FK8SIsGFKelEGVyHeZn1EBrPMn1kR5HCEQSFavtcP8pgW3/dWEqYk8eMErk90Y9ivMdRXJ6n/VkSqB+rvFulKslT2dDUyOjh9SfC2UrVD/vnbPNeGusPVyM4xRyPbTjYAImYAImsD8JWJztz/fuXpuACQyBQOkhW9F6s3kdPtyTykDoxFAcVcYAv7SLiEbuPYkGBvZNzfMblzhDOXTkbunKO6MHebCPmlAovWboAAKpRNvarbGn6XPcD/5PPxeJNARTGfCsxeYbkaZ2la9cm5Wt40EpdTChjOdKyB9doeawi4bIk/OV19wez8rYRu0Vc7LQUhZEDIHytElvsvjK6brNz3RL3pxfNUV6XDb6rueQy5xUjyIcK0AgHZ3GcQYcIk1J+hTiKjIU1hX1kpRrp4w8exz0rZYQ0i2BK9ebASVziQJxcLj0mWrXoeDLnTSnnTvbvFPV62ACJmACJrB/CQz+nd6/CNxzEzABE9hBAhrbc74Vg3EG4gi20puFeoh4VxKJQbrus8clC6ZxbTDR0O6NXW3Dz46ArE0rpEIhNHJ+ipJeCoo8rbEdUxqLajd3mMQi8I8CuzdGkr7KRwgZUnmePWkIjzylD0EXW+EX+clDyH6mLKEoW06pxDYszx+JIQkjvGFlwKPGIdmawBnPch/zd85DvCyf49hMPdnXRS6MQdjqoittRd/0zZX7XFLPJMS0UIyMqiO/D9bVsRaNahB6WVjn+6CrwlGeBqIu1ad6EM0tecxGNQU1zp0LpZfbo55BCM+Z2sJTJ4vOL6zGOkQ4OZiACZiACexfAuW/ofuXgHtuAiZgAjtAACFTiqXllXa6ML+qMb3+L1hT28rxOJt4RD6G/THmZ6COiNCNoi1tiT+mTSbiQGMEhQJjf/KEV6YoQx1RMu4lcPraql358QARQmzoWtxGWvkVwkVihofUHV4lrlGOEvmfjdLTREp41ja6gbVRN0KDLTv4kC9bnOPcU1dXnQ5PWHjZ6ClPJJ10X6bnPlJOzxBckScqIKciQSjszXfKFsIst1Km0X5L6bkH+Rm1xHuR4KWqfAYZiRKVwZHWqIGQbYsrUSUPeoM4pLzKjLI+sFi7FsX4Ii+/gaiLGlVB0UemdV6YX0lL8qaWoWyxvPfVBEzABExgfxBgQy4HEzABEzCBHSCA96Ujz8qiBuGLy/INSS1pSK9BuuSLFA4Ci5QQWgz+FbjXiF8xfZR/RGuZ8ISFaJLgQkeRC59RyJamvqlS6Y2mItIccXi1PG1ZlBTyKoQCZZVPDTajjRxHOIQw0zPiqiIH1lcpFtvhYw5xpbQkLtRseL94zgchGB42BIiMZM0YXSKNK3lizRm7GSoOBw66RjzRNs+yKNITChTSDlbanJ4EfUePoz7qxTOHAMtPgxzZlEBNud0ea/mKP1n0qW31nXPN6As7Y4alKoOghXMO5ZX6FNdzAKhkvqcH7NIolmPj42ET2ZiWSq4IKpLfp66Kk8rUxqY2Djm/sBZn34VQVHq81+h3LupvEzABEzCB/UHA4mx/vGf30gRMYOgEJA40Il/VgcNMYUOcNRpaQ6ZAOmP8Rjnvj6E9I/tihB4Ddm41nB8bHZHYYrt3rW+SSGMnx3JAT10hA2LkTyzLkPa6zt2ScBiR120w3i/zDMQHpRWKdkuRE/WRHM/4yjVgC0IDEbWAiNGTUQmbCdnMmjXyr+trTaIH4TYttYZAm1feVV2pZUrKZEwRTdDULozdhN8Ib9mY6hlVOnHyKXsItriJdlW+eK5HEefMtRyonIf6wE/XEFCFNMVO6sxfuUT0udjuPsoqb1N2j8jWfH9xAWxTmt4BU0sb8pL1tA6QT4tDw+Ec9tFaEUjgFu3HJ6rO9SPM5zXNdXlVJJSPdWpUn3uviIMJmIAJmMC+IcA/EQ4mYAImYALbToBBdyOtSJwhzNblQQtRVniOGM2HLsKOEGUauKMtQgUogtdKYmBEa5kY/PMMz07Oo2dlPhUnHn+KtBGm2KmdcOJU+hmDf0SDAmutCHkzkByneJYJ1Ea8/CeDqYHKowxNCawleX6elSnPddpaJ5bXlpH/vATPs5oueEGiBUF2VvdndZ3nmT6nup20pD60sFf2ndN5Yc9KwS0icpSP6ZJ0MC7BCUZIreyJow2lhAg8rTLU3xFjxGD2aHHNudgXkbz5Sr0Si6oYJsFLgihS2WGxIXEVndfmHlwVNvjqHgwqm+3SVZF4ruuIxLOqiGJ91CjtK0PUV5aJJApJBPJMn5W1rn4XazqMmg1FeKaLgwmYgAmYwL4jUP5Lu+867g6bgAmYwE4TYB3U2npbH3ZqZEoi4iqLAkbjGpJrUJ9F2sbYvIhFuvJqIB/iTLnz9u8k6f/KERF6VgZiiC+qH9VataYUS1viJUIlH7kihBgpbclVxRNVGz4yqs8545pFj9ZXcZaXDmteliK5EOJIuZUXkbag/AtSP+uqe17PTuvaV75D2thkSjYt6fkZiSqmDnKO25pE57ziawg2NZb/gcqtMhmRWHiWdKV9vHYrauuc8p9S/edUF+egUTaX4oogy4E0zhbjqmxRR5y7Jhv78kIqQVvgj+orbxrSknhUkvLquQpRJgI3CsVF6XCTfXJ9jozo4Op4TBmsjEe5bJV7UWdMe1T+toTcgs46W9GW+hFyEznubxMwARMwgX1DwNMa982rdkdNwASGT6AhYdZJa/ImsdZI2wHGlfVSSJHwpCnOn3JsHhtbFIbnsb2m28l7NioPTV8bfSA+CEyvi2l2iA9FS08O7eRNMJQu1dLCg6b8rNuiJOd0STHFeq+edMmInreoCwt0HYhFVdoMjx6rw7LAUSZNY0zpoETVjMotyHO2jkXKu677rgThhP6VoX/nlLcjm09oPdZ1Ej/rElKsZXtOYvWQDJmWOJvQdURsGlof11V5An1htie2xho1KSLsYH3aitLOy/w5Tdns0FFlIj/rxihDHC8bogq2pdWIpqhQuZr0nzyyh9AabaV1CccGU0ZVGi8YZUPUUYeaCeaK02SuN1/xUI6qfBkyRhmhwDdiLveKFvEw6hvGMoF1dXhUl3T+3cHpiSjjLxMwARMwgf1HwOJs/71z99gETGAIBBBLDOTX22y3LhEij9Mog30NzBXNg36N9slTiokQaRJOCIlyd0TcUk0WpylTWwutEA8x5Nc95bryIlFOmXQemjw5Ei4xdU9ioCkB1FV5xBWShGl9+JWUW6pEgqbHlL5GbGO/psRVFSRfhwoor7KIjGpoxBb06os60W2Nak1ZJ81o3dUi9lFYJRBOi6rjANP8JE5XsEEqZ1peplNKQ1yN6SPJGoJtXY3EujRdaY/W6RtXvvGOYSfb0E8jVPtSle3V1FODK2pzTKIXUUkBTAgeUZgYETxjuS9rEpBryrWqsqsSissd9V82sf6to/V8bNaB6BqRsB3VJx8eLWGF8pLSoibOkGN6IsJuZFQ1q6+8R4yPq+qKG8roGWfWxTlqsjEEm+rh99FG3MJIofy9xI2/TMAETMAE9g0BTYPnnzwHEzABEzCB7SRQDrYXNXVtQZs/rK6zs5/EGAN2BvIapeOVCRcKi6YqoS+3SuPiNA3io07KMMgP0UGcguWdRJDEwMrc6bR04WzqrK+E5ywLnZyH7OWH6YVs0nFc4uqA0vEllZaUVyUNBBNxdnls4G1SH9pqa1RChU0+2upXbNKh5wgXVlIx3XBcFTWVL8Sj2lrV85ZcR2OK541BivsQdrSwub2cojQEDh9ZgxwNwUb7qpr1amGk2is7QBKhTAI5AWHVnRhP/SPH0uSxo6lz4UJqn3xWnrReak8dTqszx9K4+scxBgitXC4Xrn7nhrJ4i3mdsj/aKhvKzQ2+8YiiHMOLJ7u72hBlamI0HZoaTxNj2Ze6IewGxRwxARMwARPY4wQszvb4C3b3TMAE6kGgFGflfw8rxQLWFTrhmg2t1vGChSWAynZfMF/l4Uu1qVLFLooWogshVWUVwkpp1wjjGrOX+jF4UVbWKK3wvu0iijbVBEzABEzg5ROwOHv5DF2DCZiACZiACZiACZiACZiACbxsAkyIcTABEzABEzABEzABEzABEzABExgyAYuzIb8AN28CJmACJmACJmACJmACJmACELA48+/ABEzABEzABEzABEzABEzABGpAwOKsBi/BJpiACZiACZiACZiACZiACZiAxZl/AyZgAiZgAiZgAiZgAiZgAiZQAwIWZzV4CTbBBEzABEzABEzABEzABEzABCzO/BswARMwARMwARMwARMwARMwgRoQsDirwUuwCSZgAiZgAiZgAiZgAiZgAiZgcebfgAmYgAmYgAmYgAmYgAmYgAnUgIDFWQ1egk0wARMwARMwARMwARMwARMwAYsz/wZMwARMwARMwARMwARMwARMoAYELM5q8BJsggmYgAmYgAmYgAmYgAmYgAlYnPk3YAImYAImYAImYAImYAImYAI1IGBxVoOXYBNMwARMwARMwARMwARMwARMwOLMvwETMAETMAETMAETMAETMAETqAEBi7MavASbYAImYAImYAImYAImYAImYAIWZ/4NmIAJmIAJmIAJmIAJmIAJmEANCFic1eAl2AQTMAETMAETMAETMAETMAETsDjzb8AETMAETMAETMAETMAETMAEakDA4qwGL8EmmIAJmIAJmIAJmIAJmIAJmIDFmX8DJmACJmACJmACJmACJmACJlADAhZnNXgJNsEETMAETMAETMAETMAETMAELM78GzABEzABEzABEzABEzABEzCBGhCwOKvBS7AJJmACJmACJmACJmACJmACJmBx5t+ACZiACZiACZiACZiACZiACdSAgMVZDV6CTTABEzABEzABEzABEzABEzABizP/BkzABEzABEzABEzABEzABEygBgQszmrwEmyCCZiACZiACZiACZiACZiACVic+TdgAiZgAiZgAiZgAiZgAiZgAjUgYHFWg5dgE0zABEzABEzABEzABEzABEzA4sy/ARMwARMwARMwARMwARMwAROoAQGLsxq8BJtgAiZgAiZgAiZgAiZgAiZgAhZn/g2YgAmYgAmYgAmYgAmYgAmYQA0IWJzV4CXYBBMwARMwARMwARMwARMwAROwOPNvwARMwARMwARMwARMwARMwARqQMDirAYvwSaYgAmYgAmYgAmYgAmYgAmYgMWZfwMmYAImYAImYAImYAImYAImUAMCFmc1eAk2wQRMwARMYOsIfPn7/1b6y/ffn57/o/+wdZW6JhMwARMwARPYAQIWZzsA2U2YgAmYwH4g0F1aSg//w3+Qvv7f//TQurv48NdS9/yZlPr99Pyv/9rQ7HDDJmACJmACJvBSCFicvRRqLmMCJmACJnAJgcVHHk7LX/lSWvjUR9Op3/7/Lnm+Ewmnf//3Bs20n3smnfrd30nd1ZVBmiMmYAImYAImUGcCFmd1fju2zQRMwAR2EYFGszWw9uS//heD+E5F8Jpd+LM/3NTcyV/652nxoYc2pfnGBEzABEzABOpKwOKsrm/GdpmACZjALiPQaG38k1IVajvVjarXrNpmozVSvXXcBEzABEzABGpLYONf0tqaaMNMwARMwAR2A4HGSEUEtTa8aDth++W8ZmW77TOny6ivJmACJmACJlBrAhZntX49Ns4ETMAEdg+BRmPjn5RGcyO+Ez2oes0ubnvpq1/ZCRPchgmYgAmYgAm8bAI7+6/nyzbXFZiACZiACdSVQGNkw1u2k9MaL/GaXSQMlx+yOKvrb8Z2mYAJmIAJbCZgcbaZh+9MwARMwAReIoFNHquLBNJLrPKqilW9ZhRoXrTGbOXhr6SVJ5+8qrqcyQRMwARMwASGScDibJj03bYJmIAJ7CUCFVHU2KE1Zxd7zabe8ObUv4wwPP/xj+4l0u6LCZiACZjAHiVgcbZHX6y7ZQImYAI7TaC6W2O6jEDaDnsu9podfPv9qbr2rWxz/hMfK6O+moAJmIAJmEBtCVic1fbV2DATMAET2F0EqlvW9yubg2xXL9pnzqSFj394UH1rciodvf+dSepskFZGVh55KM1/6Uvlra8mYAImYAImUEsCFme1fC02ygRMwAR2IYHmhihqVrfV36aunJU3rLuyNKh9+s1vS6PHjqVGxY7BQ0UufMJTG6s8HDcBEzABE6gfAYuz+r0TW2QCJmACu5JA1XN2Oe/VVndq/pOf2FTloXfIa6bQTxsisZphQfn7vV41yXETMAETMAETqBUBi7NavQ4bYwImYAK7mEBlE5BNQm0burT8ja+npS9+dlDz6PHr02GmNCpcbs0Z6evPPZPOf/xjRB1MwARMwARMoJYELM5q+VpslAmYgAnsPgLNyiYg1TPPtqMn5z/x8U3VHnz7O9JgKuXlHWeRf84bg2zi5hsTMAETMIF6EbA4q9f7sDUmYAImsGsJVLfP3+4NQRY+dfGUxndtcKuIRBKnXvfGwbOFz3wyrZ89O7h3xARMwARMwATqRMDirE5vw7aYgAmYwC4mUBVnzcoUx63u0twXv5BWvv61QbWT99yXDr3u9YN7TWysxFM69M53D+67y4vpnM88G/BwxARMwARMoF4ELM7q9T5sjQmYgAnsXgIVj1VjG3drvHhq4kyx1qwE179oK/2j731/Gr3upvJxungjkcEDR0zABEzABExgyAQszob8Aty8CZiACewVAoM1X3ToIoG0VX3sdTppvjqlUYLw8DsqUxrVUHXtG+02JybS7LvfOzCBjUSWH398cO+ICZiACZiACdSFgMVZXd6E7TABEzCB3U6g6jnbpmmN5z/xsdQ+fWpAauZNb0sHbr11cB+Ri4QhUywPv+/9m/Jc+OTmDUU2PfSNCZiACZiACQyJgMXZkMC7WRMwARPYiwQazVZ0q7r+bCv7OXfRVvgHL/Ka0VajIhLDg6f7qVfclQ6+/d0DUy7eUGTwwBETMAETMAETGCIBi7MhwnfTJmACJrDXCPRLj1l53eIOLj/45UGNramZdOTt9w/uB5HGxj9tVaE2+77/ZJBl+WsPpsWHHhzcO2ICJmACJmACdSCw8S9YHayxDSZgAiZgAruawGC91zaIsyUdPN0+89yAz/Qb35xGDh4c3JeRfhnhWnjyiB5917vTxO13EY1w4RObt+Mv0301ARMwARMwgWERsDgbFnm3awImYAJ7kMBgWmNFFG1VN5e+9tVNVR1881s33Zc3VW/ZxVv6H9LOjWWY/8ynyqivJmACJmACJlALAhZntXgNNsIETMAE9giBYr1XVSBtVc+WH354UBVTGmff+rbB/aZIdc3ZRSLx6Hd8Zxo5dDiyrz3xjTT3+c9tKuobEzABEzABExgmAYuzYdJ32yZgAiawxwgMNgLZhmmNq49uiLMrTWkE56bNGjfdpDR+/Hg6/F3fM6A+V92Wf5DqiAmYgAmYgAkMh4DF2XC4u1UTMAET2JsESjFU9V5tQU97Kytp5bFHBjUduujg6cEDItUNQVojmx5xc/xv/e2B9+zCJz6WuqrbwQRMwARMwATqQMDirA5vwTaYgAmYwF4hUIihTQdSb0HfFh76Sko6gJowfuud6eh733flWkuBSI5m45J8Ve9Z9+zz6eyHPnhJHieYgAmYgAmYwDAIWJwNg7rbNAETMIE9SqBcazaY3rhF/axuBjL7nhcQZmqvsUmc5XPXLjZjk/fsg3968WPfm4AJmIAJmMBQCFicDQW7GzUBEzCBvUlgIMq2eFrj8lcfCmCN0bF0pHJe2WUpVtq+kgev6j1bevCv0vyXvnTZqpxoAiZgAiZgAjtJwOJsJ2m7LRMwARPY6wTK9V4X7ZL4crtdbgYy85b708RNN71gdaX3jkz90p7LlKh6z87+0b+/TA4nmYAJmIAJmMDOErA421nebs0ETMAE9jSBxkjegKO8bkVnV595ZnD49Mwb3/SiVVYPoR548i5Tquo9u/ChP0ntuQuXyeUkEzABEzABE9g5AhZnO8faLZmACZjAnifQL9d7VaYWvtxOX/jMp3MV8sYdetObX7S6qucsvYgd133v96WR2SNR56nf+e0XrdsZTMAETMAETGA7CVicbSdd120CJmAC+4xAszjfrNnaun9eOufPBcXpN3x7Gr/uuhcnWgpE5XwhzxkVjR47tuncsxev3DlMwARMwARMYPsIXHoAzPa15ZpNwARMwAT2OoFCnG3ltMYb/s6PptaBqTR7/zuuil51t8ZSLL5QwZv/ix9PR97z3nTgzle8UDY/MwETMAETMIFtJ2Bxtu2I3YAJmIAJ7B8CpafqhTbiuFYarampdMOP/OjVF6tOZazGX6AGC7MXgONHJmACJmACO0Zg6+ad7JjJbsgETMAETKC2BIophVfjsdquPgzWvdFA4cnbrrZcrwmYgAmYgAlsJQGLs62k6bpMwARMYJ8TGExnvEqP1XbgalbWnL3YhiDb0b7rNAETMAETMIGXSsDi7KWSczkTMAETMIFLCPTb7UvSdjyhIgyH6cHb8X67QRMwARMwgV1PwOJs179Cd8AETMAE6kOg3+1mY3q94RlVEWepOHdteMa4ZRMwARMwARO4egIWZ1fPyjlNwARMwARehEC/08k5+tWjoF+k0FY/9rTGrSbq+kzABEzABHaIgMXZDoF2MyZgAiawHwiU4qw3TM9ZRZw1dHC1gwmYgAmYgAnsFgIWZ7vlTdlOEzABE9gNBArPWSMN03NW+aetOsVxN/CzjSZgAiZgAvuaQOVfsH3NwZ03ARMwARPYAgLdTt4QpF+TaY3luWtb0DVXYQImYAImYALbTsDibNsRuwETMAET2D8EGt1izdkQpzVWt9K3ONs/vz331ARMwAT2AgGLs73wFt0HEzABE6gLgZrt1tjwtMa6/DJshwmYgAmYwFUQsDi7CkjOYgImYAImcHUEyg1BhjqtsWLq4FDsSpqjJmACJmACJlBXAhZndX0ztssETMAEdiGBwTlnw1xzVvWWVeO7kKdNNgETMAET2F8ELM721/t2b03ABExgWwn0yg1BesPbrbFR2Uo/eSv9bX3frtwETMAETGBrCVicbS1P12YCJmAC+5cA3rJeN/e/Jp4zT2vcvz9H99wETMAEdiMBi7Pd+NZsswmYgAnUkEC53ixMG+Jujf1NnjP/M1fDn4pNMgETMAETuAIB/6t1BTBONgETMAETuDYCveIAakr1hyjOqlvpN1v+Z+7a3qJzm4AJmIAJDJOA/9UaJn23bQImYAJ7iEC/nQ+gpktV59WOd7G6CUirtePNu0ETMAETMAETeKkELM5eKjmXMwETMAET2ESgOq1xmJ6z6tlmPoR60yvyjQmYgAmYQM0JWJzV/AXZPBMwARPYLQTKnRrD3iFOa6y67RrerXG3/HxspwmYgAmYgAhYnPlnYAImYAImsCUE+u3OoJ6hHkJdmVPZ97TGwTtxxARMwARMoP4ELM7q/45soQmYgAnsCgLVDUHSMD1nlTVnTYuzXfHbsZEmYAImYAKZgMWZfwkmYAImYAJbQ6A4gJrKGsM85yw1NvpTEWobiY6ZgAmYgAmYQD0JWJzV873YKhMwARPYdQT6neIAalk+zGmN3hBk1/10bLAJmIAJmEBBwOLMPwUTMAETMIEtIVDdEGSYuzWmires2RrZkr65EhMwARMwARPYCQIWZztB2W2YgAmYwD4gsMlzNsw1ZxXW/YpQqyQ7agImYAImYAK1JGBxVsvXYqNMwARMYPcR6FfWnGle4/A6UBFkPudseK/BLZuACZiACVw7AYuza2fmEiZgAiZgApchsGm3xiGKs0ZzY0OQxkjrMpY6yQRMwARMwATqScDirJ7vxVaZgAmYwO4j0Nk452yYW+k3KuecVTcH2X1AbbEJmIAJmMB+I2Bxtt/euPtrAiZgAttEoNdub9Q8RM9ZdUOQhjcE2XgnjpmACZiACdSegMVZ7V+RDTQBEzCBXUKg4jnrDVOcVT1nPoR6l/x4bKYJmIAJmAAELM78OzABEzABE9gSAv3uxjlnQ90QpCLOql60LemkKzEBEzABEzCBbSRgcbaNcF21CZiACewnAr32+kZ3h+g5q64z826NG6/EMRMwARMwgfoTsDir/zuyhSZgAiawKwj0K9MaU783NJsbjco/bZ7WOLT34IZNwARMwASunUDlX7BrL+wSJmACJmACJlAS6LUruzUO0XNW2sO1aXFWxeG4CZiACZhAzQlYnNX8Bdk8EzABE9g1BKqes97wPGeb1plVDqTeNRxtqAmYgAmYwL4lYHG2b1+9O24CJmACW0ug3614zoYozvoVQeY1Z1v7jl2bCZiACZjA9hKwONtevq7dBEzABPYNgU3nnA2z15XdGhsjI8O0xG2bgAmYgAmYwDURsDi7JlzObAImYAImcEUClWmN/SGuOdu0zqwi1K5otx+YgAmYgAmYQE0IWJzV5EXYDBMwARPY7QR6nXYtulDdSr8WBtkIEzABEzABE7hKAhZnVwnK2UzABEzABF6EQMVz9iI5t/Vxdc3Ztjbkyk3ABEzABExgiwlYnG0xUFdnAiZgAvuVwKat9IcIoVk952yIdrhpEzABEzABE7hWAhZn10rM+U3ABEzABC5PoCaes01b6V/eUqeagAmYgAmYQC0JWJzV8rXYKBMwARPYfQQ2baU/RPO95myI8N20CZiACZjAyyJgcfay8LmwCZiACZhASaDfrseGIKnVKk3y1QRMwARMwAR2FQGLs131umysCZiACdSXQL8u0xob9WVky0zABEzABEzghQhYnL0QHT8zARMwARO4agJ18Zw17Dm76nfmjCZgAiZgAvUiYHFWr/dha0zABExg1xLodbu1sL3vg6dr8R5shAmYgAmYwLUTsDi7dmYuYQImYAImcDkCNTmEumnP2eXejtNMwARMwAR2AQGLs13wkmyiCZiACewKAjXxnDWa3hBkV/xebKQJmIAJmMAlBCzOLkHiBBMwARMwgZdCoFeT3Ro3baXf77+UrriMCZiACZiACQyFgMXZULC7URMwARPYgwRq4jlLlTVn/U491sHtwbftLpmACZiACWwDAYuzbYDqKk3ABExgPxKozVb6zY1/2vrJnrP9+Ft0n03ABExgtxLY+Bdst/bAdpuACZiACdSCQK8m55xtmtbY69WCjY0wARMwARMwgashYHF2NZScxwRMwARM4MUJ1EScpepujRZnL/7enMMETMAETKA2BCzOavMqbIgJmIAJ7G4C/ZpspV9dc5a8Icju/lHZehMwARPYZwQszvbZC3d3TcAETGC7CPS7ne2q+prqrZ5zVpeDsa+pA85sAiZgAiawbwlYnO3bV++Om4AJmMDWEqjNhiCV3RrtOdvad+zaTMAETMAEtpeAxdn28nXtJmACJrB/CNRlK32vOds/vzn31ARMwAT2GAGLsz32Qt0dEzABExgWgbp4zpqNyj9tXnM2rJ+D2zUBEzABE3gJBCr/gr2E0i5iAiZgAiZgAgWBfrddDxatyj9t3q2xHu/EVpiACZiACVwVgcq/YFeV35lMwARMwARM4LIE+nWZ1lhZc9a3OLvsu3KiCZiACZhAPQlYnNXzvdgqEzABE9hVBOpyAHVAqwiynqc17qrfkY01ARMwgf1OwOJsv/8C3H8TMAET2AoCdTmAWn3ZtH1+RahtRTddhwmYgAmYgAlsJwGLs+2k67pNwARMYJ8Q6LVrst5MvBtVb1m/t0/egLtpAiZgAiawFwhYnO2Ft+g+mIAJmMCQCdRmvZk4bLLFnrMh/zLcvAmYgAmYwLUQsDi7FlrOawImYAImcFkC/fb6ZdOHkdirCDJvCDKMN+A2TcAETMAEXioBi7OXSs7lTMAETMAEBgR67c4gPuxIw+Js2K/A7ZuACZiACbxEAhZnLxGci5mACZiACWwQqNNujf3qOrNef8NIx0zABEzABEyg5gQszmr+gmyeCZiACewGAo1OfTYE6Xe6G8iqm4NspDpmAiZgAiZgArUkYHFWy9dio0zABExgdxGo1bTGqiCrTHHcXURtrQmYgAmYwH4kYHG2H9+6+2wCJmACW0yg362R56wqyKpCbYv77OpMwARMwARMYKsJWJxtNVHXZwImYAL7kEC3RhuCVHdrTFWhtg/fi7tsAiZgAiawuwhYnO2u92VrTcAETKCeBLqVdV5DtrB6CHW3Vx+7hozFzZuACZiACewCAhZnu+Al2UQTMAETqDuBXrtG0xq7lW39Pa2x7j8d22cCJmACJlAhYHFWgeGoCZiACZjApQSe+r//z7T48NcufVBN6VQEUTX9KuLP/cHvpeXHvnEVOa8ui6c1Xh0n5zIBEzABE6gfAYuz+r0TW2QCJmACtSHw9P/1y+nMb/w/6Yn/6WdSb339ina9VM/Z4z//v6Znf/Gfpqd+4eevWPe1PqgeQp3sObtWfM5vAiZgAiYwRAIWZ0OE76ZNwARMoO4EJu98RZi4/vzJdP5jH72iuf2X4DlbP3cuzX/8w1Hn2PU3XLHua33Qrxw83feGINeKz/lNwARMwASGSMDibIjw3bQJmIAJ1J3AkXe/J6Vm/qfi3Ic+eGVzX8KGIOf+/IOpuzAXdc6+531Xrvsan/QrtlicXSM8ZzcBEzABExgqAYuzoeJ34yZgAiZQbwINCbOZt70rjFx84JNp/kt/dVmDe+0rT3m8bAElXvjwn8ej1uFj6fBb33albNecXt2tcdMUx2uuyQVMwARMwARMYGcJWJztLG+3ZgImYAK7jsCR975/YPPZP/6jQbwaudZpjec/+Ym08shDUcXx7/vBalUvO171nPmcs5eN0xWYgAmYgAnsIAGLsx2E7aZMwARMYDcSmH37/akxMhKmL3z+gct2oX+Nh1Cfr0yRvP77f+Cydb7UxF6/t1HUG4JssHDMBEzABEyg9gQszmr/imygCZiACQyXQFPC7ODb3x1GdM+fSfN//deXGHQtnrPlJ55Ic8VGIJP3vjo1xsYuqe/lJFSnMvarQu3lVOqyJmACJmACJrADBCzOdgCymzABEzCB3U7g0Lu0MUgRlr/yYBkdXHudqz+EOjYW6XWj7PEf/OFBHVsV2bQJiD1nW4XV9ZiACZiACewAAYuzHYDsJkzABExgtxNgw47m2ER0Y/ErL91z1pcom/uLvBEIlR1993u3HE2vsltjNb7lDblCEzABEzABE9hiAhZnWwzU1ZmACZjAXiTQ1NTDg+94d3Rt+UtfTJ2Fhc3dbF+d5+zsBz+Y1k8+FWUPvWtjo5HNlb28u+pujd4Q5OWxdGkTMAETMIGdJWBxtrO83ZoJmIAJ7FoCB9/29rC9u7SQ5j77mU396Hc7m+6vdFM9K+3ED2ztLo2DNiues76nNQ6wOGICJmACJlB/AhZn9X9HttAETMAEakHg8FvemlqTB8KWuc9+epNNvavwnLGRyNIXsqhrTUym6Ve9elMdW3ZTFWS9/pZV64pMwARMwARMYLsJWJxtN2HXbwImYAJ7hEATQfXWd0Rvlr7wQOqtrg561u/kDT4GCZeJnPvQnw1Sj3zv1m6fP6hYEda1DUJVqA0SHTEBEzABEzCBehKwOKvne7FVJmACJlBLAgff/NawqzN3Pp3/zIb3rP8inrP1c+fSwkf/YtCnY9/5XYP4Vkc27dbYq5x5ttUNuT4TMAETMAET2GICFmdbDNTVmYAJmMBeJjDL1MapmejifHXd2YusOTv35x9MnfkLUW7sltvTxC23bB+miiDbJNS2r0XXbAImYAImYAJbQsDibEswuhITMAET2B8ERqan0/Sb3hKdXdTUxsEUws4Lbwhy4cMb2+cf/0+3aSOQ4hVsEmQ+hHp//DDdSxPYYwQ6Wi/rWdl77KVeZXdGrjKfs5mACZiACZhAEDj47W9Jcx+RJ+zs8+n8pz+djrz9/tR9gWmN5z/9qbTyyEMDerNvz+vWBglbHOl3K1MZK160LW7G1ZmACZjAVRNod/vpL/76+fSHD5xKjz69kOYX19Pdtx5M//N/9pp04+F8hmRZ2R987mT6J//2q+nEscn0+z+dp5KXz3zd+wQszvb+O3YPTcAETGBLCcy+9a3p5MHZmKbI1EbEWapsX39xYwuf++wgaeZNb0tjR44M7rclUv3PzdX4tjTmSk3ABEzgygQeO7WUfvNTT6f/+KlnU7v6H45U5GtPzKXf/MRT6b/9m3dvquCBh8+lrv6/6+Tp5fSZR86lt9yzzf+fuan14d48/Oxi+vWPPpn+8tELaX6pnVbXO6nVaKQ7b55J3/Wm69MPvv3mNNJsDNfIbW7d4mybAbt6EzABE9hrBEYOHkoz73pvOv8f/l1afODTqbe2lnovMK1x4fOfGyA4/J1/YxDftkhVKNpztm2YXbEJmMDlCSyvddPvfuaZ9LuffCYE1uVz5dQj02OXPK74/tNjzy3tC3G2st5NP/PrD6VPfPn5S3ggVB99aj79C31+/S+eTP/Hf/2GdPPRyUvy7ZUEi7O98ibdDxMwARPYQQLHv/t7Qpy1T59K5z7yYXnOLr/mbO4vv5jWn3liYNnhbZ7SSEP9yjozH0I9QO+ICZjANhNod3rp1z76VPqVP37sEi8ZTY+2mun73nVz+iF5f54+u5IQJO9+9fFLrOp0Ns5nROjt9cCUzx/7Z59LT0mIvlg4c2E1/fi/+mL69z/zNvHcmx40i7MX+xX4uQmYgAmYwCUEpu6+O01921viUOkLH/mLdKWt9Ks7Oh75m9+XmqOjl9S15QkVb1mv6kXb8oZcoQmYgAmk9Iim4v3rP/1meuArZy4ryqanxtLffe+t6YfvvzlNjrUC2U1Hruz56VWmY//hAydTV/+f9vjzK+nkuZV0YbGdDqiOqYmR9KPvviW9+zWXirvd9k5+5UPfvESY/ZB4fftdR9L1sxPps4+eS7/8R4/FFEf6dk4C7eMPnU7v/ZYTu62rV2WvxdlVYXImEzABEzCBiwkcfMMbQ5wtPPDJNHH7XRc/jvvFL2xMaZy9/52XzbPliRVxlqrxLW/IFZqACex3Ani2/t4/fSDWiF3M4r47Z9OPvPPm9L5vvS5daZnUurxtz55dTWeX1iTAltPDzyymz3/t3KAq1p39yh9/c3Bfjcwvt/eEOPvyN+eq3Uo/q01SPvC66wZpd90wlb7jtSfS3/7HnxqI3888ct7ibEDIERMwARMwARMQgenXfuuAA4dSXxyWvv71tPrYI5HcOnI8HXrjmy7Osi33/aq3rPJfoLelMVdqAiawrwk8f2HtEmF2y3VT6Zd+8vXpxKHxF2Tz7PnV9GM//7m0uLT+gvkufsj0yJtOHEg/8TfuuPjRrrz/9nuPDgTpicOTm4RZ2aHjYvmm+46mT/316UjqairkXg32nO3VN+t+mYAJmMA2E5h+1avTtKY2Ln7hM6m7uHBJaxc++fFB2pEPfNcgvu2RqiCz52zbcbsBE9jPBG46OhG7CbJpRRlYO/U7n342/Zfvvy2NjVz5SOHf/dQzVyXMbr9hOr3urtn05rsPp9ffeTgdnrr66eELK530tWcW0tmFtfS6O2ZjmmBpZ12uf/edt6SVtU764mMX0g+87aYrmvWUPItlaO3R9Wb0z+KsfMu+moAJmIAJXDOBQ+95X4izfvvS//Jb3UJ/9h07NKVRPbDn7JpfowuYgAm8RAKjEl///Cdfl/7hL/3lphp+9U8fS7/x54+nH3nfrelH33VbOnTg0iH3wcuIrImxkdRudwfeuA+8+Yb0sz9836a6r+aGzUZ++YPfTL/1oScHdVHu8Mx4+jGJxh+VILpcYFOTf/Oxp9JvaFfEeXn0Dmq93HtefyL92LtuTbfo3LXtCC3N+fz7H7jzBatm+mh1wxDv1viCuPb2Q05o53wF/XUwARMwARO4iMCx970/Pf///kpqP39y0xM2CCkPnj5w37em6Xtfuen5dt70q96yyn/N3s42XbcJmMD+JfDmu4+k3/nv3pb+x3/7UPqKvD9l4FyzX/2zx+PznW++Mf3EB+7YdOA0gucZbfLxiA6lfsu9R9J3f9v1EkAH0q9+5Mn0S3/waFQzrimM1xq+qvr+/r/84mADjWr58/Kg/eLvPZIWV9vpJ75jsyA6r81G/t4vfC6d0k6SZUCg/cEnno4PffiZ7783IUh3OnAwdzV8620Hq7d7Kn6pjN9T3bu0Mz6h/VIm1ZTf+uTT6SOaz/tD77j5stu7VvM6bgImYALNsbE0qzPPTv/2r2+C0ZnbWOB98O075zULIyrirFeJbzLQNyZgAiawhQTwKv3Kf/Nt6WsSRv/bf3wsPfDVM5tq/5PPPpv43P+tJ9I//pH70oHxVmwS8tPfd++mfNzceHhjrdqFpcsfU3JJoSLh0ZOL6cd/4fODjTNIxlt2aHo0Pa5nZfjVP3lc0y7vSHityvCT//tfbhJmZXp5xf4ntUHJL/+DN+z4Nvb/5kNPlGYkvIuvvX12cL/XIjsvfYdEkBPaf+7fPZze81MfSf/Drz4Y/6Phvx4wR7g8of1i0y4+of3i53vt/ud+9+H0z37n4fRFnUz/xW9s/JefvdZP98cETGBrCRzW1MaLQ29xfpA0e//9g/iORKqCrHLm2Y607UZMwAT2NYFX3jyT/uV/9dr0uzqH63u0forNO6qBQ5Z/4H/5THpO28FfKcxMbqwpO6Ox6tWGueVO+s+1wQgeuzL81A+9Kv3Jz96ffusfvTn9xk+9pUyO8e+Xn9j4j2ifevhM+uazl64dvvX66fQGefXK8JA8g//oV/+6vN2R6x994WTifLMy/Mj7btnTM9o2/2LKXu+RK/NTf+2jT6bv/blPp7/zTz6T/uDjT2/6wVa7eTUntFfz77X47z/wbLis91q/3B8TMIHtJzB1r/7L78jmiRjd5fxfaKf/f/bOAzCO4vr/T9KpnHrvzbIl994brnEDDKGaFloIJZQkJKQTfiQkBAgB/oFQgwnFGBy6G+4N496riiWr997bf96edm/vdGqnO+nKd+C8M7MzszOfXUn77r15b9ps0saa3ttgrZkZaMvUgpq1LohxQQAEQMCIAO+J+v2NI2jH3+fTH24bRVFh3koLFjRu+tsBuqjSZCknRcbVzL00L3+davCe+9h1KfRDYYrIiS3HNh0rkPLyPwEqIXDT0SK5WjmmxPvTR49PpX8/MJGuERZVcmKPidtPdW4vn7fkkQXO5z7Ref3lcbWeGrpnYaIlL2FzYxn+NbW56Zk3IURo7xs3tk3+25pzfeuE1iAAAiCgIuDmqqFWUpnfdOz1CpgzV9VqgLLqfWbq/ABdHpcBARAAAZmAu/AqePWUKFoxKZL+LvZ68f4tTg1NLfTyV6n06v0T5abKsaWtVZ8XDjp6k7KEueH67/MMmr782UWxdy2NQoO9qKSswUBwY0cfQ4TLfzmdu9xZa/bXO8Yo+8ueuDaFDl0op7yiWqnLun25Fo0zxuagawSbw8J6q6G5jRobdH9P2oTvB7UnzFUL4pQ5yXN3tKNDCWeI0N73x5O1iw+/frzvHdEDBEAABHog4ObtS0GDIZypXmwQhLqHm4TTIAAC/SJwOruKArQayZFHdwPx3q7fXT+c/Lw09IHw4sjpRKrpLSQuLnrDtiYhqBinPWdL6QWxVecm4XFR9rr4xmZ9oGp2ZCcLNGziyIGsjdOz94wxMA3MFe7/1Yljtam9M2rE/H9/43D66atHpWZ5QtizVPr6cD795cOzvRru3U2XaMeJYvrJsiEiuHd4r/rYWyOHEc4Qod28R++ZT8/3KsaGeaOjFwiAgLMQaG1p7rRU3xmzyT1gEDZtq00ZoTnrdF9QAQIgYBkCn3yXQ//49II02BXjI+ivt4/qUasT7OehXJy1QqZShL/eIUiViFNmnF75Ok1y3PHqF6mKcLb7uN7M8NVHJlFRpfDKKLRm6r1aPM6QaD96ctUIGhWn93bIbvdlYU6+Vp6IKcbX9heCp5xiVa70G00IjXK7vhyPXaowKZipBUzj8dixye/ePUWzhXD29C2jyNfLzbiJXZf1xO16GUSI0G7eDazoiErPPwQeHm5SEEDzRkIvEAABZybgpvXIrX+KAABAAElEQVSm1lpDs5iAOQPspVG+ASrhzG/6LLkWRxAAARCwKIE04WxOTrtPFNI84exj+phQWjE5gmJDvCVPiA0trVRT30wX82rpsNCUHVJ5cZw5NkzubnCMDPJSypVCyOLvmORtaDUNrSRruVw7PC3yvizZCQg7IJkogk1zWjohgvLKG4QHySrhHVJDKUIwCxZeG41TWU3nOJUsrN0pvD6+//OpivDzyjfpSldTGj3lZB8yG48WGrS+cmYMPShCDrDnSHVcM4NGHYV9gvcNlyrpnZ9Npphg68RgM3Vda9c5jHCGCO3mPSps68xaR60QzF744iKtE4EHkUAABECgrwSGvvAKlW5aT5Xbt1BLtfAA5uo6OCaNYuIe8YlUe/o4xfziNxR+5dV9XQragwAIgECvCFw5MUJyNic3ZoGGnWXwp6fkK/Z8sUt9U4nd7MuaIx7zjBCuxghNV0NTG/1q9UlFyzVznE644/1rcmIhjfefJXQ4IIkWgh5/uktVQrgzlXh/2ZLf7aKhcX5CU9cgBaWW280eEyJn+3UMNArOvUnsm1u/P9dgzEjhXGXdb2fSJTGfQ6nl9JYIVVDfqJsze16/7bmDtOH/5kjhCQw62mnBYYQzRGg3/wnkXwKcqjsedKmgD3shFfEPCIAACHRHwCclhfiT4+NLxR+/T+5hkeTqNjimJlE/uotCV6wg7ZCk7qaMcyAAAiDQLwLjhYbqcbEP65XPDL0k9jTolBHB9NQqXayzrtr6+3oQCx6c7n3xECVG+VK20NSpzQ8fXaH7HRcWYCh8/V2ERuIv32VtW1fXkOvbWTXXkdg8U+vlSpsP5Es1fL2Ll/WhUbiSTSOfuFZ46bVAWjg2nD7YkqWsS70+Hp69M7764EQprlqKYMCfm0SIgtfFHjt57x4LamfE3r+pw4IsMKPBH8LtKZEGfxqWmQG7LV0mPOKcya2mYqHGlVObeLBOiLhd72/PopyKRhoe40d+KhvacQkBVFLXLF4khEedGSL6uYgJ8dg1yaQRQssh4TWG02jR5orRofKQvTqyF8RVzx6gY0LK1z/2uq4Nwr73wPlSahN7PqcMNXyYOEL7rS8cpJ3CfrixWeexh48XxA/HJ3uypTXMGRFiEDiwVxPqodHXhwoUFfKUlBCHech7WDZOgwAIWJCA/6TJFPWjuyn8+hstOGrfhtL4+JBHWDi5ajqb7/RtJLQGARAAge4JjBYarR8tTqSxSWIPlzAprG9uJxfxH+8nYycg/PEU1knBYh/ZnPFh9OjKZLp/aRL5CMcg3aUm0f/IRd07KLerEKaH6nfJh69NprmjdO+lLITVCq+Op4WJH6f80nrpXZHPd+eWP7ukns6Ld+YC8c6886RO25cs4rT95dbR5CXek48JM0x+h5YTB7O+d3kSPXXLSPJy1zstkc+bcwwVXBaMD6dzYh+Z+t2dxxoq3tfffnQyRRuZLDLT6SnBFCa8UO49rQv2fb1w9R8eoN+rZ85cbKWPi5CW9dRtZVYWmEdXEdrlodUR2uU64+MWYT/8h9WnpWr+JuH5u8YYN+myzBHajQMBmorQzmrrPS8sMBC0Vj1/0GQgQPXFRiUFWjxC+4OvH5MCUPN1Hlw5jO5akKC+JPIgAAIgAAIgAAIgAAIDQIDDQq14ap+BKSFfdrzQDj105VCakBhgMIsm0X65aF/T4UuAT/J75/1XJtEk8c7Ipo3F1U10TmiYdp0pod3C46FsGsjCZHuH6LdMxEX7v1UjpbHZX8ll4Rik3aWdIgO9pC0wBhe1cIHNNourGslD4yKUKO69MlM8nlkpzD1baYYQ1hwldS+22/Eq5QjtOeLbg/eExmyjUM/KmyV5WVKEdqGJ+s9jkylCPHCmkqUjtF8ntHKc0oVa+lYRFJsTq285Qru8ebO7CO2hAR6K8CRHaP/nPeOkcSzxT53YZConH6FGRgIBEAABEAABEAABEBh4Arxd55PfzqB3tmRSblk9TRACFsdLM+XQg2fnIdq/9/MpdN+/jlKZCHLNic0in/245zi2rNmT962xVkpOnE2M8JaLVj96ebgauO/vzQWNhdTe9LH1NpbRSdrwKhGhvfc3hx2DyMnb0+EfDXmpOIIACIAACIAACICAzREI8nGnXwrzRf4i/s758V0KZvLE+Z33f7+ZQQtEwOveJPbsOEt4jPzbPaP1zR3ToE6/PjvIOY16BBHae34aG1TCmZf74Gzk73mWaAECIAACIAACIAACIGCKADt5e/aO0VQktqd8sPsync6sorziesn0L9Dfg+LDfGhsoj/NE34UUqJ9pSHYykxOxZWd3erL53AcGAIOI5whQnv/Hxi2b5aTm3COggQCIAACIAACIAACIGB/BNg5xi+uTu7VxCNUjjRKxZ4vpMEl4BC2axyhnd2M3vDMfhH/4TSphYyu8Fo6Qrt8HeMI7U/fOYZCTexpYzek7/5iKk1O0ntq7C5Cuzw+H60RoZ3H5Q2VctKIGEVIIAACIAACIAACIAACjk2A97exiSOnPOHBEWlwCTiE5gwR2i3zELWoNGcaaM4sAxWjgAAIgAAIgAAIgICNEwgVbunzRfBq9uB4ILWMpic7jvdDG0ffaXoOIZwhQnun+2pWBcfkkJOHheJXyOPhCAIgAAIgAAIgAAIgYJsEFooYbB9uzZIm98GObAhng3ibHMJ2TY7QLqtke8uTI7R//Ktp3cZR4AjtcmLTyZufO0ALf7NTcWnP57qL0N4XpzfqkHMcV23p9Cj50pLLfY7QXqWKX2HJCO2lwt2qOio7NGcKemRAAARAAARAAARAwKEJXDVZ/8558FwJtai+sHfohdvg4hxCc8Zcb5oVS9fNiKGDqaW06VgRnbpUJQLxNVN9Q4uCXSNsav283WliSiCtEG5GexOw7qZ5cfTGN2nKGJkiuLQ6cYT2uFBdDAiOB3HLogRas033zcORC2X01Npz9MebRpBGFTdC3Z/zHKE9r7yeykRwQDlxAL6nV42i5ChfeuPrdIMYbRxU8NaF8XTbFXEGwavlvuYcg3w9iQNiywJakLdeKDVnPPQBARAAARAAARAAARCwDwJJkT4ULHwkyDHSiiobpcDV9jF7x5qli9DW6G3ZHGttFlmNM0VoX7svhz7enUMj4vzob7erYl5YhCQGAQEQAAEQAAEQAAEQsFUC7Pn8D++foVkjQ+iJH6bY6jQdfl4Qznpxi8uFBq63Edp5OI4XoY7Q3otLSE28PDRKhPYrZ8bQk0LjhgQCIAACIAACIAACIAACIOAcBCCcWek+14mAzk9/cp52HC3o8Qq8V27qqBC6cXY0/fz1E1L7K2dE05M3j+yxLxqAAAiAAAiAAAiAAAiAAAg4BgGH2XNma7cDEdpt7Y5gPiAAAiAAAiAAAiAAAiBg2wQgnFn5/iBCu5UBY3gQAAEQAAEQAAEQAAEQcBACDuFK30HuBSFCu6PcSawDBEAABEAABEAABEAABPpOAMJZ35lZtQdHaOckR2i36sUwOAiAAAiAAAiAAAiAAAiAgM0QgHBmM7dCNxGO0C4njtCOBAIgAAIgAAIgAAIgAAIg4BwEIJzZ2H1GhHYbuyGYDgiAAAiAAAiAAAiAAAgMEAEIZwMEureXkSO0y+05QjsSCIAACIAACIAACIAACICA4xOAcGaD9/j5e8ZSVJg3XX9FHEUH6fag2eA0MSUQAAEQAAEQAAEQAAEQAAELEkAQagvCxFAgAAIgAAIgAAIgAAIgAAIgYC4BaM7MJYd+IAACIAACIAACIAACIAACIGBBAhDOLAgTQ4EACIAACIAACIAACIAACICAuQQgnJlLDv1AAARAAAT6RaDmwnkq+N86qj5zul/joDMIgAAIgAAIOAoBjaMsBOsAARAAARCwfQKNhYXUWFBApZs2UMW33ygTTnr+FSnfUl1NTfn5VHvmFLVWVkh1nrFx5BETR57x8aSNTyDvuDgiV3y3qMBDBgRAAARAwGEIwCGIw9xKLAQEQAAEbJPA5VdfoepDB6i1pJha62v7PUk3Ly25R8aQz8RJFDh3PvmPH9/vMTEACIAACIAACNgCAQhntnAXMAcQAAEQcGACxxbNturqvEeNI/858yh4/gLyjIiw6rUwOAiAAAiAAAhYkwCEM2vSxdggAAIg4IQE2oRpYunWLVT1/XdUdXh/lwQ0gcHkFRFFntEx0sdLHN39/amxuIgKPltHDTmZ5B4STs2lRV2OoT7h6ulFfjPmUMC8BRQyb776FPIgAAIgAAIgYBcEIJzZxW3CJEEABEDAxgm0tQlB7BCV79xONUcOUVNJocGE3Ty8yCs2nrwSEkmbOIS8xcfNy8ugjbpw/g9PUHtzE8Xd91PSeHtTbUYG1WeKT9YlaqnS7UVTtzfOa4ePpvBbbqfguVcYn0IZBEAABEAABGyWAIQzm701mBgIgAAI2D6BtpoaKtmwniq2b6Ha1HOdJuzq7kkBM2ZTyJwryD0wsNP5riqKhMMQTuHLVnRqUnHsCFUfO0o1F850OmdcEbh4OUXc9iPyFs5EkEAABEAABEDA1glAOLP1O4T5gQAIgICNEijeuIHKvvyM6kwIZTzlgKmzKFgIZV6RkVZZQX1ONlUeOUzVp45TS3Vll9fQ+PlT8I23Usxtd3TZBidAAARAwJ4JHEwto8sl9dTY3EZzR4VSfKjWnpfj1HOHcObUtx+LBwEQAIG+Eyjft5cKV79D9RkXTXb2GzeJgmZfQT6JiSbPW7qytb6BKo8dpqrjx4TZY3qXw8PUsUs0OAECIGCHBE5kVtK2k0W0/XgxFZfXKytIjPKl/JI6EtbmpHF3Iy93V3IXH8+Oj4c4RgZrKSFMS4nh3kKQ8xZHH/L2dFPGQGbwCEA4Gzz2uDIIgAAI2BeB9nbKefN1Kv7kA5Pz9kkZJYSyueQ3YqTJ8wNRWXnyBJXv3d2tkBb92K8oYuW1AzEdXAMEQAAELEqABbJ958to/7kSuni5yqJjB/l5UqwQ1haMC6MfiE94YNf7gi16YQxmQADCmQEOFEAABEAABEwRqDpxgvLeeYPqz5zodNorKo6CrphHgZOmdDo3WBVlB/ZThdDwNRbmmpxCxD0PUDTMHE2yQSUIgIBtEcgqrqP1Rwp7LZC5uLiQl6eGPD3ERxxbWlqpTGjW2sV/fUmjkwJpzuhQWjwuHGaSfQHXz7YQzvoJEN1BAARAwNEJ5H34ARW9/47kPVG9Vo1/oGS+GCJc1/PLgK2ldmHTU7pnF5V/t4daKso6TS9oxbWU+PivOtWjAgRAAARsicCjb52kA2eLTU4pPMSHhsSFUHJCCMVH+JlsI1eWVjYI88daKqmoo5LyOiouq6Xi0hr5dLfHiSlBNFvsZVsyPpwioFHrllV/T0I46y9B9AcBEAABByVQdymD8t56g6oP7DVYoYubGwXOmEshIuizu3+AwTlbLLTWCo+Se3ZT2Y5vO01PO2wEjXjjnU71qAABEACBwSaQUVBLL2/MoO/FvjJ1Cgv1pXEpkTQ8MYQCfT3Vp/qcbxAORHIKqyi7oFJ8qihfHFt5s1oXydfHg1ZMi6SVU6MoWextQ7I8AQhnlmeKEUEABEDA7gkUff0lFf7nLRFTrNxgLb4ifljokmWkjY0zqLeHQtXp01S04SuTQa0nbttnD0vAHEEABJyAAO8re3d7Nu0/ZRgv0tfHk0KDfCg6wp/mT7ZOeBC9sFZFZ9MKqLKqwSRxN2EtsWR6lBDSImlSUpDJNqg0jwCEM/O4oRcIgAAIOCyB3NX/kcwY1Qt089JS8MKlFDpvvrra7vLNVZVU+M1XVH3iiMHc3cOjaMyadQZ1KIAACIDAQBJIF5qyN7Zk0a6j+Z0uOzolgmZPTKDQgIFz0tHU0kYnUwvp1MUiyi/sOlzJHLEn7WqhTZs/OqzTvFHRdwIQzvrODD1AAARAwGEJ5H34vtCYvW6wPklbtnQ5aWNiDertuVC6aycVf/sNtbe0KMvwmTSNUp7/p1JGBgRAAAQGgkB9Uxv9v02Z9PW+bGpq0v9O4msHC03Z7InxNHbY4Ao+5zNL6VRqEaVeMr33jed62+IEevTKYZxF6gcBCGf9gIeuIAACIOBIBPLXfkwFb/4/gyWFLFpG4cKM0RFTbUYG5a/72MDMMeiq6yjx54874nKxJhAAARsk8MXBPHpv62XKK67tNLsp4+JozqR48vawnfhj2UU1dFpo086mFVJjo6EgyQuYPDyYHluZTMOjsR+t0w3tZQWEs16C6kuz0upmSi+oFm5M3UjrrpGC/3EAQC57iWCA7m6259WsL+tDWxAAAccjUPC/dZT/mqHWKFx4MwyxczPGnu5UQ0EB5fz3PwYCWtQDj1HkjTf11BXnQQAEQMBsAgdSy+iDHdl0UMQrM07xscE0e0I8DYn2Nz5lM+XK2ibafyKHjp7O6TQnf+E05MGrkui6GTGdzqGiZwIQznpmJLXYfLyQzmRXUVVdi+4jHkp++AJ93KVPSVUjZRXWUbaIRVFb19ztqMHCBenIeH8amxggvlnwFp8ACvFz77YPToIACICAtQiw84/cl54zGD7xkcft0umHwSJ6WTAloA17+XXyGzO2lyOgGQiAAAj0jkCbCDX24pcX6dNd2Z06eHt70AwhlM0YE93pnK1WnM8qp31HMqmwpLrTFFfOiqHf3ziiUz0quicA4awbPiyQbTlWRHuMXJh208XsU/GRvjR3TAhdOy2a4sO8zR4HHUEABECgLwSKNm2k3Of/YtBl+DMvkKtGY1Dn6AVjAc1/9nwa+vQzjr5srA8EQGAACfCX/C9/lUYn0gy94PIUxo6IFNqyBAr2759r/AFcjnKp5pZ22nkkiw6duKzUyZkFkyLo2TvGyEUce0EAwpkRpNc2ptPeM6WUntv5GwCjplLRU5gt+vl5CpNFd6qsbqDqGtMuR7nxvOlJFB3mTxrXNvL3cqNXPznWaUgvEc19yZQIulq4Jx0XH9DpPCpAAARAwFIESrdtpct//ZPBcEN/8yR5BAUb1DlLwVhAi3/yGYc363SWe4t1gsBgE1i3P4de+yqDahsMrasC/L1owfShNGpIyGBPsd/XT8upoD1CSDP27LhsejT936qR/R7fWQaAcNZxpw8K2983N1+iU+kVPd77qPAAGpoQTCPFD1JYoKGWq0qYO2blV9JlDuYnjmUiErupFBsVSBzIb2hcMI0ZFi7Fkvh6x0WDpvMmRdI1U4Xr1BGhBvUogAAIgEB/CTQWFlLqYw9Rc3GBMlT8gz8jn8REpeyMmeqLFyj33Tepva2VNEEhNHbdV86IAWsGARCwIIG/fHKevt6f22nEYYmhtHjGULvUlnVaTEcFm23uEgLa/qNZBk2umRNLv7t+uEEdCqYJQDgTXN7emklvrU83TUjUenpqiIWpxJhAEY09VOwx8+iyrfGJ+qZWysqroMy8SsorqqbC4ipqbxdPriq5iEB+CbFBFOSvpbSs0k7at4WTI+knSxJpSLiPqheyIAACIGA+gUvPPkMVWzYoA0Tffg8FjB2nlJ05U7pnNxV985mEIOT6VRT/0CPOjANrBwEQ6AeBX757yuT2mJmTEmjBlIR+jGzbXU+lF9PX284ZTPLmhfH0i6uTDepQ6EzAqYWzXWeK6V/fZAgtV01nMqLGz9eLJo6Kpgki8J+vt2UcdrCwll1YJQlql3LKO6l+TU5EVLLzkbuEgHbbFXFdNUE9CIAACPSKQOm3m+ny359W2oavvIFCZs9RysgQZb3+KtVdSpVQDH3xNfIfPx5YQAAEQKBPBEwJZo5kxtgTjOMXC2nDzgsGze5eNoQeWJpkUIeCIQGnFM4u5tXQ6u1ZtO2I3pxHjYWFslkirsT4lEixP0x9xvL54op6Ss8up2xhBqkO7Md72RqbO8ePmDoylO77QQKNHxJo+clgRBAAAYcn0FRaKswZf0pN+TpPYcELllDEshUOv+6+LrD80AEqWLdG6uYzeQalPPePvg6B9iAAAk5MwJRgFh0ZSFfNS6HQAC+nIXPkfAFt3q3ftsOv1f96ZBJNTgpyGgZ9XajTCWffHM6nV75Mp8qaRoVVUkIIRYT4SnvExgkt2YThEcq5wchcyqsirYiJxuaPxy4U0mnxYLe0tipTcRNmkLcLAe2h5UOVOmRAAARAoDcEMv/xPJVv+EJqqk0cRokPPtybbk7Xhn//ZrzwLDWVFEprH/rS6+Q/Fq71ne5BwIJBwAwC7FzuvW8zDXqmJIXRDYud0ynG96fzaPt3aQqPCcnB9MZDE5UyMoYE3J4SybDKcUsvfZNGr36ZRo3CtFBOC2cNo2WzhopAf4E0XghlkaGDH9E8SHh/9BWxLvzEJzk+mEYLgdHdw10IjzpnJbxj7YRwXFJa30JzRtq/dx/5XuAIAiBgXQKlO7dT4Tv/1l3EzY2ib7pNeGbEt5emqPNe4NaGBqpL15k2kpuIaTlzlqmmqAMBEAABhcDB1HL628eGe60WCKcfS2c6rylfbLgfuYi/OVm5uhACBWX14leqG01MghWY8uCoMlY22lNdaRCzmSI49CNvnqA12/SeYwKF840bV4yzi0B/gb6eNH9yPN1x7SSKj9G7uP58dzb9/oMzg0gWlwYBELAXAi1VVVSw+j/KdEMXLSOfJOd9WVBAdJMJnDyV3LQ6j7zVu7ZSU1lZN61xCgRAAARI8vyt5nDbNRNo5rgYdZVT5udMiKVZkxOVta/enEGnRdw3pM4EHF44+/ZEIf309WN08FyJsvrE2GC6eflYSo61L4k9LtyXbr9yDPE3MHLaKvbNPfr2cbmIIwiAAAiYJFD4xWfUlH1JOueTPJLCFv3AZDtU6gm4BwaS3/hJUkVLTTWVbd2iP4kcCIAACBgReG1TugjJpNMO8anbVoyihAh/o1bOW2RFQ1yMzlqjsblNCLIZzgujm5U7tHD22qZL9MfVp6mkQh8YeuLoGLp1xRgKsePNmPwNzL03TiEfb0/p1h4QQbN//K+j3dxmnAIBEHBmAhyzq2LrtxICN08vCoMDkF4/Dv4TJittK7ZuVvLIgAAIgICaAJszvrc5U6m6dkGSCJOEOLUKkI7MrAl6r+P8/vrFwTzjJk5fdljh7MWvU8UPiaFEzvvLls/Wa53s+e5HBHnTQ6umUXCQLvYZf1MDAc2e7yjmDgLWI1C8cQM15erMuv2nzyZtrP6Po/Wu6hgja+Pjyc1LKy2mPv0CVRz43jEWhlWAAAhYlMCnqiDTE1JCaVRyrEXHd5TBhoqYwROEokRO20/oLdvkOmc/OqRw9tfPLtDa7ZeVe+uhcaPrlo61i/1lyqR7kXHXuNDd106koEDdnggW0N781lAg7cUwaAICIODgBCq26DQ+Lu4eFDR9poOv1rLLcxWb2D0i9S8SdRfOW/YCGA0EQMDuCRQIC63dx3SeXd3dXGnhVMcNLm2JmzVbaM98fTqsv84W08V80/GGLXEtexzD4YSzp9aeoy/35Cj3wlt4PLxt5XgakeCYHsk83V3p1ivHKQLaOxsv0YnMSmX9yIAACDg3gXKh6ak9dUyCwCZ6nqEws+nrE+EVo/8GvO7M6b52R3sQAAEHJ7DxqE4w42XetCiRvHx0Vk0Ovmyzlxfg40HTx+stOLafLDJ7LEfs6FDC2W+F58KN3+ttVwMDtHTf9VMoygbc41vz4eGH/IYloxUB7ScvH6bKus4BrK05B4wNAiBgmwTKN29UJhY0a46SR6b3BLRx8Urj+gvwkKvAQAYEQIAul9TTf7/VOVsaNzSQrpio/30BPF0TmD4mWnEOsu14cdcNnfCMwwhnfxIxJbYLz4Vyigj1o4dunko+Wo1c5dDHsEAtLZg2RFkjXOwrKJABAacmUClcwHPyGzOBtNF68zynhtLHxXsPSVJ6tFRXUW1qqlJGBgRAwLkJvL/jMtU16uLnLp0aTZcr2pwbSB9WLzsHuVxQQztOQ3smo3MI4WyrUIduOqDXmEVFBNC9102U1+g0xxGJITRpjM785pAIHcDeKpFAwN4JNJeUUPY7b1HJ5k3UVFpq78sZ0PkXffOVcr2gWXOVPDJ9I8Au9TWBwUqnmuM6M1GlAhkQAAGnJFBc2UjbO/aa+Xi7U3xUCLW3OyUKsxbNzkFGDAuX+m6DYxCFod0LZy2t7bR6qz64NDvHuPua8coCnS0zb0oChQbrbJ3ZW+U3h/OdDQHW62AEOD5XyUerKfu5P9OZm1bShUcforz336PqczAv6+lW157RMXLRaEibmNhTc5zvhoA2Vm+qVHMCwlk3qHAKBJyGwAYhmNXUN0vrnTIihMoaXZxm7ZZa6NA43RdfW8T76hkEpZaw2r1w9vbWS5TacTNZMHvwpimWel7schythxtdMSVRmfsL6y7CQYhCAxl7JKAVJmXuYZHK1OvOnKDC1W9S2sM/odO33UyZL75ApTt3UHMlHOEokDoydWdOSTmvmARir4NI5hPQJupNG5ty9U6nzB8RPUGgZwIN4lnL/+hDKYRDWwv2kvdMbGBbbFE5AkmO9qdG3KI+34CRQ8LI01O3Bel/3+X2ub8jdrDrDVnncqppzfZs6b6wYLZqxVhHvEd9XpNs3nj0dA7Vi98U72zLolfuHdfncdABBGyBQMiixRR0xTwq372LKvftoerv91Jboy6wfHNBDpWv58/n5Kb1Jq3YV+U3ZRoFzZlLnpF6gc4W1jHQc2jIzVVim3lhr1m/8asZ8r4zJBAYCAJZf/8r8RdSUhIacL8pM8l30hTyGz+BfIYNG4gp4BpdENh+qkhRDnCT4I6wRl00R3UXBDxEWKik+GA6l1pE64VTv/uXJVFEgM7NfhddHL7aroWzt7dkUkNTC7m7a+i6RSMpyNe5b6b6aZ01PpbOpRVSfUMzHThdTHvPFdOckWHqJsiDgN0QcHV3JxbS+NNUVEhlu3dT1b7dVHvyqLKG1vo6qjn0nfQpePs18p0yg/xnz6Xg2XNI4++vtHOWTM1pndaM16tN1DsLcpb1W3qdrlovZcj2hjoljwwIWJNA6DXXUUF5OTXlXSYSmrPq7/kLqj3EGxa0KaPIK2UExdxzL7kHBFpzGhjbBIGNx/QOLNgMLTDQj2qaTDREVY8EWHvGwhmn45cqaOmEiB77OHIDt6dEsscFrhOR2D/erttrtnxeCg2NDbLHZVhtzp7CvLGqrpnyi3Tf8NY1t9PSic79sFsNNgYeUAJuPr7kO2o0hSxbQX4zZpObXyC1CJPG1qoK/TzaWqkpJ4uqv9tDZRvXU11mJrW3tZPkEt3FOfYEFH6yhhoupUlMIsQLnpunXrjQg0KutwTaxYtx+d5dUvP2lmaKXHU78V4+JBCwJgHvpCQKv+4G8p8xhzziEsnFw4tay8qovbmJWkqLqeHiOSpa+xFVfPcdtbW1iS+iAkjj52fNKWFsQaC0upn+uuaswiI2wodGJkcrZWT6RsDb25O+P66zhAsRWrPZI0P6NoCDtbbLvyyl1Y20WmjNOMXHBNP4ZJ2nF6kC/ygEJoyIpKOncqld/LdXeLQ8mFpO05IhxCqAkLF7Ar7DRxB/Yu+7n8q/20eVe3Vmjy2V5craWoTQVrFlg/TJC48i/5mzKUBo1AImO/b+1MasTImBJjiM3MULG1L/CLh5aQ0GaCopJs+YWIM6FEDAWgR8hg8n/tCNN1FLVRWVid93VcLMu+bQfklQa0g7T3mviI+YgM+k6eQvTLtD5i+ARs1KN+RCnuEe56hQBJ3uD2pvoVAIC/Gl4tIaOpKm+qK1P4PacV+7FM6+OVJIxeW6PSfzpibaMX7rTj0iyJtGpYTTmYu6yPWffZ8L4cy6yDH6IBIImjWb+NNSUyM0HHuocs8uycSxvVUXf4an1lyUT6VfrpM+2iHJ5CMEtZDFS8g7IWEQZ26dS7dU6v7AeakCKFvnSs4xqquXoeaxVTxnSCAwGATYTDt82XLp01hQIP2+q9q/l2qPH5amU3v0APGn6D9vCtPuKyhgzhXS78bBmKujXvNCbq3B0sJCfQ3KKPSdQFSEvyScXcqrpkIRosCZ953ZpVnjs59eoIrqJoqNEpHYJ8X1/Qlwoh5enh506qIuOHdmfi1NGR5MUUGGLxlOhANLdQICrh4eYqN8MgWL/WmBC5eQJjRCOBBppOZi3ZcUMoKWijKqO3WcSr/+nOozhYm0m3A3H+84QlrB6neIze98xk4g36HD5GXjaCYBF2EOW757J7W3tkgjBM5ZIDRnCOptJk50sxABjW+HmffS5RQ4dz65hYZTe4P4fVdSRO1NjdSQfpEqdmyh8p07qVHEjHTz8yePEOc2GbME+jV7cykrX/8FzawJCeQPvwf9QssO7FIzS6Qxhsf5UXKU8wq8dqc523i0gFiq5pQsgi4jdU8gMcqfkhJCKSNL98D/77s8mjgEG4e7p4azjkLAS7w8R626RfpUnzlNlcIUqPr776ghM02/RLFPo3LXVulTOGwEBcxfJLRpPyCPMPt1oMMut1vrdd/suoeE6teKXL8IuIgvu6hJZ7VBwlwcCQRsiYA2aSjxh27/EdWcPU3lu3ZSlbAgaCrMo8bLGVTMnzXvkc/kGRS0cDGFLVlK5Gr3EZUG5Racu2Ro1tjmAo79vREJ0fp30/0Xy2j5JOf1uGx3wtn6w/pvv0cMwUtHb34YRibphTMO8nfz3FgaG+943uty3nmLij9aTcQOH8SH/+P9dqKg+1+qN/wF6sK/UMVpqT2fl5Kuv5zVNRD/ukoNpba6UTvaS9VyH75mxyjyPDqO0rzaxVn5OuLa/G28XJbGlMvSUZzioaT2unG5vdSno17uL12Tz8ntpT7cVT8fHkduz3nu48p/mPnDZem8Li+3ldp3nJfqRN6Fy5w68tK8uY7L0rjiOh1lXTsX3XXEOR5D3U66fked7vodfTvaSuN0/NHjPH8MrydekUVbXbuO9Unz0M9Hnitf24tfXpKTqUE4CKk7f47q0y5Sa7nuiwtuVy/2bfCn+MN3yWv0ePKfNIWCFy4Ugpp97WttrdDb7HuE29fcpftlo/+wMwYinZMlN1/sMbHR24RpCQK+o8ZIH3rwYREHcjuVb9sqHCTtktjUHvme+FP433cpcP5CClm+QucsCeR6RYCdgRSV1xu09XCT/voa1KHQNwKBPh7k4aGhJuGFPT1Xr5Xs2yiO0dquhLPDGeV06JzuRWpIfAhc5/fyGRwWG0xu4oW1VWgIOG08UuCQwllD5iUdERaAxIdf4tXJuMznTNWp+yDvnATYLX/t4f3Sp/DdNynyoUcpdNEPyM3HPl7Imzv2m/Hd87QzwdKWnzh2CtLcMUE3H3jEs+V7hbnpCYSwACY+zeJLm1IhpFXu2Ep1505Rc2EuFa99X/r4z55PwSuuoqAZM/UdkTNJIC1fZ72lPqnRuKmLyJtJwMfbQxLOymrk37RmDmTn3Tq+ArePVaw/pNs7xbMdNdR+TY4GmraPVkOJIsCfnI46qCccr8QhFLBgibxMHEHAIgTamhsp7+Xn6eS1yyjz+b9bZExrD9IsQgvIySMcITRkFv09ysHPeRyNnQjq/V0z+jsOAffAQIq8/gYa/q/XKfGv/6DARcvIxV2Y6opUtW8nZf7+l3T+x3dRwbpPhUdI/e8QxyFgmZU0NOudTMkjajR29TotT9vmjn6+Op8IlVWNNje3gZyQ3WjOskvqaOtRfcC/kTBp7NNzMkwIZ+kdGy15z95FsZE1xYE2W5698zZqzMnsExM0BoE+ERCa5/Jv11Pir37dp26D3Vgy+xzsSTjI9ZvLdJYbHN+MHc8gOQgB8bPN+zRFoDDh8KVV92kX+RbxEi5iJra36ur5fBt7fxVOYaQ60YbzbS3iKLUXY3Dbjnx7x3jEfTivjC/6izzX8zjSuHx9Hk/UK3MRdTyW1JfzIlaj5H2Wx+NrSHNp1c1JHo+vIeV16zEYz6ANr0PERQsKofa6GmptqBfrbaH6S6lU/++XKP/1lzvM03UaIckRjugfeutdFHfvfQ5y481bRl2jYGeUIJwZATGz6O/jKfVsFc96VX0L+QvlgjMmu1n112KvGduhckoWkcQ98C1Fn57XYXHBtFnV48CFMocSzvznzhMbnTNVK0S2VwSEuavYqSX2bYntYK66P8LSPjyxv06/f4xt6blBx94uzvP+O1GW6nkMznecb+cRRZ10nus7xuK20r4xUafeJya3k67XMYY8Hu9P4/O6/Wgde9pE2VV85H7SHjbRjk1UpfG5Dydx5LK6Xtryx+dVbaixQfJi1lJaQq3Cw1mreFExldz8AijizntNnbK5OldP3R84nlh9bg55Q8vT73vUWFoqXnp1L2W2HtA7W+y/5VTy0Wrdz4H42VP/jPNeT93PFP8cdfyscgdRr+zD5Tb8M9vxM6n8zPBeWf756ajX/XzpxlH/3Ep5bsdJPUaH8CB6CMFD9OayOLqwKXpHXlTohA8eXNTpynyehRKu5NM6IYTHEQXRjk3Z9eOISul/Pqe7jjjL7aTm+nGk8aVa/GNAgO9Hh/Cnrm9MS1UXnTLf0Kx7jtSLd3freNbVlcj3mYDa42WJcKcP4azPCAe2Q4bKZWlcFAKq9pV+gNhoGRkeQAVFOlOFIxkVdMf8+L4OY7PtY3/8E+IPEgj0hkDFge+p5vgxqj19kurOnuyyi3tEDAWIvWYhCxaSN3tBs5PkHhikzLQhJ5u8RaBupP4RaCrSO6Ny8dT2bzAr9u5kRcBCTMf1pJdtke/8vb8VJ4ShHYaAuwhLkvD4Ew6zHnMX0tDU+ScIwpm5NA37+Yt3VTmVVDdSUqR97POW52ypo91oztIK9AH/ggNs9w+jpW6MNcYZlhCsCGfHL5Zb4xIYEwRslkBdVpaI9bOdqnbvJANX+iZm7DN2IgWK4NShP1hKai2UiaY2WaUJ0H+B1ZiRTi7jJlC70BAimU+gqUhvVs+OQVz9bNMhiGxFIGm3JA2XTtMllyVtcoen2HahLeeyi1uH1py15yIveablc6yhlspCK8DtxKddfCQvq9yH61jzJh31baXxDPrqrsN9peuxNr1j3HbRXxmPb0+HpkwyDZS1Zh3Htg4TQNaeSaZ7HRo2pY9k7qc7p5yXxxBt26Q8a9nEh/vK43YcdVo40Z/nIY2tE2slzZsoSzw66uU6qR23F0mpEyNIealSOsE5gzG5n669TgvD11Zfl8/J4ynX5RYd19c15j6s3+wYm0tCg6hL3Jabd5zngkjymLpzumvr2ot/BT8eT+KiVOozgYuWkHsovGSb0pw1in1oXh66nyM9MeT6SsC/Y8+Zrp/0ZPd1CIdobxfCWbWwO80vUgln/hDOzHn6kmKDaO+hS1JXDvZ3MLWcpiXrv2E3Z0z0AQFbJtDW3ExlO7ZThRDIqr/fw28mXU7XQ2jJ/GbPocA588h//Pgu29nDCQ5Myy+/rCmpS08jF60Wwlk/b1xjsV44c4+Kkkz1+jmkVbrDisAqWB1y0DoRUqTq0EGqOXKIak8cFTJZk8E62SOpz4TJ5Dt5CgVMmUpeMbEG5521UN9kJNQKEHUNLRDOLPBANDfrti/xUJ7uzmsqahfCWXqB4R6QYH+dNxcLPAdONUREkKF6eP/FUghnTvUEOM9iOeB0xa4dVCkCsDYX5Xe5cH758Js2k/znXEHBV8yTvtXvsrGdnXDz9aeWynJqyr5ErlovatOHPrOzldjGdJtUwhnHkEICAXskUJuWRuXiyyoWyOrPn+60BI+IaBGkehr5CYEsaOo0csV+1U6MRsb6dqqrbWimYH/9Xt9ODVDRKwJVdfovCIJ83XvVxxEb2YVwlpqv15qxm032RYDUdwLuGhcKEIJtZZXOvOngeWHaeGXfx0EPELBFAk0ihg+bLVaKF4/aE0e6nCILZL5TppP/9JkUOHMWafwdLyA7L97VX5g2CuGMU+WpU+QDcySJhbn/NJcWK119J0xU8siAgK0TaKmupjLxZVXF7l1S8Gnj+boFhUpfUgXOmk1B4sNmqkhdE0g24em6odG543J1TatvZ2rr9Bz9tBDO+kZvgFunqjRngTBp7Bf9kCBfRThLy6mi5tZ2ckdk+34xRefBJdBcVkaFn/+Pyjd8RS0VZSYn4yr2CPlNn+3wApl68RohnMnfQVYdPUw+S5apTyPfBwLNNdXUUq1zpuSm9SafUaP70BtNQWBwCFQcPEgVwnqget8uSYuunoVGeJ/1ZauBGbMoSJhz2+PeWvV6BjIfG6IlX+G4oqZW/g0rzBrr9ULFQM7F0a5Vo9KcBXhDOLPp+5up0pxBOOvfrQoP9qaMLP0YFTVNFBYAVbyeCHL2QqC1poby167pVijzHjmWAubOp6AFC8jTyYIxB4i9c3VnTki3s054pXT54fXUXqu3QrCX+2wL86w6elSZhnbIMHJBjDOFBzK2RaDu8mXJbLFaCGX1aecNJufm4UXewmogUJhxO7LVgMGirVQYJkwbj4uQRHKqE2aNSP0nIAtnXp5uTm0lZxdmjZdUwllwAPab9efxDzPad1YuVMgQzvpDFH0HmkBbYyPlffQBlX/zhUlNmUaY6LDHusB5CyjAic3P+OUr/41XpNvTVJhHdQX5pPVzTBNOaz+DVSePKZfQQmumsEDGNgjw78SyXTulPbZV7PiIvU+qks+kaeQ/U5gszpnrdF9SqTBYNDsswsdAOKuohjdcSwCurWuUhvFzYq0ZA7AL4axWpS4Oglljv57/iBBDpyAVNfi2p19A0XngCIgXjrw1H1LpZ590EsrYM6Hv1FlCSzaPQubNFw4w4NHVKy6OvJJSqCHjonSPai9eIO3kqQN3vxzkSk3lZdSQnamsxnfSFCWPDAgMJoGac2eoXHij5X22zcUFBlPxHjGG/GYJ77Oz55J3YqLBORT6T2CIUfytS9msRRva/4GdfITaDrNGjs3rzMkuhDMvLw3JmwS1ns5rg2qJBzU8yNtgmLIa3bcUBpUogICNEch65Z8iPtkOaikvNZiZ9+jx5D9rLgXNvUK4eY4xOIcCiU3+MxThrO6CMHGCcNbnx6LymN6kkR0l+I4c2ecx0AEELEWgWTg+Kt22lSp3bVfMluWx3UU4EH/xuzBYfEEFj6IyFescjZ2CVFTVU2llA4XAuqtfwOvqdfv4wgKde7uNXQhn3p564axfdx2dJQIcyb6Zg3mKVF4LzZkEAv/YLIGct9+ksi/XKfNzj4zVmSyKTex+o+HSXAFjIuM3aTIVf/xf6Qx7sKwcP4kCRsOZhQlUXVbVnj+nnNMOGyG8ewovmEggMMAEKo8ekbRkVXt2UGuHcxp5Cn4zxBdUCxdTsNhb68KBxJGsTmB8YgClxPvTxctVyrVShfYsJCBaKSPTNwKnM0qUDhOSnPv3rF0IZ7wxEMlyBFzdBc8O4awSwpnlwGIkqxAIXriIak8eJ01wCIVetVIKhmqVCzngoAEiVpFX4jBqyEyTVle+bzeEsz7c54aiIqrPSld6BC78gZJHBgQGgkD5vr1UtPajTloy3lsbsGARhSxeSj7Dhw/EVHANIwIzRwQbCGdZeRU0YwyEMyNMvS6eS9eHK5mQGNjrfo7Y0C6EMy2EM4s+exphmiMbM5ZBOLMoWwxmeQLeSUNp+CuvWX5gJxkxaMXVlP/aP6XV1qdfoLLv9lKw2IuC1DOBmnNnlUaa4DAKXb5CKSMDAgNBwFgwY+ceAcK5R8jiJQ4bo3EguFriGrNHhtJ732YqQ+UV6MJtKBXI9IlAVm651N5d40qjhVbSmZNdCGds1ohkOQL84MsJmjOZBI4g4JgEwq+6mko/X0dN+dnSAsv376PAKdPIFe7ge7zhdRf0Jo1BS1eQxte3xz5oAAKWJBB6zXXExl6+02dKzo68YuMsOTzG6gcBY9PGeuFOPz23kobGOLdJnjlI2aSxqalF6jp6SKDTx9/Vv6WbQ3OA+nh7wazRkqg1KuGsApozS6LFWCBgcwQ4uGzQ8quUeTUV5VOpiIGE1D2BUmFOVis0jZzcPL2gNeseF85aiUDIosWS5UDMbXcQBDMrQe7HsGzaqE7Z0J6pcfQ6n35Z7+xrZLxfr/s5akO7EM58oDmz6POn0eg1kUE+8H5pUbgYDARskED41VeTW0i4MrOK/XupsUS/+Vo5gYxEoLGsjMp2bFFo+C9aJryBxiplZEAABECACbBpozqlZuL3qppHb/OZORVK01FxEM7sQjjz89YLE8rdQ8ZsAu3t+gCV8aGIB2U2SHQEATshwB4Gg1Xasxbh7a3oq8/tZPYDP82SrZuJGckpdPlyOYsjCIAACCgEZNNGuaK4tIYyhGkjUu8JnM8qF+GydJ4QosN9aMEY/ReJvR/FsVrahXAWHaSPd1DdcQMd6zYM7Go4HoecYiCcyShwBAGHJhB9y23EgWnlVHPhDBVu2iAXcewgUHniOFUdOaDw8J+NmFEKDGRAAAQ6EVg53dBD48UsvYlep8ao6ETgokrbuHxKpNPvN2NAdiGcTR0apNzMmo7o4UoFMn0iUNfUSo2Nuk2X3DEh1DAodZ8GQ2MQAAG7IeDq5UUxj/yM3Hz0JiNlO76lypMn7GYN1p5oW1OTCPCrN2fk6wUvg4dGa3PH+CBgzwRunBUjnIDof6+mXy6htnZ7XtHAzT01u5xOX8iXLqj1cqerhHCGZCfC2YhYP/LW6vZGVdfKTuBx+8whUFqh15px/7hQH3OGQR8QAAE7JOA7YiRFPfiowcyL1n+J/WcdRIq2bKbGwlyFT8i1N1LQrNlKGRkQAAEQMEXgh0JAk1NlVQOdz4T2TObR3fH7EznK6SVTIig6yEspO3PGLjRnfIOGxeu0Z9Cc9e9xLausUwbw9fGgED84BFGAIAMCTkAgTMTqCrnhFmWlLRVl2H8maLA5Y/nubQoX7dDhFC80jUggAAIg0BMBY+1ZahYcg/TE7OC5fMrO08U247ZXTobWTGZmN8JZQqSvNOdamDXK986sY7n4RkdOkcH4hkJmgSMIOBOB+AcfJp8pM5Ul8/6z7Pf+o5SdLVN99gzlr33fYNkxj/7coIwCCIAACHRHQK09y7hcRlW1Td01d+pztfUtdOiELvYmg5g9LpzYuQqSjoDdCGfhATqnILkihgRsec1/fNWas5gw7DcznyR6goB9E4h75DHyiI5XFlFz9qRTCmjVFy9Q/sfvU3trq8Ii7JY7yW/MWKWMDAiAAAj0RECtPeOA1PtP6k32eurrbOf3n8omNv/kxILILXMRqkSC0fGP3QhnQ0J1wll7eztl5Vep14B8HwiUV+r3nCWFw41+H9ChKQg4FAFtbBwl/uEpcg/Tm5I4m4BWm5FBBWvep9ZGvUUBe7SM/fFPHOpeYzEgAAIDQ0CtPTt6Kpeyi2oG5sJ2dBVmcui4XnC9e8VQmjpM7/jPjpZitanajXA2OyVQgXC5QB+sTqlEplcE1G70F4xFLIleQUMjEHBQAj7Dh1PS354nTYD+D6OzCGj1OdmU99F71FJn+PI0/NU3HPRuY1kgAALWJsDas5ljw6TLtFM7HTylF0KsfW17Gf/AyWxBRufOctLwYPrJDxLtZeoDNk+7Ec68Pd0oIdpfApNdAM2ZOU9IsfDUKLvRnyqi2g+P1u3jM2cs9AEBEHAMAt5Dkijl3++Qm7f+94EsoLU2OqZ33Ib8PMr5YLVBoGm+m8Neft0xbipWAQIgMGgEHlgyhNgtPKcL6UXw3Ki6E9+fyqOLGcVSjae7Kz24Ikl1FlmZgN0IZzzhSSnB0rwv55RRWg60Z/JN7O3xTLruB4LbL5qg+2ant33RDgRAwHEJeEZE0KgPPyEXdw9lkSygXX7jNapNT1fqHCHDXhmzV79NLeWGrq5Hffg/7DNzhBuMNYDAIBPg8E/3Lk1UZvH9SX14DqXSCTPns8pp+/40ZeX3LE+icfFwAqIAUWXsSjhbNFonnPH8T1woUC0D2d4QOCe+weEU4OdJPxgX0ZsuaAMCIOAkBDT+ATTu628NVtuQm0XZ//k3le7eZVBvr4XCDd8IU0ahMRPhA+TEe+7Gfr6BPCP1e+/kcziCAAiAgDkE7pgfr5g35omtOHtVe6zMGc/e+7Dl1mebTynLYO+Mdy1IUMrIGBKwK+GMNwyyYMGJVcWXC6sNV4NSlwSyi6qpvEIX42y+0Jr5erl12RYnQAAEnJOAq7s7Tdy2j3zGTlQAtLe0UNH6zyn34w+ptdZwf5bSyMYzDQUFdPntN6hs11aDmfpMmk6jPlhLLJgigQAIgIAlCajNG3cfzKBzl5wz9llLG9F/Pz+moB0W609/vW2UUkamMwG7Es54+mOT9I5BTlzI77wi1JgkcEEVrX5Rx2ZVkw1RCQIg4PQEUl76F4X/6McGHKqOHaIsYeZYfeG8Qb2tFyqOHqbst4V5Zuo5g6kGLb2KUp5/kVw1GoN6FEAABEDAEgSMzRs/33KWCst1X5JbYnx7GeOtdYepsblFmm6gvye98pNx5OUBBUF398/uhLOrp+g9DJ69UCgedL1r+O4W6uznUjN139iMSAig6cl681Bn54L1gwAImCYQc+fdFP/HPxucbCwUjjT+8zoVfPE/airXmwYaNLKhQsFXX4jg0h8Ixx+GTqRCfngTJT7xWxuaKaYCAiDgiATYvPGWRfp4kms3nKLmFp2nQkdcr/GaVn91UrHacnFxobcfmUQhHRZwxm1R1hNwe0okfdH2c4nhPnT4UhUVlNZLjjhd3VxpWJzeDbTtr2DgZ3heaM2OncmTLnzjvDiaMESvfRz42eCKIAAC9kLAO3EI+U6ZTjWnTlNrld4JU0POZao+fpTaWttIG59ALq629T0fa8vyP/mYas6eMEDtpvWm0Jtvp7j7HzKoRwEEQAAErEVgRkoIlda30PmsKmpqbqVMEat3wnDH3/f/1mdHqbBI/8XY6sen0dBIvVdga/F2hHHtTjhj6H5aV9p6TOfcoqyyjkYlR0BF2sXT2CS+oflq53mqq2uisCAtPXXLKHLX2NaLVBdTRzUIgIANEPAMD6eA2XOoLjOLmvNzlBm1NTVRXfpFqjlzlsjDnbTRMcq5wcpUX7xAhV98RmW7t3Vykx+4aBnF/fI3FLpo8WBND9cFARBwUgJzRoZQXmUjpeZUU3VNA5VWNdCIIaEOS4MFs+IS/R7lV4XGbLyw3ELqHQG7FM7U2rNW8c2tu4eGEqNx003d8h2Hsyi1I6bEjfNiadaIEFPNUAcCIAACXRLQ+PhSyA+WCiFMSw2XMqitQW9O3lpTJQS0U1SfmSkCOteRm58fabTaLseyxgmOW1a44Wsq3vAlNZfqQ4bwtbxHj6foh39O0bfdQR7BMOm2Bn+MCQIg0DOB+WPCKKOkni7l11BxWS3xLqwh0Y5lyVRQWktrNp1WBLMUIZCt++1MSgjz7hkQWigEXNpFUkp2lNl5uoh+/Y7eLeeK+SNoQop+P5odLcVqU83Iq6SPv9GZ9cQLVfI7j0wmf29sfrcacAwMAk5AoKmwkPLWfEDlX3/W5Wp9UkaR76jR5Dd6DLlb0RNiU0U5le3bSxX791B7c5PBfNxDIyj0xlso8oYbDepRAAEQAIHBJHDnS4eFiWOlNIWIMBET7YcTB3M6Frv2pv0ZdPSU3rriziWJ9NDyoRYb35kGslvhjG/SI2+dpINndd+Semjc6LaV4ykqFPas8gP8369PUU5+uVT8811jaMl4x7dxlteOIwiAgHUJcDDn4o8/ouqD+7q9kPeQZPIbN578x08g1sBZItVmZFDl0UNUdfxIJ6GMx2eHHxE330qeYWGWuBzGAAEQAAGLErj1hYOUnqsLB6X1cqcblo2luHDL/H606ER7MdjJtGLaeySTKir1FhW/u3UkXTM1uhe90cQUAbsWzmoaWunB10/QRRF1nFNosA/95IbJptbpdHW7j2XT3kOXpHWvnBVDv79xhNMxwIJBAASsT6Bk8yYqXreWGjIu3dBnCwAAMNxJREFU9ngxjbcvufkHknug+AQFkSYoWOSDJHNDjSizw45W4VmxpaaGWqqqqLmmmlpFvlXkW0SMtdbqamrMy6bWxgaDa7l5eJH3hMnkO3UaBU6fQV4xsQbnUQABEAABWyJQUNFAf/jgDJ1K1ztaWjBzqAhcPfh7d3vLqaKmkXYJoeyM8JyuTk/fOYaWToAyQM2kr3m7Fs54sbmlDXTz3/ZTs9h7xmlYYijdtGSUlHfWf3KLa+ijr45LTKKFd8u3H54sXJe6OysOrBsEQGAACJRs3UJlWzZT7eH9A3A1EoGjAylgyXIKnjdfmFCOGZBr4iIgAAIgYCkCza3t9Na3GfTet5nKkJPGxNKyWUlK2RYzlwur6UxaEZ1PL6L6hmZlinFi+8wvf5hMM1Kwt1eBYmbG7oUzXvexzCp64OVDCoL5M4bSrHH28+2DMnELZHgH4dpvz1JGVok02p/uGE0rJkVaYGQMAQIgAAI9E2Bzx7JNG6lq51ZqazLUcPXcu/sWmoAgClqxkoIXLiLvJOxl6J4WzoIACNgDgZ2ni2n19iw6d0m3Dy0xLoTGpITRuKG240ehTbxbnhIC2RlhwpiZXdoJ6wJhwvj764QJuxf8GnSCY0aFQwhnvO5PDxbSC2tOKwjuv3kahQR4KWVnyHBgw/9tO6cIZsumR9H/rXJuLaIz3HesEQRskUB9TjaVCpPH+tSLwuQxjVpKi/o8TRd3D/JMSCKvocnkP3kKhcANfp8ZogMIgIDtE8gRXhxf3ZRB248UKJMNDvKhMcnhNFaEiwrw8VDqBzJTUtlAp1IL6ZzQkqn3lMlz8PX2oPtWJNGq2c6pEJE5WProMMIZg/nzZ+n0zZ5MhdFjd8wiH61zSPH1Ta30+dZzlJlTJq0/XMQ0e1PElYgKci4BVbn5yIAACNgUgYa8PKoVglq9iEVWJ47tKnf86ol6xsaRdlgy+aSMIO+UFHL1GJyXEvWckAcBEACBgSDw6Xe59N7WLCou1zvXcHfX0MgOIS0hws/q06gT75M5hVV0WghlF9KLqSun7hNEcO3Hrx1GKVH26cjE6iD7cQGHEs6Yw//bnE0fbNJvTL/lqgkijoR/PxDZftea+mb6bOt5xTMjzxgbMm3/vmGGIAACIAACIAACIKAmkFtWT+9uy6IthwupoYmjoelTfEwwRYb5Sp7Jo0L9KNjfU3/SzFyp0I7lllRRXlEN5RdVU1FRFbV2E2WLvUuumh9LDyy17b1xZuKwiW4OJ5wx1b9+nkFf7tZ5KuTyysWjaEySY0Zir6htkjRm+YU6W2VeLwQzpoAEAiAAAiAAAiAAAvZJgE0d1x8tpI2H8ym/uM7kIvx8vYSw5kdR4hMtPjHhfqTRuFJLSxu1tLVRKx+F45FWkW9paRWO4kRefHKFAJbHH6Ehq6s3jBFp8kKi0lV8rrkijm6dG0vxod5dNUO9BQg4pHDGXJ79MpM+35muIJo2IY5mT4gnrYebUmfvGf6244ttZ6mwpEZZCgQzBQUyIAACIAACIAACIGDXBJqFgPX14QJJSDupcr0/kItaPDmSbpkXR2PiHNsSbSCZdncthxXOeNHPfZMlHGSkKesPCvSmWUJAG59iOx5wlMn1MZORW0nffpdGZeW1Sk8IZgoKZEAABEAABEAABEDAoQjsPVdK64XTELXjEGst0N3NlSaNCKJVc+No1vAQa10G45og4NDCGa/3uQ259L8t5w2WnjwkTAhpcRQj7HbtLTU0t9HeY1l08Hi2wdR/ek0y/Wh+vEEdCiAAAiAAAiAAAiAAAo5FoKSykc7kVNG5nGo6e7mGUkW+rKqx34uMDNHSxOQgmjosiGYKgSzYFzFy+w3VjAEcXjhjJm/tzKO3vzxngEfj6kZTJ8bSxOGRFOjb/w2VBoNbqXA6o4S+O5pFJWV6bRlf6lc3jaAbZsKNqZWwY1gQAAEQAAEQAAEQsGkC2WKP2unLlZSaX0sVtc3Sp0r4JaioaaaauhbivIe7G/n5uAvX/O7i3deD/L01FCTy0UIo4+DRQyN9bHqNzjI5pxDO+GZuOlFKb268RLkqxxlc7+mpodEpkTQxJYIiQmzzoeRvQ/Yeu0ynL+TzlJXEXnpeun8CDY+2Pw2gsghkQAAEQAAEQAAEQAAEQAAEJAJOI5zxaqsa2+jpT9NozxFDk0A+x15oRg6PoAkpUZQQZTsbHg+ey6f9Ry5TbZ2hunrW2HD65z1jeepIIAACIAACIAACIAACIAACDkDAqYQz+X6t3ltAa7dlUFmFPsiffI6PvCdtrNCmDY8PIhcX9ZmBybe1E51KK6IzacWUmV3a6aL3XTmUfrw4sVM9KkAABEAABEAABEAABEAABOyXgFMKZ3y7iqsb6bnP0mn3cUNTQfWt9PXxpISYQEqMCaJRQmBz11hXUisRrvFPiYjs59KLqKLSUHD0EUH/lk2LpJVTo2hErPUjxKs5IA8CIAACIAACIAACIAACIGB9Ak4rnMlod58tpk/35dNBcewuadzcKD42iJITgmlUUpjF4qU1t7TTpfwKOi2EsgvpxdRuFJWdPecsF0LZNVOjKSrIq7sp4hwIgAAIgAAIgAAIgAAIgIAdE3B64Uy+d99fLKPPD+TRThGNvTcpNjpIaNQCyUfrLj6e0tHPW+S9PLrUsNU3tYpI7M1U29BMl/MrKTOvggpEhPamppZOlxwRH0BLp0TQNdOiycfTcQJnd1ooKkAABEAABEAABEAABEAABCQCEM6MHoTjlyqEkJZPm4SgZm7y0LiRl9ZDCGwe1NzSQg1CGKsXQlmrkVbMePypI0Np5ohgmp4cTMOibNNzpPGcUQYBEAABEAABEAABEAABELAMAQhnXXAsqGigYxkVdCyzko5cKKecIsPYYl1063W1VrjwDwnwpEQRU2LReBEUWwT7CxSxJpBAAARAAARAAARAAARAAASckwCEs17e96ziOjqYWk77L5RRQWk9VdU1U3Wt0IqZMEmUh3QTrh592NRRBPnz02poiBDEJiYF0riEAAT6kyHhCAIgAAIgAAIgAAIgAAIgIBGAcNbPB6G5pU2Kwl4uhLWKmiYRdV1EXBeCmD8LZdgr1k+66A4CIAACIAACIAACIAACzkMAwpnz3GusFARAAARAAARAAARAAARAwIYJuNrw3DA1EAABEAABEAABEAABEAABEHAaAhDOnOZWY6EgAAIgAAIgAAIgAAIgAAK2TADCmS3fHcwNBEAABEAABEAABEAABEDAaQhAOHOaW42FggAIgAAIgAAIgAAIgAAI2DIBCGe2fHcwNxAAARAAARAAARAAARAAAachAOHMaW41FgoCIAACIAACIAACIAACIGDLBCCc2fLdwdxAAARAAARAAARAAARAAASchgCEM6e51VgoCIAACIAACIAACIAACICALROAcGbLdwdzAwEQAAEQAAEQAAEQAAEQcBoCEM6c5lZjoSAAAiAAAiAAAiAAAiAAArZMAMKZLd8dzA0EQAAEQAAEQAAEQAAEQMBpCGicZqVYKAiAAAg4CIHvv/+e1q1bJ60mKiqKHn/8cQdZGZYBAiAAAiAAAs5NAMKZc99/rB4EQEAQOHXqFO3cuVNiERcXR9dee61JLiUlJbRmzRrpnKurK/30pz812c7alYcPH6ZPP/1UuszIkSMhnFkbOMYHARAAARAAgQEiAOFsgEDjMiAAArZL4MyZM/TCCy9IE5w/f363wpncjhsPlnA2kCRfffVVeu2116RLzpo1i956662BvLzNXevAgQN0zz33SPPy9PSk7777jry8vAZ8nrYyjwFfOC4IAiAAAg5OAMKZg99gLA8EQAAE+kOgubmZampqpCEKCwv7M5RD9G1tbVV4MJeWlpZBWZetzGNQFo+LggAIgIADE4BDEAe+uVgaCIAACIAACIAACIAACICA/RCAcGY/9wozBQEQAAEQAAEQAAEQAAEQcGACMGt04JuLpYEACAwcATYz+/LLL5ULrly5UjJ5W79+PfGetvLyckpJSaGxY8fSlClTetynVFFRQez44/Tp05SRkUHs+GPZsmU0ZMgQ5RrdZb766iu6dOkSseMSjUZDPj4+FBMTQ2PGjKGIiIguu+bn59P+/fuV8+wsRU7p6en02WefyUWDI8+PP10lXv+JEyfo/PnzlJaWRoGBgTR8+HBpPt3162o8c+vZDJG5Xrx4UbonZWVlVFlZSR4eHhQcHCzNizldffXV0iW4PbOUE/dTJ+bh6+urrpLyPBbvXzROx44do127dpGbm5t0b7RaLYWGhkocEhMTpTrjPly29DzkazQ1NdHRo0clHhcuXCAXFxfpOeVnddq0aV3OR+6PIwiAAAiAgGUJQDizLE+MBgIg4KQEWDj7+c9/rqx+3Lhx9MADD1BqaqpSJ2dYQHr77beJ3eCbSidPnqS77rqLSktLldMs+D377LN09913Sy/zyokuMu+//z4dPHjQ5Nnk5GT6wx/+YFJ44Bd09TrUA/Aeq67OXXfddfTPf/5T3VzJb9u2jR599FFlr5ZyoiPDa/31r39N3t7exqcsVm5vb6dXXnmF3nzzzS7nIV+MhWdZOGtoaOhyzdz+j3/8o9zN4BgSEiIJPQaVorB79+4uObGQ98QTT9Ctt95K7u7uBl0tPQ8enIVtdmpz7tw5g2vJhRkzZtCLL74oCfVyHY4gAAIgAALWJQCzRuvyxeggAAJOSoAFFVOCGeNgbdiVV15JRUVFneiwZoUFA7Vgpm707rvvUmZmprrKZD47O9tkPVfyvO68805JWOmykYVOsHdH9m4oOxUxNezq1avpjjvuIBagrJVefvllSdDobh7ytePj4+WsxY+smewq8dyefPJJ+tGPfkQs7FszsfZw4cKFXQpmfG2Op7dkyZIun0Vrzg9jgwAIgICzEoDmzFnvPNYNAiBgVQLffPONNP4vfvELGj16NNXW1tLHH38suV7nEyx8vffee/SrX/3KYB4vvfSSQfnGG2+UzBn5ZX3Pnj3EGjE5xplBQ6PCQw89JAlE7G2xvr6e2NMim+SxYCinf/zjHzR79myaPHmyXCWZtKnDBWzatIm2bt0qne8u4LUpgYaFz7/85S/K2Jx58MEHaejQodKc1q5dq8yHhYWNGzfSihUrDNpbosDrNtbqLV++nCZMmCCZMrJpIQuGrJ1i80s2t5QTu8tX82CTzNdff10+La3PlCt9U3XcafHixcTmi2xOyB9+DljYZpf8cuI8a1bvv/9+uYosOY+2tjb605/+pIzNmWuuuYY4VAJz4OeMzXE5scDIgu3TTz8tlfEPCIAACICAdQm4iF/E1vuq0rpzx+ggAAIgYBECLDSxWR0n3ifEQpOpxPulli5dqpzKyspS8vyizeaC6rRhwwZJMJPrWFD68Y9/rAS85nre0+Xv7y814b1pauHkvvvuk8wP5f585D1OatNC3q/FAlRvE1+DtVSyZo41eHIcM1NjsLAoCzbjx4832H9lqr26jrVAMks22WPTzGHDhilNmAeb1W3evFmqY36yIKg0skCGr8tmlXJiAfeKK66Qi306suB0yy23KH2Yp6k9Z0qDXmZYkH3ssccUIY3H5LG7Sv2ZBz+XLCTLic09WThTJzb/fOaZZ5QqFp7DwsKUMjIgAAIgAALWIQCzRutwxaggAAJOToA1XqwxUyfeR/Twww+rqxTNEVeqHXFwmbVfxunaa6/tJAQat+muzHP685//rDRRa9KUSgtlvv32W2Uk3kulFsz4BPPgvW9yYnNLa8QNY62lOtXV1amLNpEPDw8ntdaUNVbWiivHmjE58ZcNxoIZn2NT1ISEBLmZ5FxGKSADAiAAAiBgNQIwa7QaWgwMAiDgzATYIYipxM4mWCsi730qKChQmqn3I7EGjz3+GSf2vjhp0qQu97MZt2dPhPySz9dpbGyUtB8sCMiJtX9cz2ZzlkysFVOvh835TCU2h2TnGbImLy8vj0yZSJrq29s6Y40mmwuyFpD3U7FHQvagyaaNA5nY1JTvCd8bvkfsvZLXrX422Etnd541zZ0vjysntaZWruMje/icOnUqydrhnJwciZW6DfIgAAIgAAKWJwDhzPJMMSIIgAAIdOmJkV2V854jWWOVm5ur0FI78YiNjVXqjTPs6r27xNbqO3bsoH//+99demxU92eTTEsLZyxkqdO9996rLhrkZcGMK1mgs7RwxgIxm2/K+6j4OuzWnz9yYmGY52iuuaM8Tk9H9ozIJoNdhSRQ92eh2RpJ7ajm97//vcEeOvX11M+j+jlVt0EeBEAABEDAsgQgnFmWJ0YDARAAAYmAsSt0NZagoCClyPHM5KQWUrpyKMFtOUZWd4nN49Qmct21tdY5WTMoj9+Vu3b5vHxkzaClEwvE//rXvyQvmOwERS2cyNfauXOntBeQBTneY2dpAZGvw8KhKVNVeQ4DdVQ/Z3yfenNvmCESCIAACICA9QlAOLM+Y1wBBEDAxgmohR1joUI9dTZFs0QqKSlRhuEAxHJSm7AZ75OS2/R03Lt3r4FgxvuGWGvEnhZZO1ZVVSU5mvj88897GqrT+b64d4+LizPoz/Ng88WekilTzp769OY8C33soZE/7L2R3cQzq3379ikmpjwOO77gfYHqwNPdjd9bn1oshKsFMzZf5P2DSUlJ5OfnJ3mKZBNC9tJoTurtPHhsjrMna265zGayPSVrCKs9XRPnQQAEQMAZCUA4c8a7jjWDAAgYEFAHg7506ZLBOXVBbapnvI9J3Y7z7K7cVGKHF2pzMbWJotqUsbt5dPcizkGO5cRBhNkzoYeHh1wlHdkLYG+FM7W5o3oPmcGAJgrsgVK9l4y9A6q9HJroMmBVKSkpUsgAjifG9+PIkSOSZ0LZzJGPvBdMLSzLk1Pz4DrWQrFw1VNSO0dhwWzLli0UHR1t0I3nwp5Du/uCQO5g7jy4P69fFs7YGQh7a0QCARAAARCwDQKWtx+xjXVhFiAAAiDQawJqoYhftll7Yip98cUXSjW/4HaX1AKYuh3vBVO/fKtf0NXzYM1OV2PIThrU48p5ds0vJ45hZiyY8bldu3bJTXo8qjUmzIYdQ/Q2saMNOT3//POkFm7l+sE+suOL6dOnG7iW5zl1JYiq7xG346DhvUnqtbPmSn3f5f4sFKqfDbne1NHcefBY6jhuHGaANYhIIAACIAACtkEAwplt3AfMAgRAYBAJsPaKtRly4sDQ6hhT7HmQA/GqtR9deWOUx+D2xnubONiw2o09a+w4ELKcjD0asvt5NkNUJ94b1V0QavV+tnXr1pGxKSJrbP7+97+rh+zWfb2xeSLHWDMlHJpyga8OG8CC3apVq4hjbJlqyyajpuoNJmpmgb0hskOLrsa/fPky/fe//zUYPTIy0qAsF4xjffH9PHjwoHxaOTJ3tYZTba5pSvBmTSnHfFMnfu66SubOg8fj+6B+3m+77TbJnLK6urrT5ZgZB+dGAgEQAAEQGBgCCEI9MJxxFRAAARsn8OGHH9Lvfvc7g1myWR67nTd2mMD1rG3w9vZW2psKQs0neT8PCzhlZWWkji/F51hI4hdldWLveR988IFSxS/RbJ7IDkIuXLjQSeAzDkLNHhqfffZZpT/PdcGCBdLLOGsEZXM2431HLCjedddd9MADDyh9OcMv51dddVUnBryHjAUEfnFnLRMLX6aENo5jxqaV6sRrGjp0qGT2yDHHuB+PwcIku2+3dGLB649//KM0LK+TmfC9Y2+IrAnkuasTu9lnjVJXTjCeeuopevfdd9VdpDGZCQtlHFCa16MOQs6mk9ddd51BHxbGmQNrOzmoNCfj+8KsWAPKHh6NkznzkMf4+uuvO8Xc43NsrstfVvA62NU/f8HAX1aoBW15DBxBAARAAAQsTwDCmeWZYkQQAAE7JMBCCAsnxgKUqaW89957xK7X1clYOJs7d263Y3GMrVdffbWT2SHvdbrzzjs7CUPqa91xxx2KwGMsnLEjERam1LGs1H05z3NnQYLXoU633367tPdKXcd5Nt1j5xU9JW6n1hBxezbTe/rpp2nt2rU9dacXX3yRrr/++h7b9bXB3/72ty7dxRuPxYLbxo0bTe43k9uyhmnevHmdhDr5vHx86623pFhqcvlnP/tZt3v9WJC/++676ZFHHpG7SEe+V+q9hPJJc+ch9+f5/eUvf5GLXR45oPoLL7zQ5XmcAAEQAAEQsBwBmDVajiVGAgEQsGMCvPeINSz8Eqp2EKJeEgs9bJJmLJip28h5fsn+xS9+IXnjk+v4yC//v/71r+mNN97oJJjxeXZCwc46WFBSm57xOdaq8PxWrlzJRZPJx8eH1qxZQ7feeqvJ81zPbuXVWj+TDVWVEydOlBxYsEDZXWLB0jjxGp577jni/XossBqvSd2etYvWSL3xfMn3nO8Xe2k05QhEPS92ALJt2zZJiO5uPcY8WEjke2+qDz9T7MLf1Dn1tdV5c+chj3HfffdJz/MNN9wgPZdyvfHRWLNofB5lEAABEAAByxGA5sxyLDESCICAAxGQze143w+/rLN5Y1dmbrxsY82ZWrvGL7e8p4ida/A4vU28Z4mdgvCeqSFDhigv7jwnHpMFLP6wYGkqySaH7MZdq9VK5pUsvHHiMXmfFzsMkT8cm627NXI/1jCyVo61Nuyenj8sJLCDi+5is3FfObEQxmZ/fH2ee2BgoNTflPMSuU9/j8yCnXLwvJkfr5Ovx3v0+Pp9EYrUc+F7xGaZ5eXlSjUz5j1r7LHSVGKGbPooCz3MjoV2TjxPHkt9TzjfU/w3c+ZhPDcWYnn/Hc+BE6+DTRzlZ8a4PcogAAIgAAKWJwDhzPJMMSIIgIATEuhOOHNCHFgyCIAACIAACICAGQRg1mgGNHQBARAAARAAARAAARAAARAAAUsTgHBmaaIYDwRAAARAAARAAARAAARAAATMIADhzAxo6AICIAACIAACIAACIAACIAACliYA4czSRDEeCIAACIAACIAACIAACIAACJhBwLSLLzMGQhcQAAEQcGYC7Onwl7/8pRS8lzkMGzbMmXFg7SAAAiAAAiAAAmYQgLdGM6ChCwiAAAiAAAiAAAiAAAiAAAhYmgDMGi1NFOOBAAiAAAiAAAiAAAiAAAiAgBkEIJyZAQ1dQAAEQAAEQAAEQAAEQAAEQMDSBCCcWZooxgMBEAABEAABEAABEAABEAABMwhAODMDGrqAAAiAAAiAAAiAAAiAAAiAgKUJQDizNFGMBwIgAAIgAAIgAAIgAAIgAAJmEIBwZgY0dAEBELB/AnV1dfTkk0/S6NGj6aGHHqLs7GyrL2rz5s2Su312uf/SSy9Z/Xq4wOAT2L17N1155ZU0Y8YM+uCDDwZ/QpgBCIAACICATROAK32bvj2YHAiAgLUIrF27lp544gll+Ntvv52eeeYZpWyNzMsvv0wvvviiNPSYMWNo/fr11rgMxrQRAi0tLTRt2jQqLS1VZrR//36Kjo5WysiAAAiAAAiAgJoANGdqGsiDAAg4DYGzZ88arPXUqVMG5a4KBw4ckLRtrHGbNGkSNTQ0dNUU9U5OoKyszEAwYxxpaWlOR+XVV19Vfmbuu+8+p1s/FgwCIAACfSEA4awvtNAWBEDAYQhcf/31BmtZtWqVQbmrQmtrK9XU1Egf1oiwdgQJBEwRCA8Pp8WLFyunQkL+f3t3AjJV9cZx/NiulS3/pKLVerMF2qAFssWCSiOypAUK0yTKoKC0aMEwqCgiMKLNCkolW5E2MmwzWqB9oTSLyqK9qCzCUqi/vwPP5dwzd953zrxz3pn3ne+B17nLuWfu/dyZus+cc5/7P3fYYYcV890ysXbt2uI78+OPP3bLYXOcCCCAQFMCGzS1FRshgAACg1xgv/32cy+//LJ78cUX3YEHHuj/BvkhsfsdKHDXXXe5JUuWON3jePzxx7sNNuB/ux14mtglBBBAoGME+L9Ex5wKdgQBBAZaYNddd3XTpk0b6Lfl/bpIYMMNN/QJQbrokDlUBBBAAIF+CBCc9QOPTRFAIK/AU0895TQkSuXwww93Giam8uSTTxbDCY855hi35ZZb+uXKhvjXX3/5aWXHCxMvLF261OkeoHpFw8222267mtUatqj3s/Lpp5/apH9dtGiR22yzzUrLNLP11lu7cePG1SyPF3z88ce+Z+Xbb791I0aMcGPGjHFHHXWU22mnneKqLZvXfU8ffvihb2+TTTZxJ5xwQk3bGn722muvFXUmTJjghg0bVlPPFsjlnXfe8fdY/f777+63337z9XVutthiC3+OTj31VDd8+HDbpOb1o48+csuXL3crVqxwaqOnp8ftsccePtPhpptuWlNfC15//XX3ww8/lNbJcfz48X7Z+++/73tIV65c6UaNGuXbO/HEE3vdj1JjCTOhWdVmstDntV7RkNknnniiWH3SSSe5NWvWOH0PZCJT/aCg+x3Vznrr1d6ZUNWGPsNKPqPPmtrQZ2zfffd1Bx10kNP5ryraD7Wlonsr9b5xUa+zzpOKep9Hjx7tp7///nunxCdWwvs5P//8c6fvTFXZe++9nf4oCCCAQDcLkK2xm88+x45AhwscffTR7osvvvB7ee+997pjjz3WrV692u21117Fns+bN68IgnTRqvvBVJSq/pRTTinq6R6zt99+u5iPJ6677jo3efLkeLFvT+2mFt1f9O6775Y2i7M1nnbaaW727NmlOjZz8803O63PUebPn++uvvpq33TVfmrFq6++6s4666zi7RU0KeiJy+LFi91NN91UnKd4fTivAKMqGFi1apWbNWtWKQgOt9t+++2d7A499NBwsZ8+77zznILyuCgY05DCG2+8MV7lAzQZhMF7TaUmFiiYPfPMM+tuqSBeAVK9okBMwaiVZ5991p1//vnuq6++skXFq35MuPXWW33AWSxcNxG38cILL7jp06e7zz77LKzmp5UxVN8r+cYl/C7NmTPHTZo0Ka7iv1/2Gb/ooov8YyJUST+ETJkypaZ+Xwv0HnovCgIIINDNArU/u3WzBseOAAIdJbDzzjsX+2O9Xt99912xTBP2fLJ//vmnCMy0PNxW851WdA9SvcBM+6pnoX3yySedttul/dEzvHThbwF0aWU0oyCwKjBTT47uxQp7J6NNnXpiTj/9dB8wxuvqzSurZlVgpvoKVO644456m3bMcj12oSow0w6qx/Caa67pc18V7FQFZtpQPZV6BttPP/3UZztUQAABBBAYGAGGNQ6MM++CAAJNCIQ9Gz///LNvwYIxa84uXsNnSWndjjvuaFX8q3pY4gyNeq6Z9bSVKgczG2+8sVMvlhUNCVSPjBX1uFUFHVXLbBu9WkBz3HHH+R4I9Qg+9NBD7s033yyqqWekU4OI//77z11wwQXFvmpCw+Q0/FQ9Mepl0zBIDUtVALbRRhuV6tqMLBV8WVEbEydO9KYaKnnPPffYKh+MqDcpTKqhHhr1qKoooLWycOFCP6nhfxoeqABk7ty5RWr7BQsW+N5Dnd9Wld12283dcMMNpeY01POxxx4rLWt0RolEVKZOnep23313H5yGvYRPP/20u/DCC3sdCqg6KjNmzPDDITXsV58zBXcq+t6o9/myyy7z8634R8Mmw++Mztnzzz/vm9ZnY+bMmZVv0+k/qFTuNAsRQACBFgsQnLUYlOYQQKB1AmFw9ssvv/iGv/7669IbfPnll34+/vXf7k+zyuqdiYsuIPsKzpTQIRxeqIvaMDjT0Mmqe87i96qa1xC48GJewaMCiQ8++MBXtyFjVdu2e5kCqtAuHNbW6L6pFzS0PPvss30Atv766xdNqGfn5JNP9vPqAdIwvfBcjh07tqirniTbJ90zpXvLNBzSgjndzxQOXdVnaocddii27++EAo94WKOCv2aDM+2PjuOAAw7wuyYfBWf6ocGKelf7uk/rmWee8YGZbaN7DM8991w//FDLbrvtNj98cuTIkValX6/63obfGd1PacGZvpfhun69ERsjgAACQ1CAYY1D8KRySAgMFYEwQYf1nFlPmSVWsB4oC9507LpY7S15Raf4KKCJiwIKKwqANFyzE4slXrF9U8/fv//+a7MNvb711ltFPQ17vPLKK10YmGmlEk1oSKOVOCGLLa96veqqq4rATOvjewfjgL6qjXYuUwIVC8xsP5QsJizxjxXhOk0rEIqPWz84qMctLBriSEEAAQQQaL8AwVn7zwF7gAACdQTCRAV2IW3BmD3cV/PKRmfr1ZSGgHV6UW9b2DNo+7v//vvbpH9V0NOJZZdddintlhJLKEmFegLVu6WhjH2VMLDQ8M6qhCNq45BDDimaCrcpFlZMaJu4V0zZEjWc0f6qMhBWNNW2RQpM46LhsspEauXPP/+0ycpXPc+vqmj4aNjjG2e8rNqGZQgggAAC+QUY1pjfmHdAAIEmBcKeM6UpV1EqbhWl99bFpYax6cLSeta0Tvf+dHqplyq/Kj16Jx6L7iG79tpri6yP2kf19GmYog1VVDZAZXxU7416a+JivaBa/uCDDzqlva8qYeDdaHCmVPFxUa/ckUceGS/u2Pltt922ct9SPiPhDxxhY+pZVnBqPWYaekhBAAEEEGi/AD1n7T8H7AECCNQRCC9OdSGvHjK7oFfyAEup/80335SedTUYEgvEw/fqELRtcSNDFHUPlLIs2hDTeGd14a+hikcccYRTZse4/PHHH6VFStdf9Rcme2k0MNFz5gZ7qQpoU4+ptza22mqrojl7XlmxgAkEEEAAgbYI0HPWFnbeFAEEGhHYfPPNS9WUKVFF9yfpocQavqhnl6k3JRyWFQ+5KzXS4hllLRyKpdGLdQ3DvO+++5x6NpUsRX8a1hgGVOpRU2bHl156qXiQuMziHk71hvZVqnrEqrYZCsFZ1XG1cll4n+Y222yT3HR4jhvd2B5s3Wh96iGAAALdJkBw1m1nnONFYJAJ6KG89pwmy2K45557+qOwi3v1pikAsFJvyKCt789rnHpdF6hxENmf9gd6WyUcUYAZJ1CxYaSN7o96OZW50h78reGnygK4aNEi34SGnyoBiLIvWrHzp3kF3I8++mgpgYfVa+a103smmzmmZrap1wOqXujwsRTx/Xnhe/3999/hrJ9Wu9aLXbMyWhB+Z8LvaVSNWQQQQACBdQIMa+RjgAACHS0Q9oKpl0ylp6fHv44ePdq/Kp1+eKEZDof0FVr4T/z8tPfee6+FrQ9MUwqErChoqkrZ//jjj1uVpl7Vqzlr1qzStvF9TQq8rSjIVTBHaa1A+L0IW1Yvpj12QMvj5DThoyjCZ6tZG0uXLrXJPl/DYcY6zxqGTEEAAQQQqBYgOKt2YSkCCHSIQPiL/htvvOH3yoIyu+jTvU12oakL/kbvS2rmEEeNGlXaTEkxwgdH20oN3+rUIY9hwKv91YOu7Xlx6lG55ZZbikQR4fHYtF5VT9vUS/W/atWqmgdox8kplCZe96NZmTNnjk8wUhVQyDJO32/b8VpfQM95s55nq7Vy5UqfzMXmdV7ilP32A4jqKBBbvHixsx40tadnyoVFn4d6Je7JvuSSSyp73Xpro17bLEcAAQSGmgDDGofaGeV4EBhiAuEFvQ2jUpY5FQvObLmWjRkzRi+lcv/99xfD68IV4T0zt99+ux9WF65fuHBhKd241inwO+ecc/x9VppXG8pGqN4oBT0KypRdUMO34of/qn4nFAWwcrUhZnpAsP60/zoeBbpKtW49ldpnZV7Udg8//LA/Vl2gjx8/3h+Ojl09LRreqQts3a9mjzyw41VmzYMPPthm/auGUl5//fWlDIrz5893+tO+6PxqeKLS8mtIq1L1K6ujFb3HxRdfbLNFgK4FCvQeeOCBYp3uiwt7DIsVLZpQMDpt2jS3du3aUouWXVQL5aqHjIdFD9jWdrmKzqceO6H7+RQk/frrr+6VV14pvZ0M7UHdtkJJXpYsWWKzbvr06X5a58W+b3qeoBK4qNx5553+EQV6fpruLwzLPvvs4589aHX1Y4ayZqot/dihoE+fRe2rtR1uzzQCCCDQTQIEZ910tjlWBAahQBic2e7rok5FSUF0wa2LOisWuNm8XpctW+Yv7sNl8bQuDi1YsXX1khfMnDnTZykM31fT4bza0DC++AHA1nY7X3UP0OzZs4sLbtsXuzCWqXpGwgdiq44CsjVr1vjqoVXVsVub9nr33Xe78NEItlznct68ee6KK64o+WtfbH+s7ooVK2zSv+q+OLsPsbRi3Ux8PnP3yijzZBjMxvtj8/H+hs8sszqtfFXPpIIxDV2tGr6q58tNmjSp5i31g4POiwVUVsHOiYas6nyE6xV8Vg1ZVOCn598pEA1L1TlW8Egyl1CJaQQQ6DYBhjV22xnneBEYZAJVF/ThfV/6VT4sVcFZuL4V0+ohUkbCKVOm1PSshe3HSTX0bDArephwO8uECRP8xXcc/Cow07DG0LhqP/t6+LG2UW/Z5MmTne5fGzt2bFUzftm4ceN8JkcNd7PAu6qygsDcQVbV+w7Usng4bm9p8BvdJ/XyzpgxoyYzps7z5Zdf7ubOnevCz6W1q4DqkUcecWeccYYtKl4nTpzopk6d6vRQ70aLHqj93HPPOQWDvZX4O9NbXdYhgAACQ1Fg2Lpx/EMzD/RQPFscEwIIdJyA/hOqHgANvbOiHj0FlSNHjrRFHf1qwzCHDx/uFNzqYl3Hpd4nBQia15+mw+FvCpT0CAMdu3rULLjQRbueoaXjt2UpABoaqB4YDY9UVsARI0b4YZMKKCi9C+g8hIlW1Pul4FdFwa3uE9Rw0TDhh1/Zyz+6r1Db6bzoc233XWoo5+rVq4vPh31G4syfcdP63GhIqgJ8fT70px88lJSk3T9axPvKPAIIIDDQAgRnAy3O+yGAAAIIIJBJoLfgLNNb0iwCCCCAQAsFGNbYQkyaQgABBBBAAAEEEEAAAQSaFSA4a1aO7RBAAAEEEEAAAQQQQACBFgoQnLUQk6YQQAABBBBAAAEEEEAAgWYFCM6alWM7BBBAAAEEEEAAAQQQQKCFAjznrIWYNIUAAggggEA7BZRR89JLL/UPQ9d+9PT0tHN3eG8EEEAAgUQBsjUmglEdAQQQQAABBBBAAAEEEMghwLDGHKq0iQACCCCAAAIIIIAAAggkChCcJYJRHQEEEEAAAQQQQAABBBDIIUBwlkOVNhFAAAEEEEAAAQQQQACBRAGCs0QwqiOAAAIIIIAAAggggAACOQQIznKo0iYCCCCAAAIIIIAAAgggkChAcJYIRnUEEEAAAQQQQAABBBBAIIcAwVkOVdpEAAEEEEAAAQQQQAABBBIFCM4SwaiOAAIIIIAAAggggAACCOQQIDjLoUqbCCCAAAIIIIAAAggggECiAMFZIhjVEUAAAQQQQAABBBBAAIEcAgRnOVRpEwEEEEAAAQQQQAABBBBIFCA4SwSjOgIIIIAAAggggAACCCCQQ4DgLIcqbSKAAAIIIIAAAggggAACiQIEZ4lgVEcAAQQQQAABBBBAAAEEcggQnOVQpU0EEEAAAQQQQAABBBBAIFGA4CwRjOoIIIAAAggggAACCCCAQA4BgrMcqrSJAAIIIIAAAggggAACCCQKEJwlglEdAQQQQAABBBBAAAEEEMghQHCWQ5U2EUAAAQQQQAABBBBAAIFEAYKzRDCqI4AAAggggAACCCCAAAI5BAjOcqjSJgIIIIAAAggggAACCCCQKEBwlghGdQQQQAABBBBAAAEEEEAghwDBWQ5V2kQAAQQQQAABBBBAAAEEEgUIzhLBqI4AAggggAACCCCAAAII5BAgOMuhSpsIIIAAAggggAACCCCAQKIAwVkiGNURQAABBBBAAAEEEEAAgRwCBGc5VGkTAQQQQAABBBBAAAEEEEgUIDhLBKM6AggggAACCCCAAAIIIJBDgOAshyptIoAAAggggAACCCCAAAKJAgRniWBURwABBBBAAAEEEEAAAQRyCBCc5VClTQQQQAABBBBAAAEEEEAgUYDgLBGM6ggggAACCCCAAAIIIIBADgGCsxyqtIkAAggggAACCCCAAAIIJAoQnCWCUR0BBBBAAAEEEEAAAQQQyCFAcJZDlTYRQAABBBBAAAEEEEAAgUQBgrNEMKojgAACCCCAAAIIIIAAAjkECM5yqNImAggggAACCCCAAAIIIJAoQHCWCEZ1BBBAAAEEEEAAAQQQQCCHAMFZDlXaRAABBBBAAAEEEEAAAQQSBQjOEsGojgACCCCAAAIIIIAAAgjkECA4y6FKmwgggAACCCCAAAIIIIBAogDBWSIY1RFAAAEEEEAAAQQQQACBHAIEZzlUaRMBBBBAAAEEEEAAAQQQSBQgOEsEozoCCCCAAAIIIIAAAgggkEOA4CyHKm0igAACCCCAAAIIIIAAAokCBGeJYFRHAAEEEEAAAQQQQAABBHIIEJzlUKVNBBBAAAEEEEAAAQQQQCBRgOAsEYzqCCCAAAIIIIAAAggggEAOAYKzHKq0iQACCCCAAAIIIIAAAggkChCcJYJRHQEEEEAAAQQQQAABBBDIIUBwlkOVNhFAAAEEEEAAAQQQQACBRAGCs0QwqiOAAAIIIIAAAggggAACOQQIznKo0iYCCCCAAAIIIIAAAgggkChAcJYIRnUEEEAAAQQQQAABBBBAIIcAwVkOVdpEAAEEEEAAAQQQQAABBBIFCM4SwaiOAAIIIIAAAggggAACCOQQIDjLoUqbCCCAAAIIIIAAAggggECiwP8BiF+EZ0NiwtwAAAAASUVORK5CYII="
        }
      },
      "cell_type": "markdown",
      "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
      "metadata": {},
      "source": [
        "# How to wait for user input\n",
        "\n",
        "!!! tip \"Prerequisites\"\n",
        "\n",
        "    This guide assumes familiarity with the following concepts:\n",
        "\n",
        "    * [Human-in-the-loop](/langgraphjs/concepts/human_in_the_loop)\n",
        "    * [Breakpoints](/langgraphjs/concepts/breakpoints)\n",
        "    * [LangGraph Glossary](/langgraphjs/concepts/low_level)\n",
        "\n",
        "Human-in-the-loop (HIL) interactions are crucial for [agentic systems](/langgraphjs/concepts/agentic_concepts/#human-in-the-loop). Waiting for human input is a common HIL interaction pattern, allowing the agent to ask the user clarifying questions and await input before proceeding. \n",
        "\n",
        "We can implement these in LangGraph using the [`interrupt()`](/langgraphjs/reference/functions/langgraph.interrupt-1.html) function. `interrupt` allows us to stop graph execution to collect input from a user and continue execution with collected input.\n",
        "\n",
        "![Screenshot 2024-07-08 at 5.26.26 PM.png](attachment:02ae42da-d1a4-4849-984a-6ab0bbf759bd.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cbd446a-808f-4394-be92-d45ab818953c",
      "metadata": {},
      "source": [
        "## Setup\n",
        "First we need to install the packages required\n",
        "\n",
        "```bash\n",
        "npm install @langchain/langgraph @langchain/anthropic @langchain/core zod\n",
        "```\n",
        "\n",
        "Next, we need to set API keys for Anthropic (the LLM we will use)\n",
        "\n",
        "```bash\n",
        "export ANTHROPIC_API_KEY=your-api-key\n",
        "```\n",
        "\n",
        "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
        "\n",
        "```bash\n",
        "export LANGCHAIN_TRACING_V2=\"true\"\n",
        "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
        "export LANGCHAIN_API_KEY=your-api-key\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6cf1fad-5ab6-49c5-b0c8-15a1b6e8cf21",
      "metadata": {},
      "source": [
        "## Simple Usage\n",
        "\n",
        "Let's explore a basic example of using human feedback. A straightforward approach is to create a node, **`human_feedback`**, designed specifically to collect user input. This allows us to gather feedback at a specific, chosen point in our graph.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Call `interrupt()`** inside the **`human_feedback`** node.  \n",
        "2. We set up a [checkpointer](/langgraphjs/concepts/low_level/#persistence) to save the state of the graph up until this node.\n",
        "3. **Use `new Command({ resume: ... })`** to provide the requested value to the **`human_feedback`** node and resume execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58eae42d-be32-48da-8d0a-ab64471657d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph, Annotation, START, END, interrupt, MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  input: Annotation<string>,\n",
        "  userFeedback: Annotation<string>\n",
        "});\n",
        "\n",
        "const step1 = (_state: typeof StateAnnotation.State) => {\n",
        "  console.log(\"---Step 1---\");\n",
        "  return {};\n",
        "}\n",
        "\n",
        "const humanFeedback = (_state: typeof StateAnnotation.State) => {\n",
        "  console.log(\"--- humanFeedback ---\");\n",
        "  const feedback: string = interrupt(\"Please provide feedback\");\n",
        "  return {\n",
        "    userFeedback: feedback\n",
        "  };\n",
        "}\n",
        "\n",
        "const step3 = (_state: typeof StateAnnotation.State) => {\n",
        "  console.log(\"---Step 3---\");\n",
        "  return {};\n",
        "}\n",
        "\n",
        "const builder = new StateGraph(StateAnnotation)\n",
        "    .addNode(\"step1\", step1)\n",
        "    .addNode(\"humanFeedback\", humanFeedback)\n",
        "    .addNode(\"step3\", step3)\n",
        "    .addEdge(START, \"step1\")\n",
        "    .addEdge(\"step1\", \"humanFeedback\")\n",
        "    .addEdge(\"humanFeedback\", \"step3\")\n",
        "    .addEdge(\"step3\", END);\n",
        "\n",
        "\n",
        "// Set up memory\n",
        "const memory = new MemorySaver()\n",
        "\n",
        "// Add \n",
        "const graph = builder.compile({\n",
        "  checkpointer: memory,\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e990a56",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const drawableGraph = graph.getGraph();\n",
        "const image = await drawableGraph.drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce0fe2bc-86fc-465f-956c-729805d50404",
      "metadata": {},
      "source": [
        "Run until our breakpoint in the `humanFeedback` node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eb8e7d47-e7c9-4217-b72c-08394a2c4d3e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---Step 1---\n",
            "{}\n",
            "--- humanFeedback ---\n",
            "{\n",
            "  __interrupt__: [\n",
            "    {\n",
            "      value: 'Please provide feedback',\n",
            "      when: 'during',\n",
            "      resumable: true,\n",
            "      ns: [Array]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "--- GRAPH INTERRUPTED ---\n"
          ]
        }
      ],
      "source": [
        "// Input\n",
        "const initialInput = { input: \"hello world\" };\n",
        "\n",
        "// Thread\n",
        "const config = { configurable: { thread_id: \"1\" } };\n",
        "\n",
        "// Run the graph until the first interruption\n",
        "for await (const event of await graph.stream(initialInput, config)) {\n",
        "  console.log(event);\n",
        "}\n",
        "\n",
        "// Will log when the graph is interrupted, after step 2.\n",
        "console.log(\"--- GRAPH INTERRUPTED ---\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28a7d545-ab19-4800-985b-62837d060809",
      "metadata": {},
      "source": [
        "Now, we can just manually update our graph state with with the user input - "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2165a1bc-1c5b-411f-9e9c-a2b9627e5d56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- humanFeedback ---\n",
            "{ humanFeedback: { userFeedback: 'go to step 3! ' } }\n",
            "\n",
            "====\n",
            "\n",
            "---Step 3---\n",
            "{}\n",
            "\n",
            "====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import { Command } from \"@langchain/langgraph\";\n",
        "\n",
        "// Continue the graph execution\n",
        "for await (const event of await graph.stream(\n",
        "  new Command({ resume: \"go to step 3! \"}),\n",
        "  config,\n",
        ")) {\n",
        "  console.log(event);\n",
        "  console.log(\"\\n====\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75a1060-47aa-4cc6-8c41-e6ba2e9d7923",
      "metadata": {},
      "source": [
        "We can see our feedback was added to state - "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2b83e5ca-8497-43ca-bff7-7203e654c4d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ input: 'hello world', userFeedback: 'go to step 3! ' }\n"
          ]
        }
      ],
      "source": [
        "(await graph.getState(config)).values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36f89e5",
      "metadata": {},
      "source": [
        "## Agent\n",
        "\n",
        "In the context of agents, waiting for user feedback is useful to ask clarifying questions.\n",
        " \n",
        "To show this, we will build a relatively simple ReAct-style agent that does tool calling. \n",
        "\n",
        "We will use Anthropic's models and a mock tool (purely for demonstration purposes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f5319e01",
      "metadata": {},
      "outputs": [],
      "source": [
        "// Set up the tool\n",
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { StateGraph, MessagesAnnotation, START, END, MemorySaver } from \"@langchain/langgraph\";\n",
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "import { AIMessage, ToolMessage } from \"@langchain/core/messages\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const search = tool((_) => {\n",
        "  return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\";\n",
        "}, {\n",
        "  name: \"search\",\n",
        "  description: \"Call to surf the web.\",\n",
        "  schema: z.string(),\n",
        "})\n",
        "\n",
        "const tools = [search]\n",
        "const toolNode = new ToolNode<typeof MessagesAnnotation.State>(tools)\n",
        "\n",
        "// Set up the model\n",
        "const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" })\n",
        "\n",
        "const askHumanTool = tool((_) => {\n",
        "  return \"The human said XYZ\";\n",
        "}, {\n",
        "  name: \"askHuman\",\n",
        "  description: \"Ask the human for input.\",\n",
        "  schema: z.string(),\n",
        "});\n",
        "\n",
        "\n",
        "const modelWithTools = model.bindTools([...tools, askHumanTool])\n",
        "\n",
        "// Define nodes and conditional edges\n",
        "\n",
        "// Define the function that determines whether to continue or not\n",
        "function shouldContinue(state: typeof MessagesAnnotation.State): \"action\" | \"askHuman\" | typeof END {\n",
        "  const lastMessage = state.messages[state.messages.length - 1] as AIMessage;\n",
        "  // If there is no function call, then we finish\n",
        "  if (lastMessage && !lastMessage.tool_calls?.length) {\n",
        "    return END;\n",
        "  }\n",
        "  // If tool call is askHuman, we return that node\n",
        "  // You could also add logic here to let some system know that there's something that requires Human input\n",
        "  // For example, send a slack message, etc\n",
        "  if (lastMessage.tool_calls?.[0]?.name === \"askHuman\") {\n",
        "    console.log(\"--- ASKING HUMAN ---\")\n",
        "    return \"askHuman\";\n",
        "  }\n",
        "  // Otherwise if it isn't, we continue with the action node\n",
        "  return \"action\";\n",
        "}\n",
        "\n",
        "\n",
        "// Define the function that calls the model\n",
        "async function callModel(state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> {\n",
        "  const messages = state.messages;\n",
        "  const response = await modelWithTools.invoke(messages);\n",
        "  // We return an object with a messages property, because this will get added to the existing list\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "\n",
        "// We define a fake node to ask the human\n",
        "function askHuman(state: typeof MessagesAnnotation.State): Partial<typeof MessagesAnnotation.State> {\n",
        "  const lastMessage = state.messages[state.messages.length - 1] as AIMessage;\n",
        "  const toolCallId = lastMessage.tool_calls?.[0].id;\n",
        "  const location: string = interrupt(\"Please provide your location:\");\n",
        "  const newToolMessage = new ToolMessage({\n",
        "    tool_call_id: toolCallId!,\n",
        "    content: location,\n",
        "  })\n",
        "  return { messages: [newToolMessage] };\n",
        "}\n",
        "\n",
        "// Define a new graph\n",
        "const messagesWorkflow = new StateGraph(MessagesAnnotation)\n",
        "  // Define the two nodes we will cycle between\n",
        "  .addNode(\"agent\", callModel)\n",
        "  .addNode(\"action\", toolNode)\n",
        "  .addNode(\"askHuman\", askHuman)\n",
        "  // We now add a conditional edge\n",
        "  .addConditionalEdges(\n",
        "    // First, we define the start node. We use `agent`.\n",
        "    // This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    // Next, we pass in the function that will determine which node is called next.\n",
        "    shouldContinue\n",
        "  )\n",
        "  // We now add a normal edge from `action` to `agent`.\n",
        "  // This means that after `action` is called, `agent` node is called next.\n",
        "  .addEdge(\"action\", \"agent\")\n",
        "  // After we get back the human response, we go back to the agent\n",
        "  .addEdge(\"askHuman\", \"agent\")\n",
        "  // Set the entrypoint as `agent`\n",
        "  // This means that this node is the first one called\n",
        "  .addEdge(START, \"agent\");\n",
        "\n",
        "\n",
        "// Setup memory\n",
        "const messagesMemory = new MemorySaver();\n",
        "\n",
        "// Finally, we compile it!\n",
        "// This compiles it into a LangChain Runnable,\n",
        "// meaning you can use it as you would any other runnable\n",
        "const messagesApp = messagesWorkflow.compile({\n",
        "    checkpointer: messagesMemory,\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4b816850",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const drawableGraph2 = messagesApp.getGraph();\n",
        "const image2 = await drawableGraph2.drawMermaidPng();\n",
        "const arrayBuffer2 = await image2.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer2));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
      "metadata": {},
      "source": [
        "## Interacting with the Agent\n",
        "\n",
        "We can now interact with the agent. Let's ask it to ask the user where they are, then tell them the weather. \n",
        "\n",
        "This should make it use the `askHuman` tool first, then use the normal tool. Note that we switch to use `streamMode: \"values\"` to just return the update messages at each point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================ human Message (1) =================================\n",
            "Use the search tool to ask the user where they are, then look up the weather there\n",
            "--- ASKING HUMAN ---\n",
            "================================ ai Message (1) =================================\n",
            "[\n",
            "  {\n",
            "    type: 'text',\n",
            "    text: \"Certainly! I'll use the askHuman tool to ask the user about their location, and then use the search tool to look up the weather for that location. Let's start by asking the user where they are.\"\n",
            "  },\n",
            "  {\n",
            "    type: 'tool_use',\n",
            "    id: 'toolu_015UDVFoXcMV7KjRqPY78Umk',\n",
            "    name: 'askHuman',\n",
            "    input: {\n",
            "      input: 'Where are you currently located? Please provide a city and country or region.'\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "// Input\n",
        "const input = {\n",
        "  role: \"user\",\n",
        "  content: \"Use the search tool to ask the user where they are, then look up the weather there\",\n",
        "}\n",
        "\n",
        "// Thread\n",
        "const config2 = { configurable: { thread_id: \"3\" }, streamMode: \"values\" as const };\n",
        "\n",
        "for await (const event of await messagesApp.stream({\n",
        "  messages: [input]\n",
        "}, config2)) {\n",
        "  const recentMsg = event.messages[event.messages.length - 1];\n",
        "  console.log(`================================ ${recentMsg.getType()} Message (1) =================================`)\n",
        "  console.log(recentMsg.content);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ea9a777b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "next:  [ 'askHuman' ]\n"
          ]
        }
      ],
      "source": [
        "console.log(\"next: \", (await messagesApp.getState(config2)).next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc168c90-a374-4280-a9a6-8bc232dbb006",
      "metadata": {},
      "source": [
        "You can see that our graph got interrupted inside the `askHuman` node, which is now waiting for a `location` to be provided. We can provide this value by invoking the graph with a `new Command({ resume: \"<location>\" })` input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63598092-d565-4170-9773-e092d345f8c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"cfb461cb-0da1-48b4-acef-e3bf0d3a4e6c\",\n",
            "      \"content\": \"Use the search tool to ask the user where they are, then look up the weather there\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I'll use the askHuman tool to ask the user about their location, and then use the search tool to look up the weather for that location. Let's start by asking the user where they are.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"name\": \"askHuman\",\n",
            "          \"input\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"askHuman\",\n",
            "          \"args\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          },\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"cfb461cb-0da1-48b4-acef-e3bf0d3a4e6c\",\n",
            "      \"content\": \"Use the search tool to ask the user where they are, then look up the weather there\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I'll use the askHuman tool to ask the user about their location, and then use the search tool to look up the weather for that location. Let's start by asking the user where they are.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"name\": \"askHuman\",\n",
            "          \"input\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"askHuman\",\n",
            "          \"args\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          },\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"0225d971-1756-468e-996d-9af93e608a95\",\n",
            "      \"content\": \"San Francisco\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"cfb461cb-0da1-48b4-acef-e3bf0d3a4e6c\",\n",
            "      \"content\": \"Use the search tool to ask the user where they are, then look up the weather there\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I'll use the askHuman tool to ask the user about their location, and then use the search tool to look up the weather for that location. Let's start by asking the user where they are.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"name\": \"askHuman\",\n",
            "          \"input\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"askHuman\",\n",
            "          \"args\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          },\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"0225d971-1756-468e-996d-9af93e608a95\",\n",
            "      \"content\": \"San Francisco\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\"\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Thank you for providing your location. Now, I'll use the search tool to look up the weather in San Francisco.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\",\n",
            "          \"name\": \"search\",\n",
            "          \"input\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 590,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 81\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 590,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 81\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          },\n",
            "          \"id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 590,\n",
            "        \"output_tokens\": 81,\n",
            "        \"total_tokens\": 671\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"cfb461cb-0da1-48b4-acef-e3bf0d3a4e6c\",\n",
            "      \"content\": \"Use the search tool to ask the user where they are, then look up the weather there\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I'll use the askHuman tool to ask the user about their location, and then use the search tool to look up the weather for that location. Let's start by asking the user where they are.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"name\": \"askHuman\",\n",
            "          \"input\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"askHuman\",\n",
            "          \"args\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          },\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"0225d971-1756-468e-996d-9af93e608a95\",\n",
            "      \"content\": \"San Francisco\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\"\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Thank you for providing your location. Now, I'll use the search tool to look up the weather in San Francisco.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\",\n",
            "          \"name\": \"search\",\n",
            "          \"input\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 590,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 81\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 590,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 81\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          },\n",
            "          \"id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 590,\n",
            "        \"output_tokens\": 81,\n",
            "        \"total_tokens\": 671\n",
            "      }\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"1c52e9b5-1d0b-4889-abd0-10572053e1d8\",\n",
            "      \"content\": \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\",\n",
            "      \"name\": \"search\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"cfb461cb-0da1-48b4-acef-e3bf0d3a4e6c\",\n",
            "      \"content\": \"Use the search tool to ask the user where they are, then look up the weather there\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Certainly! I'll use the askHuman tool to ask the user about their location, and then use the search tool to look up the weather for that location. Let's start by asking the user where they are.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"name\": \"askHuman\",\n",
            "          \"input\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01TA2zHbbrenm7KXSUdcFdXD\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 465,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 112\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"askHuman\",\n",
            "          \"args\": {\n",
            "            \"input\": \"Where are you currently located? Please provide a city and country or region.\"\n",
            "          },\n",
            "          \"id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"0225d971-1756-468e-996d-9af93e608a95\",\n",
            "      \"content\": \"San Francisco\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_015UDVFoXcMV7KjRqPY78Umk\"\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"type\": \"text\",\n",
            "          \"text\": \"Thank you for providing your location. Now, I'll use the search tool to look up the weather in San Francisco.\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"tool_use\",\n",
            "          \"id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\",\n",
            "          \"name\": \"search\",\n",
            "          \"input\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          }\n",
            "        }\n",
            "      ],\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 590,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 81\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01QDw6TTXvFXKPEX4mYjzU8Y\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"tool_use\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 590,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 81\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"name\": \"search\",\n",
            "          \"args\": {\n",
            "            \"input\": \"current weather in San Francisco\"\n",
            "          },\n",
            "          \"id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\",\n",
            "          \"type\": \"tool_call\"\n",
            "        }\n",
            "      ],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 590,\n",
            "        \"output_tokens\": 81,\n",
            "        \"total_tokens\": 671\n",
            "      }\n",
            "    },\n",
            "    ToolMessage {\n",
            "      \"id\": \"1c52e9b5-1d0b-4889-abd0-10572053e1d8\",\n",
            "      \"content\": \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\",\n",
            "      \"name\": \"search\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_call_id\": \"toolu_01AzffiwYncLArNb9SbKHK2a\"\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"msg_01UAbWbZ9RVV3L6ABgQWsy9f\",\n",
            "      \"content\": \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's many attractions.\\n\\nHowever, I should note that the search result included an unusual comment about Geminis. This appears to be unrelated to the weather and might be a joke or reference included in the search results. It's not typical for weather reports to include astrological information, so I'd recommend focusing on the factual weather information provided.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\",\n",
            "      \"additional_kwargs\": {\n",
            "        \"id\": \"msg_01UAbWbZ9RVV3L6ABgQWsy9f\",\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"end_turn\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 704,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 140\n",
            "        }\n",
            "      },\n",
            "      \"response_metadata\": {\n",
            "        \"id\": \"msg_01UAbWbZ9RVV3L6ABgQWsy9f\",\n",
            "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
            "        \"stop_reason\": \"end_turn\",\n",
            "        \"stop_sequence\": null,\n",
            "        \"usage\": {\n",
            "          \"input_tokens\": 704,\n",
            "          \"cache_creation_input_tokens\": 0,\n",
            "          \"cache_read_input_tokens\": 0,\n",
            "          \"output_tokens\": 140\n",
            "        },\n",
            "        \"type\": \"message\",\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 704,\n",
            "        \"output_tokens\": 140,\n",
            "        \"total_tokens\": 844\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import { Command } from \"@langchain/langgraph\";\n",
        "\n",
        "// Continue the graph execution\n",
        "for await (const event of await messagesApp.stream(\n",
        "  new Command({ resume: \"San Francisco\" }),\n",
        "  config2,\n",
        ")) {\n",
        "  console.log(event);\n",
        "  console.log(\"\\n====\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35098a24",
      "metadata": {},
      "source": [
        "**Note:** The `interrupt` function propagates by throwing a special `GraphInterrupt` error. Therefore, you should avoid using `try/catch` blocks around the `interrupt` function - or if you do, ensure that the `GraphInterrupt` error is thrown again within your `catch` block."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="multi_agent/agent_supervisor.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
   "metadata": {},
   "source": [
    "# Agent Supervisor\n",
    "\n",
    "The [previous example](./multi-agent-collaboration.ipynb) routed messages\n",
    "automatically based on the output of the initial researcher agent.\n",
    "\n",
    "We can also choose to use an LLM to orchestrate the different agents.\n",
    "\n",
    "Below, we will create an agent group, with an agent supervisor to help delegate\n",
    "tasks.\n",
    "\n",
    "![diagram](./img/supervisor-diagram.png)\n",
    "\n",
    "To simplify the code in each agent node, we will use the AgentExecutor class\n",
    "from LangChain. This and other \"advanced agent\" notebooks are designed to show\n",
    "how you can implement certain design patterns in LangGraph. If the pattern suits\n",
    "your needs, we recommend combining it with some of the other fundamental\n",
    "patterns described elsewhere in the docs for best performance.\n",
    "\n",
    "Before we build, let's configure our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c2f3de-c730-4aec-85a6-af2c2f058803",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "// process.env.TAVILY_API_KEY = \"sk_...\";\n",
    "// Optional tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"sk_...\";\n",
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_PROJECT = \"Agent Supervisor: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48ac3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c46b34",
   "metadata": {},
   "source": [
    "### Define State\n",
    "\n",
    "We first define the state of the graph. This will just a list of messages, along\n",
    "with a key to track the most recent sender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfb0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, Annotation } from \"@langchain/langgraph\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// This defines the object that is passed between each node\n",
    "// in the graph. We will create different nodes for each agent and tool\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "    default: () => [],\n",
    "  }),\n",
    "  // The agent node that last performed work\n",
    "  next: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x ?? END,\n",
    "    default: () => END,\n",
    "  }),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac25624-4d83-45a4-b9ef-a10589aacfb7",
   "metadata": {},
   "source": [
    "## Create tools\n",
    "\n",
    "For this example, you will make an agent to do web research with a search\n",
    "engine, and one agent to create plots. Define the tools they'll use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04c6778-403b-4b49-9b93-678e910d5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "require(\"esm-hook\"); // Only for running this in TSLab. See: https://github.com/yunabe/tslab/issues/72\n",
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
    "import * as d3 from \"d3\";\n",
    "// ----------ATTENTION----------\n",
    "// If attempting to run this notebook locally, you must follow these instructions\n",
    "// to install the necessary system dependencies for the `canvas` package.\n",
    "// https://www.npmjs.com/package/canvas#compiling\n",
    "// -----------------------------\n",
    "import { createCanvas } from \"canvas\";\n",
    "import { z } from \"zod\";\n",
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const chartTool = new DynamicStructuredTool({\n",
    "  name: \"generate_bar_chart\",\n",
    "  description:\n",
    "    \"Generates a bar chart from an array of data points using D3.js and displays it for the user.\",\n",
    "  schema: z.object({\n",
    "    data: z\n",
    "      .object({\n",
    "        label: z.string(),\n",
    "        value: z.number(),\n",
    "      })\n",
    "      .array(),\n",
    "  }),\n",
    "  func: async ({ data }) => {\n",
    "    const width = 500;\n",
    "    const height = 500;\n",
    "    const margin = { top: 20, right: 30, bottom: 30, left: 40 };\n",
    "\n",
    "    const canvas = createCanvas(width, height);\n",
    "    const ctx = canvas.getContext(\"2d\");\n",
    "\n",
    "    const x = d3\n",
    "      .scaleBand()\n",
    "      .domain(data.map((d) => d.label))\n",
    "      .range([margin.left, width - margin.right])\n",
    "      .padding(0.1);\n",
    "\n",
    "    const y = d3\n",
    "      .scaleLinear()\n",
    "      .domain([0, d3.max(data, (d) => d.value) ?? 0])\n",
    "      .nice()\n",
    "      .range([height - margin.bottom, margin.top]);\n",
    "\n",
    "    const colorPalette = [\n",
    "      \"#e6194B\",\n",
    "      \"#3cb44b\",\n",
    "      \"#ffe119\",\n",
    "      \"#4363d8\",\n",
    "      \"#f58231\",\n",
    "      \"#911eb4\",\n",
    "      \"#42d4f4\",\n",
    "      \"#f032e6\",\n",
    "      \"#bfef45\",\n",
    "      \"#fabebe\",\n",
    "    ];\n",
    "\n",
    "    data.forEach((d, idx) => {\n",
    "      ctx.fillStyle = colorPalette[idx % colorPalette.length];\n",
    "      ctx.fillRect(\n",
    "        x(d.label) ?? 0,\n",
    "        y(d.value),\n",
    "        x.bandwidth(),\n",
    "        height - margin.bottom - y(d.value),\n",
    "      );\n",
    "    });\n",
    "\n",
    "    ctx.beginPath();\n",
    "    ctx.strokeStyle = \"black\";\n",
    "    ctx.moveTo(margin.left, height - margin.bottom);\n",
    "    ctx.lineTo(width - margin.right, height - margin.bottom);\n",
    "    ctx.stroke();\n",
    "\n",
    "    ctx.textAlign = \"center\";\n",
    "    ctx.textBaseline = \"top\";\n",
    "    x.domain().forEach((d) => {\n",
    "      const xCoord = (x(d) ?? 0) + x.bandwidth() / 2;\n",
    "      ctx.fillText(d, xCoord, height - margin.bottom + 6);\n",
    "    });\n",
    "\n",
    "    ctx.beginPath();\n",
    "    ctx.moveTo(margin.left, height - margin.top);\n",
    "    ctx.lineTo(margin.left, height - margin.bottom);\n",
    "    ctx.stroke();\n",
    "\n",
    "    ctx.textAlign = \"right\";\n",
    "    ctx.textBaseline = \"middle\";\n",
    "    const ticks = y.ticks();\n",
    "    ticks.forEach((d) => {\n",
    "      const yCoord = y(d); // height - margin.bottom - y(d);\n",
    "      ctx.moveTo(margin.left, yCoord);\n",
    "      ctx.lineTo(margin.left - 6, yCoord);\n",
    "      ctx.stroke();\n",
    "      ctx.fillText(d.toString(), margin.left - 8, yCoord);\n",
    "    });\n",
    "    await tslab.display.png(canvas.toBuffer());\n",
    "    return \"Chart has been generated and displayed to the user!\";\n",
    "  },\n",
    "});\n",
    "\n",
    "const tavilyTool = new TavilySearchResults();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b883c63-460d-45b0-82a3-61257db20af0",
   "metadata": {},
   "source": [
    "## Create Agent Supervisor\n",
    "\n",
    "The supervisor routes the work between our worker agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c108a0-6dc3-46fd-a5e6-a1fcfad5458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "\n",
    "const members = [\"researcher\", \"chart_generator\"] as const;\n",
    "\n",
    "const systemPrompt =\n",
    "  \"You are a supervisor tasked with managing a conversation between the\" +\n",
    "  \" following workers: {members}. Given the following user request,\" +\n",
    "  \" respond with the worker to act next. Each worker will perform a\" +\n",
    "  \" task and respond with their results and status. When finished,\" +\n",
    "  \" respond with FINISH.\";\n",
    "const options = [END, ...members];\n",
    "\n",
    "// Define the routing function\n",
    "const routingTool = {\n",
    "  name: \"route\",\n",
    "  description: \"Select the next role.\",\n",
    "  schema: z.object({\n",
    "    next: z.enum([END, ...members]),\n",
    "  }),\n",
    "}\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", systemPrompt],\n",
    "  new MessagesPlaceholder(\"messages\"),\n",
    "  [\n",
    "    \"human\",\n",
    "    \"Given the conversation above, who should act next?\" +\n",
    "    \" Or should we FINISH? Select one of: {options}\",\n",
    "  ],\n",
    "]);\n",
    "\n",
    "const formattedPrompt = await prompt.partial({\n",
    "  options: options.join(\", \"),\n",
    "  members: members.join(\", \"),\n",
    "});\n",
    "\n",
    "const llm = new ChatAnthropic({\n",
    "  modelName: \"claude-3-5-sonnet-20241022\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "const supervisorChain = formattedPrompt\n",
    "  .pipe(llm.bindTools(\n",
    "    [routingTool],\n",
    "    {\n",
    "      tool_choice: \"route\",\n",
    "    },\n",
    "  ))\n",
    "  // select the first one\n",
    "  .pipe((x) => (x.tool_calls[0].args));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "204c8c8c-dd24-454a-8eba-8600886eafb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ next: 'researcher' }\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "await supervisorChain.invoke({\n",
    "  messages: [\n",
    "    new HumanMessage({\n",
    "      content: \"write a report on birds.\",\n",
    "    }),\n",
    "  ],\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d507f-34d1-4f1b-8dde-5e58d17b2166",
   "metadata": {},
   "source": [
    "## Construct Graph\n",
    "\n",
    "We're ready to start building the graph. First, create the agents to add to the\n",
    "graph.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Compatibility</p>\n",
    "    <p>\n",
    "        The <a href=\"https://langchain-ai.github.io/langgraphjs/reference/types/langgraph_prebuilt.CreateReactAgentParams.html\"><code>stateModifier</code></a> parameter was added in <code>@langchain/langgraph>=0.2.27</code>.\n",
    "        <br />\n",
    "        If you are on an older version, you will need to use the deprecated <code>messageModifier</code> parameter.\n",
    "        <br />\n",
    "        For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14778e86-077b-4e6a-893c-400e59b0cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { SystemMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// Recall llm was defined as ChatOpenAI above\n",
    "// It could be any other language model\n",
    "const researcherAgent = createReactAgent({\n",
    "  llm,\n",
    "  tools: [tavilyTool],\n",
    "  stateModifier: new SystemMessage(\"You are a web researcher. You may use the Tavily search engine to search the web for\" +\n",
    "    \" important information, so the Chart Generator in your team can make useful plots.\")\n",
    "})\n",
    "\n",
    "const researcherNode = async (\n",
    "  state: typeof AgentState.State,\n",
    "  config?: RunnableConfig,\n",
    ") => {\n",
    "  const result = await researcherAgent.invoke(state, config);\n",
    "  const lastMessage = result.messages[result.messages.length - 1];\n",
    "  return {\n",
    "    messages: [\n",
    "      new HumanMessage({ content: lastMessage.content, name: \"Researcher\" }),\n",
    "    ],\n",
    "  };\n",
    "};\n",
    "\n",
    "const chartGenAgent = createReactAgent({\n",
    "  llm,\n",
    "  tools: [chartTool],\n",
    "  stateModifier: new SystemMessage(\"You excel at generating bar charts. Use the researcher's information to generate the charts.\")\n",
    "})\n",
    "\n",
    "const chartGenNode = async (\n",
    "  state: typeof AgentState.State,\n",
    "  config?: RunnableConfig,\n",
    ") => {\n",
    "  const result = await chartGenAgent.invoke(state, config);\n",
    "  const lastMessage = result.messages[result.messages.length - 1];\n",
    "  return {\n",
    "    messages: [\n",
    "      new HumanMessage({ content: lastMessage.content, name: \"ChartGenerator\" }),\n",
    "    ],\n",
    "  };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac913ae-0dfc-44bf-a26c-8f23f7e3e4a2",
   "metadata": {},
   "source": [
    "Now we can create the graph itself! Add the nodes, and add edges to define how\n",
    "how work will be performed in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f874f7-dac1-4b81-9ddc-a161f9b5bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// 1. Create the graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "  // 2. Add the nodes; these will do the work\n",
    "  .addNode(\"researcher\", researcherNode)\n",
    "  .addNode(\"chart_generator\", chartGenNode)\n",
    "  .addNode(\"supervisor\", supervisorChain);\n",
    "// 3. Define the edges. We will define both regular and conditional ones\n",
    "// After a worker completes, report to supervisor\n",
    "members.forEach((member) => {\n",
    "  workflow.addEdge(member, \"supervisor\");\n",
    "});\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "  \"supervisor\",\n",
    "  (x: typeof AgentState.State) => x.next,\n",
    ");\n",
    "\n",
    "workflow.addEdge(START, \"supervisor\");\n",
    "\n",
    "const graph = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36496de-7121-4c49-8cb6-58c943c66628",
   "metadata": {},
   "source": [
    "## Invoke the team\n",
    "\n",
    "With the graph created, we can now invoke it and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ba78e9-d9c1-457c-a073-d606d5d3e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ supervisor: { next: 'researcher' } }\n",
      "----\n",
      "{\n",
      "  researcher: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"Based on the search results, I can tell you about the most popular TV shows in 2023 across both traditional television and streaming platforms:\\n\\n1. \\\"Succession\\\" (HBO) - The final season was one of the most critically acclaimed and watched shows of 2023\\n2. \\\"The Last of Us\\\" (HBO) - This adaptation became a massive hit and one of HBO's most-watched series ever\\n3. \\\"Wednesday\\\" (Netflix) - This show continued its popularity from late 2022 into 2023 and remained one of the most-streamed shows\\n\\nIt's worth noting that different metrics (network TV vs. streaming, total viewers vs. ratings) can yield different results. For traditional network television, \\\"NFL Sunday Night Football\\\" was technically the most-watched program, but I focused on scripted series for this list.\\n\\nGiven the conversation above, we should have the chart_generator act next, as they can create visualizations showing the viewership numbers or ratings for these popular shows.\\n\\nAnswer: chart_generator\",\n",
      "        \"name\": \"Researcher\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: 'researcher' } }\n",
      "----\n",
      "{\n",
      "  researcher: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"Based on the search results, I can provide information about the most popular TV shows in 2023:\\n\\n1. \\\"Succession\\\" (HBO) - The series finale drew 2.9 million viewers, with Season 4 averaging 710,000 viewers per episode, making it the show's most popular season.\\n\\n2. \\\"The Last of Us\\\" (HBO) - The show had a strong debut with 837 million minutes watched in just its first full week of availability on HBO Max.\\n\\n3. \\\"Wednesday\\\" (Netflix) - While specific 2023 numbers aren't readily available in the search results, the show maintained its popularity from its record-breaking 2022 debut throughout 2023.\\n\\nIt's worth noting that measuring TV show popularity has become complex due to different platforms and viewing methods. Traditional TV measurements (like Nielsen ratings) differ from streaming minutes watched, and some shows are popular across both formats. Additionally, NFL's Sunday Night Football remained the most-watched program overall on traditional television, averaging 19.73 million viewers in the 2023-24 season.\\n\\nGiven the conversation above, we should have the chart_generator act next, as they can create visualizations showing these viewership numbers across different metrics.\\n\\nAnswer: chart_generator\",\n",
      "        \"name\": \"Researcher\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: 'researcher' } }\n",
      "----\n",
      "{\n",
      "  researcher: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"Based on the search results, I can provide detailed information about the most popular TV shows in 2023:\\n\\n1. \\\"Succession\\\" (HBO)\\n- Series finale drew 2.9 million viewers\\n- Season 4 averaged 710,000 viewers per episode (live viewing)\\n- When including delayed viewing, Season 4 averaged 8.7 million viewers per episode\\n- Most watched season of the series\\n\\n2. \\\"The Last of Us\\\" (HBO)\\n- Became one of HBO's biggest hits\\n- Demonstrated extremely high demand in streaming metrics\\n- Outperformed many other popular streaming shows including \\\"Wednesday\\\" in terms of viewer demand\\n- Specific episode viewership data varied throughout the season\\n\\n3. \\\"Wednesday\\\" (Netflix)\\n- Remained one of Netflix's top English-language series of all time\\n- Continued strong performance into 2023 from its late 2022 debut\\n- Exact viewing hours for 2023 aren't specifically broken out in Netflix's engagement reports\\n\\nIt's worth noting that comparing shows across different platforms is challenging due to varying measurement methods:\\n- Traditional TV uses Nielsen ratings\\n- Streaming services use hours viewed\\n- Some platforms count delayed viewing while others don't\\n- HBO shows often have both cable and streaming numbers\\n\\nGiven the conversation above, we should have the chart_generator act next, as they can create visualizations showing these viewership numbers, particularly for Succession where we have the most concrete data across different viewing methods.\\n\\nAnswer: chart_generator\",\n",
      "        \"name\": \"Researcher\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: 'researcher' } }\n",
      "----\n",
      "{\n",
      "  researcher: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"Based on the additional search results, I can now provide more precise viewership data:\\n\\n1. \\\"Succession\\\" (HBO)\\n- Series finale: 2.928 million viewers (live)\\n- Season 4 cumulative average: 8.7 million viewers per episode (including repeats and HBO Max)\\n- This was up 1.5 million viewers from Season 3\\n\\n2. \\\"The Last of Us\\\" (HBO)\\n- Averaged over 20 million viewers per episode (including delayed viewing and HBO Max)\\n- Viewership nearly doubled from its first episode\\n- One of HBO's biggest hits of 2023\\n\\n3. \\\"Wednesday\\\" (Netflix)\\n- While specific 2023 numbers aren't broken out in the search results, it remained one of Netflix's most popular shows\\n\\nGiven this more detailed information, we should have the chart_generator act next to create visualizations showing:\\n1. The comparison between live vs. cumulative viewing for Succession\\n2. The growth trajectory of The Last of Us viewership\\n3. A comparison of total viewership between these shows where metrics are comparable\\n\\nAnswer: chart_generator\",\n",
      "        \"name\": \"Researcher\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: 'researcher' } }\n",
      "----\n",
      "{\n",
      "  researcher: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"Based on the comprehensive search results, I can now provide the most accurate viewership data for these shows in 2023:\\n\\n1. \\\"Succession\\\" (HBO)\\n- Series finale: 2.928 million viewers (live)\\n- Season 4 average: 8.7 million viewers per episode (including delayed viewing and HBO Max)\\n- Significant growth from previous seasons\\n\\n2. \\\"The Last of Us\\\" (HBO)\\n- Started with 4.7 million viewers (premiere)\\n- Grew to 8.1 million viewers by Episode 8\\n- Final viewership numbers showed consistent growth:\\n  * Episode 1: 4.7M\\n  * Episode 2: 5.7M (+22%)\\n  * Episode 4: 7.5M\\n  * Episode 8: 8.1M\\n  * Overall series average reached about 20 million viewers per episode when including all viewing methods\\n\\n3. \\\"Wednesday\\\" (Netflix)\\n- While specific 2023 numbers aren't broken out in detail, it maintained its position as one of Netflix's most-watched shows\\n\\nGiven this comprehensive data, we should have the chart_generator act next to create visualizations showing:\\n1. The episode-by-episode growth of The Last of Us viewership\\n2. The comparison between live vs. cumulative viewing for Succession's final season\\n3. A comparison of total viewership between these HBO shows where metrics are comparable\\n\\nAnswer: chart_generator\",\n",
      "        \"name\": \"Researcher\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: 'chart_generator' } }\n",
      "----\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  chart_generator: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"These visualizations help illustrate:\\n1. The steady growth in viewership for The Last of Us over its first season\\n2. The significant difference between live viewing and total viewing numbers for Succession\\n3. The overall viewership comparison between the two HBO shows when including all viewing methods\\n\\nNote that Wednesday (Netflix) isn't included in these comparisons because its 2023 viewership numbers weren't specifically broken out in the available data.\",\n",
      "        \"name\": \"ChartGenerator\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: '__end__' } }\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "let streamResults = graph.stream(\n",
    "  {\n",
    "    messages: [\n",
    "      new HumanMessage({\n",
    "        content: \"What were the 3 most popular tv shows in 2023?\",\n",
    "      }),\n",
    "    ],\n",
    "  },\n",
    "  { recursionLimit: 100 },\n",
    ");\n",
    "\n",
    "for await (const output of await streamResults) {\n",
    "  if (!output?.__end__) {\n",
    "    console.log(output);\n",
    "    console.log(\"----\");\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a92dfd-0e11-47f5-aad4-b68d24990e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ supervisor: { next: 'researcher' } }\n",
      "----\n",
      "{\n",
      "  researcher: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"Based on the search results, I have found the annual GDP growth rates for the United States from 2021 to 2023:\\n\\n2021: 5.80%\\n2022: 1.94%\\n2023: 2.54%\\n\\nNow that I have gathered the necessary data, the Chart Generator should create a bar chart using these values.\\n\\nThe next actor should be: chart_generator\",\n",
      "        \"name\": \"Researcher\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: 'chart_generator' } }\n",
      "----\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  chart_generator: {\n",
      "    messages: [\n",
      "      HumanMessage {\n",
      "        \"content\": \"I've generated a bar chart showing the US GDP growth rates for the years 2021-2023. The chart displays:\\n- 2021: 5.80%\\n- 2022: 1.94%\\n- 2023: 2.54%\\n\\nThe chart clearly shows that 2021 had the highest GDP growth rate at 5.80%, followed by a significant decrease in 2022 to 1.94%, and then a slight increase in 2023 to 2.54%.\",\n",
      "        \"name\": \"ChartGenerator\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----\n",
      "{ supervisor: { next: '__end__' } }\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "streamResults = graph.stream(\n",
    "  {\n",
    "    messages: [\n",
    "      new HumanMessage({\n",
    "        content: \"Generate a bar chart of the US GDP growth from 2021-2023.\",\n",
    "      }),\n",
    "    ],\n",
    "  },\n",
    "  { recursionLimit: 150 },\n",
    ");\n",
    "\n",
    "for await (const output of await streamResults) {\n",
    "  if (!output?.__end__) {\n",
    "    console.log(output);\n",
    "    console.log(\"----\");\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb18b41",
   "metadata": {},
   "source": [
    "You can\n",
    "[click here](https://smith.langchain.com/public/c5e026d4-3551-4704-aa29-7b7488a1a7a7/r)\n",
    "to see a LangSmith trace of the above query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="multi_agent/hierarchical_agent_teams.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e1f6bc2",
      "metadata": {},
      "source": [
        "# Hierarchical Agent Teams\n",
        "\n",
        "In our previous example ([Agent Supervisor](./agent_supervisor.ipynb)), we\n",
        "introduced the concept of a single supervisor node to route work between\n",
        "different worker nodes.\n",
        "\n",
        "But what if the job for a single worker becomes too complex? What if the number\n",
        "of workers becomes too large?\n",
        "\n",
        "For some applications, the system may be more effective if work is distributed\n",
        "_hierarchically_.\n",
        "\n",
        "You can do this by composing different subgraphs and creating a top-level\n",
        "supervisor, along with mid-level supervisors.\n",
        "\n",
        "To do this, let's build a simple research assistant! The graph will look\n",
        "something like the following:\n",
        "\n",
        "![diagram](./img/hierarchical-diagram.png)\n",
        "\n",
        "This notebook is inspired by the paper\n",
        "[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155),\n",
        "by Wu, et. al. In the rest of this notebook, you will:\n",
        "\n",
        "1. Define the agents' tools to access the web and write files\n",
        "2. Define some utilities to help create the graph and agents\n",
        "3. Create and define each team (web research + doc writing)\n",
        "4. Compose everything together.\n",
        "\n",
        "But before all of that, some setup:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12fb0acf",
      "metadata": {},
      "outputs": [],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "// process.env.TAVILY_API_KEY = \"sk_...\";\n",
        "\n",
        "// Optional, add tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"sk_...\";\n",
        "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "// process.env.LANGCHAIN_PROJECT = \"Multi-agent Collaboration: LangGraphJS\";\n",
        "\n",
        "// Or use a dotenv file:\n",
        "// import \"dotenv/config\";\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f28f8d",
      "metadata": {},
      "source": [
        "## Create Tools\n",
        "\n",
        "Each team will be composed of one or more agents each with one or more tools.\n",
        "Below, define all the tools to be used by your different teams.\n",
        "\n",
        "We'll start with the research team.\n",
        "\n",
        "**Research team tools**\n",
        "\n",
        "The research team can use a search engine and url scraper to find information on\n",
        "the web. Feel free to add additional functionality below to boost the team\n",
        "performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "193df01b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
        "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const tavilyTool = new TavilySearchResults();\n",
        "\n",
        "const scrapeWebpage = tool(async (input) => {\n",
        "    const loader = new CheerioWebBaseLoader(input.url);\n",
        "    const docs = await loader.load();\n",
        "    const formattedDocs = docs.map(\n",
        "      (doc) =>\n",
        "        `<Document name=\"${doc.metadata?.title}\">\\n${doc.pageContent}\\n</Document>`,\n",
        "    );\n",
        "    return formattedDocs.join(\"\\n\\n\");\n",
        "  },\n",
        "  {\n",
        "    name: \"scrape_webpage\",\n",
        "    description: \"Scrape the contents of a webpage.\",\n",
        "    schema: z.object({\n",
        "      url: z.string(),\n",
        "    }),\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148bbce7",
      "metadata": {},
      "source": [
        "**Document writing team tools**\n",
        "\n",
        "Next up, we will give some tools for the doc writing team to use. We define some\n",
        "bare-bones file-access tools below.\n",
        "\n",
        "Note that this gives the agents access to your file-system, which can be unsafe.\n",
        "We also haven't optimized the tool descriptions for performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f1e07f",
      "metadata": {},
      "outputs": [],
      "source": [
        "require(\"esm-hook\"); // Only for running this in TSLab in Jupyter. See: https://github.com/yunabe/tslab/issues/72\n",
        "// ----------ATTENTION----------\n",
        "// If attempting to run this notebook locally, you must follow these instructions\n",
        "// to install the necessary system dependencies for the `canvas` package.\n",
        "// https://www.npmjs.com/package/canvas#compiling\n",
        "// -----------------------------\n",
        "import { createCanvas } from \"canvas\";\n",
        "import * as d3 from \"d3\";\n",
        "import * as tslab from \"tslab\";\n",
        "import * as fs from \"fs/promises\";\n",
        "import * as path from \"path\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const WORKING_DIRECTORY = \"./temp\";\n",
        "await fs.mkdir(WORKING_DIRECTORY, { recursive: true });\n",
        "\n",
        "const createOutlineTool = tool(\n",
        "  async ({ points, file_name }) => {\n",
        "    const filePath = path.join(WORKING_DIRECTORY, file_name);\n",
        "    const data = points\n",
        "      .map((point, index) => `${index + 1}. ${point}\\n`)\n",
        "      .join(\"\");\n",
        "    await fs.writeFile(filePath, data);\n",
        "    return `Outline saved to ${file_name}`;\n",
        "  },\n",
        "  {\n",
        "    name: \"create_outline\",\n",
        "    description: \"Create and save an outline.\",\n",
        "    schema: z.object({\n",
        "      points: z\n",
        "        .array(z.string())\n",
        "        .nonempty(\"List of main points or sections must not be empty.\"),\n",
        "      file_name: z.string(),\n",
        "    }),\n",
        "  }\n",
        ");\n",
        "\n",
        "const readDocumentTool = tool(\n",
        "  async ({ file_name, start, end }) => {\n",
        "    const filePath = path.join(WORKING_DIRECTORY, file_name);\n",
        "    const data = await fs.readFile(filePath, \"utf-8\");\n",
        "    const lines = data.split(\"\\n\");\n",
        "    return lines.slice(start ?? 0, end).join(\"\\n\");\n",
        "  },\n",
        "  {\n",
        "    name: \"read_document\",\n",
        "    description: \"Read the specified document.\",\n",
        "    schema: z.object({\n",
        "      file_name: z.string(),\n",
        "      start: z.number().optional(),\n",
        "      end: z.number().optional(),\n",
        "    }),\n",
        "  }\n",
        ");\n",
        "\n",
        "const writeDocumentTool = tool(\n",
        "  async ({ content, file_name }) => {\n",
        "    const filePath = path.join(WORKING_DIRECTORY, file_name);\n",
        "    await fs.writeFile(filePath, content);\n",
        "    return `Document saved to ${file_name}`;\n",
        "  },\n",
        "  {\n",
        "    name: \"write_document\",\n",
        "    description: \"Create and save a text document.\",\n",
        "    schema: z.object({\n",
        "      content: z.string(),\n",
        "      file_name: z.string(),\n",
        "    }),\n",
        "  }\n",
        ");\n",
        "\n",
        "const editDocumentTool = tool(\n",
        "  async ({ file_name, inserts }) => {\n",
        "    const filePath = path.join(WORKING_DIRECTORY, file_name);\n",
        "    const data = await fs.readFile(filePath, \"utf-8\");\n",
        "    let lines = data.split(\"\\n\");\n",
        "\n",
        "    const sortedInserts = Object.entries(inserts).sort(\n",
        "      ([a], [b]) => parseInt(a) - parseInt(b),\n",
        "    );\n",
        "\n",
        "    for (const [line_number_str, text] of sortedInserts) {\n",
        "      const line_number = parseInt(line_number_str);\n",
        "      if (1 <= line_number && line_number <= lines.length + 1) {\n",
        "        lines.splice(line_number - 1, 0, text);\n",
        "      } else {\n",
        "        return `Error: Line number ${line_number} is out of range.`;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    await fs.writeFile(filePath, lines.join(\"\\n\"));\n",
        "    return `Document edited and saved to ${file_name}`;\n",
        "  },\n",
        "  {\n",
        "    name: \"edit_document\",\n",
        "    description: \"Edit a document by inserting text at specific line numbers.\",\n",
        "    schema: z.object({\n",
        "      file_name: z.string(),\n",
        "      inserts: z.record(z.number(), z.string()),\n",
        "    }),\n",
        "  }\n",
        ");\n",
        "\n",
        "const chartTool = tool(\n",
        "  async ({ data }) => {\n",
        "    const width = 500;\n",
        "    const height = 500;\n",
        "    const margin = { top: 20, right: 30, bottom: 30, left: 40 };\n",
        "\n",
        "    const canvas = createCanvas(width, height);\n",
        "    const ctx = canvas.getContext(\"2d\");\n",
        "\n",
        "    const x = d3\n",
        "      .scaleBand()\n",
        "      .domain(data.map((d) => d.label))\n",
        "      .range([margin.left, width - margin.right])\n",
        "      .padding(0.1);\n",
        "\n",
        "    const y = d3\n",
        "      .scaleLinear()\n",
        "      .domain([0, d3.max(data, (d) => d.value) ?? 0])\n",
        "      .nice()\n",
        "      .range([height - margin.bottom, margin.top]);\n",
        "\n",
        "    const colorPalette = [\n",
        "      \"#e6194B\",\n",
        "      \"#3cb44b\",\n",
        "      \"#ffe119\",\n",
        "      \"#4363d8\",\n",
        "      \"#f58231\",\n",
        "      \"#911eb4\",\n",
        "      \"#42d4f4\",\n",
        "      \"#f032e6\",\n",
        "      \"#bfef45\",\n",
        "      \"#fabebe\",\n",
        "    ];\n",
        "\n",
        "    for (let i = 0; i < data.length; i++) {\n",
        "      const d = data[i];\n",
        "      ctx.fillStyle = colorPalette[i % colorPalette.length];\n",
        "      ctx.fillRect(\n",
        "        x(d.label) ?? 0,\n",
        "        y(d.value),\n",
        "        x.bandwidth(),\n",
        "        height - margin.bottom - y(d.value),\n",
        "      ); \n",
        "    }\n",
        "\n",
        "    ctx.beginPath();\n",
        "    ctx.strokeStyle = \"black\";\n",
        "    ctx.moveTo(margin.left, height - margin.bottom);\n",
        "    ctx.lineTo(width - margin.right, height - margin.bottom);\n",
        "    ctx.stroke();\n",
        "\n",
        "    ctx.textAlign = \"center\";\n",
        "    ctx.textBaseline = \"top\";\n",
        "    x.domain().forEach((d) => {\n",
        "      const xCoord = (x(d) ?? 0) + x.bandwidth() / 2;\n",
        "      ctx.fillText(d, xCoord, height - margin.bottom + 6);\n",
        "    });\n",
        "\n",
        "    ctx.beginPath();\n",
        "    ctx.moveTo(margin.left, height - margin.top);\n",
        "    ctx.lineTo(margin.left, height - margin.bottom);\n",
        "    ctx.stroke();\n",
        "\n",
        "    ctx.textAlign = \"right\";\n",
        "    ctx.textBaseline = \"middle\";\n",
        "    const ticks = y.ticks();\n",
        "    ticks.forEach((d) => {\n",
        "      const yCoord = y(d);\n",
        "      ctx.moveTo(margin.left, yCoord);\n",
        "      ctx.lineTo(margin.left - 6, yCoord);\n",
        "      ctx.stroke();\n",
        "      ctx.fillText(d.toString(), margin.left - 8, yCoord);\n",
        "    });\n",
        "\n",
        "    tslab.display.png(canvas.toBuffer());\n",
        "    return \"Chart has been generated and displayed to the user!\";\n",
        "  },\n",
        "  {\n",
        "    name: \"generate_bar_chart\",\n",
        "    description:\n",
        "      \"Generates a bar chart from an array of data points using D3.js and displays it for the user.\",\n",
        "    schema: z.object({\n",
        "      data: z\n",
        "        .object({\n",
        "          label: z.string(),\n",
        "          value: z.number(),\n",
        "        })\n",
        "        .array(),\n",
        "    }),\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "46625540",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document saved to hello.txt\n"
          ]
        }
      ],
      "source": [
        "// Example invocation\n",
        "await writeDocumentTool.invoke({\n",
        "  content: \"Hello from LangGraph!\",\n",
        "  file_name: \"hello.txt\",\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e9ab0a5c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from LangGraph!\n"
          ]
        }
      ],
      "source": [
        "await readDocumentTool.invoke({ file_name: \"hello.txt\" });"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "797487d5",
      "metadata": {},
      "outputs": [
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chart has been generated and displayed to the user!\n"
          ]
        }
      ],
      "source": [
        "await chartTool.invoke({\n",
        "  data: [\n",
        "    { label: \"People who like graphs\", value: 5000 },\n",
        "    {\n",
        "      label: \"People who like LangGraph\",\n",
        "      value: 10000,\n",
        "    },\n",
        "  ],\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de85df8",
      "metadata": {},
      "source": [
        "## Helper Utilities\n",
        "\n",
        "We are going to create a few utility functions to make it more concise when we\n",
        "want to:\n",
        "\n",
        "1. Create a worker agent.\n",
        "2. Create a supervisor for the sub-graph.\n",
        "\n",
        "These will simplify the graph compositional code at the end for us so it's\n",
        "easier to see what's going on.\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">Compatibility</p>\n",
        "    <p>\n",
        "        The <a href=\"https://langchain-ai.github.io/langgraphjs/reference/types/langgraph_prebuilt.CreateReactAgentParams.html\"><code>stateModifier</code></a> parameter for <code>createReactAgent</code> below was added in <code>@langchain/langgraph>=0.2.27</code>.\n",
        "        <br />\n",
        "        If you are on an older version, you will need to use the deprecated <code>messageModifier</code> parameter.\n",
        "        <br />\n",
        "        For help upgrading, see <a href=\"/langgraphjs/how-tos/manage-ecosystem-dependencies/\">this guide</a>.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be11102",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { z } from \"zod\";\n",
        "import { HumanMessage, BaseMessage, SystemMessage } from \"@langchain/core/messages\";\n",
        "import {\n",
        "  ChatPromptTemplate,\n",
        "  MessagesPlaceholder,\n",
        "} from \"@langchain/core/prompts\";\n",
        "import { JsonOutputToolsParser } from \"langchain/output_parsers\";\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "import { Runnable } from \"@langchain/core/runnables\";\n",
        "import { StructuredToolInterface } from \"@langchain/core/tools\";\n",
        "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
        "\n",
        "const agentStateModifier = (\n",
        "  systemPrompt: string,\n",
        "  tools: StructuredToolInterface[],\n",
        "  teamMembers: string[],\n",
        "): ((state: typeof MessagesAnnotation.State) => BaseMessage[]) => {\n",
        "  const toolNames = tools.map((t) => t.name).join(\", \");\n",
        "  const systemMsgStart = new SystemMessage(systemPrompt +\n",
        "    \"\\nWork autonomously according to your specialty, using the tools available to you.\" +\n",
        "    \" Do not ask for clarification.\" +\n",
        "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\" +\n",
        "    ` You are chosen for a reason! You are one of the following team members: ${teamMembers.join(\", \")}.`)\n",
        "  const systemMsgEnd = new SystemMessage(`Supervisor instructions: ${systemPrompt}\\n` +\n",
        "      `Remember, you individually can only use these tools: ${toolNames}` +\n",
        "      \"\\n\\nEnd if you have already completed the requested task. Communicate the work completed.\");\n",
        "\n",
        "  return (state: typeof MessagesAnnotation.State): any[] => \n",
        "    [systemMsgStart, ...state.messages, systemMsgEnd];\n",
        "}\n",
        "\n",
        "async function runAgentNode(params: {\n",
        "  state: any;\n",
        "  agent: Runnable;\n",
        "  name: string;\n",
        "}) {\n",
        "  const { state, agent, name } = params;\n",
        "  const result = await agent.invoke({\n",
        "    messages: state.messages,\n",
        "  });\n",
        "  const lastMessage = result.messages[result.messages.length - 1];\n",
        "  return {\n",
        "    messages: [new HumanMessage({ content: lastMessage.content, name })],\n",
        "  };\n",
        "}\n",
        "\n",
        "async function createTeamSupervisor(\n",
        "  llm: ChatOpenAI,\n",
        "  systemPrompt: string,\n",
        "  members: string[],\n",
        "): Promise<Runnable> {\n",
        "  const options = [\"FINISH\", ...members];\n",
        "  const routeTool = {\n",
        "    name: \"route\",\n",
        "    description: \"Select the next role.\",\n",
        "    schema: z.object({\n",
        "      reasoning: z.string(),\n",
        "      next: z.enum([\"FINISH\", ...members]),\n",
        "      instructions: z.string().describe(\"The specific instructions of the sub-task the next role should accomplish.\"),\n",
        "    })\n",
        "  }\n",
        "  let prompt = ChatPromptTemplate.fromMessages([\n",
        "    [\"system\", systemPrompt],\n",
        "    new MessagesPlaceholder(\"messages\"),\n",
        "    [\n",
        "      \"system\",\n",
        "      \"Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}\",\n",
        "    ],\n",
        "  ]);\n",
        "  prompt = await prompt.partial({\n",
        "    options: options.join(\", \"),\n",
        "    team_members: members.join(\", \"),\n",
        "  });\n",
        "\n",
        "  const supervisor = prompt\n",
        "    .pipe(\n",
        "      llm.bindTools([routeTool], {\n",
        "        tool_choice: \"route\",\n",
        "      }),\n",
        "    )\n",
        "    .pipe(new JsonOutputToolsParser())\n",
        "    // select the first one\n",
        "    .pipe((x) => ({\n",
        "      next: x[0].args.next,\n",
        "      instructions: x[0].args.instructions,\n",
        "    }));\n",
        "\n",
        "  return supervisor;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34f84e3",
      "metadata": {},
      "source": [
        "## Define Agent Teams\n",
        "\n",
        "Now we can get to define our hierachical teams. \"Choose your player!\"\n",
        "\n",
        "### Research Team\n",
        "\n",
        "The research team will have a search agent and a web scraping \"research_agent\"\n",
        "as the two worker nodes. Let's create those, as well as the team supervisor.\n",
        "(Note: If you are running deno in a jupyter notebook, the web scraper won't work\n",
        "out of the box. We have commented out this code to accomodate this challenge)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "751f1587",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const ResearchTeamState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  team_members: Annotation<string[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  next: Annotation<string>({\n",
        "    reducer: (x, y) => y ?? x,\n",
        "    default: () => \"supervisor\",\n",
        "  }),\n",
        "  instructions: Annotation<string>({\n",
        "    reducer: (x, y) => y ?? x,\n",
        "    default: () => \"Solve the human's question.\",\n",
        "  }),\n",
        "})\n",
        "\n",
        "const llm = new ChatOpenAI({ modelName: \"gpt-4o\" });\n",
        "\n",
        "const searchNode = (state: typeof ResearchTeamState.State) => {\n",
        "  const stateModifier = agentStateModifier(\n",
        "    \"You are a research assistant who can search for up-to-date info using the tavily search engine.\",\n",
        "    [tavilyTool],\n",
        "    state.team_members ?? [\"Search\"],\n",
        "  )\n",
        "  const searchAgent = createReactAgent({\n",
        "    llm,\n",
        "    tools: [tavilyTool],\n",
        "    stateModifier,\n",
        "  })\n",
        "  return runAgentNode({ state, agent: searchAgent, name: \"Search\" });\n",
        "};\n",
        "\n",
        "const researchNode = (state: typeof ResearchTeamState.State) => {\n",
        "  const stateModifier = agentStateModifier(\n",
        "    \"You are a research assistant who can scrape specified urls for more detailed information using the scrapeWebpage function.\",\n",
        "    [scrapeWebpage],\n",
        "    state.team_members ?? [\"WebScraper\"],\n",
        "  )\n",
        "  const researchAgent = createReactAgent({\n",
        "    llm,\n",
        "    tools: [scrapeWebpage],\n",
        "    stateModifier,\n",
        "  })\n",
        "  return runAgentNode({ state, agent: researchAgent, name: \"WebScraper\" });\n",
        "}\n",
        "\n",
        "const supervisorAgent = await createTeamSupervisor(\n",
        "  llm,\n",
        "  \"You are a supervisor tasked with managing a conversation between the\" +\n",
        "    \" following workers:  {team_members}. Given the following user request,\" +\n",
        "    \" respond with the worker to act next. Each worker will perform a\" +\n",
        "    \" task and respond with their results and status. When finished,\" +\n",
        "    \" respond with FINISH.\\n\\n\" +\n",
        "    \" Select strategically to minimize the number of steps taken.\",\n",
        "  [\"Search\", \"WebScraper\"],\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb5f120",
      "metadata": {},
      "source": [
        "Now that we've created the necessary components, defining their interactions is\n",
        "easy. Add the nodes to the team graph, and define the edges, which determine the\n",
        "transition criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dff6a285",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "const researchGraph = new StateGraph(ResearchTeamState)\n",
        "  .addNode(\"Search\", searchNode)\n",
        "  .addNode(\"supervisor\", supervisorAgent)\n",
        "  .addNode(\"WebScraper\", researchNode)\n",
        "  // Define the control flow\n",
        "  .addEdge(\"Search\", \"supervisor\")\n",
        "  .addEdge(\"WebScraper\", \"supervisor\")\n",
        "  .addConditionalEdges(\"supervisor\", (x) => x.next, {\n",
        "    Search: \"Search\",\n",
        "    WebScraper: \"WebScraper\",\n",
        "    FINISH: END,\n",
        "  })\n",
        "  .addEdge(START, \"supervisor\");\n",
        "\n",
        "const researchChain = researchGraph.compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e55c4b46",
      "metadata": {},
      "source": [
        "Since each team is itself a complete computational graph, you can directly query\n",
        "it like so:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "49e94ade",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  supervisor: {\n",
            "    next: 'WebScraper',\n",
            "    instructions: 'Find the current price of a Big Mac in Argentina.'\n",
            "  }\n",
            "}\n",
            "----\n",
            "{\n",
            "  WebScraper: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"I attempted to scrape a relevant article from The Guardian but encountered a 404 error, indicating that the page could not be found.\\n\\nPlease provide another URL or specify another way I can assist you.\",\n",
            "        \"name\": \"WebScraper\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "----\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'WebScraper',\n",
            "    instructions: 'Find the price of a Big Mac in Argentina from any available and reliable online sources.'\n",
            "  }\n",
            "}\n",
            "----\n",
            "{\n",
            "  WebScraper: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"I couldn't retrieve the specific price information for a Big Mac in Argentina from the sources attempted.\\n\\nFor accurate and updated details, you might want to check the latest economic reports or specific websites that track the Big Mac Index.\",\n",
            "        \"name\": \"WebScraper\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "----\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'Search',\n",
            "    instructions: 'Find the current price of a Big Mac in Argentina.'\n",
            "  }\n",
            "}\n",
            "----\n",
            "{\n",
            "  Search: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"The current price of a Big Mac in Argentina as of December 2023 is $5.43 USD. For further details, you can visit the source [here](https://www.globalproductprices.com/Argentina/big_mac_menu_prices/).\",\n",
            "        \"name\": \"Search\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "----\n",
            "{ supervisor: { next: 'FINISH', instructions: '' } }\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "const streamResults = researchChain.stream(\n",
        "  {\n",
        "    messages: [new HumanMessage(\"What's the price of a big mac in Argentina?\")],\n",
        "  },\n",
        "  { recursionLimit: 100 },\n",
        ");\n",
        "for await (const output of await streamResults) {\n",
        "  if (!output?.__end__) {\n",
        "    console.log(output);\n",
        "    console.log(\"----\");\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adbfeef9",
      "metadata": {},
      "source": [
        "You can\n",
        "[click here](https://smith.langchain.com/public/3cf3f0d1-2e37-40fd-8345-d60fc7639c44/r)\n",
        "to see a LangSmith trace of the above run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5932c2b",
      "metadata": {},
      "source": [
        "### Document Writing Team\n",
        "\n",
        "Create the document writing team below using a similar approach. This time, we\n",
        "will give each agent access to different file-writing tools.\n",
        "\n",
        "Note that we are giving file-system access to our agent here, which is not safe\n",
        "in all cases.\n",
        "\n",
        "For the doc writing team, each agent will be writing to the same workspace. We\n",
        "don't want them to waste time checking which files are available, so we will\n",
        "force a call to a \"prelude\" function before an agent is invoked to populate the\n",
        "prompt template with the current directory's contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "13327b87",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { RunnableLambda } from \"@langchain/core/runnables\";\n",
        "\n",
        "const prelude = new RunnableLambda({\n",
        "  func: async (state: {\n",
        "    messages: BaseMessage[];\n",
        "    next: string;\n",
        "    instructions: string;\n",
        "  }) => {\n",
        "    let writtenFiles: string[] = [];\n",
        "    if (\n",
        "      !(await fs\n",
        "        .stat(WORKING_DIRECTORY)\n",
        "        .then(() => true)\n",
        "        .catch(() => false))\n",
        "    ) {\n",
        "      await fs.mkdir(WORKING_DIRECTORY, { recursive: true });\n",
        "    }\n",
        "    try {\n",
        "      const files = await fs.readdir(WORKING_DIRECTORY);\n",
        "      for (const file of files) {\n",
        "        writtenFiles.push(file);\n",
        "      }\n",
        "    } catch (error) {\n",
        "      console.error(error);\n",
        "    }\n",
        "    const filesList = writtenFiles.length > 0\n",
        "      ? \"\\nBelow are files your team has written to the directory:\\n\" +\n",
        "        writtenFiles.map((f) => ` - ${f}`).join(\"\\n\")\n",
        "      : \"No files written.\";\n",
        "    return { ...state, current_files: filesList };\n",
        "  },\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9498c9",
      "metadata": {},
      "source": [
        "The doc writing state then is similar to that of the research team. We will add\n",
        "the additional `current_files` state variable to reflect the shared workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "06610610",
      "metadata": {},
      "outputs": [],
      "source": [
        "// This defines the agent state for the document writing team\n",
        "const DocWritingState = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  team_members: Annotation<string[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  next: Annotation<string>({\n",
        "    reducer: (x, y) => y ?? x,\n",
        "    default: () => \"supervisor\",\n",
        "  }),\n",
        "  current_files: Annotation<string>({\n",
        "    reducer: (x, y) => (y ? `${x}\\n${y}` : x),\n",
        "    default: () => \"No files written.\",\n",
        "  }),\n",
        "  instructions: Annotation<string>({\n",
        "    reducer: (x, y) => y ?? x,\n",
        "    default: () => \"Solve the human's question.\",\n",
        "  }),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a71b41",
      "metadata": {},
      "source": [
        "The team will be comprised of three agents:\n",
        "\n",
        "- A doc writing agent\n",
        "- A note taking agent\n",
        "- A chart generating agent\n",
        "\n",
        "Note this isn't optimized for performance, but is illustrative of the pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cb86fb24",
      "metadata": {},
      "outputs": [],
      "source": [
        "const docWritingLlm = new ChatOpenAI({ modelName: \"gpt-4o\" });\n",
        "\n",
        "const docWritingNode = (state: typeof DocWritingState.State) => {\n",
        "  const stateModifier = agentStateModifier(\n",
        "    `You are an expert writing a research document.\\nBelow are files currently in your directory:\\n${state.current_files}`,\n",
        "    [writeDocumentTool, editDocumentTool, readDocumentTool],\n",
        "    state.team_members ?? [],\n",
        "  )\n",
        "  const docWriterAgent = createReactAgent({\n",
        "    llm: docWritingLlm,\n",
        "    tools: [writeDocumentTool, editDocumentTool, readDocumentTool],\n",
        "    stateModifier,\n",
        "  })\n",
        "  const contextAwareDocWriterAgent = prelude.pipe(docWriterAgent);\n",
        "  return runAgentNode({ state, agent: contextAwareDocWriterAgent, name: \"DocWriter\" });\n",
        "}\n",
        "\n",
        "const noteTakingNode = (state: typeof DocWritingState.State) => {\n",
        "  const stateModifier = agentStateModifier(\n",
        "    \"You are an expert senior researcher tasked with writing a paper outline and\" +\n",
        "    ` taking notes to craft a perfect paper. ${state.current_files}`,\n",
        "    [createOutlineTool, readDocumentTool],\n",
        "    state.team_members ?? [],\n",
        "  )\n",
        "  const noteTakingAgent = createReactAgent({\n",
        "    llm: docWritingLlm,\n",
        "    tools: [createOutlineTool, readDocumentTool],\n",
        "    stateModifier,\n",
        "  })\n",
        "  const contextAwareNoteTakingAgent = prelude.pipe(noteTakingAgent);\n",
        "  return runAgentNode({ state, agent: contextAwareNoteTakingAgent, name: \"NoteTaker\" });\n",
        "}\n",
        "\n",
        "const chartGeneratingNode = async (\n",
        "  state: typeof DocWritingState.State,\n",
        ") => {\n",
        "  const stateModifier = agentStateModifier(\n",
        "    \"You are a data viz expert tasked with generating charts for a research project.\" +\n",
        "    `${state.current_files}`,\n",
        "    [readDocumentTool, chartTool],\n",
        "    state.team_members ?? [],\n",
        "  )\n",
        "  const chartGeneratingAgent = createReactAgent({\n",
        "    llm: docWritingLlm,\n",
        "    tools: [readDocumentTool, chartTool],\n",
        "    stateModifier,\n",
        "  })\n",
        "  const contextAwareChartGeneratingAgent = prelude.pipe(chartGeneratingAgent);\n",
        "  return runAgentNode({ state, agent: contextAwareChartGeneratingAgent, name: \"ChartGenerator\" });\n",
        "}\n",
        "\n",
        "const docTeamMembers = [\"DocWriter\", \"NoteTaker\", \"ChartGenerator\"];\n",
        "const docWritingSupervisor = await createTeamSupervisor(\n",
        "  docWritingLlm,\n",
        "  \"You are a supervisor tasked with managing a conversation between the\" +\n",
        "    \" following workers:  {team_members}. Given the following user request,\" +\n",
        "    \" respond with the worker to act next. Each worker will perform a\" +\n",
        "    \" task and respond with their results and status. When finished,\" +\n",
        "    \" respond with FINISH.\\n\\n\" +\n",
        "    \" Select strategically to minimize the number of steps taken.\",\n",
        "  docTeamMembers,\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7814b63",
      "metadata": {},
      "source": [
        "With the objects themselves created, we can form the graph. Start by creating\n",
        "the \"nodes\", which will do the actual work, then define the edges to control how\n",
        "the program will progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2ab60a98",
      "metadata": {},
      "outputs": [],
      "source": [
        "// Create the graph here:\n",
        "const authoringGraph = new StateGraph(DocWritingState)\n",
        "  .addNode(\"DocWriter\", docWritingNode)\n",
        "  .addNode(\"NoteTaker\", noteTakingNode)\n",
        "  .addNode(\"ChartGenerator\", chartGeneratingNode)\n",
        "  .addNode(\"supervisor\", docWritingSupervisor)\n",
        "  // Add the edges that always occur\n",
        "  .addEdge(\"DocWriter\", \"supervisor\")\n",
        "  .addEdge(\"NoteTaker\", \"supervisor\")\n",
        "  .addEdge(\"ChartGenerator\", \"supervisor\")\n",
        "  // Add the edges where routing applies\n",
        "  .addConditionalEdges(\"supervisor\", (x) => x.next, {\n",
        "    DocWriter: \"DocWriter\",\n",
        "    NoteTaker: \"NoteTaker\",\n",
        "    ChartGenerator: \"ChartGenerator\",\n",
        "    FINISH: END,\n",
        "  })\n",
        "  .addEdge(START, \"supervisor\");\n",
        "\n",
        "const enterAuthoringChain = RunnableLambda.from(\n",
        "  ({ messages }: { messages: BaseMessage[] }) => {\n",
        "    return {\n",
        "      messages: messages,\n",
        "      team_members: [\"Doc Writer\", \"Note Taker\", \"Chart Generator\"],\n",
        "    };\n",
        "  },\n",
        ");\n",
        "const authoringChain = enterAuthoringChain.pipe(authoringGraph.compile());"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4941b52e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  supervisor: { next: 'DocWriter', instructions: 'Write a limerick.' }\n",
            "}\n",
            "---\n",
            "{\n",
            "  DocWriter: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"The limerick and the character frequency data have been successfully written to the files \\\"limerick.txt\\\" and \\\"character_count.json,\\\" respectively. Task completed.\",\n",
            "        \"name\": \"DocWriter\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'NoteTaker',\n",
            "    instructions: 'Please take notes on the limerick and record the character frequency data provided by the Doc Writer.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  NoteTaker: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"I have completed the requested task as follows:\\n\\n- Created a limerick.\\n- Generated a bar chart of the characters used in the limerick.\\n\\nNo further action is required. Task completed successfully.\",\n",
            "        \"name\": \"NoteTaker\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'ChartGenerator',\n",
            "    instructions: \"Create a bar chart from the character frequency data in 'character_count.json.'\"\n",
            "  }\n",
            "}\n",
            "---\n"
          ]
        },
        {
          "data": {
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  ChartGenerator: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"The limerick has been created, and a bar chart illustrating the frequency of characters used in the limerick has been successfully generated. Task completed.\",\n",
            "        \"name\": \"ChartGenerator\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: { next: 'DocWriter', instructions: 'Write a limerick.' }\n",
            "}\n",
            "---\n",
            "{\n",
            "  DocWriter: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"The requested task has been successfully completed. Here is a summary:\\n\\n1. Written a limerick and saved it to the file \\\"limerick.txt\\\".\\n2. Generated a bar chart of the characters used within that limerick.\\n\\nNo further action is required from my side. Task completed successfully.\",\n",
            "        \"name\": \"DocWriter\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: { next: 'DocWriter', instructions: 'Please write a limerick.' }\n",
            "}\n",
            "---\n",
            "{\n",
            "  DocWriter: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"I have written the requested limerick and saved it in \\\"limerick.txt.\\\" Below is the content of the limerick:\\n\\n```\\nThere once was a coder so bright,\\nWho worked through the day and the night.\\nWith lines of pure gold,\\nThe stories were told,\\nAnd the bugs, they all took to flight.\\n```\\n\\nThe task has been completed as requested.\",\n",
            "        \"name\": \"DocWriter\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'ChartGenerator',\n",
            "    instructions: 'Use the following limerick to create a bar chart of the character frequency:\\n' +\n",
            "      '\\n' +\n",
            "      'There once was a coder so bright,\\n' +\n",
            "      'Who worked through the day and the night.\\n' +\n",
            "      'With lines of pure gold,\\n' +\n",
            "      'The stories were told,\\n' +\n",
            "      'And the bugs, they all took to flight.'\n",
            "  }\n",
            "}\n",
            "---\n"
          ]
        },
        {
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  ChartGenerator: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"The task has been successfully completed. Here is a summary of the actions performed:\\n\\n1. Written a limerick.\\n2. Generated a bar chart illustrating the frequency of characters used in the limerick.\\n\\nThe bar chart has been generated and displayed. Task completed successfully.\",\n",
            "        \"name\": \"ChartGenerator\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{ supervisor: { next: 'FINISH', instructions: '' } }\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "let resultStream = await authoringChain.stream(\n",
        "  {\n",
        "    messages: [\n",
        "      new HumanMessage(\n",
        "        \"Write a limerick and make a bar chart of the characters used.\",\n",
        "      ),\n",
        "    ],\n",
        "  },\n",
        "  { recursionLimit: 100 },\n",
        ");\n",
        "\n",
        "for await (const step of resultStream) {\n",
        "  console.log(step);\n",
        "  console.log(\"---\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033b9d56",
      "metadata": {},
      "source": [
        "You can\n",
        "[click here](https://smith.langchain.com/public/076c5524-fb41-4a19-9d4e-f251b17af983/r)\n",
        "to see a representative LangSmith trace of the above run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b6df80",
      "metadata": {},
      "source": [
        "## Add Layers\n",
        "\n",
        "In this design, we are enforcing a top-down planning policy. We've created two\n",
        "graphs already, but we have to decide how to route work between the two.\n",
        "\n",
        "We'll create a _third_ graph to orchestrate the previous two, and add some\n",
        "connectors to define how this top-level state is shared between the different\n",
        "graphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "59910ece",
      "metadata": {},
      "outputs": [],
      "source": [
        "// Define the top-level State interface\n",
        "const State = Annotation.Root({\n",
        "  messages: Annotation<BaseMessage[]>({\n",
        "    reducer: (x, y) => x.concat(y),\n",
        "  }),\n",
        "  next: Annotation<string>({\n",
        "    reducer: (x, y) => y ?? x,\n",
        "    default: () => \"ResearchTeam\",\n",
        "  }),\n",
        "  instructions: Annotation<string>({\n",
        "    reducer: (x, y) => y ?? x,\n",
        "    default: () => \"Resolve the user's request.\",\n",
        "  }),\n",
        "});\n",
        "\n",
        "const supervisorNode = await createTeamSupervisor(\n",
        "  llm,\n",
        "  \"You are a supervisor tasked with managing a conversation between the\" +\n",
        "    \" following teams: {team_members}. Given the following user request,\" +\n",
        "    \" respond with the worker to act next. Each worker will perform a\" +\n",
        "    \" task and respond with their results and status. When finished,\" +\n",
        "    \" respond with FINISH.\\n\\n\" +\n",
        "    \" Select strategically to minimize the number of steps taken.\",\n",
        "  [\"ResearchTeam\", \"PaperWritingTeam\"],\n",
        ");\n",
        "\n",
        "const getMessages = RunnableLambda.from((state: typeof State.State) => {\n",
        "  return { messages: state.messages };\n",
        "});\n",
        "\n",
        "const joinGraph = RunnableLambda.from((response: any) => {\n",
        "  return {\n",
        "    messages: [response.messages[response.messages.length - 1]],\n",
        "  };\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14da3813",
      "metadata": {},
      "source": [
        "Now we can finally create the top-level graph below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "0f40c195",
      "metadata": {},
      "outputs": [],
      "source": [
        "const superGraph = new StateGraph(State)\n",
        "  .addNode(\"ResearchTeam\", async (input) => {\n",
        "    const getMessagesResult = await getMessages.invoke(input);\n",
        "    const researchChainResult = await researchChain.invoke({\n",
        "      messages: getMessagesResult.messages,\n",
        "    });\n",
        "    const joinGraphResult = await joinGraph.invoke({\n",
        "      messages: researchChainResult.messages,\n",
        "    });\n",
        "  })\n",
        "  .addNode(\"PaperWritingTeam\", getMessages.pipe(authoringChain).pipe(joinGraph))\n",
        "  .addNode(\"supervisor\", supervisorNode)\n",
        "  .addEdge(\"ResearchTeam\", \"supervisor\")\n",
        "  .addEdge(\"PaperWritingTeam\", \"supervisor\")\n",
        "  .addConditionalEdges(\"supervisor\", (x) => x.next, {\n",
        "    PaperWritingTeam: \"PaperWritingTeam\",\n",
        "    ResearchTeam: \"ResearchTeam\",\n",
        "    FINISH: END,\n",
        "  })\n",
        "  .addEdge(START, \"supervisor\");\n",
        "\n",
        "const compiledSuperGraph = superGraph.compile();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012366f4",
      "metadata": {},
      "source": [
        "With the full graph defined, try invoking it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "7989fd5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  supervisor: {\n",
            "    next: 'ResearchTeam',\n",
            "    instructions: 'Please look up a current event and summarize it briefly.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  ResearchTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"I encountered an access issue while trying to scrape the BBC News website for recent events. However, I was able to gather information about recent events in October 2023 from other sources and produced the poem along with the word distribution chart as per your request. Here is a summary of tasks completed:\\n\\n- **Created a poem inspired by recent events including conflicts, humanitarian crises, and legal decisions.**\\n- **Presented a word distribution list for the poem.**\\n\\nIf further specific details are needed or additional tasks are required, please let me know!\",\n",
            "        \"name\": \"WebScraper\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'ResearchTeam',\n",
            "    instructions: 'Look up a current event from October 2023. Provide a summary of the event including the key details and any relevant sources.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  ResearchTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"I have completed the requested tasks:\\n\\n1. **Created a poem inspired by current events in October 2023.**\\n2. **Generated a word distribution list for the poem.**\\n\\nIf you need further tasks or additional information, please let me know!\",\n",
            "        \"name\": \"WebScraper\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'PaperWritingTeam',\n",
            "    instructions: 'Review the gathered information about recent events in October 2023 and create a poem inspired by these events. Then, generate a word distribution list from the poem.'\n",
            "  }\n",
            "}\n",
            "---\n"
          ]
        },
        {
          "data": { 
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  PaperWritingTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Completed:\\n\\n- **Generated a bar chart depicting the word frequency distribution for the poem inspired by current events in October 2023.**\\n\\nThe bar chart has been successfully displayed. If you need any further assistance, please let me know!\",\n",
            "        \"name\": \"ChartGenerator\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'PaperWritingTeam',\n",
            "    instructions: 'Utilize the latest available information about a recent event from October 2023 and write a poem reflecting it.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  PaperWritingTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Completed:\\n\\n- **Generated a bar chart depicting the word frequency distribution for the poem inspired by current events in October 2023.**\\n\\nThe bar chart has been successfully displayed. If you need any further assistance, please let me know!\",\n",
            "        \"name\": \"ChartGenerator\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'ResearchTeam',\n",
            "    instructions: \"Identify a current event from a reliable news source, ensuring it's up-to-date and relevant for the specified location and date, i.e., October 2023. Provide a brief summary including key details and context.\"\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  ResearchTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Completed:\\n\\n- **Generated a bar chart depicting the word frequency distribution for the poem inspired by current events in October 2023.**\\n\\nThe bar chart has been successfully displayed. If you need any further assistance, please let me know!\",\n",
            "        \"name\": \"ChartGenerator\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'PaperWritingTeam',\n",
            "    instructions: 'Compile the poem inspired by current events and the word distribution to finalize documentation. Also, ensure the generated bar chart is included along with any additional relevant details.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  PaperWritingTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Completed:\\n\\n- **Crafted a detailed poem inspired by current events in October 2023.**\\n- **Generated a bar chart illustrating the word distribution within the poem.**\\n\\nIf there are additional tasks or further details needed, please let me know!\",\n",
            "        \"name\": \"NoteTaker\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'ResearchTeam',\n",
            "    instructions: 'Look up a current event from a reliable news source and summarize the main details.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  ResearchTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Completed:\\n\\n- **Crafted a detailed poem inspired by current events in October 2023.**\\n- **Generated a bar chart illustrating the word distribution within the poem.**\\n\\nIf there are additional tasks or further details needed, please let me know!\",\n",
            "        \"name\": \"NoteTaker\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'ResearchTeam',\n",
            "    instructions: 'Look up a current event from October 2023 to collect detailed information that can be used as a basis for writing a poem.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  ResearchTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Overview:\\n1. **Research Current Events**: Looked up current events from October 2023.\\n2. **Create a Poem**: Based on these events.\\n3. **Plot a Word Distribution Bar Chart**: From the poem.\\n\\n### Current Events Summary from October 2023:\\n1. **Gaza Conflicts**: Israel launched airstrikes resulting in civilian casualties, and humanitarian concerns rise over fuel and food deprivation in Gaza.\\n2. **Global Political Movements**: Protests around the world related to various conflicts.\\n3. **Ecuador Political Assassination**: Six suspects in the assassination of an anti-corruption presidential candidate were killed in prison.\\n4. **Impact of Natural Disasters**: A dense fog caused a massive vehicle crash in Louisiana.\\n5. **India's Legal Decisions**: The Supreme Court of India declined to legalize same-sex marriage.\\n\\n### Poem Based on Current Events (October 2023):\\n\\n*In October’s shadow, skies are torn,\\nBy cries of anguish, lives forlorn.\\nIn Gaza’s streets, the echo sounds,\\nOf airstrikes fierce, where pain abounds.*\\n\\n*The world's a stage of silent screams,\\nIn Ecuador, the end of dreams.\\nA candidate for change succumbed,\\nIn prison cells, dark fates are plumbed.*\\n\\n*O’er foggy roads in states afar,\\nIn dense embrace, the wrecks of car.\\nIn havoc’s path and choking gray,\\nLives dispersed in tragic fray.*\\n\\n*In India’s courts, the closed doors stay,\\nLove unsanctioned, cast away.\\nUnyielding laws in hearts conflate,\\nFor freedoms still, we arbitrate.*\\n\\n*Across the globe, in voices loud,\\nPeace and justice we avowed.\\nBound by hope and tempests’ test,\\nDreams of solace, unexpressed.*\\n\\n### Bar Chart of Word Distribution in the Poem:\\n\\nI’ll now share the word frequency data that can be used for plotting a bar chart.\\n\\n### Word Frequency Data:\\n- October: 1\\n- shadow: 1\\n- skies: 1\\n- torn: 1\\n- cries: 1\\n- anguish: 1\\n- lives: 2\\n- forlorn: 1\\n- Gaza: 1\\n- streets: 1\\n- echo: 1\\n- sounds: 1\\n- airstrikes: 1\\n- fierce: 1\\n- pain: 1\\n- abounds: 1\\n- world: 1\\n- stage: 1\\n- silent: 1\\n- screams: 1\\n- Ecuador: 1\\n- end: 1\\n- dreams: 2\\n- candidate: 1\\n- change: 1\\n- succumbed: 1\\n- prison: 1\\n- cells: 1\\n- dark: 1\\n- fates: 1\\n- plumbed: 1\\n- foggy: 1\\n- roads: 1\\n- states: 1\\n- afar: 1\\n- dense: 1\\n- embrace: 1\\n- wrecks: 1\\n- car: 1\\n- havoc: 1\\n- path: 1\\n- choking: 1\\n- gray: 1\\n- dispersed: 1\\n- tragic: 1\\n- fray: 1\\n- India: 1\\n- courts: 1\\n- closed: 1\\n- doors: 1\\n- stay: 1\\n- unsanctioned: 1\\n- cast: 1\\n- unyielding: 1\\n- laws: 1\\n- hearts: 1\\n- conflate: 1\\n- freedoms: 1\\n- arbitrate: 1\\n- across: 1\\n- globe: 1\\n- voices: 1\\n- loud: 1\\n- peace: 1\\n- justice: 1\\n- avowed: 1\\n- bound: 1\\n- hope: 1\\n- tempests: 1\\n- test: 1\\n- solace: 1\\n- unexpressed: 1\\n\\nThese word frequencies can be used to plot a bar chart.\\n\\n### Conclusion:\\nThe poem and the word distribution data are ready. If you require further assistance or specific visual plots, please let me know!\",\n",
            "        \"name\": \"Search\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  supervisor: {\n",
            "    next: 'PaperWritingTeam',\n",
            "    instructions: 'Using the provided summary of current events from October 2023, craft a poem about the events. Then create a list of word frequencies within the poem, which will be used to generate a bar chart.'\n",
            "  }\n",
            "}\n",
            "---\n",
            "{\n",
            "  PaperWritingTeam: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        \"content\": \"### Task Completed:\\n\\n- **Created a Poem on Current Events**: A poem inspired by events in October 2023, including conflicts, political issues, and natural disasters.\\n- **Document Created**: A document titled \\\"Poem_and_Word_Distribution_October_2023.doc\\\" containing the poem as well as a word frequency distribution list.\\n\\nDocument: **Poem_and_Word_Distribution_October_2023.doc**\\n\\nIf further tasks or details are needed, please let me know!\",\n",
            "        \"name\": \"DocWriter\",\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {}\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---\n",
            "{ supervisor: { next: 'FINISH', instructions: '' } }\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "resultStream = compiledSuperGraph.stream(\n",
        "  {\n",
        "    messages: [\n",
        "      new HumanMessage(\n",
        "        \"Look up a current event, write a poem about it, then plot a bar chart of the distribution of words therein.\",\n",
        "      ),\n",
        "    ],\n",
        "  },\n",
        "  { recursionLimit: 150 },\n",
        ");\n",
        "\n",
        "for await (const step of await resultStream) {\n",
        "  if (!step.__end__) {\n",
        "    console.log(step);\n",
        "    console.log(\"---\");\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32bd6e42",
      "metadata": {},
      "source": [
        "As before, you can\n",
        "[click here](https://smith.langchain.com/public/338b0d73-e699-43d2-8822-90b3787c0111/r)\n",
        "to see a LangSmith run of the above graph.\n",
        "\n",
        "```\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.14.5"
      }
    },
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file>

<file path="multi_agent/multi_agent_collaboration.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fd1948-b5c3-48c4-b10e-2ae7e8c83334",
   "metadata": {},
   "source": [
    "# Basic Multi-agent Collaboration\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a\n",
    "single domain, but even using powerful models like `gpt-4`, it can be less\n",
    "effective at using many tools.\n",
    "\n",
    "One way to approach complicated tasks is through a \"divide-and-conquer\"\n",
    "approach: create an specialized agent for each task or domain and route tasks to\n",
    "the correct \"expert\".\n",
    "\n",
    "This notebook (inspired by the paper\n",
    "[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155),\n",
    "by Wu, et. al.) shows one way to do this using LangGraph.\n",
    "\n",
    "The resulting graph will look something like the following diagram:\n",
    "\n",
    "![](./img/simple_multi_agent_diagram.png)\n",
    "\n",
    "Before we get started, a quick note: this and other multi-agent notebooks are\n",
    "designed to show _how_ you can implement certain design patterns in LangGraph.\n",
    "If the pattern suits your needs, we recommend combining it with some of the\n",
    "other fundamental patterns described elsewhere in the docs for best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743c19df-6da9-4d1e-b2d2-ea40080b9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "// process.env.TAVILY_API_KEY = \"sk_...\";\n",
    "// process.env.LANGCHAIN_API_KEY = \"sk_...\";\n",
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_PROJECT = \"Multi-agent Collaboration: LangGraphJS\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4344a7-21df-4d54-90d2-9d19b3416ffb",
   "metadata": {},
   "source": [
    "## Helper Utilities\n",
    "\n",
    "The following helper functions will help create agents. These agents will then\n",
    "be nodes in the graph.\n",
    "\n",
    "You can skip ahead if you just want to see what the graph looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4325a10e-38dc-4a98-9004-e1525eaba377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { StructuredTool } from \"@langchain/core/tools\";\n",
    "import { convertToOpenAITool } from \"@langchain/core/utils/function_calling\";\n",
    "import { Runnable } from \"@langchain/core/runnables\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "/**\n",
    " * Create an agent that can run a set of tools.\n",
    " */\n",
    "async function createAgent({\n",
    "  llm,\n",
    "  tools,\n",
    "  systemMessage,\n",
    "}: {\n",
    "  llm: ChatOpenAI;\n",
    "  tools: StructuredTool[];\n",
    "  systemMessage: string;\n",
    "}): Promise<Runnable> {\n",
    "  const toolNames = tools.map((tool) => tool.name).join(\", \");\n",
    "  const formattedTools = tools.map((t) => convertToOpenAITool(t));\n",
    "\n",
    "  let prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\n",
    "      \"system\",\n",
    "      \"You are a helpful AI assistant, collaborating with other assistants.\" +\n",
    "      \" Use the provided tools to progress towards answering the question.\" +\n",
    "      \" If you are unable to fully answer, that's OK, another assistant with different tools \" +\n",
    "      \" will help where you left off. Execute what you can to make progress.\" +\n",
    "      \" If you or any of the other assistants have the final answer or deliverable,\" +\n",
    "      \" prefix your response with FINAL ANSWER so the team knows to stop.\" +\n",
    "      \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "    ],\n",
    "    new MessagesPlaceholder(\"messages\"),\n",
    "  ]);\n",
    "  prompt = await prompt.partial({\n",
    "    system_message: systemMessage,\n",
    "    tool_names: toolNames,\n",
    "  });\n",
    "\n",
    "  return prompt.pipe(llm.bind({ tools: formattedTools }));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a8c3c-86a0-46aa-b970-ab070fb787d9",
   "metadata": {},
   "source": [
    "### Define State\n",
    "\n",
    "We first define the state of the graph. This will just a list of messages, along\n",
    "with a key to track the most recent sender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290c91d4-f6f4-443c-8181-233d39102974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// This defines the object that is passed between each node\n",
    "// in the graph. We will create different nodes for each agent and tool\n",
    "const AgentState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "  sender: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x ?? \"user\",\n",
    "    default: () => \"user\",\n",
    "  }),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b40de2-5dd4-4d5b-882e-577210723ff4",
   "metadata": {},
   "source": [
    "## Define tools\n",
    "\n",
    "These tools will be used by our worker agents to answer our questions.\n",
    "\n",
    "We will create a chart tool (using d3.js), and the LangChain TavilySearchResults\n",
    "tool for web search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca076f3b-a729-4ca9-8f91-05c2ba58d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "require(\"esm-hook\"); // Only for running this in TSLab. See: https://github.com/yunabe/tslab/issues/72\n",
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import * as d3 from \"d3\";\n",
    "// ----------ATTENTION----------\n",
    "// If attempting to run this notebook locally, you must follow these instructions\n",
    "// to install the necessary system dependencies for the `canvas` package.\n",
    "// https://www.npmjs.com/package/canvas#compiling\n",
    "// -----------------------------\n",
    "import { createCanvas } from \"canvas\";\n",
    "import { z } from \"zod\";\n",
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const chartTool = tool(\n",
    "  ({ data }) => {\n",
    "    const width = 500;\n",
    "    const height = 500;\n",
    "    const margin = { top: 20, right: 30, bottom: 30, left: 40 };\n",
    "\n",
    "    const canvas = createCanvas(width, height);\n",
    "    const ctx = canvas.getContext(\"2d\");\n",
    "\n",
    "    const x = d3\n",
    "      .scaleBand()\n",
    "      .domain(data.map((d) => d.label))\n",
    "      .range([margin.left, width - margin.right])\n",
    "      .padding(0.1);\n",
    "\n",
    "    const y = d3\n",
    "      .scaleLinear()\n",
    "      .domain([0, d3.max(data, (d) => d.value) ?? 0])\n",
    "      .nice()\n",
    "      .range([height - margin.bottom, margin.top]);\n",
    "\n",
    "    const colorPalette = [\n",
    "      \"#e6194B\",\n",
    "      \"#3cb44b\",\n",
    "      \"#ffe119\",\n",
    "      \"#4363d8\",\n",
    "      \"#f58231\",\n",
    "      \"#911eb4\",\n",
    "      \"#42d4f4\",\n",
    "      \"#f032e6\",\n",
    "      \"#bfef45\",\n",
    "      \"#fabebe\",\n",
    "    ];\n",
    "\n",
    "    data.forEach((d, idx) => {\n",
    "      ctx.fillStyle = colorPalette[idx % colorPalette.length];\n",
    "      ctx.fillRect(\n",
    "        x(d.label) ?? 0,\n",
    "        y(d.value),\n",
    "        x.bandwidth(),\n",
    "        height - margin.bottom - y(d.value),\n",
    "      );\n",
    "    });\n",
    "\n",
    "    ctx.beginPath();\n",
    "    ctx.strokeStyle = \"black\";\n",
    "    ctx.moveTo(margin.left, height - margin.bottom);\n",
    "    ctx.lineTo(width - margin.right, height - margin.bottom);\n",
    "    ctx.stroke();\n",
    "\n",
    "    ctx.textAlign = \"center\";\n",
    "    ctx.textBaseline = \"top\";\n",
    "    x.domain().forEach((d) => {\n",
    "      const xCoord = (x(d) ?? 0) + x.bandwidth() / 2;\n",
    "      ctx.fillText(d, xCoord, height - margin.bottom + 6);\n",
    "    });\n",
    "\n",
    "    ctx.beginPath();\n",
    "    ctx.moveTo(margin.left, height - margin.top);\n",
    "    ctx.lineTo(margin.left, height - margin.bottom);\n",
    "    ctx.stroke();\n",
    "\n",
    "    ctx.textAlign = \"right\";\n",
    "    ctx.textBaseline = \"middle\";\n",
    "    const ticks = y.ticks();\n",
    "    ticks.forEach((d) => {\n",
    "      const yCoord = y(d); // height - margin.bottom - y(d);\n",
    "      ctx.moveTo(margin.left, yCoord);\n",
    "      ctx.lineTo(margin.left - 6, yCoord);\n",
    "      ctx.stroke();\n",
    "      ctx.fillText(d.toString(), margin.left - 8, yCoord);\n",
    "    });\n",
    "    tslab.display.png(canvas.toBuffer());\n",
    "    return \"Chart has been generated and displayed to the user!\";\n",
    "  },\n",
    "  {\n",
    "    name: \"generate_bar_chart\",\n",
    "    description:\n",
    "      \"Generates a bar chart from an array of data points using D3.js and displays it for the user.\",\n",
    "    schema: z.object({\n",
    "      data: z\n",
    "        .object({\n",
    "          label: z.string(),\n",
    "          value: z.number(),\n",
    "        })\n",
    "        .array(),\n",
    "    }),\n",
    "  }\n",
    ")\n",
    "\n",
    "const tavilyTool = new TavilySearchResults();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b54c0c-0b09-408b-abc5-86308929afb6",
   "metadata": {},
   "source": [
    "## Create graph\n",
    "\n",
    "Now that we've defined our tools and made some helper functions, will create the\n",
    "individual agents below and tell them how to talk to each other using LangGraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a283e-ea04-40c1-b792-f9e5f7d81203",
   "metadata": {},
   "source": [
    "### Define Agent Nodes\n",
    "\n",
    "In LangGraph, nodes represent functions that perform the work. In our example,\n",
    "we will have \"agent\" nodes and a \"callTool\" node.\n",
    "\n",
    "The input for every node is the graph's state. In our case, the state will have\n",
    "a list of messages as input, as well as the name of the previous node.\n",
    "\n",
    "First, let's define the nodes for the agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b790ca-9cef-4b22-b469-4b1d5d8424d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import type { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "\n",
    "// Helper function to run a node for a given agent\n",
    "async function runAgentNode(props: {\n",
    "  state: typeof AgentState.State;\n",
    "  agent: Runnable;\n",
    "  name: string;\n",
    "  config?: RunnableConfig;\n",
    "}) {\n",
    "  const { state, agent, name, config } = props;\n",
    "  let result = await agent.invoke(state, config);\n",
    "  // We convert the agent output into a format that is suitable\n",
    "  // to append to the global state\n",
    "  if (!result?.tool_calls || result.tool_calls.length === 0) {\n",
    "    // If the agent is NOT calling a tool, we want it to\n",
    "    // look like a human message.\n",
    "    result = new HumanMessage({ ...result, name: name });\n",
    "  }\n",
    "  return {\n",
    "    messages: [result],\n",
    "    // Since we have a strict workflow, we can\n",
    "    // track the sender so we know who to pass to next.\n",
    "    sender: name,\n",
    "  };\n",
    "}\n",
    "\n",
    "const llm = new ChatOpenAI({ modelName: \"gpt-4o\" });\n",
    "\n",
    "// Research agent and node\n",
    "const researchAgent = await createAgent({\n",
    "  llm,\n",
    "  tools: [tavilyTool],\n",
    "  systemMessage:\n",
    "    \"You should provide accurate data for the chart generator to use.\",\n",
    "});\n",
    "\n",
    "async function researchNode(\n",
    "  state: typeof AgentState.State,\n",
    "  config?: RunnableConfig,\n",
    ") {\n",
    "  return runAgentNode({\n",
    "    state: state,\n",
    "    agent: researchAgent,\n",
    "    name: \"Researcher\",\n",
    "    config,\n",
    "  });\n",
    "}\n",
    "\n",
    "// Chart Generator\n",
    "const chartAgent = await createAgent({\n",
    "  llm,\n",
    "  tools: [chartTool],\n",
    "  systemMessage: \"Any charts you display will be visible by the user.\",\n",
    "});\n",
    "\n",
    "async function chartNode(state: typeof AgentState.State) {\n",
    "  return runAgentNode({\n",
    "    state: state,\n",
    "    agent: chartAgent,\n",
    "    name: \"ChartGenerator\",\n",
    "  });\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd9f7d1-a839-4f28-8ef3-3cf665c6016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    AIMessage {\n",
      "      \"id\": \"chatcmpl-9yopin5fBlXtA15wWiUlDyiKT9T9P\",\n",
      "      \"content\": \"\",\n",
      "      \"additional_kwargs\": {\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_SRihR0BFFtw3TlHQtiBDPR3v\",\n",
      "            \"type\": \"function\",\n",
      "            \"function\": \"[Object]\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"response_metadata\": {\n",
      "        \"tokenUsage\": {\n",
      "          \"completionTokens\": 22,\n",
      "          \"promptTokens\": 192,\n",
      "          \"totalTokens\": 214\n",
      "        },\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"system_fingerprint\": \"fp_3aa7262c27\"\n",
      "      },\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"name\": \"tavily_search_results_json\",\n",
      "          \"args\": {\n",
      "            \"input\": \"US primaries 2024 updates\"\n",
      "          },\n",
      "          \"type\": \"tool_call\",\n",
      "          \"id\": \"call_SRihR0BFFtw3TlHQtiBDPR3v\"\n",
      "        }\n",
      "      ],\n",
      "      \"invalid_tool_calls\": [],\n",
      "      \"usage_metadata\": {\n",
      "        \"input_tokens\": 192,\n",
      "        \"output_tokens\": 22,\n",
      "        \"total_tokens\": 214\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  sender: 'Researcher'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Example invocation\n",
    "const researchResults = await researchNode({\n",
    "  messages: [new HumanMessage(\"Research the US primaries in 2024\")],\n",
    "  sender: \"User\",\n",
    "});\n",
    "\n",
    "researchResults;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7f1b2-24a3-4340-bcb2-feb22e344fb6",
   "metadata": {},
   "source": [
    "### Define Tool Node\n",
    "\n",
    "We now define a node to run the tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a79c76-5c7c-42f6-91cf-635bc8305804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const tools = [tavilyTool, chartTool];\n",
    "// This runs tools in the graph\n",
    "const toolNode = new ToolNode<typeof AgentState.State>(tools);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac38a474-2354-4bbf-b35a-b4b0e8135f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  messages: [\n",
      "    ToolMessage {\n",
      "      \"content\": \"[{\\\"title\\\":\\\"Election Results 2024: Live Election Map | Races by State - POLITICO\\\",\\\"url\\\":\\\"https://www.politico.com/2024-election/results/\\\",\\\"content\\\":\\\"Live 2024 election results and maps by state. POLITICO's coverage of 2024 primary races for President, Senate, House and Governors.\\\",\\\"score\\\":0.9798227,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Presidential Primary Election Results 2024 | Delegate Count Map by ...\\\",\\\"url\\\":\\\"https://www.politico.com/2024-election/results/president/\\\",\\\"content\\\":\\\"Live 2024 Presidential election results, maps and delegate counts by state. POLITICO's coverage of 2024 primary races for President, Senate, House and Governors.\\\",\\\"score\\\":0.97666925,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Election 2024: Presidential campaign news, polls and results | CNN Politics\\\",\\\"url\\\":\\\"https://www.cnn.com/election/2024\\\",\\\"content\\\":\\\"2024 election guide: Presidential candidates, polls, primaries and caucuses, voter information and results for November 5, 2024\\\",\\\"score\\\":0.92455155,\\\"raw_content\\\":null},{\\\"title\\\":\\\"2024 Election news: Latest on the presidential race, polls & results\\\",\\\"url\\\":\\\"https://www.politico.com/news/2024-elections\\\",\\\"content\\\":\\\"POLITICO\\\\nPolitico Logo\\\\nWASHINGTON & POLITICS\\\\n2024 ELECTIONS\\\\nSTATE POLITICS & POLICY\\\\nGLOBAL POLITICS & POLICY\\\\nPOLICY NEWS\\\\nNEWSLETTERS\\\\nCOLUMNISTS\\\\nSERIES & MORE\\\\nPOLITICO Live\\\\nFollow us\\\\n2024 Elections\\\\nThe latest coverage of the 2024 presidential, House and Senate elections.\\\\n It's a question being debated in courtrooms across the country: Does the 14th Amendment of the U.S. Constitution bar Trump from running for president again because he supported or “engaged in insurrection or rebellion” for his role in the Jan. 6 attack on the Capitol?\\\\nRead More »\\\\nFormer House Speaker Kevin McCarthy has endorsed Trump in his 2024 run, but he hasn't always had the nicest things to say about the former president. By ERICA ORDEN\\\\n12/14/2023 03:15 PM EST\\\\nUpdated 12/14/2023 04:06 PM EST\\\\nNEW YORK — An appeals court on Thursday rejected former President Donald Trump’s effort to overturn the gag order barring him from making comments about the staff of the judge presiding over his $250 million civil fraud trial, dealing him another setback in his attempts to fight the restrictions.\\\\n | Charlie Neibergall/AP\\\\nPlaybook Deep Dive\\\\nHow Hunter Biden, Jack Smith, and Trump’s legal troubles are setting the stage for 2024\\\\nA week of new developments in impeachment, Donald Trump’s D.C. case, and Hunter Biden’s congressional inquiry showcased how the collision of law and politics will determine much of Republicans’ and Democrats’ political fortunes in 2024.\\\\n | Francis Chung/POLITICO\\\\nPolitics\\\\nCornel West thinks Biden won’t make it to the general election\\\\nThe independent candidate rejected the idea he could be a “spoiler” for Biden in an exclusive meeting with POLITICO.\\\\n\\\",\\\"score\\\":0.9069832,\\\"raw_content\\\":null},{\\\"title\\\":\\\"DNC 2024 live updates: Walz speaks tonight ahead of Harris' remarks ...\\\",\\\"url\\\":\\\"https://www.nbcnews.com/politics/2024-election/live-blog/election-2024-dnc-live-updates-rcna165228\\\",\\\"content\\\":\\\"Latest news and live updates on the Democratic National Convention and the 2024 presidential election campaigns as Harris and ... Navy, Coast Guard, Air Force Space Force, or the United States ...\\\",\\\"score\\\":0.8344069,\\\"raw_content\\\":null}]\",\n",
      "      \"name\": \"tavily_search_results_json\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {},\n",
      "      \"tool_call_id\": \"call_SRihR0BFFtw3TlHQtiBDPR3v\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Example invocation\n",
    "await toolNode.invoke(researchResults);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb30498-dbc4-4b20-980f-da08ebc9da56",
   "metadata": {},
   "source": [
    "### Define Edge Logic\n",
    "\n",
    "We can define some of the edge logic that is needed to decide what to do based\n",
    "on results of the agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4b4d37-e8a3-4abb-8d42-eaea26016f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { AIMessage } from \"@langchain/core/messages\";\n",
    "// Either agent can decide to end\n",
    "function router(state: typeof AgentState.State) {\n",
    "  const messages = state.messages;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "  if (lastMessage?.tool_calls && lastMessage.tool_calls.length > 0) {\n",
    "    // The previous agent is invoking a tool\n",
    "    return \"call_tool\";\n",
    "  }\n",
    "  if (\n",
    "    typeof lastMessage.content === \"string\" &&\n",
    "    lastMessage.content.includes(\"FINAL ANSWER\")\n",
    "  ) {\n",
    "    // Any agent decided the work is done\n",
    "    return \"end\";\n",
    "  }\n",
    "  return \"continue\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9359c34-e191-43a2-a3d4-f2dea636dfd2",
   "metadata": {},
   "source": [
    "### Define the Graph\n",
    "\n",
    "We can now put it all together and define the graph!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c59d31-e753-472e-ae7b-94ab068ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// 1. Create the graph\n",
    "const workflow = new StateGraph(AgentState)\n",
    "   // 2. Add the nodes; these will do the work\n",
    "  .addNode(\"Researcher\", researchNode)\n",
    "  .addNode(\"ChartGenerator\", chartNode)\n",
    "  .addNode(\"call_tool\", toolNode);\n",
    "\n",
    "// 3. Define the edges. We will define both regular and conditional ones\n",
    "// After a worker completes, report to supervisor\n",
    "workflow.addConditionalEdges(\"Researcher\", router, {\n",
    "  // We will transition to the other agent\n",
    "  continue: \"ChartGenerator\",\n",
    "  call_tool: \"call_tool\",\n",
    "  end: END,\n",
    "});\n",
    "\n",
    "workflow.addConditionalEdges(\"ChartGenerator\", router, {\n",
    "  // We will transition to the other agent\n",
    "  continue: \"Researcher\",\n",
    "  call_tool: \"call_tool\",\n",
    "  end: END,\n",
    "});\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "  \"call_tool\",\n",
    "  // Each agent node updates the 'sender' field\n",
    "  // the tool calling node does not, meaning\n",
    "  // this edge will route back to the original agent\n",
    "  // who invoked the tool\n",
    "  (x) => x.sender,\n",
    "  {\n",
    "    Researcher: \"Researcher\",\n",
    "    ChartGenerator: \"ChartGenerator\",\n",
    "  },\n",
    ");\n",
    "\n",
    "workflow.addEdge(START, \"Researcher\");\n",
    "const graph = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9447e7-9ab6-43eb-8ae6-9b52f8ba8425",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "With the graph created, you can invoke it! Let's have it chart some stats for\n",
    "us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3264bf5-19d9-4f36-af7c-e189d0d9e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  type: 'ai',\n",
      "  content: '',\n",
      "  tool_calls: [\n",
      "    {\n",
      "      name: 'tavily_search_results_json',\n",
      "      args: { input: 'US GDP over the past 3 years' },\n",
      "      type: 'tool_call',\n",
      "      id: 'call_ZrmEsfu4B8SKpDhUJY5vcps8'\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{ sender: 'Researcher' }\n",
      "----\n",
      "{\n",
      "  type: 'tool',\n",
      "  content: `[{\"title\":\"United States GDP Annual Growth Rate - TRADING ECONOMICS\",\"url\":\"https://tradingeconomics.com/united-states/gdp-growth-annual\",\"content\":\"The Gross Domestic Product (GDP) in the United States expanded 3.10 percent in the second quarter of 2024 over the same quarter of the previous year. This page provides the latest reported value for - United States GDP Annual Growth Rate - plus previous releases, historical high and low, short-term forecast and long-term prediction, economic calendar, survey consensus and news.\",\"score\":0.88798404,\"raw_content\":null},{\"title\":\"U.S. GDP 1960-2024 | MacroTrends\",\"url\":\"https://www.macrotrends.net/global-metrics/countries/USA/united-states/gdp-gross-domestic-product\",\"content\":\"U.S. gdp for 2021 was $23,315.08B, a 10.71% increase from 2020. U.S. gdp for 2020 was $21,060.47B, a 1.5% decline from 2019. U.S. gdp for 2019 was $21,380.98B, a 4.13% increase from 2018. GDP at purchaser's prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included ...\",\"score\":0.7666432,\"raw_content\":null},{\"title\":\"U.S. GDP Growth Rate 1960-2024 | MacroTrends\",\"url\":\"https://www.macrotrends.net/global-metrics/countries/USA/united-states/gdp-growth-rate\",\"content\":\"U.S. gdp growth rate for 2021 was 5.95%, a 8.71% increase from 2020. U.S. gdp growth rate for 2020 was -2.77%, a 5.06% decline from 2019. U.S. gdp growth rate for 2019 was 2.29%, a 0.65% decline from 2018. Annual percentage growth rate of GDP at market prices based on constant local currency. Aggregates are based on constant 2010 U.S. dollars.\",\"score\":0.6904547,\"raw_content\":null},{\"title\":\"U.S. GDP by Year, Compared to Recessions and Events - The Balance\",\"url\":\"https://www.thebalancemoney.com/us-gdp-by-year-3305543\",\"content\":\"U.S. GDP by Year, Compared to Recessions and Events\\\\nThe Strange Ups and Downs of the U.S. Economy Since 1929\\\\nThe Balance / Julie Bang\\\\nU.S. gross domestic product (GDP) by year is a good overview of economic growth in the United States. Rebasing changes the reference year (or base year) for the real (chained dollar and quantity index) estimates and price indexes and expresses GDP and other NIPA aggregates in terms of the prices of one year. You can compare the GDP by year to fiscal and monetary policies to get a complete picture of what works and what doesn't in the U.S. economy.\\\\n Real GDP is important because without canceling out the effects of inflation, the GDP could appear to grow, when really all that's happened is an increase in prices.\\\\n Key Takeaways\\\\nTypes of GDP\\\\nThe Bureau of Economic Analysis compiles the data.\",\"score\":0.5998954,\"raw_content\":null},{\"title\":\"US GDP over time - USAFacts\",\"url\":\"https://usafacts.org/data/topics/economy/economic-indicators/gdp/gross-domestic-product/\",\"content\":\"Data Adjustments\\\\nIs the economy growing?\\\\nRelated Metrics\\\\nAnnual percent change in real GDP\\\\n5.7%\\\\n2021\\\\nAnnual percent change in real GDP\\\\n5.7%\\\\n2021\\\\nExplore Gross domestic product\\\\nInteract with the data\\\\nData Adjustments\\\\nState Display\\\\nOur nation, in numbers\\\\nUSAFacts is a not-for-profit, nonpartisan civic initiative making government data easy for all Americans to access and understand.\\\\n • Check your spelling\\\\n• Try other search terms\\\\n• Use fewer words\\\\nGross domestic product\\\\nGross domestic product\\\\nGross domestic product (GDP) is the value of all goods and services produced in the US. All topics\\\\nExplore articles, data and trends by topic\\\\nAbout\\\\nWhat makes USAFacts different\\\\nWe frequently add data and we're interested in what would be useful to people. Newsletter\\\\nData delivered to your inbox\\\\nKeep up with the latest data and most popular content. But only the official BEA inflation-adjusted \\\\\"real GDP\\\\\" value is used to calculate annual percent change in GDP and therefore how well the economy is doing.\",\"score\":0.42083758,\"raw_content\":null}]`,\n",
      "  tool_calls: undefined\n",
      "}\n",
      "----\n",
      "{\n",
      "  type: 'human',\n",
      "  content: 'Here are the U.S. GDP values over the past 3 years:\\n' +\n",
      "    '\\n' +\n",
      "    '1. **2021**: $23,315.08 billion\\n' +\n",
      "    '2. **2022:** No specific value obtained from the search, but we can infer trends from growth rates\\n' +\n",
      "    '3. **2023:** No specific value obtained from the search, but we can infer trends from growth rates\\n' +\n",
      "    '\\n' +\n",
      "    'Given this, further insight can be derived from additional data sources, economic reports, or databases to fill in the missing GDP values for 2022 and 2023.',\n",
      "  tool_calls: undefined\n",
      "}\n",
      "{ sender: 'Researcher' }\n",
      "----\n",
      "{\n",
      "  type: 'ai',\n",
      "  content: '',\n",
      "  tool_calls: [\n",
      "    {\n",
      "      name: 'generate_bar_chart',\n",
      "      args: {\n",
      "        data: [\n",
      "          { label: '2021', value: 23315.08 },\n",
      "          { label: '2022', value: 25514.3 },\n",
      "          { label: '2023', value: 27857.73 }\n",
      "        ]\n",
      "      },\n",
      "      type: 'tool_call',\n",
      "      id: 'call_CKH68vipCp9DshSVmw2vIZpK'\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{ sender: 'ChartGenerator' }\n",
      "----\n"
     ]
    },  
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  type: 'tool',\n",
      "  content: 'Chart has been generated and displayed to the user!',\n",
      "  tool_calls: undefined\n",
      "}\n",
      "----\n",
      "{\n",
      "  type: 'human',\n",
      "  content: 'FINAL ANSWER: The bar chart displaying the U.S. GDP over the past 3 years has been generated and displayed. The values used are as follows:\\n' +\n",
      "    '\\n' +\n",
      "    '- **2021**: $23,315.08 billion\\n' +\n",
      "    '- **2022**: $25,514.3 billion (approximate)\\n' +\n",
      "    '- **2023**: $27,857.73 billion (approximate)\\n' +\n",
      "    '\\n' +\n",
      "    'Please refer to the chart for a visual representation of the data.\\n',\n",
      "  tool_calls: undefined\n",
      "}\n",
      "{ sender: 'ChartGenerator' }\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "const streamResults = await graph.stream(\n",
    "  {\n",
    "    messages: [\n",
    "      new HumanMessage({\n",
    "        content: \"Generate a bar chart of the US gdp over the past 3 years.\",\n",
    "      }),\n",
    "    ],\n",
    "  },\n",
    "  { recursionLimit: 150 },\n",
    ");\n",
    "\n",
    "const prettifyOutput = (output: Record<string, any>) => {\n",
    "  const keys = Object.keys(output);\n",
    "  const firstItem = output[keys[0]];\n",
    "\n",
    "  if (\"messages\" in firstItem && Array.isArray(firstItem.messages)) {\n",
    "    const lastMessage = firstItem.messages[firstItem.messages.length - 1];\n",
    "    console.dir({\n",
    "      type: lastMessage._getType(),\n",
    "      content: lastMessage.content,\n",
    "      tool_calls: lastMessage.tool_calls,\n",
    "    }, { depth: null });\n",
    "  }\n",
    "\n",
    "  if (\"sender\" in firstItem) {\n",
    "    console.log({\n",
    "      sender: firstItem.sender,\n",
    "    })\n",
    "  }\n",
    "}\n",
    "\n",
    "for await (const output of await streamResults) {\n",
    "  if (!output?.__end__) {\n",
    "    prettifyOutput(output);\n",
    "    console.log(\"----\");\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52f960",
   "metadata": {},
   "source": [
    "[Click here](https://smith.langchain.com/public/a79e2a74-5993-4542-b041-c77f1aa02200/r)\n",
    "to see a LangSmith trace of the above run.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="package.json">
{
  "name": "examples",
  "type": "module",
  "description": "Dependencies for LangGraph examples.",
  "scripts": {
    "start": "tsx --experimental-wasm-modules -r dotenv/config src/index.ts"
  },
  "devDependencies": {
    "@langchain/anthropic": "^0.3.12",
    "@langchain/community": "^0.3.27",
    "@langchain/core": "^0.3.40",
    "@langchain/groq": "^0.1.3",
    "@langchain/langgraph": "workspace:*",
    "@langchain/mistralai": "^0.2.0",
    "@langchain/ollama": "^0.1.0",
    "@langchain/openai": "^0.4.0",
    "@langchain/textsplitters": "^0.1.0",
    "@xenova/transformers": "2.17.2",
    "cheerio": "^1.0.0",
    "chromadb": "^1.8.1",
    "d3": "^7.9.0",
    "dotenv": "^16.4.5",
    "langchain": "^0.3.13",
    "pg": "^8.11.0",
    "tslab": "^1.0.22",
    "tsx": "^4.18.0",
    "uuid": "^10.0.0",
    "zod": "^3.23.8",
    "zod-to-json-schema": "^3.23.2"
  }
}
</file>

<file path="plan-and-execute/plan-and-execute.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5011ae",
   "metadata": {},
   "source": [
    "# Plan-and-Execute\n",
    "\n",
    "This notebook shows how to create a \"plan-and-execute\" style agent. This is\n",
    "heavily inspired by the [Plan-and-Solve](https://arxiv.org/abs/2305.04091) paper\n",
    "as well as the [Baby-AGI](https://github.com/yoheinakajima/babyagi) project.\n",
    "\n",
    "The core idea is to first come up with a multi-step plan, and then go through\n",
    "that plan one item at a time. After accomplishing a particular task, you can\n",
    "then revisit the plan and modify as appropriate.\n",
    "\n",
    "This compares to a typical [ReAct](https://arxiv.org/abs/2210.03629) style agent\n",
    "where you think one step at a time. The advantages of this \"plan-and-execute\"\n",
    "style agent are:\n",
    "\n",
    "1. Explicit long term planning (which even really strong LLMs can struggle with)\n",
    "2. Ability to use smaller/weaker models for the execution step, only using\n",
    "   larger/better models for the planning step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d34776",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to install the packages required.\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai langchain @langchain/core\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c72ec",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the\n",
    "search tool we will use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54485855",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
    "// process.env.TAVILY_API_KEY = \"YOUR_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c681775",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for LangSmith tracing, which will give us\n",
    "best-in-class observability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57574807",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\"\n",
    "// process.env.LANGCHAIN_API_KEY = \"YOUR_API_KEY\"\n",
    "// process.env.LANGCHAIN_PROJECT = \"YOUR_PROJECT_NAME\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5c3b9",
   "metadata": {},
   "source": [
    "## Define the State\n",
    "\n",
    "Let's start by defining the state to track for this agent.\n",
    "\n",
    "First, we will need to track the current plan. Let's represent that as a list of\n",
    "strings.\n",
    "\n",
    "Next, we should track previously executed steps. Let's represent that as a list\n",
    "of tuples (these tuples will contain the step and then the result)\n",
    "\n",
    "Finally, we need to have some state to represent the final response as well as\n",
    "the original input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e49ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const PlanExecuteState = Annotation.Root({\n",
    "  input: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x ?? \"\",\n",
    "  }),\n",
    "  plan: Annotation<string[]>({\n",
    "    reducer: (x, y) => y ?? x ?? [],\n",
    "  }),\n",
    "  pastSteps: Annotation<[string, string][]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  }),\n",
    "  response: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x,\n",
    "  }),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ee3fd",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will\n",
    "use a built-in search tool via Tavily. However, it is really easy to create your\n",
    "own tools - see documentation\n",
    "[here](https://js.langchain.com/docs/modules/agents/tools/dynamic) on how to do\n",
    "that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a13860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "const tools = [new TavilySearchResults({ maxResults: 3 })];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9aa1f",
   "metadata": {},
   "source": [
    "## Define our Execution Agent\n",
    "\n",
    "Now we will create the execution agent we want to use to execute tasks. Note\n",
    "that for this example, we will be using the same execution agent for each task,\n",
    "but this doesn't HAVE to be the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa0509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const agentExecutor = createReactAgent({\n",
    "  llm: new ChatOpenAI({ model: \"gpt-4o\" }),\n",
    "  tools: tools,\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d6c87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  messages: [\n",
       "    HumanMessage {\n",
       "      \"content\": \"who is the winner of the us open\",\n",
       "      \"additional_kwargs\": {},\n",
       "      \"response_metadata\": {}\n",
       "    },\n",
       "    AIMessage {\n",
       "      \"content\": \"\",\n",
       "      \"additional_kwargs\": {\n",
       "        \"tool_calls\": [\n",
       "          {\n",
       "            \"id\": \"call_c2N7Z1RX31qKJaSlpOJ0K7Wm\",\n",
       "            \"type\": \"function\",\n",
       "            \"function\": \"[Object]\"\n",
       "          }\n",
       "        ]\n",
       "      },\n",
       "      \"response_metadata\": {\n",
       "        \"tokenUsage\": {\n",
       "          \"completionTokens\": 25,\n",
       "          \"promptTokens\": 80,\n",
       "          \"totalTokens\": 105\n",
       "        },\n",
       "        \"finish_reason\": \"tool_calls\"\n",
       "      },\n",
       "      \"tool_calls\": [\n",
       "        {\n",
       "          \"name\": \"tavily_search_results_json\",\n",
       "          \"args\": {\n",
       "            \"input\": \"winner of the US Open 2023\"\n",
       "          },\n",
       "          \"type\": \"tool_call\",\n",
       "          \"id\": \"call_c2N7Z1RX31qKJaSlpOJ0K7Wm\"\n",
       "        }\n",
       "      ],\n",
       "      \"invalid_tool_calls\": []\n",
       "    },\n",
       "    ToolMessage {\n",
       "      \"content\": \"[{\\\"title\\\":\\\"How Wyndham Clark won the 2023 U.S. Open over Rory McIlroy, Scottie ...\\\",\\\"url\\\":\\\"https://www.nytimes.com/athletic/live-blogs/us-open-leaderboard-live-scores-results-tee-times/mhPUFgLsyFfM/\\\",\\\"content\\\":\\\"Wyndham Clark is your 2023 U.S. Open champion. Wyndham Clark has won his first major championship, besting some of the best players in the world on Sunday at Los Angeles Country Club to claim the ...\\\",\\\"score\\\":0.9981324,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Championship Point | Coco Gauff Wins Women's Singles Title | 2023 US Open\\\",\\\"url\\\":\\\"https://www.youtube.com/watch?v=rZ0XQWWFIAo\\\",\\\"content\\\":\\\"The moment Coco Gauff beat Aryna Sabalenka in the final of the 2023 US Open.Don't miss a moment of the US Open! Subscribe now: https://bit.ly/2Pdr81iThe 2023...\\\",\\\"score\\\":0.997459,\\\"raw_content\\\":null},{\\\"title\\\":\\\"2023 U.S. Open leaderboard: Wyndham Clark breaks through edging Rory ...\\\",\\\"url\\\":\\\"https://www.cbssports.com/golf/news/2023-u-s-open-leaderboard-wyndham-clark-breaks-through-edging-rory-mcilroy-for-first-major-championship/live/\\\",\\\"content\\\":\\\"College Pick'em\\\\nA Daily SportsLine Betting Podcast\\\\nNFL Playoff Time!\\\\n2023 U.S. Open leaderboard: Wyndham Clark breaks through edging Rory McIlroy for first major championship\\\\nClark beat one of the game's best clinching his second PGA Tour victory, both in the last six weeks\\\\nWith Rickie Fowler, Rory McIlroy and Scottie Scheffler atop the 2023 U.S. Open leaderboard, it appeared as if Los Angeles Country Club was set to crown a shining star as its national champion. After making birdie on No. 1 to momentarily pull even with the leaders, McIlroy was unable to take advantage of the short par-4 6th before leaving one on the table on the par-5 8th when his birdie putt from less than four feet failed to even touch the hole.\\\\n The shot on 14 was kind of the shot of the week for me -- to make a birdie there and grind it on the way in. The Champion Golfer of the Year now goes to defend the Claret Jug at Hoylake where he will relish the opportunity to put his creativity and imagination on display again.\\\\n Instead, the City of Angels saw a breakout performance from perhaps one of the game's rising stars as 29-year-old Wyndham Clark (-10) outlasted the veteran McIlroy (-9) to capture his first major championship and clinch his second professional victory.\\\\n\\\",\\\"score\\\":0.99586606,\\\"raw_content\\\":null}]\",\n",
       "      \"name\": \"tavily_search_results_json\",\n",
       "      \"additional_kwargs\": {},\n",
       "      \"response_metadata\": {},\n",
       "      \"tool_call_id\": \"call_c2N7Z1RX31qKJaSlpOJ0K7Wm\"\n",
       "    },\n",
       "    AIMessage {\n",
       "      \"content\": \"The winners of the 2023 US Open are:\\n\\n- **Men's Singles**: Wyndham Clark, who won his first major championship.\\n- **Women's Singles**: Coco Gauff, who defeated Aryna Sabalenka in the final.\",\n",
       "      \"additional_kwargs\": {},\n",
       "      \"response_metadata\": {\n",
       "        \"tokenUsage\": {\n",
       "          \"completionTokens\": 50,\n",
       "          \"promptTokens\": 717,\n",
       "          \"totalTokens\": 767\n",
       "        },\n",
       "        \"finish_reason\": \"stop\"\n",
       "      },\n",
       "      \"tool_calls\": [],\n",
       "      \"invalid_tool_calls\": []\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "await agentExecutor.invoke({\n",
    "  messages: [new HumanMessage(\"who is the winner of the us open\")],\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d42a9",
   "metadata": {},
   "source": [
    "## Planning Step\n",
    "\n",
    "Let's now think about creating the planning step. This will use function calling\n",
    "to create a plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b5b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "const plan = zodToJsonSchema(\n",
    "  z.object({\n",
    "    steps: z\n",
    "      .array(z.string())\n",
    "      .describe(\"different steps to follow, should be in sorted order\"),\n",
    "  }),\n",
    ");\n",
    "const planFunction = {\n",
    "  name: \"plan\",\n",
    "  description: \"This tool is used to plan the steps to follow\",\n",
    "  parameters: plan,\n",
    "};\n",
    "\n",
    "const planTool = {\n",
    "  type: \"function\",\n",
    "  function: planFunction,\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba67c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const plannerPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  `For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "{objective}`,\n",
    ");\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  modelName: \"gpt-4-0125-preview\",\n",
    "}).withStructuredOutput(planFunction);\n",
    "\n",
    "const planner = plannerPrompt.pipe(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53fec065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  steps: [\n",
       "    \u001b[32m\"Identify the current Australia Open winner.\"\u001b[39m,\n",
       "    \u001b[32m\"Research the hometown of the identified Australia Open winner.\"\u001b[39m,\n",
       "    \u001b[32m\"Report the hometown of the Australia Open winner.\"\u001b[39m\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await planner.invoke({\n",
    "  objective: \"what is the hometown of the current Australia open winner?\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a106e",
   "metadata": {},
   "source": [
    "## Re-Plan Step\n",
    "\n",
    "Now, let's create a step that re-does the plan based on the result of the\n",
    "previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb888fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { JsonOutputToolsParser } from \"@langchain/core/output_parsers/openai_tools\";\n",
    "\n",
    "const response = zodToJsonSchema(\n",
    "  z.object({\n",
    "    response: z.string().describe(\"Response to user.\"),\n",
    "  }),\n",
    ");\n",
    "\n",
    "const responseTool = {\n",
    "  type: \"function\",\n",
    "  function: {\n",
    "    name: \"response\",\n",
    "    description: \"Response to user.\",\n",
    "    parameters: response,\n",
    "  },\n",
    "};\n",
    "\n",
    "const replannerPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  `For the given objective, come up with a simple step by step plan. \n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{pastSteps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that and use the 'response' function.\n",
    "Otherwise, fill out the plan.  \n",
    "Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.`,\n",
    ");\n",
    "\n",
    "const parser = new JsonOutputToolsParser();\n",
    "const replanner = replannerPrompt\n",
    "  .pipe(\n",
    "    new ChatOpenAI({ model: \"gpt-4o\" }).bindTools([\n",
    "      planTool,\n",
    "      responseTool,\n",
    "    ]),\n",
    "  )\n",
    "  .pipe(parser);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6ab14",
   "metadata": {},
   "source": [
    "## Create the Graph\n",
    "\n",
    "We can now create the graph!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef97a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "\n",
    "async function executeStep(\n",
    "  state: typeof PlanExecuteState.State,\n",
    "  config?: RunnableConfig,\n",
    "): Promise<Partial<typeof PlanExecuteState.State>> {\n",
    "  const task = state.plan[0];\n",
    "  const input = {\n",
    "    messages: [new HumanMessage(task)],\n",
    "  };\n",
    "  const { messages } = await agentExecutor.invoke(input, config);\n",
    "\n",
    "  return {\n",
    "    pastSteps: [[task, messages[messages.length - 1].content.toString()]],\n",
    "    plan: state.plan.slice(1),\n",
    "  };\n",
    "}\n",
    "\n",
    "async function planStep(\n",
    "  state: typeof PlanExecuteState.State,\n",
    "): Promise<Partial<typeof PlanExecuteState.State>> {\n",
    "  const plan = await planner.invoke({ objective: state.input });\n",
    "  return { plan: plan.steps };\n",
    "}\n",
    "\n",
    "async function replanStep(\n",
    "  state: typeof PlanExecuteState.State,\n",
    "): Promise<Partial<typeof PlanExecuteState.State>> {\n",
    "  const output = await replanner.invoke({\n",
    "    input: state.input,\n",
    "    plan: state.plan.join(\"\\n\"),\n",
    "    pastSteps: state.pastSteps\n",
    "      .map(([step, result]) => `${step}: ${result}`)\n",
    "      .join(\"\\n\"),\n",
    "  });\n",
    "  const toolCall = output[0];\n",
    "\n",
    "  if (toolCall.type == \"response\") {\n",
    "    return { response: toolCall.args?.response };\n",
    "  }\n",
    "\n",
    "  return { plan: toolCall.args?.steps };\n",
    "}\n",
    "\n",
    "function shouldEnd(state: typeof PlanExecuteState.State) {\n",
    "  return state.response ? \"true\" : \"false\";\n",
    "}\n",
    "\n",
    "const workflow = new StateGraph(PlanExecuteState)\n",
    "  .addNode(\"planner\", planStep)\n",
    "  .addNode(\"agent\", executeStep)\n",
    "  .addNode(\"replan\", replanStep)\n",
    "  .addEdge(START, \"planner\")\n",
    "  .addEdge(\"planner\", \"agent\")\n",
    "  .addEdge(\"agent\", \"replan\")\n",
    "  .addConditionalEdges(\"replan\", shouldEnd, {\n",
    "    true: END,\n",
    "    false: \"agent\",\n",
    "  });\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4bb886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  planner: {\n",
      "    plan: [\n",
      "      \"Identify the winner of the 2024 Australian Open.\",\n",
      "      \"Research the hometown of the identified winner.\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  agent: {\n",
      "    plan: [ \"Research the hometown of the identified winner.\" ],\n",
      "    pastSteps: [\n",
      "      [\n",
      "        \"Identify the winner of the 2024 Australian Open.\",\n",
      "        \"The winner of the 2024 Australian Open men's singles title is Jannik Sinner of Italy. He achieved a \"... 175 more characters\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{ replan: { plan: [ \"Research the hometown of Jannik Sinner.\" ] } }\n",
      "{\n",
      "  agent: {\n",
      "    plan: [],\n",
      "    pastSteps: [\n",
      "      [\n",
      "        \"Research the hometown of Jannik Sinner.\",\n",
      "        \"Jannik Sinner's hometown is Sexten (also known as Sesto) in northern Italy. Located in the Dolomites\"... 126 more characters\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  replan: {\n",
      "    response: \"The objective has been achieved. The hometown of the 2024 Australian Open winner, Jannik Sinner, is \"... 47 more characters\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config = { recursionLimit: 50 };\n",
    "const inputs = {\n",
    "  input: \"what is the hometown of the 2024 Australian open winner?\",\n",
    "};\n",
    "\n",
    "for await (const event of await app.stream(inputs, config)) {\n",
    "  console.log(event);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7f2be",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/5bb4f582-d111-417d-ba91-29bcced272bb/r).\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="quickstart.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph.js - Quickstart\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this quickstart guide, you'll get up and running with a simple Reason + Act Agent (often\n",
    "called a ReAct Agent) that can search the web using [Tavily Search API](https://tavily.com/).\n",
    "The code is fully configurable. You can:\n",
    "\n",
    "- swap out components\n",
    "- customize the execution flow\n",
    "- extend it with custom code or tooling\n",
    "- change the Large Language Model (LLM) and provider being used\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To follow along, you'll need to have the following:\n",
    "\n",
    "- NodeJS version 18 or newer\n",
    "- A [Tavily](https://tavily.com/) account and API key\n",
    "- An [OpenAI developer platform](https://platform.openai.com/docs/overview) account and API key\n",
    "\n",
    "Start by creating a new folder for the project. Open your terminal and run the following code:\n",
    "\n",
    "```bash\n",
    "mkdir langgraph-agent\n",
    "cd langgraph-agent\n",
    "```\n",
    "\n",
    "You'll also need to install a few dependencies to create an agent:\n",
    "\n",
    "- **@langchain/langgraph** contains the building blocks used to assemble an agent\n",
    "- **@langchain/openai** enable your agent to use OpenAI's LLMs\n",
    "- **@langchain/community** includes the Tavily integration give your agent search capabilities\n",
    "\n",
    "You can install these dependencies using by running following npm command in your terminal:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/core @langchain/langgraph @langchain/openai @langchain/community\n",
    "```\n",
    "\n",
    "## LangSmith\n",
    "\n",
    "Optionally, set up [LangSmith](https://docs.smith.langchain.com/) for best-in-class observability. Setup is simple - add the following variables to your environment and update the `LANGCHAIN_API_KEY` value with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
    "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_PROJECT = \"Quickstart: LangGraphJS\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making your first agent using LangGraph\n",
    "\n",
    "Create a file named `agent.ts` (short for Reason + Act Agent) and add the below TypeScript code to it.\n",
    "\n",
    "Make sure you update the environment variables at the top of the file to contain your API keys. If you don't, the OpenAI and Tavily API calls will produce errors and your agent will not work correctly.\n",
    "\n",
    "Once you've added your API keys, save the file and run the code with the following command:\n",
    "\n",
    "```bash\n",
    "npx tsx agent.ts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in San Francisco is as follows:\n",
      "- Temperature: 82.0°F (27.8°C)\n",
      "- Condition: Sunny\n",
      "- Wind: 11.9 mph from the NW\n",
      "- Humidity: 41%\n",
      "- Pressure: 29.98 in\n",
      "- Visibility: 9.0 miles\n",
      "- UV Index: 6.0\n",
      "\n",
      "For more details, you can visit [Weather in San Francisco](https://www.weatherapi.com/).\n",
      "The current weather in New York is as follows:\n",
      "- Temperature: 84.0°F (28.9°C)\n",
      "- Condition: Sunny\n",
      "- Wind: 2.2 mph from SSE\n",
      "- Humidity: 57%\n",
      "- Pressure: 29.89 in\n",
      "- Precipitation: 0.01 in\n",
      "- Visibility: 9.0 miles\n",
      "- UV Index: 6.0\n",
      "\n",
      "For more details, you can visit [Weather in New York](https://www.weatherapi.com/).\n"
     ]
    }
   ],
   "source": [
    "// agent.ts\n",
    "\n",
    "// IMPORTANT - Add your API keys here. Be careful not to publish them.\n",
    "process.env.OPENAI_API_KEY = \"sk-...\";\n",
    "process.env.TAVILY_API_KEY = \"tvly-...\";\n",
    "\n",
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "// Define the tools for the agent to use\n",
    "const agentTools = [new TavilySearchResults({ maxResults: 3 })];\n",
    "const agentModel = new ChatOpenAI({ temperature: 0 });\n",
    "\n",
    "// Initialize memory to persist state between graph runs\n",
    "const agentCheckpointer = new MemorySaver();\n",
    "const agent = createReactAgent({\n",
    "  llm: agentModel,\n",
    "  tools: agentTools,\n",
    "  checkpointSaver: agentCheckpointer,\n",
    "});\n",
    "\n",
    "// Now it's time to use!\n",
    "const agentFinalState = await agent.invoke(\n",
    "  { messages: [new HumanMessage(\"what is the current weather in sf\")] },\n",
    "  { configurable: { thread_id: \"42\" } },\n",
    ");\n",
    "\n",
    "console.log(\n",
    "  agentFinalState.messages[agentFinalState.messages.length - 1].content,\n",
    ");\n",
    "\n",
    "const agentNextState = await agent.invoke(\n",
    "  { messages: [new HumanMessage(\"what about ny\")] },\n",
    "  { configurable: { thread_id: \"42\" } },\n",
    ");\n",
    "\n",
    "console.log(\n",
    "  agentNextState.messages[agentNextState.messages.length - 1].content,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "The\n",
    "[createReactAgent](/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html)\n",
    "constructor lets you create a simple tool-using LangGraph agent in a single line\n",
    "of code. Here's a visual representation of the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "metadata": {},
      "output_type": "display_data"
     }
   ],
   "source": [
    "// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!\n",
    "import * as tslab from \"tslab\";\n",
    "\n",
    "const graph = agent.getGraph();\n",
    "const image = await graph.drawMermaidPng();\n",
    "const arrayBuffer = await image.arrayBuffer();\n",
    "\n",
    "await tslab.display.png(new Uint8Array(arrayBuffer));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e342f",
   "metadata": {},
   "source": [
    "Alternatively, you can save the graph as a PNG file locally using the following approach:\n",
    "\n",
    "```ts\n",
    "import { writeFileSync } from \"node:fs\";\n",
    "\n",
    "const graphStateImage = await drawableGraphGraphState.drawMermaidPng();\n",
    "const graphStateArrayBuffer = await graphStateImage.arrayBuffer();\n",
    "\n",
    "const filePath = \"./graphState.png\";\n",
    "writeFileSync(filePath, new Uint8Array(graphStateArrayBuffer));\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing agent behavior\n",
    "\n",
    "createReactAgent can be great for simple agents, but sometimes you need something more powerful.\n",
    "\n",
    "LangGraph really shines when you need fine-grained control over an agent's behavior. The following\n",
    "code creates an agent with the same behavior as the example above, but you can\n",
    "clearly see the execution logic and how you could customize it.\n",
    "\n",
    "Update the code in your `agent.ts` file to match the example below. Once again, be sure to update\n",
    "the environment variables at the top.\n",
    "\n",
    "After you've updated your environment variables and saved the file, you can run it with the same command as before:\n",
    "\n",
    "```bash\n",
    "npx tsx agent.ts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// agent.ts\n",
    "\n",
    "// IMPORTANT - Add your API keys here. Be careful not to publish them.\n",
    "process.env.OPENAI_API_KEY = \"sk-...\";\n",
    "process.env.TAVILY_API_KEY = \"tvly-...\";\n",
    "\n",
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { StateGraph, MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the tools for the agent to use\n",
    "const tools = [new TavilySearchResults({ maxResults: 3 })];\n",
    "const toolNode = new ToolNode(tools);\n",
    "\n",
    "// Create a model and give it access to the tools\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o-mini\",\n",
    "  temperature: 0,\n",
    "}).bindTools(tools);\n",
    "\n",
    "// Define the function that determines whether to continue or not\n",
    "function shouldContinue({ messages }: typeof MessagesAnnotation.State) {\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "\n",
    "  // If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "  if (lastMessage.tool_calls?.length) {\n",
    "    return \"tools\";\n",
    "  }\n",
    "  // Otherwise, we stop (reply to the user) using the special \"__end__\" node\n",
    "  return \"__end__\";\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModel(state: typeof MessagesAnnotation.State) {\n",
    "  const response = await model.invoke(state.messages);\n",
    "\n",
    "  // We return a list, because this will get added to the existing list\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addEdge(\"__start__\", \"agent\") // __start__ is a special name for the entrypoint\n",
    "  .addNode(\"tools\", toolNode)\n",
    "  .addEdge(\"tools\", \"agent\")\n",
    "  .addConditionalEdges(\"agent\", shouldContinue);\n",
    "\n",
    "// Finally, we compile it into a LangChain Runnable.\n",
    "const app = workflow.compile();\n",
    "\n",
    "// Use the agent\n",
    "const finalState = await app.invoke({\n",
    "  messages: [new HumanMessage(\"what is the weather in sf\")],\n",
    "});\n",
    "console.log(finalState.messages[finalState.messages.length - 1].content);\n",
    "\n",
    "const nextState = await app.invoke({\n",
    "  // Including the messages from the previous run gives the LLM context.\n",
    "  // This way it knows we're asking about the weather in NY\n",
    "  messages: [...finalState.messages, new HumanMessage(\"what about ny\")],\n",
    "});\n",
    "console.log(nextState.messages[nextState.messages.length - 1].content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few new things going on in this version of our ReAct Agent.\n",
    "\n",
    "A [`ToolNode`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph_prebuilt.ToolNode.html) enables the LLM to use tools.\n",
    "In this example, we made a `shouldContinue` function and passed it to [`addConditionalEdge`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html#addConditionalEdges) so our ReAct Agent can either call a tool or respond to the request.\n",
    "\n",
    "[Annotations](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#annotation) are how graph state is represented in LangGraph. We're using [`MessagesAnnotation`](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#messagesannotation), a helper that implements a common pattern: keeping the message history in an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Great job creating your first AI agent using LangGraph! If you're ready to build\n",
    "something more, check out our other [tutorials](/langgraphjs/tutorials/)\n",
    "to learn how to implement other end-to-end agentic workflows such as:\n",
    "\n",
    "- [Retrieval-Augmented Generation (RAG)](/langgraphjs/tutorials/rag/langgraph_agentic_rag/)\n",
    "- [Multi-agent collaboration](/langgraphjs/tutorials/multi_agent/multi_agent_collaboration/)\n",
    "- [Reflection](/langgraphjs/tutorials/reflection/reflection/), where the agent evaluates its work\n",
    "\n",
    "If you'd rather improve your agent we have [how-to guides](/langgraphjs/how-tos/) to help, including:\n",
    "\n",
    "- [Tool calling](/langgraphjs/how-tos/tool-calling/) that enables agents to interact with APIs\n",
    "- give your agent [persistent memory](/langgraphjs/how-tos/persistence/) to continue conversations and debug unexpected behavior\n",
    "- Put a [human in the loop](/langgraphjs/how-tos/breakpoints/) for actions you want a human to verify\n",
    "- [Streaming the agent output](/langgraphjs/how-tos/stream-values/) to make your application feel more responsive\n",
    "- [Change the AI model in one line of code](https://js.langchain.com/docs/how_to/chat_models_universal_init/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="rag/langgraph_adaptive_rag_local.ipynb">
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb89d3f0-7ade-43a8-a527-4bec45971cf6",
   "metadata": {},
   "source": [
    "# Adaptive RAG with local LLMs\n",
    "\n",
    "Adaptive RAG is a strategy for RAG that unites (1)\n",
    "[query analysis](https://blog.langchain.dev/query-construction/) with (2)\n",
    "[active / self-corrective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/).\n",
    "\n",
    "In the [paper](https://arxiv.org/abs/2403.14403), they report query analysis to\n",
    "route across:\n",
    "\n",
    "- No Retrieval\n",
    "- Single-shot RAG\n",
    "- Iterative RAG\n",
    "\n",
    "Let's build on this using LangGraph.\n",
    "\n",
    "In our implementation, we will route between:\n",
    "\n",
    "- Web search: for questions related to recent events\n",
    "- Self-corrective RAG: for questions related to our index\n",
    "\n",
    "![Adaptive RAG graph](attachment:3755396d-c4a8-45bd-87d4-00cb56339fe5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d4a26-249b-4551-aa13-6c373429618e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, you'll need to install some required dependencies:\n",
    "\n",
    "```bash\n",
    "npm install cheerio langchain @langchain/community @langchain/ollama @langchain/core\n",
    "```\n",
    "\n",
    "For the fallback web search, you'll also need to obtain a\n",
    "[Tavily API key](https://tavily.com) and set it as an environment variable named\n",
    "`TAVILY_API_KEY`.\n",
    "\n",
    "### Models\n",
    "\n",
    "Next, choose which local models you'll use.\n",
    "\n",
    "#### Local Embeddings\n",
    "\n",
    "We'll be using the [`mxbai-embed-large`](https://ollama.com/library/mxbai-embed-large) embeddings model from Ollama.\n",
    "\n",
    "#### Local LLM\n",
    "\n",
    "(1) Download [Ollama app](https://ollama.ai/).\n",
    "\n",
    "(2) Pull a `Llama 3` model [here](https://ollama.com/library/llama3). You can\n",
    "also try `Mistral` models [here](https://ollama.ai/library/mixtral), one of the\n",
    "[quantized Cohere Command-R models](https://ollama.com/library/command-r), or\n",
    "any other model you'd like to try from the\n",
    "[Ollama library](https://ollama.com/library) - just be sure that your computer\n",
    "has sufficient RAM.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3 mxbai-embed-large\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c104495-a8a4-4517-b6a6-9a2cbe1e82f2",
   "metadata": {},
   "source": [
    "### Tracing\n",
    "\n",
    "Optionally, use [LangSmith](https://docs.smith.langchain.com/) for tracing\n",
    "(shown at bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cb8dce-f580-421d-a05f-9fc71de2b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_ENDPOINT = \"https://api.smith.langchain.com\";\n",
    "// process.env.LANGCHAIN_API_KEY = \"<your-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04718a0c-7a48-4243-97a2-940a0239cc12",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "Now that you've chosen and set up your local models, load and index some source\n",
    "documents. The code below uses some of\n",
    "[Lilian Weng's blog posts](https://lilianweng.github.io/) on LLMs and agents as\n",
    "a data source, then loads them into a demo\n",
    "[`MemoryVectorStore`](https://js.langchain.com/docs/integrations/vectorstores/memory)\n",
    "instance. It then creates a\n",
    "[retriever](https://js.langchain.com/docs/concepts#retrievers) from that\n",
    "vector store for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ff6b99-080d-4827-b2cb-f775543d76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { OllamaEmbeddings } from \"@langchain/ollama\";\n",
    "\n",
    "const urls = [\n",
    "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "];\n",
    "\n",
    "const docs = await Promise.all(urls.map((url) => {\n",
    "  const loader = new CheerioWebBaseLoader(url);\n",
    "  return loader.load();\n",
    "}));\n",
    "\n",
    "const docsList = docs.flat();\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 250,\n",
    "  chunkOverlap: 0,\n",
    "});\n",
    "\n",
    "const splitDocs = await textSplitter.splitDocuments(docsList);\n",
    "\n",
    "const embeddings = new OllamaEmbeddings({\n",
    "  model: \"mxbai-embed-large\",\n",
    "});\n",
    "\n",
    "// Add to vector store\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "  splitDocs,\n",
    "  embeddings,\n",
    ");\n",
    "\n",
    "const retriever = vectorStore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3eb922-27a1-4a72-a727-85fbf5b3daf1",
   "metadata": {},
   "source": [
    "## Creating components\n",
    "\n",
    "Here, you'll create the components of the graph.\n",
    "\n",
    "### Question router\n",
    "\n",
    "First, create a chain that will route incoming questions towards either your\n",
    "vector store if they are related to LLMs or agents, or to a general web search\n",
    "if they are not.\n",
    "\n",
    "You'll use Ollama's\n",
    "[JSON mode](https://js.langchain.com/docs/integrations/chat/ollama/#json-mode)\n",
    "to help keep the output format consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7045e064-e666-4aea-9111-6e9d2007f27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ datasource: 'vectorstore' }\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { JsonOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { ChatOllama } from \"@langchain/ollama\";\n",
    "\n",
    "const jsonModeLlm = new ChatOllama({\n",
    "  model: \"llama3\",\n",
    "  format: \"json\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "const QUESTION_ROUTER_SYSTEM_TEMPLATE =\n",
    "  `You are an expert at routing a user question to a vectorstore or web search.\n",
    "Use the vectorstore for questions on LLM agents, prompt engineering, and adversarial attacks.\n",
    "You do not need to be stringent with the keywords in the question related to these topics.\n",
    "Otherwise, use web-search. Give a binary choice 'web_search' or 'vectorstore' based on the question.\n",
    "Return the a JSON with a single key 'datasource' and no preamble or explanation.`;\n",
    "\n",
    "const questionRouterPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", QUESTION_ROUTER_SYSTEM_TEMPLATE],\n",
    "  [\"human\", \"{question}\"],\n",
    "]);\n",
    "\n",
    "const questionRouter = questionRouterPrompt.pipe(jsonModeLlm).pipe(\n",
    "  new JsonOutputParser(),\n",
    ");\n",
    "\n",
    "await questionRouter.invoke({ question: \"llm agent memory\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84b5ee",
   "metadata": {},
   "source": [
    "Above, notice that you invoked the router with a query related to the knowledge\n",
    "our vector store contains, so it responds accordingly. Here's what happens if\n",
    "you ask something irrelevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1bdaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ datasource: 'web_search' }\n"
     ]
    }
   ],
   "source": [
    "await questionRouter.invoke({ question: \"red robin\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f6be3",
   "metadata": {},
   "source": [
    "In this case, you can see that execution would be routed to our web search.\n",
    "\n",
    "### Retrieval grader\n",
    "\n",
    "Create a grader that will check retrieved documents from our vector store for\n",
    "relevancy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813cdcef-8b75-4214-a2ed-b89077b3d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ score: 'yes' }\n"
     ]
    }
   ],
   "source": [
    "const GRADER_TEMPLATE =\n",
    "  `You are a grader assessing relevance of a retrieved document to a user question.\n",
    "Here is the retrieved document:\n",
    "\n",
    "<document>\n",
    "{content}\n",
    "</document>\n",
    "\n",
    "Here is the user question:\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "If the document contains keywords related to the user question, grade it as relevant.\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.`;\n",
    "\n",
    "const graderPrompt = ChatPromptTemplate.fromTemplate(GRADER_TEMPLATE);\n",
    "\n",
    "const retrievalGrader = graderPrompt.pipe(jsonModeLlm).pipe(\n",
    "  new JsonOutputParser(),\n",
    ");\n",
    "\n",
    "// Test run\n",
    "const testQuestion = \"agent memory\";\n",
    "\n",
    "const docs2 = await retriever.invoke(testQuestion);\n",
    "\n",
    "await retrievalGrader.invoke({\n",
    "  question: testQuestion,\n",
    "  content: docs2[0].pageContent,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1cabd",
   "metadata": {},
   "source": [
    "And you can see that it marks the first retrieved document as related to\n",
    "`\"agent memory\"`.\n",
    "\n",
    "### Generation\n",
    "\n",
    "Next, create a chain that generates an answer based on retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb8b373-0289-4dec-bd4b-8b2701200301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that an agent's memory refers to its ability to record and reflect on past experiences, using both long-term and short-term memory modules. The long-term memory module, or \"memory stream,\" stores a comprehensive list of agents' experiences in natural language, while the reflection mechanism synthesizes these memories into higher-level inferences over time to guide future behavior.\n"
     ]
    }
   ],
   "source": [
    "import * as hub from \"langchain/hub\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import type { Document } from \"@langchain/core/documents\";\n",
    "\n",
    "// https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "const ragPrompt = await hub.pull(\"rlm/rag-prompt\");\n",
    "\n",
    "// Post-processing\n",
    "const formatDocs = (docs: Document[]) => {\n",
    "  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n",
    "};\n",
    "\n",
    "// Initialize a new model without JSON mode active\n",
    "const llm = new ChatOllama({\n",
    "  model: \"llama3\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "// Chain\n",
    "const ragChain = ragPrompt.pipe(llm).pipe(new StringOutputParser());\n",
    "\n",
    "// Test run\n",
    "const testQuestion2 = \"agent memory\";\n",
    "const docs3 = await retriever.invoke(testQuestion2);\n",
    "\n",
    "await ragChain.invoke({ context: formatDocs(docs3), question: testQuestion2 });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f3b3a",
   "metadata": {},
   "source": [
    "### Hallucination grader\n",
    "\n",
    "Create a chain that reviews a generated answer and checks for hallucinations.\n",
    "We'll return to using JSON mode for this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38345cff-e2d0-436e-aa09-599522a61eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ score: 'yes' }\n"
     ]
    }
   ],
   "source": [
    "const HALLUCINATION_GRADER_TEMPLATE =\n",
    "  `You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "Here are the facts used as context to generate the answer:\n",
    "\n",
    "<context>\n",
    "{context} \n",
    "</context>\n",
    "\n",
    "Here is the answer:\n",
    "\n",
    "<answer>\n",
    "{generation}\n",
    "</answer>\n",
    "\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.\n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.`;\n",
    "\n",
    "const hallucinationGraderPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  HALLUCINATION_GRADER_TEMPLATE,\n",
    ");\n",
    "\n",
    "const hallucinationGrader = hallucinationGraderPrompt.pipe(llm).pipe(\n",
    "  new JsonOutputParser(),\n",
    ");\n",
    "\n",
    "// Test run\n",
    "const generation2 = await ragChain.invoke({\n",
    "  context: formatDocs(docs3),\n",
    "  question: testQuestion2,\n",
    "});\n",
    "\n",
    "await hallucinationGrader.invoke({ context: formatDocs(docs3), generation: generation2 });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25643a76",
   "metadata": {},
   "source": [
    "### Answer Grader\n",
    "\n",
    "Create a chain for checking the relevancy of the final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9771caa1-5542-47c3-8354-aeeafcf51964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ score: 'yes' }\n"
     ]
    }
   ],
   "source": [
    "const ANSWER_GRADER_PROMPT_TEMPLATE =\n",
    "  `You are a grader assessing whether an answer is useful to resolve a question.\n",
    "Here is the answer:\n",
    "\n",
    "<answer>\n",
    "{generation} \n",
    "</answer>\n",
    "\n",
    "Here is the question:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.`;\n",
    "\n",
    "const answerGraderPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  ANSWER_GRADER_PROMPT_TEMPLATE,\n",
    ");\n",
    "\n",
    "const answerGrader = answerGraderPrompt.pipe(jsonModeLlm).pipe(\n",
    "  new JsonOutputParser(),\n",
    ");\n",
    "\n",
    "// Test run\n",
    "const generation3 = await ragChain.invoke({\n",
    "  context: formatDocs(docs3),\n",
    "  question: testQuestion2,\n",
    "});\n",
    "\n",
    "await answerGrader.invoke({ question: testQuestion2, generation: generation3 });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98248f",
   "metadata": {},
   "source": [
    "### Question rewriter\n",
    "\n",
    "Create a question rewriter. This chain performs\n",
    "[query analysis](https://js.langchain.com/docs/tutorials/query_analysis/)\n",
    "on the user questions and optimizes them for RAG to help handle difficult\n",
    "queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830ba5f7-9c8d-4c01-83b1-e4d51d40d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are memories stored in by agents?\n"
     ]
    }
   ],
   "source": [
    "const REWRITER_PROMPT_TEMPLATE =\n",
    "  `You a question re-writer that converts an input question to a better version that is optimized\n",
    "for vectorstore retrieval. Look at the initial and formulate an improved question.\n",
    "\n",
    "Here is the initial question:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Respond only with an improved question. Do not include any preamble or explanation.`;\n",
    "\n",
    "const rewriterPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  REWRITER_PROMPT_TEMPLATE,\n",
    ");\n",
    "\n",
    "const rewriter = rewriterPrompt.pipe(llm).pipe(new StringOutputParser());\n",
    "\n",
    "// Test run\n",
    "\n",
    "// Test question is \"agent memory\"\n",
    "await rewriter.invoke({ question: testQuestion2 });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c9bb1-5069-45f9-8a7e-cba34fe07dd9",
   "metadata": {},
   "source": [
    "### Web Search Tool\n",
    "\n",
    "Finally, you'll need a web search tool that can handle questions out of scope\n",
    "from the indexed documents. The code below initializes a\n",
    "[Tavily-powered](https://js.langchain.com/docs/integrations/tools/tavily_search)\n",
    "search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3c1c70-ff84-41e8-bf72-738ed52f2dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"title\":\"Family Friendly Burger Restaurant | Red Robin\",\"url\":\"https://www.redrobin.com/\",\"content\":\"Red Robin is donating 10¢ to Make-A-Wish ® for every Kids Meal purchased. You can contribute to life-changing wishes by simply purchasing a Kids Meal at Red Robin for Dine-in or To-Go. Join us for a memorable meal or order online and help transform lives, one wish at a time.\",\"score\":0.998043,\"raw_content\":null},{\"title\":\"Red Robin United States of America Directory\",\"url\":\"https://locations.redrobin.com/locations-list/us/\",\"content\":\"Maps, Driving Directions and Local Restaurant Information for Red Robin Restaurants in United States\",\"score\":0.99786776,\"raw_content\":null},{\"title\":\"Red Robin Restaurant Locations\",\"url\":\"https://locations.redrobin.com/\",\"content\":\"Maps, Driving Directions and Local Restaurant Information for Red Robin\",\"score\":0.99718815,\"raw_content\":null}]\n"
     ]
    }
   ],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "const webSearchTool = new TavilySearchResults({ maxResults: 3 });\n",
    "\n",
    "await webSearchTool.invoke(\"red robin\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d1751-a20b-4858-b3fd-0312de4f3ad7",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "Now that you've created all the necessary components, it's time to capture the\n",
    "flow as a graph.\n",
    "\n",
    "### Graph state\n",
    "\n",
    "Define the graph state like this. Since `question` and `generation` are simple\n",
    "strings, we can use `null` as a shorthand for default behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e09087e-b2a9-437a-abee-129e426df799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import type { Document } from \"@langchain/core/documents\";\n",
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// This defines the agent state.\n",
    "// Returned documents from a node will override the current\n",
    "// \"documents\" value in the state object.\n",
    "const GraphState = Annotation.Root({\n",
    "  question: Annotation<string>,\n",
    "  generation: Annotation<string>,\n",
    "  documents: Annotation<Document[]>({\n",
    "    reducer: (_, y) => y,\n",
    "    default: () => [],\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac1f3c",
   "metadata": {},
   "source": [
    "### Preparing nodes and edges\n",
    "\n",
    "Let's wrap our components in functions that match the interfaces required by\n",
    "LangGraph. These functions will handle formatting inputs and outputs.\n",
    "\n",
    "We'll use some components within nodes, and others to define conditional edges.\n",
    "Each will take the graph state as a parameter. Nodes return state properties to\n",
    "be updated, while conditional edges return the name of the next node to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5fa507-77ae-426a-a65f-f518b9525bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Document } from \"@langchain/core/documents\";\n",
    "\n",
    "/* ---Nodes--- */\n",
    "\n",
    "// Retrieve documents for a question\n",
    "const retrieve = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => {\n",
    "  console.log(\"---RETRIEVE---\");\n",
    "  const documents = await retriever.invoke(state.question);\n",
    "  // Add sources to the state\n",
    "  return { documents };\n",
    "};\n",
    "\n",
    "// RAG generation\n",
    "const generate = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => {\n",
    "  console.log(\"---GENERATE---\");\n",
    "  const generation = await ragChain.invoke({\n",
    "    context: formatDocs(state.documents),\n",
    "    question: state.question,\n",
    "  });\n",
    "  // Add generation to the state\n",
    "  return { generation };\n",
    "};\n",
    "\n",
    "// Determines whether the retrieved documents are relevant to the question.\n",
    "const gradeDocuments = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => {\n",
    "  console.log(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\");\n",
    "  // Score each doc\n",
    "  const relevantDocs: Document[] = [];\n",
    "  for (const doc of state.documents) {\n",
    "    const grade: { score: string } = await retrievalGrader.invoke({\n",
    "      question: state.question,\n",
    "      content: doc.pageContent,\n",
    "    });\n",
    "    if (grade.score === \"yes\") {\n",
    "      console.log(\"---GRADE: DOCUMENT RELEVANT---\");\n",
    "      relevantDocs.push(doc);\n",
    "    } else {\n",
    "      console.log(\"---GRADE: DOCUMENT NOT RELEVANT---\");\n",
    "    }\n",
    "  }\n",
    "  return { documents: relevantDocs };\n",
    "};\n",
    "\n",
    "// Re-write question\n",
    "const transformQuery = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => {\n",
    "  console.log(\"---TRANSFORM QUERY---\");\n",
    "  const betterQuestion = await rewriter.invoke({ question: state.question });\n",
    "  return { question: betterQuestion };\n",
    "};\n",
    "\n",
    "// Web search based on the re-phrased question\n",
    "const webSearch = async (state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> => {\n",
    "  console.log(\"---WEB SEARCH---\");\n",
    "  const stringifiedSearchResults = await webSearchTool.invoke(state.question);\n",
    "  return {\n",
    "    documents: [new Document({ pageContent: stringifiedSearchResults })],\n",
    "  };\n",
    "};\n",
    "\n",
    "/* ---Edges--- */\n",
    "\n",
    "// Decide on the datasource to route the initial question to.\n",
    "const routeQuestion = async (state: typeof GraphState.State) => {\n",
    "  const source: { datasource: string } = await questionRouter.invoke({\n",
    "    question: state.question,\n",
    "  });\n",
    "  if (source.datasource === \"web_search\") {\n",
    "    console.log(`---ROUTING QUESTION \"${state.question} TO WEB SEARCH---`);\n",
    "    return \"web_search\";\n",
    "  } else {\n",
    "    console.log(`---ROUTING QUESTION \"${state.question} TO RAG---`);\n",
    "    return \"retrieve\";\n",
    "  }\n",
    "};\n",
    "\n",
    "// Decide whether the current documents are sufficiently relevant\n",
    "// to come up with a good answer.\n",
    "const decideToGenerate = async (state: typeof GraphState.State) => {\n",
    "  const filteredDocuments = state.documents;\n",
    "  // All documents have been filtered as irrelevant\n",
    "  // Regenerate a new query and try again\n",
    "  if (filteredDocuments.length === 0) {\n",
    "    console.log(\n",
    "      \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\",\n",
    "    );\n",
    "    return \"transform_query\";\n",
    "  } else {\n",
    "    // We have relevant documents, so generate answer.\n",
    "    console.log(\"---DECISION: GENERATE---\");\n",
    "    return \"generate\";\n",
    "  }\n",
    "};\n",
    "\n",
    "// Determines whether the generation is grounded in the document and answers question.\n",
    "const gradeGenerationDocumentsAndQuestion = async (\n",
    "  state: typeof GraphState.State,\n",
    ") => {\n",
    "  const hallucinationGrade: { score: string } = await hallucinationGrader\n",
    "    .invoke({\n",
    "      generation: state.generation,\n",
    "      context: formatDocs(state.documents),\n",
    "    });\n",
    "  // Check for hallucination\n",
    "  if (hallucinationGrade.score === \"yes\") {\n",
    "    console.log(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\");\n",
    "    // Check question answering\n",
    "    console.log(\"---GRADING GENERATION vs. QUESTION---\");\n",
    "    const onTopicGrade: { score: string } = await answerGrader.invoke({\n",
    "      question: state.question,\n",
    "      generation: state.generation,\n",
    "    });\n",
    "    if (onTopicGrade.score === \"yes\") {\n",
    "      console.log(\"---DECISION: GENERATION ADDRESSES QUESTION---\");\n",
    "      return \"useful\";\n",
    "    } else {\n",
    "      console.log(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\");\n",
    "      return \"not_useful\";\n",
    "    }\n",
    "  } else {\n",
    "    console.log(\n",
    "      \"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RETRY---\",\n",
    "    );\n",
    "    return \"not_supported\";\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d8eb6-31d7-4ab5-8a88-7081b64582bb",
   "metadata": {},
   "source": [
    "## Build the graph\n",
    "\n",
    "Now we build the graph. For fun, let's add a checkpointer and have the compiled\n",
    "graph pause before making a web search. This will simulate asking for\n",
    "permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "450eb313-ca75-4a43-b57e-7034bd3f40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, MemorySaver, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const graph = new StateGraph(GraphState)\n",
    "  .addNode(\"web_search\", webSearch)\n",
    "  .addNode(\"retrieve\", retrieve)\n",
    "  .addNode(\"grade_documents\", gradeDocuments)\n",
    "  .addNode(\"generate\", generate)\n",
    "  .addNode(\"transform_query\", transformQuery)\n",
    "  .addConditionalEdges(START, routeQuestion)\n",
    "  .addEdge(\"web_search\", \"generate\")\n",
    "  .addEdge(\"retrieve\", \"grade_documents\")\n",
    "  .addConditionalEdges(\"grade_documents\", decideToGenerate)\n",
    "  .addEdge(\"transform_query\", \"retrieve\")\n",
    "  .addConditionalEdges(\"generate\", gradeGenerationDocumentsAndQuestion, {\n",
    "    not_supported: \"generate\",\n",
    "    useful: END,\n",
    "    not_useful: \"transform_query\",\n",
    "  });\n",
    "\n",
    "const app = graph.compile({\n",
    "  checkpointer: new MemorySaver(),\n",
    "  interruptBefore: [\"web_search\"],\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ea270",
   "metadata": {},
   "source": [
    "## Running the graph\n",
    "\n",
    "You're all set! Time to ask some questions. First, try a question about\n",
    "something related to agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b095c1db-8bd1-4a34-937c-1a9b74ae74ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTING QUESTION \"What are some features of long-term memory? TO WEB SEARCH---\n",
      "{\n",
      "  question: 'What are some features of long-term memory?',\n",
      "  documents: []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await app.invoke(\n",
    "  {\n",
    "    question: \"What are some features of long-term memory?\",\n",
    "  },\n",
    "  { configurable: { thread_id: \"1\" } },\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450921be",
   "metadata": {},
   "source": [
    "You can see that your graph correctly routes the query to the vector store and\n",
    "answers the question, filtering out some documents as necessary.\n",
    "\n",
    "If you ask something not related to agents or LLMs, the graph should fall back\n",
    "to information gleaned from the web. The graph will pause before executing, as\n",
    "specified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4620ede9-b014-499f-8acf-80f80ce0d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTING QUESTION \"Where are the 2024 Euros being held? TO WEB SEARCH---\n",
      "{ question: 'Where are the 2024 Euros being held?', documents: [] }\n"
     ]
    }
   ],
   "source": [
    "await app.invoke(\n",
    "  {\n",
    "    question: \"Where are the 2024 Euros being held?\",\n",
    "  },\n",
    "  { configurable: { thread_id: \"2\" } },\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aeaedb",
   "metadata": {},
   "source": [
    "You can see the graph paused before running the web search. And now we continue\n",
    "by invoking the graph with `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a19e7786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADING GENERATION vs. QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "{\n",
      "  question: 'Where are the 2024 Euros being held?',\n",
      "  generation: 'The 2024 Euros are being held in Germany. The final match will take place at Olympiastadion Berlin on July 14, 2024.',\n",
      "  documents: [\n",
      "    Document {\n",
      "      pageContent: `[{\"title\":\"Where is Euro 2024? Country, host cities and venues\",\"url\":\"https://www.radiotimes.com/tv/sport/football/euro-2024-location/\",\"content\":\"Euro 2024 stadiums The Olympiastadion Berlin, the biggest stadium in Germany with a capacity of around 74,000, will host games as well as the final on Sunday, 14th July, 2024.\",\"score\":0.99743915,\"raw_content\":null},{\"title\":\"UEFA EURO 2024 venues - complete list: When and where will the opening ...\",\"url\":\"https://olympics.com/en/news/uefa-euro-2024-venues-complete-list-when-where-final-opening-game\",\"content\":\"UEFA EURO 2024 will be held in Germany across June and July, with 10 host cities staging the major football tournament.. It all begins in Munich on June 14, when hosts Germany take on Scotland in the tournament's opening game at Bayern Munich's stadium.. The final takes place a month later on July 14 at Olympiastadion Berlin in the German capital, which hosted the 2006 FIFA World Cup final ...\",\"score\":0.9973061,\"raw_content\":null},{\"title\":\"EURO 2024: All you need to know | UEFA EURO 2024\",\"url\":\"https://www.uefa.com/euro2024/news/0257-0e13b161b2e8-4a3fd5615e0c-1000--euro-2024-all-you-need-to-know/\",\"content\":\"Article top media content\\\\nArticle body\\\\nWhere will EURO 2024 be held?\\\\nGermany will host EURO 2024, having been chosen to stage the 17th edition of the UEFA European Championship at a UEFA Executive Committee meeting in Nyon on 27 September 2018. Host cities\\\\nEURO 2024 fixtures by venue\\\\nEURO 2024 fixtures by team\\\\nAlso visit\\\\nChange language\\\\nServices links and disclaimer\\\\n© 1998-2024 UEFA. Where and when will the final of UEFA EURO 2024 be played?\\\\nBerlin's Olympiastadion will stage the final on Sunday 14 July 2024.\\\\n The ten venues chosen to host games at the tournament include nine of the stadiums used at the 2006 World Cup plus the Düsseldorf Arena.\\\\n All you need to know\\\\nThursday, January 11, 2024\\\\nArticle summary\\\\nThree-time winners Germany will stage the UEFA European Championship in 2024.\\\\n\",\"score\":0.99497885,\"raw_content\":null}]`,\n",
      "      metadata: {},\n",
      "      id: undefined\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "await app.invoke(null, { configurable: { thread_id: \"2\" } });"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="rag/langgraph_agentic_rag.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Retrieval Agent\n",
    "\n",
    "We can implement\n",
    "[Retrieval Agents](https://js.langchain.com/docs/tutorials/rag/)\n",
    "in [LangGraph](https://js.langchain.com/docs/langgraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load env vars\n",
    "\n",
    "Add a `.env` variable in the root of the `./examples` folder with your\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import dotenv from 'dotenv';\n",
    "\n",
    "// dotenv.config();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "```bash\n",
    "npm install cheerio zod zod-to-json-schema langchain @langchain/openai @langchain/core @langchain/community @langchain/textsplitters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
    "import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "\n",
    "const urls = [\n",
    "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "];\n",
    "\n",
    "const docs = await Promise.all(\n",
    "  urls.map((url) => new CheerioWebBaseLoader(url).load()),\n",
    ");\n",
    "const docsList = docs.flat();\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 500,\n",
    "  chunkOverlap: 50,\n",
    "});\n",
    "const docSplits = await textSplitter.splitDocuments(docsList);\n",
    "\n",
    "// Add to vectorDB\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "  docSplits,\n",
    "  new OpenAIEmbeddings(),\n",
    ");\n",
    "\n",
    "const retriever = vectorStore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent state\n",
    "\n",
    "We will define a graph.\n",
    "\n",
    "You may pass a custom `state` object to the graph, or use a simple list of\n",
    "`messages`.\n",
    "\n",
    "Our state will be a list of `messages`.\n",
    "\n",
    "Each node in our graph will append to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const GraphState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "    default: () => [],\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createRetrieverTool } from \"langchain/tools/retriever\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const tool = createRetrieverTool(\n",
    "  retriever,\n",
    "  {\n",
    "    name: \"retrieve_blog_posts\",\n",
    "    description:\n",
    "      \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    "  },\n",
    ");\n",
    "const tools = [tool];\n",
    "\n",
    "const toolNode = new ToolNode<typeof GraphState.State>(tools);"
   ]
  },
  {
   "attachments": 
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes and Edges\n",
    "\n",
    "Each node will -\n",
    "\n",
    "1/ Either be a function or a runnable.\n",
    "\n",
    "2/ Modify the `state`.\n",
    "\n",
    "The edges choose which node to call next.\n",
    "\n",
    "We can lay out an agentic RAG graph like this:\n",
    "\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END } from \"@langchain/langgraph\";\n",
    "import { pull } from \"langchain/hub\";\n",
    "import { z } from \"zod\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { AIMessage, BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "/**\n",
    " * Decides whether the agent should retrieve more information or end the process.\n",
    " * This function checks the last message in the state for a function call. If a tool call is\n",
    " * present, the process continues to retrieve information. Otherwise, it ends the process.\n",
    " * @param {typeof GraphState.State} state - The current state of the agent, including all messages.\n",
    " * @returns {string} - A decision to either \"continue\" the retrieval process or \"end\" it.\n",
    " */\n",
    "function shouldRetrieve(state: typeof GraphState.State): string {\n",
    "  const { messages } = state;\n",
    "  console.log(\"---DECIDE TO RETRIEVE---\");\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "\n",
    "  if (\"tool_calls\" in lastMessage && Array.isArray(lastMessage.tool_calls) && lastMessage.tool_calls.length) {\n",
    "    console.log(\"---DECISION: RETRIEVE---\");\n",
    "    return \"retrieve\";\n",
    "  }\n",
    "  // If there are no tool calls then we finish.\n",
    "  return END;\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether the Agent should continue based on the relevance of retrieved documents.\n",
    " * This function checks if the last message in the conversation is of type FunctionMessage, indicating\n",
    " * that document retrieval has been performed. It then evaluates the relevance of these documents to the user's\n",
    " * initial question using a predefined model and output parser. If the documents are relevant, the conversation\n",
    " * is considered complete. Otherwise, the retrieval process is continued.\n",
    " * @param {typeof GraphState.State} state - The current state of the agent, including all messages.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} - The updated state with the new message added to the list of messages.\n",
    " */\n",
    "async function gradeDocuments(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---GET RELEVANCE---\");\n",
    "\n",
    "  const { messages } = state;\n",
    "  const tool = {\n",
    "    name: \"give_relevance_score\",\n",
    "    description: \"Give a relevance score to the retrieved documents.\",\n",
    "    schema: z.object({\n",
    "      binaryScore: z.string().describe(\"Relevance score 'yes' or 'no'\"),\n",
    "    })\n",
    "  }\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing relevance of retrieved docs to a user question.\n",
    "  Here are the retrieved docs:\n",
    "  \\n ------- \\n\n",
    "  {context} \n",
    "  \\n ------- \\n\n",
    "  Here is the user question: {question}\n",
    "  If the content of the docs are relevant to the users question, score them as relevant.\n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the docs are relevant to the question.\n",
    "  Yes: The docs are relevant to the question.\n",
    "  No: The docs are not relevant to the question.`,\n",
    "  );\n",
    "\n",
    "  const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "    temperature: 0,\n",
    "  }).bindTools([tool], {\n",
    "    tool_choice: tool.name,\n",
    "  });\n",
    "\n",
    "  const chain = prompt.pipe(model);\n",
    "\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "\n",
    "  const score = await chain.invoke({\n",
    "    question: messages[0].content as string,\n",
    "    context: lastMessage.content as string,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    messages: [score]\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Check the relevance of the previous LLM tool call.\n",
    " *\n",
    " * @param {typeof GraphState.State} state - The current state of the agent, including all messages.\n",
    " * @returns {string} - A directive to either \"yes\" or \"no\" based on the relevance of the documents.\n",
    " */\n",
    "function checkRelevance(state: typeof GraphState.State): string {\n",
    "  console.log(\"---CHECK RELEVANCE---\");\n",
    "\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "  if (!(\"tool_calls\" in lastMessage)) {\n",
    "    throw new Error(\"The 'checkRelevance' node requires the most recent message to contain tool calls.\")\n",
    "  }\n",
    "  const toolCalls = (lastMessage as AIMessage).tool_calls;\n",
    "  if (!toolCalls || !toolCalls.length) {\n",
    "    throw new Error(\"Last message was not a function message\");\n",
    "  }\n",
    "\n",
    "  if (toolCalls[0].args.binaryScore === \"yes\") {\n",
    "    console.log(\"---DECISION: DOCS RELEVANT---\");\n",
    "    return \"yes\";\n",
    "  }\n",
    "  console.log(\"---DECISION: DOCS NOT RELEVANT---\");\n",
    "  return \"no\";\n",
    "}\n",
    "\n",
    "// Nodes\n",
    "\n",
    "/**\n",
    " * Invokes the agent model to generate a response based on the current state.\n",
    " * This function calls the agent model to generate a response to the current conversation state.\n",
    " * The response is added to the state's messages.\n",
    " * @param {typeof GraphState.State} state - The current state of the agent, including all messages.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} - The updated state with the new message added to the list of messages.\n",
    " */\n",
    "async function agent(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---CALL AGENT---\");\n",
    "  \n",
    "  const { messages } = state;\n",
    "  // Find the AIMessage which contains the `give_relevance_score` tool call,\n",
    "  // and remove it if it exists. This is because the agent does not need to know\n",
    "  // the relevance score.\n",
    "  const filteredMessages = messages.filter((message) => {\n",
    "    if (\"tool_calls\" in message && Array.isArray(message.tool_calls) && message.tool_calls.length > 0) {\n",
    "      return message.tool_calls[0].name !== \"give_relevance_score\";\n",
    "    }\n",
    "    return true;\n",
    "  });\n",
    "\n",
    "  const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "    temperature: 0,\n",
    "    streaming: true,\n",
    "  }).bindTools(tools);\n",
    "\n",
    "  const response = await model.invoke(filteredMessages);\n",
    "  return {\n",
    "    messages: [response],\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Transform the query to produce a better question.\n",
    " * @param {typeof GraphState.State} state - The current state of the agent, including all messages.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} - The updated state with the new message added to the list of messages.\n",
    " */\n",
    "async function rewrite(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---TRANSFORM QUERY---\");\n",
    "\n",
    "  const { messages } = state;\n",
    "  const question = messages[0].content as string;\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "Here is the initial question:\n",
    "\\n ------- \\n\n",
    "{question} \n",
    "\\n ------- \\n\n",
    "Formulate an improved question:`,\n",
    "  );\n",
    "\n",
    "  // Grader\n",
    "  const model = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "    temperature: 0,\n",
    "    streaming: true,\n",
    "  });\n",
    "  const response = await prompt.pipe(model).invoke({ question });\n",
    "  return {\n",
    "    messages: [response],\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Generate answer\n",
    " * @param {typeof GraphState.State} state - The current state of the agent, including all messages.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} - The updated state with the new message added to the list of messages.\n",
    " */\n",
    "async function generate(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---GENERATE---\");\n",
    "  \n",
    "  const { messages } = state;\n",
    "  const question = messages[0].content as string;\n",
    "  // Extract the most recent ToolMessage\n",
    "  const lastToolMessage = messages.slice().reverse().find((msg) => msg._getType() === \"tool\");\n",
    "  if (!lastToolMessage) {\n",
    "    throw new Error(\"No tool message found in the conversation history\");\n",
    "  }\n",
    "\n",
    "  const docs = lastToolMessage.content as string;\n",
    "\n",
    "  const prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
    "\n",
    "  const llm = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "    temperature: 0,\n",
    "    streaming: true,\n",
    "  });\n",
    "\n",
    "  const ragChain = prompt.pipe(llm);\n",
    "\n",
    "  const response = await ragChain.invoke({\n",
    "    context: docs,\n",
    "    question,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    messages: [response],\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "- Start with an agent, `callModel`\n",
    "- Agent make a decision to call a function\n",
    "- If so, then `action` to call tool (retriever)\n",
    "- Then call agent with the tool output added to messages (`state`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the graph\n",
    "const workflow = new StateGraph(GraphState)\n",
    "  // Define the nodes which we'll cycle between.\n",
    "  .addNode(\"agent\", agent)\n",
    "  .addNode(\"retrieve\", toolNode)\n",
    "  .addNode(\"gradeDocuments\", gradeDocuments)\n",
    "  .addNode(\"rewrite\", rewrite)\n",
    "  .addNode(\"generate\", generate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { START } from \"@langchain/langgraph\";\n",
    "\n",
    "// Call agent node to decide to retrieve or not\n",
    "workflow.addEdge(START, \"agent\");\n",
    "\n",
    "// Decide whether to retrieve\n",
    "workflow.addConditionalEdges(\n",
    "  \"agent\",\n",
    "  // Assess agent decision\n",
    "  shouldRetrieve,\n",
    ");\n",
    "\n",
    "workflow.addEdge(\"retrieve\", \"gradeDocuments\");\n",
    "\n",
    "// Edges taken after the `action` node is called.\n",
    "workflow.addConditionalEdges(\n",
    "  \"gradeDocuments\",\n",
    "  // Assess agent decision\n",
    "  checkRelevance,\n",
    "  {\n",
    "    // Call tool node\n",
    "    yes: \"generate\",\n",
    "    no: \"rewrite\", // placeholder\n",
    "  },\n",
    ");\n",
    "\n",
    "workflow.addEdge(\"generate\", END);\n",
    "workflow.addEdge(\"rewrite\", \"agent\");\n",
    "\n",
    "// Compile\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---DECIDE TO RETRIEVE---\n",
      "---DECISION: RETRIEVE---\n",
      "Output from node: 'agent'\n",
      "{\n",
      "  type: 'ai',\n",
      "  content: '',\n",
      "  tool_calls: [\n",
      "    {\n",
      "      name: 'retrieve_blog_posts',\n",
      "      args: { query: 'types of agent memory' },\n",
      "      id: 'call_adLYkV7T2ry1EZFboT0jPuwn',\n",
      "      type: 'tool_call'\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---\n",
      "\n",
      "Output from node: 'retrieve'\n",
      "{\n",
      "  type: 'tool',\n",
      "  content: 'Agent System Overview\\n' +\n",
      "    '                \\n' +\n",
      "    '                    Component One: Planning\\n' +\n",
      "    '                        \\n' +\n",
      "    '                \\n' +\n",
      "    '                    Task Decomposition\\n' +\n",
      "    '                \\n' +\n",
      "    '                    Self-Reflection\\n' +\n",
      "    '                \\n' +\n",
      "    '                \\n' +\n",
      "    '                    Component Two: Memory\\n' +\n",
      "    '                        \\n' +\n",
      "    '                \\n' +\n",
      "    '                    Types of Memory\\n' +\n",
      "    '                \\n' +\n",
      "    '                    Maximum Inner Product Search (MIPS)\\n' +\n",
      "    '\\n' +\n",
      "    'Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n' +\n",
      "    '\\n' +\n",
      "    'Each element is an observation, an event directly provided by the agent.\\n' +\n",
      "    '- Inter-agent communication can trigger new natural language statements.\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n' +\n",
      "    '\\n' +\n",
      "    'Planning\\n' +\n",
      "    '\\n' +\n",
      "    'Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\n' +\n",
      "    'Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'Memory\\n' +\n",
      "    '\\n' +\n",
      "    'The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.',\n",
      "  tool_calls: undefined\n",
      "}\n",
      "---\n",
      "\n",
      "---GET RELEVANCE---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "Output from node: 'gradeDocuments'\n",
      "{\n",
      "  type: 'ai',\n",
      "  content: '',\n",
      "  tool_calls: [\n",
      "    {\n",
      "      name: 'give_relevance_score',\n",
      "      args: { binaryScore: 'no' },\n",
      "      type: 'tool_call',\n",
      "      id: 'call_AGE7gORVFubExfJWcjb0C2nV'\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---\n",
      "\n",
      "---TRANSFORM QUERY---\n",
      "Output from node: 'rewrite'\n",
      "{\n",
      "  type: 'ai',\n",
      "  content: \"What are the different types of agent memory described in Lilian Weng's blog post?\",\n",
      "  tool_calls: []\n",
      "}\n",
      "---\n",
      "\n",
      "---CALL AGENT---\n",
      "---DECIDE TO RETRIEVE---\n",
      "Output from node: 'agent'\n",
      "{\n",
      "  type: 'ai',\n",
      "  content: \"Lilian Weng's blog post describes the following types of agent memory:\\n\" +\n",
      "    '\\n' +\n",
      "    '1. **Memory Stream**:\\n' +\n",
      "    '   - This is a long-term memory module (external database) that records a comprehensive list of agents’ experiences in natural language.\\n' +\n",
      "    '   - Each element in the memory stream is an observation or an event directly provided by the agent.\\n' +\n",
      "    '   - Inter-agent communication can trigger new natural language statements to be added to the memory stream.\\n' +\n",
      "    '\\n' +\n",
      "    '2. **Retrieval Model**:\\n' +\n",
      "    '   - This model surfaces the context to inform the agent’s behavior based on relevance, recency, and importance.\\n' +\n",
      "    '\\n' +\n",
      "    'These memory types are part of a broader design that combines generative agents with memory, planning, and reflection mechanisms to enable agents to behave based on past experiences and interact with other agents.',\n",
      "  tool_calls: []\n",
      "}\n",
      "---\n",
      "\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain_core\",\n",
      "        \"messages\",\n",
      "        \"AIMessageChunk\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"Lilian Weng's blog post describes the following types of agent memory:\\n\\n1. **Memory Stream**:\\n   - This is a long-term memory module (external database) that records a comprehensive list of agents’ experiences in natural language.\\n   - Each element in the memory stream is an observation or an event directly provided by the agent.\\n   - Inter-agent communication can trigger new natural language statements to be added to the memory stream.\\n\\n2. **Retrieval Model**:\\n   - This model surfaces the context to inform the agent’s behavior based on relevance, recency, and importance.\\n\\nThese memory types are part of a broader design that combines generative agents with memory, planning, and reflection mechanisms to enable agents to behave based on past experiences and interact with other agents.\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {\n",
      "          \"estimatedTokenUsage\": {\n",
      "            \"promptTokens\": 280,\n",
      "            \"completionTokens\": 155,\n",
      "            \"totalTokens\": 435\n",
      "          },\n",
      "          \"prompt\": 0,\n",
      "          \"completion\": 0,\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3bfp_3cd8b62c3b\"\n",
      "        },\n",
      "        \"tool_call_chunks\": [],\n",
      "        \"id\": \"chatcmpl-9zAaVQGmTLiCaFvtbxUK60qMFsSmU\",\n",
      "        \"usage_metadata\": {\n",
      "          \"input_tokens\": 363,\n",
      "          \"output_tokens\": 156,\n",
      "          \"total_tokens\": 519\n",
      "        },\n",
      "        \"tool_calls\": [],\n",
      "        \"invalid_tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const inputs = {\n",
    "  messages: [\n",
    "    new HumanMessage(\n",
    "      \"What are the types of agent memory based on Lilian Weng's blog post?\",\n",
    "    ),\n",
    "  ],\n",
    "};\n",
    "let finalState;\n",
    "for await (const output of await app.stream(inputs)) {\n",
    "  for (const [key, value] of Object.entries(output)) {\n",
    "    const lastMsg = output[key].messages[output[key].messages.length - 1];\n",
    "    console.log(`Output from node: '${key}'`);\n",
    "    console.dir({\n",
    "      type: lastMsg._getType(),\n",
    "      content: lastMsg.content,\n",
    "      tool_calls: lastMsg.tool_calls,\n",
    "    }, { depth: null });\n",
    "    console.log(\"---\\n\");\n",
    "    finalState = value;\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(JSON.stringify(finalState, null, 2));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="rag/langgraph_crag_mistral.ipynb">
{
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrective RAG\n",
    "\n",
    "Self-reflection can enhance RAG, enabling correction of poor quality retrieval\n",
    "or generations.\n",
    "\n",
    "Several recent papers focus on this theme, but implementing the ideas can be\n",
    "tricky.\n",
    "\n",
    "Here we show how to implement self-reflective RAG using `Mistral` and\n",
    "`LangGraph`.\n",
    "\n",
    "We'll focus on ideas from one paper, `Corrective RAG (CRAG)`\n",
    "[here](https://arxiv.org/pdf/2401.15884.pdf).\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "### Running Locally\n",
    "\n",
    "If you want to run this locally (e.g., on your laptop), use\n",
    "[Ollama](https://ollama.ai/library/mistral/tags):\n",
    "\n",
    "- Download [Ollama app](https://ollama.ai/).\n",
    "- Download a `Mistral` model e.g., `ollama pull mistral:7b-instruct`, from\n",
    "  various Mistral versions [here](https://ollama.ai/library/mistral) and Mixtral\n",
    "  versions [here](https://ollama.ai/library/mixtral) available.\n",
    "- Download LLaMA2 `ollama pull llama2:latest` to use Ollama embeddings.\n",
    "- Set flags indicating we will run locally and the Mistral model downloaded:\n",
    "\n",
    "```typescript\n",
    "const runLocal = true;\n",
    "const localLlm = \"mistral\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load env vars\n",
    "\n",
    "Add a `.env` variable in the root of the repo with your variables.\n",
    "\n",
    "- Set `TOGETHER_AI_API_KEY` (optional if you don't want to run the chat model\n",
    "  locally via Ollama). You can create an account\n",
    "  [here](https://www.together.ai/).\n",
    "- Set `MISTRAL_API_KEY` (optional if you don't want to run embeddings locally\n",
    "  via Ollama) for Mistral AI embeddings.\n",
    "- Set `TAVILY_API_KEY` to enable web search\n",
    "  [here](https://app.tavily.com/sign-in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "```bash\n",
    "npm install cheerio zod zod-to-json-schema langchain @langchain/community @langchain/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing\n",
    "\n",
    "- Optionally, use [LangSmith](https://docs.smith.langchain.com/) for tracing\n",
    "  (shown at bottom) by setting:\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "export LANGCHAIN_API_KEY=<your-api-key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const runLocal = true;\n",
    "const localLlm = \"mistral\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "First, let's index a popular blog post on agents.\n",
    "\n",
    "We can use\n",
    "[Mistral embeddings](https://js.langchain.com/docs/integrations/text_embedding/mistralai).\n",
    "\n",
    "For local embeddings, we can use\n",
    "[Ollama](https://js.langchain.com/docs/integrations/text_embedding/ollama).\n",
    "You'll need to run `ollama pull nomic-embed-text` to pull the embeddings model\n",
    "locally.\n",
    "\n",
    "We'll use a local demo vectorstore, but you can swap in\n",
    "[your preferred production-ready choice](https://js.langchain.com/docs/integrations/vectorstores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings {\n",
       "  caller: AsyncCaller {\n",
       "    maxConcurrency: \u001b[33m1\u001b[39m,\n",
       "    maxRetries: \u001b[33m6\u001b[39m,\n",
       "    onFailedAttempt: \u001b[36m[Function: defaultFailedAttemptHandler]\u001b[39m,\n",
       "    queue: PQueue {\n",
       "      _events: Events <[Object: null prototype] {}> {},\n",
       "      _eventsCount: \u001b[33m0\u001b[39m,\n",
       "      _intervalCount: \u001b[33m1\u001b[39m,\n",
       "      _intervalEnd: \u001b[33m0\u001b[39m,\n",
       "      _pendingCount: \u001b[33m0\u001b[39m,\n",
       "      _resolveEmpty: \u001b[36m[Function: empty]\u001b[39m,\n",
       "      _resolveIdle: \u001b[36m[Function: empty]\u001b[39m,\n",
       "      _carryoverConcurrencyCount: \u001b[33mfalse\u001b[39m,\n",
       "      _isIntervalIgnored: \u001b[33mtrue\u001b[39m,\n",
       "      _intervalCap: \u001b[33mInfinity\u001b[39m,\n",
       "      _interval: \u001b[33m0\u001b[39m,\n",
       "      _queue: PriorityQueue { _queue: [] },\n",
       "      _queueClass: \u001b[36m[class PriorityQueue]\u001b[39m,\n",
       "      _concurrency: \u001b[33m1\u001b[39m,\n",
       "      _intervalId: \u001b[90mundefined\u001b[39m,\n",
       "      _timeout: \u001b[90mundefined\u001b[39m,\n",
       "      _throwOnTimeout: \u001b[33mfalse\u001b[39m,\n",
       "      _isPaused: \u001b[33mfalse\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  model: \u001b[32m\"nomic-embed-text\"\u001b[39m,\n",
       "  baseUrl: \u001b[32m\"http://localhost:11434\"\u001b[39m,\n",
       "  headers: \u001b[90mundefined\u001b[39m,\n",
       "  keepAlive: \u001b[32m\"5m\"\u001b[39m,\n",
       "  requestOptions: \u001b[90mundefined\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"cheerio\";\n",
    "import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n",
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "import { OllamaEmbeddings } from \"@langchain/community/embeddings/ollama\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { MistralAIEmbeddings } from \"@langchain/mistralai\";\n",
    "\n",
    "const url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\";\n",
    "const loader = new CheerioWebBaseLoader(url);\n",
    "const docs = await loader.load();\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 500,\n",
    "  chunkOverlap: 100,\n",
    "});\n",
    "const docSplits = await textSplitter.splitDocuments(docs);\n",
    "\n",
    "let embeddings;\n",
    "\n",
    "if (runLocal) {\n",
    "  embeddings = new OllamaEmbeddings({ model: \"nomic-embed-text\" });\n",
    "} else {\n",
    "  embeddings = new MistralAIEmbeddings();\n",
    "}\n",
    "\n",
    "// Add to vectorDB\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "  docSplits,\n",
    "  embeddings,\n",
    ");\n",
    "const retriever = vectorStore.asRetriever();"
   ]
  },
  {
    "attachments"
    "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrective RAG\n",
    "\n",
    "Let's implement self-reflective RAG with some ideas from the CRAG (Corrective\n",
    "RAG) [paper](https://arxiv.org/pdf/2401.15884.pdf):\n",
    "\n",
    "- Grade documents for relevance relative to the question.\n",
    "- If any are irrelevant, then we will supplement the context used for generation\n",
    "  with web search.\n",
    "- For web search, we will re-phrase the question and use Tavily API.\n",
    "- We will then pass retrieved documents and web results to an LLM for final\n",
    "  answer generation.\n",
    "\n",
    "Here is a schematic of our graph in more detail:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We will implement this using\n",
    "[LangGraph](https://js.langchain.com/docs/langgraph):\n",
    "\n",
    "- See video\n",
    "  [here](https://www.youtube.com/watch?ref=blog.langchain.dev&v=pbAd8O1Lvm4&feature=youtu.be)\n",
    "- See blog post [here](https://blog.langchain.dev/agentic-rag-with-langgraph/)\n",
    "\n",
    "---\n",
    "\n",
    "### State\n",
    "\n",
    "Every node in our graph will modify `state`, which is dict that contains values\n",
    "(`question`, `documents`, etc) relevant to RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Represents the state of our graph.\n",
    " */\n",
    "type GraphState = {\n",
    "  /**\n",
    "   * An object where each key is a string.\n",
    "   */\n",
    "  keys: Record<string, any>;\n",
    "};\n",
    "\n",
    "const graphState = {\n",
    "  keys: {\n",
    "    value: null,\n",
    "    default: () => ({}),\n",
    "  },\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes and Edges\n",
    "\n",
    "Every node in the graph we laid out above is a function.\n",
    "\n",
    "Each node will modify the state in some way.\n",
    "\n",
    "Each edge will choose which node to call next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { Document, DocumentInterface } from \"@langchain/core/documents\";\n",
    "import { z } from \"zod\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { pull } from \"langchain/hub\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { ChatOllama } from \"@langchain/community/chat_models/ollama\";\n",
    "import { ChatTogetherAI } from \"@langchain/community/chat_models/togetherai\";\n",
    "import { BaseMessageChunk } from \"@langchain/core/messages\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "import type { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "\n",
    "/**\n",
    " * Retrieve documents\n",
    " *\n",
    " * @param {GraphState} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<GraphState>} The new state object.\n",
    " */\n",
    "async function retrieve(state: GraphState, config?: RunnableConfig) {\n",
    "  console.log(\"---RETRIEVE---\");\n",
    "  const stateObject = state.keys;\n",
    "  const question = stateObject.question;\n",
    "  const documents = await retriever.invoke(question, config);\n",
    "  return {\n",
    "    keys: {\n",
    "      ...stateObject,\n",
    "      documents,\n",
    "      question,\n",
    "    },\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Generate answer\n",
    " *\n",
    " * @param {GraphState} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<GraphState>} The new state object.\n",
    " */\n",
    "async function generate(state: GraphState) {\n",
    "  console.log(\"---GENERATE---\");\n",
    "  const stateObject = state.keys;\n",
    "  const documents = stateObject.documents;\n",
    "  const question = stateObject.question;\n",
    "  const local = stateObject.local;\n",
    "\n",
    "  // Pull in the prompt\n",
    "  const prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
    "\n",
    "  let llm;\n",
    "  if (local) {\n",
    "    llm = new ChatOllama({ model: localLlm, temperature: 0 });\n",
    "  } else {\n",
    "    llm = new ChatTogetherAI({\n",
    "      modelName: \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "      temperature: 0,\n",
    "    });\n",
    "  }\n",
    "\n",
    "  // RAG Chain\n",
    "  const ragChain = prompt.pipe(llm).pipe(new StringOutputParser());\n",
    "\n",
    "  const formattedDocs = documents.map((doc) => doc.pageContent).join(\"\\n\\n\");\n",
    "\n",
    "  const generation = await ragChain.invoke({\n",
    "    context: formattedDocs,\n",
    "    question,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    keys: {\n",
    "      ...stateObject,\n",
    "      generation,\n",
    "    },\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether the retrieved documents are relevant to the question.\n",
    " *\n",
    " * @param {GraphState} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<GraphState>} The new state object.\n",
    " */\n",
    "async function gradeDocuments(state: GraphState) {\n",
    "  console.log(\"---CHECK RELEVANCE---\");\n",
    "  const stateObject = state.keys;\n",
    "  const documents = stateObject.documents;\n",
    "  const question = stateObject.question;\n",
    "  const local = stateObject.local;\n",
    "\n",
    "  let llm;\n",
    "  if (local) {\n",
    "    llm = new ChatOllama({\n",
    "      model: localLlm,\n",
    "      temperature: 0,\n",
    "      format: \"json\",\n",
    "    });\n",
    "  } else {\n",
    "    const zodScore = z.object({\n",
    "      binaryScore: z.enum([\"yes\", \"no\"]).describe(\n",
    "        \"Relevance score 'yes' or 'no'\",\n",
    "      ),\n",
    "    });\n",
    "    llm = new ChatTogetherAI({\n",
    "      modelName: \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "      temperature: 0,\n",
    "    }).bind({\n",
    "      response_format: {\n",
    "        type: \"json_object\",\n",
    "        schema: zodToJsonSchema(zodScore),\n",
    "      },\n",
    "    });\n",
    "  }\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "  Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "  Here is the user question: {question} \\n\n",
    "  If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "  It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "  Use the 'grade' tool to provide the score.\n",
    "  Instructions: {formatInstructions}`,\n",
    "  );\n",
    "\n",
    "  const formatInstructions =\n",
    "    \"Respond with a valid JSON object containing a single key 'binaryScore' with a value of 'yes' or 'no'.\";\n",
    "\n",
    "  const filteredDocs: Array<DocumentInterface> = [];\n",
    "  let runWebSearch = \"No\";\n",
    "  for await (const doc of documents) {\n",
    "    let finalRes = \"\";\n",
    "\n",
    "    // Ollama JSON mode has a bug (as of 02/15/2024) where it logs infinite tab unicode characters.\n",
    "    // To hack around this, we attach a stop signal to the LLM, and if the last character in the\n",
    "    // response returns '9' from .charCodeAt() we know it has started logging infinite tab unicode characters.\n",
    "    try {\n",
    "      const controller = new AbortController();\n",
    "      // Chain\n",
    "      const chain = prompt.pipe(llm.bind({ signal: controller.signal }));\n",
    "      const stream = await chain.stream({\n",
    "        context: doc.pageContent,\n",
    "        question,\n",
    "        formatInstructions,\n",
    "      });\n",
    "      for await (const item of stream) {\n",
    "        finalRes += (item as BaseMessageChunk).content;\n",
    "        const prevCharCodeAt = finalRes.length > 1 &&\n",
    "          finalRes.charCodeAt(finalRes.length - 2);\n",
    "        const charCodeAt = finalRes.charCodeAt(finalRes.length - 1);\n",
    "        if (prevCharCodeAt === 9 && charCodeAt === 9) {\n",
    "          controller.abort();\n",
    "        }\n",
    "      }\n",
    "    } catch (e) {\n",
    "      // no-op\n",
    "      console.log(\"Infinite tab response detected. Aborting.\");\n",
    "    }\n",
    "\n",
    "    const parsed = JSON.parse(finalRes.trim());\n",
    "    if (parsed.binaryScore === \"yes\") {\n",
    "      console.log(\"---GRADE: DOCUMENT RELEVANT---\");\n",
    "      filteredDocs.push(doc);\n",
    "    } else {\n",
    "      runWebSearch = \"Yes\";\n",
    "      console.log(\"---GRADE: DOCUMENT NOT RELEVANT---\");\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return {\n",
    "    keys: {\n",
    "      ...stateObject,\n",
    "      documents: filteredDocs,\n",
    "      runWebSearch,\n",
    "    },\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Transform the query to produce a better question.\n",
    " *\n",
    " * @param {GraphState} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<GraphState>} The new state object.\n",
    " */\n",
    "async function transformQuery(state: GraphState) {\n",
    "  console.log(\"---TRANSFORM QUERY---\");\n",
    "  const stateObject = state.keys;\n",
    "  const question = stateObject.question;\n",
    "  const local = stateObject.local;\n",
    "\n",
    "  // Pull in the prompt\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are generating questions that is well optimized for semantic search retrieval. \\n \n",
    "  Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "  Here is the initial question:\n",
    "  \\n ------- \\n\n",
    "  {question} \n",
    "  \\n ------- \\n\n",
    "  Provide an improved question without any preamble, only respond with the updated question: `,\n",
    "  );\n",
    "\n",
    "  // Grader\n",
    "  let llm;\n",
    "  if (local) {\n",
    "    llm = new ChatOllama({ model: localLlm, temperature: 0 });\n",
    "  } else {\n",
    "    llm = new ChatTogetherAI({\n",
    "      modelName: \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "      temperature: 0,\n",
    "    });\n",
    "  }\n",
    "\n",
    "  // Prompt\n",
    "  const chain = prompt.pipe(llm).pipe(new StringOutputParser());\n",
    "  const betterQuestion = await chain.invoke({ question });\n",
    "\n",
    "  return {\n",
    "    keys: {\n",
    "      ...stateObject,\n",
    "      question: betterQuestion,\n",
    "    },\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Web search based on the re-phrased question using Tavily API.\n",
    " *\n",
    " * @param {GraphState} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<GraphState>} The new state object.\n",
    " */\n",
    "async function webSearch(state: GraphState) {\n",
    "  console.log(\"---WEB SEARCH---\");\n",
    "  const stateObject = state.keys;\n",
    "  const question = stateObject.question;\n",
    "  const documents = stateObject.documents;\n",
    "\n",
    "  const tool = new TavilySearchResults();\n",
    "  const docs = await tool.invoke(question);\n",
    "  const webResults = new Document({ pageContent: docs });\n",
    "  const newDocs = documents.concat(webResults);\n",
    "\n",
    "  return {\n",
    "    keys: {\n",
    "      ...stateObject,\n",
    "      documents: newDocs,\n",
    "    },\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether to generate an answer or re-generate a question for web search.\n",
    " *\n",
    " * @param {GraphState} state The current state of the graph.\n",
    " * @returns {Promise<GraphState>} The new state object.\n",
    " */\n",
    "function decideToGenerate(state: GraphState) {\n",
    "  console.log(\"---DECIDE TO GENERATE---\");\n",
    "  const stateObject = state.keys;\n",
    "  const search = stateObject.runWebSearch;\n",
    "\n",
    "  if (search === \"Yes\") {\n",
    "    // All documents have been filtered checkRelevance\n",
    "    // We will re-generate a new query\n",
    "    console.log(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\");\n",
    "    return \"transformQuery\";\n",
    "  }\n",
    "  // We have relevant documents, so generate answer\n",
    "  console.log(\"---DECISION: GENERATE---\");\n",
    "  return \"generate\";\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph\n",
    "\n",
    "The just follows the flow we outlined in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const workflow = new StateGraph<GraphState>({\n",
    "  channels: graphState,\n",
    "});\n",
    "\n",
    "// Define the nodes\n",
    "workflow.addNode(\"retrieve\", retrieve);\n",
    "workflow.addNode(\"gradeDocuments\", gradeDocuments);\n",
    "workflow.addNode(\"generate\", generate);\n",
    "workflow.addNode(\"transformQuery\", transformQuery);\n",
    "workflow.addNode(\"webSearch\", webSearch);\n",
    "\n",
    "// Build graph\n",
    "workflow.addEdge(START, \"retrieve\");\n",
    "workflow.addEdge(\"retrieve\", \"gradeDocuments\");\n",
    "workflow.addConditionalEdges(\n",
    "  \"gradeDocuments\",\n",
    "  decideToGenerate,\n",
    "  {\n",
    "    transformQuery: \"transformQuery\",\n",
    "    generate: \"generate\",\n",
    "  },\n",
    ");\n",
    "workflow.addEdge(\"transformQuery\", \"webSearch\");\n",
    "workflow.addEdge(\"webSearch\", \"generate\");\n",
    "workflow.addEdge(\"generate\", END);\n",
    "\n",
    "// Compile\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "Node: 'retrieve'\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK RELEVANCE---\n",
      "Infinite tab response detected. Aborting.\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Node: 'gradeDocuments'\n",
      "\n",
      "---\n",
      "\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\n",
      "---TRANSFORM QUERY---\n",
      "Node: 'transformQuery'\n",
      "\n",
      "---\n",
      "\n",
      "---WEB SEARCH---\n",
      "Node: 'webSearch'\n",
      "\n",
      "---\n",
      "\n",
      "---GENERATE---\n",
      "Node: 'generate'\n",
      "\n",
      "---\n",
      "\n",
      "Node: '__end__'\n",
      "\n",
      "---\n",
      "\n",
      "{\n",
      "  \"keys\": {\n",
      "    \"question\": \" What are the functions and workings of different types of agent memory?\",\n",
      "    \"local\": true,\n",
      "    \"documents\": [\n",
      "      {\n",
      "        \"pageContent\": \"Agent System Overview\\n                \\n                    Component One: Planning\\n                        \\n                \\n                    Task Decomposition\\n                \\n                    Self-Reflection\\n                \\n                \\n                    Component Two: Memory\\n                        \\n                \\n                    Types of Memory\\n                \\n                    Maximum Inner Product Search (MIPS)\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
      "          \"loc\": {\n",
      "            \"lines\": {\n",
      "              \"from\": 112,\n",
      "              \"to\": 127\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"pageContent\": \"The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
      "          \"loc\": {\n",
      "            \"lines\": {\n",
      "              \"from\": 318,\n",
      "              \"to\": 318\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"pageContent\": \"Fig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\",\n",
      "        \"metadata\": {\n",
      "          \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
      "          \"loc\": {\n",
      "            \"lines\": {\n",
      "              \"from\": 235,\n",
      "              \"to\": 240\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"pageContent\": \"[{\\\"title\\\":\\\"Different Types of Memories and the Function of Each - Verywell Mind\\\",\\\"url\\\":\\\"https://www.verywellmind.com/different-types-of-memory-and-their-functions-5194859\\\",\\\"content\\\":\\\"People who classify memory into only two distinctive types, implicit and explicit memory, view that other types of memories like sensory, short-term, and long-term memories aren’t types of memory but stages of memory.\\\\n While some experts view working memory as a fourth distinct type of memory, working memory can fall under the classification of short-term memory and, in many cases, is even used interchangeably.\\\\n Working Memory\\\\nWorking memory is a type of memory that involves the immediate and small amount of information that a person actively uses as they perform cognitive tasks.\\\\n These processes are also how sensory memory might be turned into short-term memory or short-term memory into long-term memory.\\\\n The four general types of memories are sensory memory, short-term memory, working memory, and long-term memory.\\\",\\\"score\\\":0.97278,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Memory: Neurobiological mechanisms and assessment - PMC\\\",\\\"url\\\":\\\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8611531/\\\",\\\"content\\\":\\\"Memory: Neurobiological mechanisms and assessment\\\\nSwaleha Mujawar\\\\nDepartment of Psychiatry, Regional Mental Hospital, Nagpur, Maharashtra, India\\\\nJaideep Patil\\\\n1Department of Psychiatry, Dr. D. Y. Patil Medical College, Hospital and Research Center, Dr D Y Patil Vidyapeeth, Pune, Maharashtra, India\\\\nBhushan Chaudhari\\\\n1Department of Psychiatry, Dr. D. Y. Patil Medical College, Hospital and Research Center, Dr D Y Patil Vidyapeeth, Pune, Maharashtra, India\\\\nDaniel Saldanha\\\\n1Department of Psychiatry, Dr. D. Y. Patil Medical College, Hospital and Research Center, Dr D Y Patil Vidyapeeth, Pune, Maharashtra, India\\\\nAbstract\\\\nMemory is the process of retaining of knowledge over a period for the function of affecting future actions. It is observed that suffering from extended periods of amnesia after a trauma can be a prognostic indicator and that the improvement from the symptoms of concussion may take more than usual.[20]\\\\nASSESSMENT\\\\nA scale consisting of 30 questions which is employed to gauge impairment inmemory is the Mini-Mental State Examination.[21] The book: The Principles of Psychology written by famous psychologist William James suggested that there is a difference between memory and habit.[3] The case of Henry Molaison was first described as a result of research of two main researchers, namely William Beecher Scoville and Brenda Milner.[4] A few health behaviors like exercise can prevent forgetting from happening.[15]\\\\nFORGETTING\\\\nForgetting was classified under various types by Paul Connerton: They are-prescriptive forgetting, planned obsolescence, formation of the new identity, repressive erasure, structural amnesia, annulment, and humiliated silence.[16]\\\\nRetroactive interference can be defined as the phenomenon when new information or memories disturb the old information. Studies suggest that long-term memory (LTM) storage may be preserved by DNA methylation or prions.[8,9]\\\\nMULTI-STORE MODEL OF MEMORY\\\\nRichard Atkinson and Richard Shiffrin put forth a model of memory which is known as “The multi-store model or modal model.\\\",\\\"score\\\":0.94663,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Types of Memory | Psychology Today\\\",\\\"url\\\":\\\"https://www.psychologytoday.com/us/basics/memory/types-memory\\\",\\\"content\\\":\\\"The appearance of someone you met a minute ago\\\\n• The current temperature, immediately after looking it up\\\\n• What happened moments ago in a movie\\\\n• A number you have calculated as part of a mental math problem\\\\n• The person named at the beginning of a sentence\\\\n• Holding a concept in mind (such as ball) and combining it with another (orange)\\\\n The sound of a piano note that was just played\\\\n• The appearance of a car that drove by\\\\n• The smell of a restaurant you passed\\\\nProspective memory is forward-thinking memory: It means recalling an intention from the past in order to do something in the future. Overcome burnout, your burdens, and that endless to-do list.\\\\nVerified by Psychology Today\\\\nTypes of Memory\\\\nReviewed by Psychology Today Staff\\\\nA person’s memory is a sea of images and other sensory impressions, facts and meanings, echoes of past feelings, and ingrained codes for how to behave—a diverse well of information. The details of a phone call you had 20 minutes ago\\\\n• How you felt during your last argument\\\\n• What it was like receiving your high-school diploma\\\\nSemantic memory is someone’s long-term store of knowledge: It’s composed of pieces of information such as facts learned in school, what concepts mean and how they are related, or the definition of a particular word. • How to tie your shoes\\\\n• How to send an email\\\\n• How to shoot a basketball\\\\nThe terms short-term memory and working memory are sometimes used interchangeably, and both refer to storage of information for a brief amount of time.\\\",\\\"score\\\":0.92346,\\\"raw_content\\\":null},{\\\"title\\\":\\\"The Neuroanatomical, Neurophysiological and Psychological Basis of ...\\\",\\\"url\\\":\\\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5491610/\\\",\\\"content\\\":\\\"The Neuroanatomical, Neurophysiological and Psychological Basis of Memory: Current Models and Their Origins\\\\nEduardo Camina\\\\n1Mind-Brain Group: Biology and Subjectivity in Philosophy and Contemporary Neuroscience, Institute for Culture and Society, University of Navarra, Pamplona, Spain\\\\n2Department of Learning and Curriculum, Faculty of Education and Psychology, University of Navarra, Pamplona, Spain\\\\nFrancisco Güell\\\\n1Mind-Brain Group: Biology and Subjectivity in Philosophy and Contemporary Neuroscience, Institute for Culture and Society, University of Navarra, Pamplona, Spain\\\\nAbstract\\\\nThis review aims to classify and clarify, from a neuroanatomical, neurophysiological, and psychological perspective, different memory models that are currently widespread in the literature as well as to describe their origins. Binder and Desai showed two striking results related to neuroimaging research: on the one hand, the participation of the specific sensory, motor and emotional modality in understanding language and, on the other hand, the existence of large regions of the brain (the inferior parietal lobe and a large part of the temporal lobe) involved in tasks related to understanding. There are a number of neural components that are closely related to the proper functioning of episodic memory, which include the following: the cortex near the hippocampus [as discussed below, the perirhinal cortex (PRC), the entorhinal cortex, and the parahippocampal cortex (PHC)], cortical and subcortical structures, and the circuits within the medial temporal lobe and hippocampus.\\\\n For example, an adverse event in childhood (e.g., seeing your grandfather being run over by a combine) can, on the one hand, consolidate as a stable declarative memory for the event itself (the sound of a combine always makes you remember that moment-episodic memory) and, on the other hand, can crystallize in non-declarative memory and result in a phobia experienced as a personality trait rather than as a mere memory (being near a combine will always produce panic and induces a desire to escape that situation-associative memory). Regarding short-term, Sperling interpreted the results of the partial report as due to the rapid decline of the visual sign and reaffirmed this short duration by obtaining a decrease in the number of letters reported by the subject in delaying the audio signal for choosing a row to remember in the presentation.\\\",\\\"score\\\":0.92177,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Architecture of AI Framework: Comparing AI Agent Memory to ... - Medium\\\",\\\"url\\\":\\\"https://medium.com/springchain-ai/architecture-of-ai-framework-comparing-ai-agent-memory-to-human-brain-5b446ebc99dd\\\",\\\"content\\\":\\\"Types of Information in the Knowledge Base\\\\nAnalyzing the structure of the human brain and the demands of AI applications and agents, the knowledge base can be segmented into:\\\\nCategorizing Information Storage\\\\nInformation in the knowledge base falls into two brackets:\\\\nRelated Modules in an AI Framework\\\\nDeveloping artificial intelligence (AI) capabilities requires more than just training algorithms on data. --\\\\n--\\\\nWritten by Jarosław Wasowski\\\\nspringchain.ai\\\\nAI Engineer, Software Engineer, Founder of springchain.ai\\\\nHelp\\\\nStatus\\\\nAbout\\\\nCareers\\\\nBlog\\\\nPrivacy\\\\nTerms\\\\nText to speech\\\\nTeams The Key Architectures: Exploring the Landscape of the Mind\\\\nA marvel of nature, the brain’s structure can be broken down into critical areas, each responsible for unique aspects of memory:\\\\nThe Hippocampus: The Heart of Declarative Memory\\\\n Architecture of AI Framework: Comparing AI Agent Memory to Human Brain\\\\nJarosław Wasowski\\\\nFollow\\\\nspringchain.ai\\\\n--\\\\nListen\\\\nShare\\\\n“The true sign of intelligence is not knowledge but imagination.” Graphs and Memory Networks in AI Systems\\\\nDiving deeper into the architecture of AI systems, especially in the intriguing realm of the Knowledge Module, the sheer brilliance of graphs and memory networks unfolds.\\\",\\\"score\\\":0.91643,\\\"raw_content\\\":null}]\",\n",
      "        \"metadata\": {}\n",
      "      }\n",
      "    ],\n",
      "    \"runWebSearch\": \"Yes\",\n",
      "    \"generation\": \" The multi-store model or modal model of memory, proposed by Richard Atkinson and Richard Shiffrin, suggests that memory consists of multiple stores or systems for different types of information. These include sensory memory, short-term memory or working memory, and long-term memory. Sensory memory holds incoming sensory information for a very brief period, while short-term memory retains information for a limited time to perform tasks such as mental arithmetic or remembering a phone number. Long-term memory is responsible for storing information for extended periods, including facts, skills, and experiences. The model also includes the concept of \\\"prospective memory,\\\" which refers to recalling an intention from the past in order to do something in the future.\\n\\nThe human brain has various neural components that support episodic memory, such as the cortex near the hippocampus (perirhinal cortex, entorhinal cortex, and parahippocampal cortex), cortical and subcortical structures, and circuits within the medial temporal lobe and hippocampus. These components play a crucial role in encoding, consolidating, and retrieving memories.\\n\\nIn contrast, AI agents have different memory architectures to store and process information. Graphs and memory networks are essential components of AI systems, enabling them to learn from data and make intelligent decisions based on the knowledge they acquire. The architecture of these memory systems varies significantly from human memory, as AI agents do not possess emotions or consciousness.\\n\\nIn summary, while both humans and AI agents have memory capabilities, their underlying structures and functions differ significantly. Human memory is a complex system involving various brain regions and processes, whereas AI memory architectures are designed to store and process information for specific tasks.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const inputs = {\n",
    "  keys: {\n",
    "    question: \"Explain how the different types of agent memory work.\",\n",
    "    local: runLocal,\n",
    "  },\n",
    "};\n",
    "const config = { recursionLimit: 50 };\n",
    "let finalGeneration;\n",
    "for await (const output of await app.stream(inputs, config)) {\n",
    "  for (const [key, value] of Object.entries(output)) {\n",
    "    console.log(`Node: '${key}'`);\n",
    "    // Optional: log full state at each node\n",
    "    // console.log(JSON.stringify(value, null, 2));\n",
    "    finalGeneration = value;\n",
    "  }\n",
    "  console.log(\"\\n---\\n\");\n",
    "}\n",
    "\n",
    "// Log the final generation.\n",
    "console.log(JSON.stringify(finalGeneration, null, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/a289cd5f-507f-4f03-9bfc-d1ef6281c080/r)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="rag/langgraph_crag.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrective RAG (CRAG)\n",
    "\n",
    "Self-reflection can enhance RAG, enabling correction of poor quality retrieval\n",
    "or generations.\n",
    "\n",
    "Several recent papers focus on this theme, but implementing the ideas can be\n",
    "tricky.\n",
    "\n",
    "Here we show how to implement ideas from the `Corrective RAG (CRAG)` paper\n",
    "[here](https://arxiv.org/pdf/2401.15884.pdf) using LangGraph.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Set `OPENAI_API_KEY`\n",
    "\n",
    "Set `TAVILY_API_KEY` to enable web search [here](https://app.tavily.com/sign-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load env vars\n",
    "\n",
    "Add a `.env` variable in the root of the repo with your variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "```bash\n",
    "npm install cheerio zod langchain @langchain/community @langchain/openai @langchain/core @langchain/textsplitters @langchain/langgraph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRAG Detail\n",
    "\n",
    "Corrective-RAG (CRAG) is a recent paper that introduces an interesting approach\n",
    "for self-reflective RAG.\n",
    "\n",
    "The framework grades retrieved documents relative to the question:\n",
    "\n",
    "1. Correct documents -\n",
    "\n",
    "- If at least one document exceeds the threshold for relevance, then it proceeds\n",
    "  to generation\n",
    "- Before generation, it performns knowledge refinement\n",
    "- This paritions the document into \"knowledge strips\"\n",
    "- It grades each strip, and filters our irrelevant ones\n",
    "\n",
    "2. Ambiguous or incorrect documents -\n",
    "\n",
    "- If all documents fall below the relevance threshold or if the grader is\n",
    "  unsure, then the framework seeks an additional datasource\n",
    "- It will use web search to supplement retrieval\n",
    "- The diagrams in the paper also suggest that query re-writing is used here\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement some of these ideas from scratch using\n",
    "[LangGraph](https://js.langchain.com/docs/langgraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "Let's index 3 blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
    "import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "\n",
    "const urls = [\n",
    "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "];\n",
    "\n",
    "const docs = await Promise.all(\n",
    "  urls.map((url) => new CheerioWebBaseLoader(url).load()),\n",
    ");\n",
    "const docsList = docs.flat();\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 250,\n",
    "  chunkOverlap: 0,\n",
    "});\n",
    "const docSplits = await textSplitter.splitDocuments(docsList);\n",
    "\n",
    "// Add to vectorDB\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "  docSplits,\n",
    "  new OpenAIEmbeddings(),\n",
    ");\n",
    "const retriever = vectorStore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State\n",
    "\n",
    "We will define a graph.\n",
    "\n",
    "Our state will be an `object`.\n",
    "\n",
    "We can access this from any graph node as `state.key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "import { DocumentInterface } from \"@langchain/core/documents\";\n",
    "\n",
    "// Represents the state of our graph.\n",
    "const GraphState = Annotation.Root({\n",
    "  documents: Annotation<DocumentInterface[]>({\n",
    "    reducer: (x, y) => y ?? x ?? [],\n",
    "  }),\n",
    "  question: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x ?? \"\",\n",
    "  }),\n",
    "  generation: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x,\n",
    "  }),\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes and Edges\n",
    "\n",
    "Each `node` will simply modify the `state`.\n",
    "\n",
    "Each `edge` will choose which `node` to call next.\n",
    "\n",
    "We can make some simplifications from the paper:\n",
    "\n",
    "- Let's skip the knowledge refinement phase as a first pass. This can be added\n",
    "  back as a node, if desired.\n",
    "- If _any_ document is irrelevant, let's opt to supplement retrieval with web\n",
    "  search.\n",
    "- We'll use\n",
    "  [Tavily Search](https://js.langchain.com/docs/integrations/tools/tavily_search)\n",
    "  for web search.\n",
    "- Let's use query re-writing to optimize the query for web search.\n",
    "\n",
    "Here is our graph flow:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { Document } from \"@langchain/core/documents\";\n",
    "import { z } from \"zod\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { pull } from \"langchain/hub\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { formatDocumentsAsString } from \"langchain/util/document\";\n",
    "\n",
    "// Define the LLM once. We'll reuse it throughout the graph.\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "/**\n",
    " * Retrieve documents\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function retrieve(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---RETRIEVE---\");\n",
    "\n",
    "  const documents = await retriever\n",
    "    .withConfig({ runName: \"FetchRelevantDocuments\" })\n",
    "    .invoke(state.question);\n",
    "\n",
    "  return {\n",
    "    documents,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Generate answer\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function generate(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---GENERATE---\");\n",
    "\n",
    "  const prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
    "  // Construct the RAG chain by piping the prompt, model, and output parser\n",
    "  const ragChain = prompt.pipe(model).pipe(new StringOutputParser());\n",
    "\n",
    "  const generation = await ragChain.invoke({\n",
    "    context: formatDocumentsAsString(state.documents),\n",
    "    question: state.question,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    generation,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether the retrieved documents are relevant to the question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function gradeDocuments(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---CHECK RELEVANCE---\");\n",
    "\n",
    "  // pass the name & schema to `withStructuredOutput` which will force the model to call this tool.\n",
    "  const llmWithTool = model.withStructuredOutput(\n",
    "    z\n",
    "      .object({\n",
    "        binaryScore: z\n",
    "          .enum([\"yes\", \"no\"])\n",
    "          .describe(\"Relevance score 'yes' or 'no'\"),\n",
    "      })\n",
    "      .describe(\n",
    "        \"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"\n",
    "      ),\n",
    "    {\n",
    "      name: \"grade\",\n",
    "    }\n",
    "  );\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing relevance of a retrieved document to a user question.\n",
    "  Here is the retrieved document:\n",
    "  \n",
    "  {context}\n",
    "  \n",
    "  Here is the user question: {question}\n",
    "\n",
    "  If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.`\n",
    "  );\n",
    "\n",
    "  // Chain\n",
    "  const chain = prompt.pipe(llmWithTool);\n",
    "\n",
    "  const filteredDocs: Array<DocumentInterface> = [];\n",
    "  for await (const doc of state.documents) {\n",
    "    const grade = await chain.invoke({\n",
    "      context: doc.pageContent,\n",
    "      question: state.question,\n",
    "    });\n",
    "    if (grade.binaryScore === \"yes\") {\n",
    "      console.log(\"---GRADE: DOCUMENT RELEVANT---\");\n",
    "      filteredDocs.push(doc);\n",
    "    } else {\n",
    "      console.log(\"---GRADE: DOCUMENT NOT RELEVANT---\");\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return {\n",
    "    documents: filteredDocs,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Transform the query to produce a better question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function transformQuery(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---TRANSFORM QUERY---\");\n",
    "\n",
    "  // Pull in the prompt\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are generating a question that is well optimized for semantic search retrieval.\n",
    "  Look at the input and try to reason about the underlying sematic intent / meaning.\n",
    "  Here is the initial question:\n",
    "  \\n ------- \\n\n",
    "  {question} \n",
    "  \\n ------- \\n\n",
    "  Formulate an improved question: `\n",
    "  );\n",
    "\n",
    "  // Prompt\n",
    "  const chain = prompt.pipe(model).pipe(new StringOutputParser());\n",
    "  const betterQuestion = await chain.invoke({ question: state.question });\n",
    "\n",
    "  return {\n",
    "    question: betterQuestion,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Web search based on the re-phrased question using Tavily API.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function webSearch(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---WEB SEARCH---\");\n",
    "\n",
    "  const tool = new TavilySearchResults();\n",
    "  const docs = await tool.invoke({ input: state.question });\n",
    "  const webResults = new Document({ pageContent: docs });\n",
    "  const newDocuments = state.documents.concat(webResults);\n",
    "\n",
    "  return {\n",
    "    documents: newDocuments,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether to generate an answer, or re-generate a question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @returns {\"transformQuery\" | \"generate\"} Next node to call\n",
    " */\n",
    "function decideToGenerate(state: typeof GraphState.State) {\n",
    "  console.log(\"---DECIDE TO GENERATE---\");\n",
    "\n",
    "  const filteredDocs = state.documents;\n",
    "  if (filteredDocs.length === 0) {\n",
    "    // All documents have been filtered checkRelevance\n",
    "    // We will re-generate a new query\n",
    "    console.log(\"---DECISION: TRANSFORM QUERY---\");\n",
    "    return \"transformQuery\";\n",
    "  }\n",
    "\n",
    "  // We have relevant documents, so generate answer\n",
    "  console.log(\"---DECISION: GENERATE---\");\n",
    "  return \"generate\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph\n",
    "\n",
    "The just follows the flow we outlined in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const workflow = new StateGraph(GraphState)\n",
    "  // Define the nodes\n",
    "  .addNode(\"retrieve\", retrieve)\n",
    "  .addNode(\"gradeDocuments\", gradeDocuments)\n",
    "  .addNode(\"generate\", generate)\n",
    "  .addNode(\"transformQuery\", transformQuery)\n",
    "  .addNode(\"webSearch\", webSearch);\n",
    "\n",
    "// Build graph\n",
    "workflow.addEdge(START, \"retrieve\");\n",
    "workflow.addEdge(\"retrieve\", \"gradeDocuments\");\n",
    "workflow.addConditionalEdges(\n",
    "  \"gradeDocuments\",\n",
    "  decideToGenerate,\n",
    ");\n",
    "workflow.addEdge(\"transformQuery\", \"webSearch\");\n",
    "workflow.addEdge(\"webSearch\", \"generate\");\n",
    "workflow.addEdge(\"generate\", END);\n",
    "\n",
    "// Compile\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "Node: 'retrieve'\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: GENERATE---\n",
      "Node: 'gradeDocuments'\n",
      "\n",
      "---\n",
      "\n",
      "---GENERATE---\n",
      "Node: 'generate'\n",
      "\n",
      "---\n",
      "\n",
      "{\n",
      "  \"generation\": \"Different types of agent memory include long-term memory, which allows the agent to retain and recall information over extended periods, often using an external vector store for fast retrieval. This enables the agent to remember and utilize vast amounts of information efficiently.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const inputs = {\n",
    "  question: \"Explain how the different types of agent memory work.\",\n",
    "};\n",
    "const config = { recursionLimit: 50 };\n",
    "let finalGeneration;\n",
    "for await (const output of await app.stream(inputs, config)) {\n",
    "  for (const [key, value] of Object.entries(output)) {\n",
    "    console.log(`Node: '${key}'`);\n",
    "    // Optional: log full state at each node\n",
    "    // console.log(JSON.stringify(value, null, 2));\n",
    "    finalGeneration = value;\n",
    "  }\n",
    "  console.log(\"\\n---\\n\");\n",
    "}\n",
    "\n",
    "// Log the final generation.\n",
    "console.log(JSON.stringify(finalGeneration, null, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/d2160835-8610-467f-a91a-ff19a34d1f5b/r)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="rag/langgraph_self_rag.ipynb">
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-RAG\n",
    "\n",
    "Self-reflection can enhance RAG, enabling correction of poor quality retrieval\n",
    "or generations.\n",
    "\n",
    "Several recent papers focus on this theme, but implementing the ideas can be\n",
    "tricky.\n",
    "\n",
    "Here we show how to implement ideas from the `Self RAG` paper\n",
    "[here](https://arxiv.org/abs/2310.11511) using LangGraph.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Set `OPENAI_API_KEY`\n",
    "\n",
    "## Self-RAG Detail\n",
    "\n",
    "Self-RAG is a recent paper that introduces an interesting approach for\n",
    "self-reflective RAG.\n",
    "\n",
    "The framework trains an LLM (e.g., LLaMA2-7b or 13b) to generate tokens that\n",
    "govern the RAG process in a few ways:\n",
    "\n",
    "1. Should I retrieve from retriever, `R` -\n",
    "\n",
    "- Token: `Retrieve`\n",
    "- Input: `x (question)` OR `x (question)`, `y (generation)`\n",
    "- Decides when to retrieve `D` chunks with `R`\n",
    "- Output: `yes, no, continue`\n",
    "\n",
    "2. Are the retrieved passages `D` relevant to the question `x` -\n",
    "\n",
    "- Token: `ISREL`\n",
    "-\n",
    "  - Input: (`x (question)`, `d (chunk)`) for `d` in `D`\n",
    "- `d` provides useful information to solve `x`\n",
    "- Output: `relevant, irrelevant`\n",
    "\n",
    "3. Are the LLM generation from each chunk in `D` is relevant to the chunk\n",
    "   (hallucinations, etc) -\n",
    "\n",
    "- Token: `ISSUP`\n",
    "- Input: `x (question)`, `d (chunk)`, `y (generation)` for `d` in `D`\n",
    "- All of the verification-worthy statements in `y (generation)` are supported by\n",
    "  `d`\n",
    "- Output: `{fully supported, partially supported, no support`\n",
    "\n",
    "4. The LLM generation from each chunk in `D` is a useful response to\n",
    "   `x (question)` -\n",
    "\n",
    "- Token: `ISUSE`\n",
    "- Input: `x (question)`, `y (generation)` for `d` in `D`\n",
    "- `y (generation)` is a useful response to `x (question)`.\n",
    "- Output: `{5, 4, 3, 2, 1}`\n",
    "\n",
    "We can represent this as a graph:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement some of these ideas from scratch using\n",
    "[LangGraph](https://js.langchain.com/docs/langgraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load env vars\n",
    "\n",
    "Add a `.env` variable in the root of the repo folder with your variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "```bash\n",
    "npm install cheerio zod langchain @langchain/community @langchain/openai @langchain/core @langchain/textsplitters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
    "import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "\n",
    "const urls = [\n",
    "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "  \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "];\n",
    "\n",
    "const docs = await Promise.all(\n",
    "  urls.map((url) => new CheerioWebBaseLoader(url).load()),\n",
    ");\n",
    "const docsList = docs.flat();\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 500,\n",
    "  chunkOverlap: 250,\n",
    "});\n",
    "const docSplits = await textSplitter.splitDocuments(docsList);\n",
    "\n",
    "// Add to vectorDB\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "  docSplits,\n",
    "  new OpenAIEmbeddings({ model: \"text-embedding-3-large\" }),\n",
    ");\n",
    "const retriever = vectorStore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State\n",
    "\n",
    "We will define a graph.\n",
    "\n",
    "Our state will be an `object`.\n",
    "\n",
    "We can access this from any graph node as `state.key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "import { type DocumentInterface } from \"@langchain/core/documents\";\n",
    "\n",
    "// Represents the state of our graph.\n",
    "const GraphState = Annotation.Root({\n",
    "  documents: Annotation<DocumentInterface[]>({\n",
    "    reducer: (x, y) => y ?? x ?? [],\n",
    "  }),\n",
    "  question: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x ?? \"\",\n",
    "  }),\n",
    "  generation: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x,\n",
    "    default: () => \"\",\n",
    "  }),\n",
    "  generationVQuestionGrade: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x,\n",
    "  }),\n",
    "  generationVDocumentsGrade: Annotation<string>({\n",
    "    reducer: (x, y) => y ?? x,\n",
    "  }),\n",
    "});"
   ]
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes and Edges\n",
    "\n",
    "Each `node` will simply modify the `state`.\n",
    "\n",
    "Each `edge` will choose which `node` to call next.\n",
    "\n",
    "We can lay out `self-RAG` as a graph.\n",
    "\n",
    "Here is our graph flow:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { pull } from \"langchain/hub\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import type { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { formatDocumentsAsString } from \"langchain/util/document\";\n",
    "\n",
    "// Define the LLM once. We'll reuse it throughout the graph.\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "/**\n",
    " * Retrieve documents\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function retrieve(\n",
    "  state: typeof GraphState.State,\n",
    "  config?: RunnableConfig\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---RETRIEVE---\");\n",
    "\n",
    "  const documents = await retriever\n",
    "    .withConfig({ runName: \"FetchRelevantDocuments\" })\n",
    "    .invoke(state.question, config);\n",
    "\n",
    "  return {\n",
    "    documents,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Generate answer\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function generate(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---GENERATE---\");\n",
    "\n",
    "  // Pull in the prompt\n",
    "  const prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
    "  // Construct the RAG chain by piping the prompt, model, and output parser\n",
    "  const ragChain = prompt.pipe(model).pipe(new StringOutputParser());\n",
    "\n",
    "  const generation = await ragChain.invoke({\n",
    "    context: formatDocumentsAsString(state.documents),\n",
    "    question: state.question,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    generation,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether the retrieved documents are relevant to the question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function gradeDocuments(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---CHECK RELEVANCE---\");\n",
    "\n",
    "  // pass the name & schema to `withStructuredOutput` which will force the model to call this tool.\n",
    "  const llmWithTool = model.withStructuredOutput(\n",
    "    z\n",
    "      .object({\n",
    "        binaryScore: z\n",
    "          .enum([\"yes\", \"no\"])\n",
    "          .describe(\"Relevance score 'yes' or 'no'\"),\n",
    "      })\n",
    "      .describe(\n",
    "        \"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"\n",
    "      ),\n",
    "    {\n",
    "      name: \"grade\",\n",
    "    }\n",
    "  );\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing relevance of a retrieved document to a user question.\n",
    "  Here is the retrieved document:\n",
    "  \n",
    "  {context}\n",
    "  \n",
    "  Here is the user question: {question}\n",
    "\n",
    "  If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.`\n",
    "  );\n",
    "\n",
    "  // Chain\n",
    "  const chain = prompt.pipe(llmWithTool);\n",
    "\n",
    "  const filteredDocs: Array<DocumentInterface> = [];\n",
    "  for await (const doc of state.documents) {\n",
    "    const grade = await chain.invoke({\n",
    "      context: doc.pageContent,\n",
    "      question: state.question,\n",
    "    });\n",
    "    if (grade.binaryScore === \"yes\") {\n",
    "      console.log(\"---GRADE: DOCUMENT RELEVANT---\");\n",
    "      filteredDocs.push(doc);\n",
    "    } else {\n",
    "      console.log(\"---GRADE: DOCUMENT NOT RELEVANT---\");\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return {\n",
    "    documents: filteredDocs,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Transform the query to produce a better question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function transformQuery(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---TRANSFORM QUERY---\");\n",
    "\n",
    "  // Pull in the prompt\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are generating a question that is well optimized for semantic search retrieval.\n",
    "  Look at the input and try to reason about the underlying sematic intent / meaning.\n",
    "  Here is the initial question:\n",
    "  \\n ------- \\n\n",
    "  {question} \n",
    "  \\n ------- \\n\n",
    "  Formulate an improved question: `\n",
    "  );\n",
    "\n",
    "  // Construct the chain\n",
    "  const chain = prompt.pipe(model).pipe(new StringOutputParser());\n",
    "  const betterQuestion = await chain.invoke({ question: state.question });\n",
    "\n",
    "  return {\n",
    "    question: betterQuestion,\n",
    "  };\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether to generate an answer, or re-generate a question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @returns {\"transformQuery\" | \"generate\"} Next node to call\n",
    " */\n",
    "function decideToGenerate(state: typeof GraphState.State) {\n",
    "  console.log(\"---DECIDE TO GENERATE---\");\n",
    "\n",
    "  const filteredDocs = state.documents;\n",
    "  if (filteredDocs.length === 0) {\n",
    "    // All documents have been filtered checkRelevance\n",
    "    // We will re-generate a new query\n",
    "    console.log(\"---DECISION: TRANSFORM QUERY---\");\n",
    "    return \"transformQuery\";\n",
    "  }\n",
    "\n",
    "  // We have relevant documents, so generate answer\n",
    "  console.log(\"---DECISION: GENERATE---\");\n",
    "  return \"generate\";\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether the generation is grounded in the document.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function generateGenerationVDocumentsGrade(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---GENERATE GENERATION vs DOCUMENTS GRADE---\");\n",
    "\n",
    "  const llmWithTool = model.withStructuredOutput(\n",
    "    z\n",
    "      .object({\n",
    "        binaryScore: z\n",
    "          .enum([\"yes\", \"no\"])\n",
    "          .describe(\"Relevance score 'yes' or 'no'\"),\n",
    "      })\n",
    "      .describe(\n",
    "        \"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"\n",
    "      ),\n",
    "    {\n",
    "      name: \"grade\",\n",
    "    }\n",
    "  );\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "  Here are the facts:\n",
    "  \\n ------- \\n\n",
    "  {documents} \n",
    "  \\n ------- \\n\n",
    "  Here is the answer: {generation}\n",
    "  Give a binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by a set of facts.`\n",
    "  );\n",
    "\n",
    "  const chain = prompt.pipe(llmWithTool);\n",
    "\n",
    "  const score = await chain.invoke({\n",
    "    documents: formatDocumentsAsString(state.documents),\n",
    "    generation: state.generation,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    generationVDocumentsGrade: score.binaryScore,\n",
    "  };\n",
    "}\n",
    "\n",
    "function gradeGenerationVDocuments(state: typeof GraphState.State) {\n",
    "  console.log(\"---GRADE GENERATION vs DOCUMENTS---\");\n",
    "\n",
    "  const grade = state.generationVDocumentsGrade;\n",
    "  if (grade === \"yes\") {\n",
    "    console.log(\"---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\");\n",
    "    return \"supported\";\n",
    "  }\n",
    "\n",
    "  console.log(\"---DECISION: NOT SUPPORTED, GENERATE AGAIN---\");\n",
    "  return \"not supported\";\n",
    "}\n",
    "\n",
    "/**\n",
    " * Determines whether the generation addresses the question.\n",
    " *\n",
    " * @param {typeof GraphState.State} state The current state of the graph.\n",
    " * @param {RunnableConfig | undefined} config The configuration object for tracing.\n",
    " * @returns {Promise<Partial<typeof GraphState.State>>} The new state object.\n",
    " */\n",
    "async function generateGenerationVQuestionGrade(\n",
    "  state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log(\"---GENERATE GENERATION vs QUESTION GRADE---\");\n",
    "\n",
    "  const llmWithTool = model.withStructuredOutput(\n",
    "    z\n",
    "      .object({\n",
    "        binaryScore: z\n",
    "          .enum([\"yes\", \"no\"])\n",
    "          .describe(\"Relevance score 'yes' or 'no'\"),\n",
    "      })\n",
    "      .describe(\n",
    "        \"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"\n",
    "      ),\n",
    "    {\n",
    "      name: \"grade\",\n",
    "    }\n",
    "  );\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing whether an answer is useful to resolve a question.\n",
    "  Here is the answer:\n",
    "  \\n ------- \\n\n",
    "  {generation} \n",
    "  \\n ------- \\n\n",
    "  Here is the question: {question}\n",
    "  Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.`\n",
    "  );\n",
    "\n",
    "  const chain = prompt.pipe(llmWithTool);\n",
    "\n",
    "  const score = await chain.invoke({\n",
    "    question: state.question,\n",
    "    generation: state.generation,\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    generationVQuestionGrade: score.binaryScore,\n",
    "  };\n",
    "}\n",
    "\n",
    "function gradeGenerationVQuestion(state: typeof GraphState.State) {\n",
    "  console.log(\"---GRADE GENERATION vs QUESTION---\");\n",
    "\n",
    "  const grade = state.generationVQuestionGrade;\n",
    "  if (grade === \"yes\") {\n",
    "    console.log(\"---DECISION: USEFUL---\");\n",
    "    return \"useful\";\n",
    "  }\n",
    "\n",
    "  console.log(\"---DECISION: NOT USEFUL---\");\n",
    "  return \"not useful\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph\n",
    "\n",
    "The just follows the flow we outlined in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const workflow = new StateGraph(GraphState)\n",
    "  // Define the nodes\n",
    "  .addNode(\"retrieve\", retrieve)\n",
    "  .addNode(\"gradeDocuments\", gradeDocuments)\n",
    "  .addNode(\"generate\", generate)\n",
    "  .addNode(\n",
    "    \"generateGenerationVDocumentsGrade\",\n",
    "    generateGenerationVDocumentsGrade\n",
    "  )\n",
    "  .addNode(\"transformQuery\", transformQuery)\n",
    "  .addNode(\n",
    "    \"generateGenerationVQuestionGrade\",\n",
    "    generateGenerationVQuestionGrade\n",
    "  );\n",
    "\n",
    "// Build graph\n",
    "workflow.addEdge(START, \"retrieve\");\n",
    "workflow.addEdge(\"retrieve\", \"gradeDocuments\");\n",
    "workflow.addConditionalEdges(\"gradeDocuments\", decideToGenerate, {\n",
    "  transformQuery: \"transformQuery\",\n",
    "  generate: \"generate\",\n",
    "});\n",
    "workflow.addEdge(\"transformQuery\", \"retrieve\");\n",
    "workflow.addEdge(\"generate\", \"generateGenerationVDocumentsGrade\");\n",
    "workflow.addConditionalEdges(\n",
    "  \"generateGenerationVDocumentsGrade\",\n",
    "  gradeGenerationVDocuments,\n",
    "  {\n",
    "    supported: \"generateGenerationVQuestionGrade\",\n",
    "    \"not supported\": \"generate\",\n",
    "  }\n",
    ");\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "  \"generateGenerationVQuestionGrade\",\n",
    "  gradeGenerationVQuestion,\n",
    "  {\n",
    "    useful: END,\n",
    "    \"not useful\": \"transformQuery\",\n",
    "  }\n",
    ");\n",
    "\n",
    "// Compile\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "Node: 'retrieve'\n",
      "Retrieved 4 documents.\n",
      "\n",
      "---ITERATION END---\n",
      "\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: GENERATE---\n",
      "Node: 'gradeDocuments'\n",
      "Graded documents. Found 3 relevant document(s).\n",
      "\n",
      "---ITERATION END---\n",
      "\n",
      "---GENERATE---\n",
      "Node: 'generate'\n",
      "{\n",
      "  generation: 'Short-term memory in agents involves in-context learning, which is limited by the finite context window length of the model. Long-term memory allows the agent to retain and recall extensive information over extended periods by using an external vector store and fast retrieval mechanisms. Sensory memory involves learning embedding representations for raw inputs like text and images.'\n",
      "}\n",
      "\n",
      "---ITERATION END---\n",
      "\n",
      "---GENERATE GENERATION vs DOCUMENTS GRADE---\n",
      "---GRADE GENERATION vs DOCUMENTS---\n",
      "---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\n",
      "Node: 'generateGenerationVDocumentsGrade'\n",
      "{ generationVDocumentsGrade: 'yes' }\n",
      "\n",
      "---ITERATION END---\n",
      "\n",
      "---GENERATE GENERATION vs QUESTION GRADE---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: USEFUL---\n",
      "Node: 'generateGenerationVQuestionGrade'\n",
      "{ generationVQuestionGrade: 'yes' }\n",
      "\n",
      "---ITERATION END---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const inputs = {\n",
    "  question: \"Explain how the different types of agent memory work.\",\n",
    "};\n",
    "const config = { recursionLimit: 50 };\n",
    "\n",
    "const prettifyOutput = (output: Record<string, any>) => {\n",
    "  const key = Object.keys(output)[0];\n",
    "  const value = output[key];\n",
    "  console.log(`Node: '${key}'`);\n",
    "  if (key === \"retrieve\" && \"documents\" in value) {\n",
    "    console.log(`Retrieved ${value.documents.length} documents.`);\n",
    "  } else if (key === \"gradeDocuments\" && \"documents\" in value) {\n",
    "    console.log(`Graded documents. Found ${value.documents.length} relevant document(s).`);\n",
    "  } else {\n",
    "    console.dir(value, { depth: null });\n",
    "  }\n",
    "}\n",
    "\n",
    "for await (const output of await app.stream(inputs, config)) {\n",
    "  prettifyOutput(output);\n",
    "  console.log(\"\\n---ITERATION END---\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/cbf3c09a-5104-45f4-bd32-6e992e67f67a/r)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="reflection/reflection.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084acbec",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "In the context of LLM agent building, reflection refers to the process of\n",
    "prompting an LLM to observe its past steps (along with potential observations\n",
    "from tools/the environment) to assess the quality of the chosen actions. This is\n",
    "then used downstream for things like re-planning, search, or evaluation.\n",
    "\n",
    "![Reflection](./img/reflection.png)\n",
    "\n",
    "This notebook demonstrates a very simple form of reflection in LangGraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedf181",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "\n",
    "We will be using a basic agent with a search tool here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a2b8a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load env vars\n",
    "\n",
    "Add a `.env` variable in the root of the repo folder with your variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4724b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import \"dotenv/config\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cceaea8",
   "metadata": {},
   "source": [
    "## Generate\n",
    "\n",
    "For our example, we will create a \"5 paragraph essay\" generator. First, create\n",
    "the generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7326d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatFireworks } from \"@langchain/community/chat_models/fireworks\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are an essay assistant tasked with writing excellent 5-paragraph essays.\n",
    "Generate the best essay possible for the user's request.  \n",
    "If the user provides critique, respond with a revised version of your previous attempts.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"messages\"),\n",
    "]);\n",
    "const llm = new ChatFireworks({\n",
    "  model: \"accounts/fireworks/models/firefunction-v2\",\n",
    "  temperature: 0,\n",
    "  modelKwargs: {\n",
    "    max_tokens: 32768,\n",
    "  },\n",
    "});\n",
    "const essayGenerationChain = prompt.pipe(llm);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c5026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Little\n",
      " Prince, a\n",
      " novella written\n",
      " by Antoine de\n",
      " Saint-Ex\n",
      "upéry in\n",
      " 1943\n",
      ", has been\n",
      " a beloved classic\n",
      " for generations of\n",
      " children and adults\n",
      " alike. Despite\n",
      " being written over\n",
      " 80 years\n",
      " ago,\n",
      " the story\n",
      " remains remarkably\n",
      " relevant to modern\n",
      " childhood. This\n",
      " essay will explore\n",
      " the ways in\n",
      " which The Little\n",
      " Prince continues to\n",
      " resonate with\n",
      " children today,\n",
      " highlighting its timeless\n",
      " themes, rel\n",
      "atable characters,\n",
      " and poignant commentary\n",
      " on the\n",
      " human experience.\n",
      "\n",
      "\n",
      "One of the\n",
      " primary reasons The\n",
      " Little Prince remains\n",
      " relevant is its\n",
      " exploration of universal\n",
      " themes that transcend\n",
      " time and culture\n",
      ". The story\n",
      " delves into\n",
      " complex emotions such\n",
      " as loneliness,\n",
      " friendship, and\n",
      " the importance of\n",
      " human connection.\n",
      " These themes are\n",
      " just as relevant\n",
      " today as\n",
      " they were\n",
      " when the\n",
      " book was first\n",
      " published. Children\n",
      " today face many\n",
      " of the same\n",
      " challenges as the\n",
      " Little Prince,\n",
      " including feeling isolated\n",
      " and struggling to\n",
      " form meaningful relationships\n",
      ". The nov\n",
      "ella's exploration\n",
      " of these themes\n",
      " provides a rel\n",
      "atable and comforting\n",
      " narrative for young\n",
      " readers.\n",
      "\n",
      "The\n",
      " characters in The\n",
      " Little Prince are\n",
      " also remarkably rel\n",
      "atable to modern\n",
      " children. The\n",
      " Little Prince himself\n",
      " is a curious\n",
      " and adventurous young\n",
      " boy who embodies\n",
      " the\n",
      " sense of\n",
      " wonder and\n",
      " curiosity that\n",
      " defines childhood\n",
      ". His\n",
      " relationships with the\n",
      " various characters\n",
      " he encounters\n",
      " on\n",
      " his journey\n",
      ", including\n",
      " the fox and\n",
      " the rose,\n",
      " serve as powerful\n",
      " reminders of the\n",
      " importance of empathy\n",
      ", kindness,\n",
      " and understanding.\n",
      " These characters,\n",
      " along with the\n",
      " Little Prince,\n",
      " provide a diverse\n",
      " and inclusive cast\n",
      " that reflects the\n",
      " complexity of modern\n",
      " childhood.\n",
      "\n",
      "Furthermore\n",
      ", The Little\n",
      " Prince offers a\n",
      " poignant commentary on\n",
      " the human experience\n",
      " that is just\n",
      " as relevant today\n",
      " as it was\n",
      " when the book\n",
      " was first\n",
      " published.\n",
      " The novella\n",
      "'s exploration\n",
      " of the adult\n",
      " world,\n",
      " with its\n",
      " emphasis on material\n",
      " possessions and superficial\n",
      " relationships, serves\n",
      " as a powerful\n",
      " critique of modern\n",
      " society. The\n",
      " Little Prince\n",
      "'s observations\n",
      " on\n",
      " the adult\n",
      " world,\n",
      " including his famous\n",
      " line \"You\n",
      " see, grown\n",
      "-ups never understand\n",
      " anything by themselves\n",
      ", and it\n",
      " is exhausting for\n",
      " children\n",
      " to be\n",
      " always and\n",
      " forever explaining things\n",
      " to them,\"\n",
      " remain a\n",
      " powerful commentary\n",
      " on the challenges\n",
      " of growing up\n",
      " and navigating the\n",
      " complexities of adulthood\n",
      ".\n",
      "\n",
      "In conclusion\n",
      ", The Little\n",
      " Prince remains a\n",
      " remarkably relevant and\n",
      " powerful work of\n",
      " children's literature\n",
      ". Its exploration\n",
      " of universal themes\n",
      ", relatable\n",
      " characters, and\n",
      " poignant commentary on\n",
      " the human experience\n",
      " make it a\n",
      " must-read for\n",
      " children today.\n",
      " As a work\n",
      " of literature,\n",
      " it continues to\n",
      " inspire and comfort\n",
      " young readers,\n",
      " providing a powerful\n",
      " reminder of the\n",
      " importance of empathy\n",
      ", kindness,\n",
      " and understanding\n",
      ". As a\n",
      " cultural artifact,\n",
      " it serves as\n",
      " a powerful commentary\n",
      " on the challenges\n",
      " of growing up\n",
      " and navigating the\n",
      " complexities of modern\n",
      " society.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "let essay = \"\";\n",
    "const request = new HumanMessage({\n",
    "  content:\n",
    "    \"Write an essay on why the little prince is relevant in modern childhood\",\n",
    "});\n",
    "\n",
    "for await (\n",
    "  const chunk of await essayGenerationChain.stream({\n",
    "    messages: [request],\n",
    "  })\n",
    ") {\n",
    "  console.log(chunk.content);\n",
    "  essay += chunk.content;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f84f5",
   "metadata": {},
   "source": [
    "### Reflect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00094238",
   "metadata": {},
   "outputs": [],
   "source": [
    "const reflectionPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a teacher grading an essay submission.\n",
    "Generate critique and recommendations for the user's submission.\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"messages\"),\n",
    "]);\n",
    "const reflect = reflectionPrompt.pipe(llm);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173aa5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The essay\n",
      " provides a\n",
      " good overview of\n",
      " the relevance of\n",
      " The Little Prince\n",
      " in modern childhood\n",
      ". However,\n",
      " there are some\n",
      " areas that need\n",
      " improvement. Firstly\n",
      ", the introduction\n",
      " could be stronger\n",
      ". Instead of\n",
      " simply stating that\n",
      " the book is\n",
      " a classic,\n",
      " try to provide\n",
      " a more nuanced\n",
      " explanation of its\n",
      " enduring popularity.\n",
      " Additionally, the\n",
      " body paragraphs could\n",
      " be more detailed\n",
      " and provide more\n",
      " specific examples from\n",
      " the text to\n",
      " support the arguments\n",
      ".\n",
      "\n",
      "In terms\n",
      " of style,\n",
      " the writing is\n",
      " clear and concise\n",
      ", but could\n",
      " benefit from more\n",
      " varied sentence structures\n",
      " and vocabulary.\n",
      " The use of\n",
      " transitions between paragraphs\n",
      " could also be\n",
      " improved to create\n",
      " a\n",
      " more cohesive\n",
      " flow.\n",
      "\n",
      "\n",
      "One area that\n",
      " could be explored\n",
      " further is the\n",
      " way in which\n",
      " The Little Prince\n",
      " reflects the experiences\n",
      " of modern children\n",
      ". While the\n",
      " essay touches on\n",
      " this, it\n",
      " could be developed\n",
      " more fully to\n",
      " provide\n",
      " a more\n",
      " nuanced understanding\n",
      " of how the\n",
      " book continues to\n",
      " resonate with young\n",
      " readers.\n",
      "\n",
      "Overall\n",
      ", the essay\n",
      " provides a good\n",
      " foundation for exploring\n",
      " the relevance of\n",
      " The Little Prince\n",
      " in modern childhood\n",
      ". With some\n",
      " revisions to address\n",
      " the areas mentioned\n",
      " above, it\n",
      " could be even\n",
      " stronger.\n",
      "\n",
      "Grade\n",
      ": B\n",
      "+\n",
      "\n",
      "Recommendations\n",
      ":\n",
      "\n",
      "* Rev\n",
      "ise the introduction\n",
      " to provide a\n",
      " more nuanced explanation\n",
      " of the book\n",
      "'s enduring popularity\n",
      ".\n",
      "* Provide\n",
      " more specific examples\n",
      " from the text\n",
      " to support the\n",
      " arguments\n",
      " in the\n",
      " body paragraphs\n",
      ".\n",
      "* V\n",
      "ary sentence structures\n",
      " and vocabulary to\n",
      " create a more\n",
      " engaging writing style\n",
      ".\n",
      "* Explore\n",
      " the way in\n",
      " which The Little\n",
      " Prince reflects the\n",
      " experiences of modern\n",
      " children in more\n",
      " detail.\n",
      "*\n",
      " Improve transitions between\n",
      " paragraphs to create\n",
      " a more cohesive\n",
      " flow.\n",
      "\n",
      "Length\n",
      ": The essay\n",
      " is a good\n",
      " length, but\n",
      " could be expanded\n",
      " to provide more\n",
      " detail and examples\n",
      ".\n",
      "\n",
      "Depth:\n",
      " The essay provides\n",
      " a good overview\n",
      " of the relevance\n",
      " of The Little\n",
      " Prince, but\n",
      " could delve deeper\n",
      " into the themes\n",
      " and characters to\n",
      " provide a more\n",
      " nuanced understanding.\n",
      "\n",
      "\n",
      "Style: The\n",
      " writing is clear\n",
      " and concise\n",
      ",\n",
      " but could\n",
      " benefit from\n",
      " more varied sentence\n",
      " structures and vocabulary\n",
      ".\n",
      "\n",
      "Overall\n",
      ", the essay\n",
      " provides a good\n",
      " foundation for exploring\n",
      " the relevance of\n",
      " The Little Prince\n",
      " in modern childhood\n",
      ". With some\n",
      " revisions to address\n",
      " the areas mentioned\n",
      " above, it\n",
      " could be even\n",
      " stronger.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let reflection = \"\";\n",
    "\n",
    "for await (\n",
    "  const chunk of await reflect.stream({\n",
    "    messages: [request, new HumanMessage({ content: essay })],\n",
    "  })\n",
    ") {\n",
    "  console.log(chunk.content);\n",
    "  reflection += chunk.content;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27725f96",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "\n",
    "And... that's all there is too it! You can repeat in a loop for a fixed number\n",
    "of steps, or use an LLM (or other check) to decide when the finished product is\n",
    "good enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa75d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is\n",
      " a revised\n",
      " version of\n",
      " the essay that\n",
      " addresses the areas\n",
      " mentioned above:\n",
      "\n",
      "\n",
      "The Little Prince\n",
      ", a nov\n",
      "ella written by\n",
      " Antoine de Saint\n",
      "-Exup\n",
      "éry in \n",
      "1943,\n",
      " has captivated\n",
      " the hearts of\n",
      " readers of all\n",
      " ages with its\n",
      " poignant and timeless\n",
      " tale of friendship\n",
      ", love,\n",
      " and the human\n",
      " experience. One\n",
      " of the primary\n",
      " reasons for its enduring\n",
      " popularity is its\n",
      " ability to tap\n",
      " into the universal\n",
      " emotions and experiences\n",
      " that define childhood\n",
      ". The story\n",
      "'s exploration of\n",
      " complex themes such\n",
      " as loneliness,\n",
      " friendship, and\n",
      " the importance of\n",
      " human connection reson\n",
      "ates deeply with\n",
      " children today,\n",
      " who face many\n",
      " of the same\n",
      " challenges as the\n",
      " Little Prince.\n",
      "\n",
      "\n",
      "One of the\n",
      " most relatable\n",
      " aspects of\n",
      " The Little\n",
      " Prince is\n",
      " its\n",
      " exploration of\n",
      " the complexities\n",
      " of human relationships\n",
      ". The Little\n",
      " Prince's journey\n",
      " to different planets\n",
      ", where he\n",
      " encounters various strange\n",
      " characters,\n",
      " serves as\n",
      " a powerful metaphor\n",
      " for the challenges\n",
      " of forming meaningful\n",
      " connections with others\n",
      ". For example\n",
      ", his relationship\n",
      " with the fox\n",
      ", who teaches\n",
      " him the importance\n",
      " of human connection\n",
      " and the value\n",
      " of friendship,\n",
      " is a powerful\n",
      " reminder of the\n",
      " importance of empathy\n",
      " and understanding in\n",
      " building strong relationships\n",
      ". This theme\n",
      " is particularly relevant\n",
      " in modern childhood\n",
      ", where children\n",
      " are often encouraged\n",
      " to focus on\n",
      " individual achievement and\n",
      " success, rather\n",
      " than building strong\n",
      " relationships with others\n",
      ".\n",
      "\n",
      "Furthermore,\n",
      " The Little Prince\n",
      " offers a powerful\n",
      " commentary on the\n",
      " adult world,\n",
      " which is just\n",
      " as relevant today\n",
      " as it was\n",
      " when the book\n",
      " was first published\n",
      ". The nov\n",
      "ella's exploration\n",
      " of the superficial\n",
      "ity of adult\n",
      " relationships, where\n",
      " people are often\n",
      " more concerned with\n",
      " material possessions and\n",
      " appearances than with\n",
      " genuine human connection\n",
      ", serves as\n",
      " a powerful critique\n",
      " of modern society\n",
      ". The Little\n",
      " Prince's observations\n",
      " on the adult\n",
      " world, including\n",
      " his famous line\n",
      " \"You see\n",
      ", grown-ups\n",
      " never understand anything\n",
      " by themselves,\n",
      " and it is\n",
      " exhausting for children\n",
      " to be always\n",
      " and forever explaining\n",
      " things to them\n",
      ",\" remain a\n",
      " powerful commentary on\n",
      " the challenges of\n",
      " growing up and\n",
      " navigating the complexities\n",
      " of adulthood.\n",
      "\n",
      "\n",
      "In addition to\n",
      " its exploration of\n",
      " universal themes and\n",
      " relatable characters\n",
      ", The Little\n",
      " Prince also reflects\n",
      " the experiences of\n",
      " modern children in\n",
      " a number of\n",
      " ways. For\n",
      " example, the\n",
      " Little Prince's\n",
      " sense of wonder\n",
      " and curiosity,\n",
      " as well as\n",
      " his desire for\n",
      " adventure and exploration\n",
      ", are all\n",
      " qualities that are\n",
      " highly valued in\n",
      " modern childhood.\n",
      " Furthermore, the\n",
      " novella's\n",
      " exploration of the\n",
      " importance of human\n",
      " connection and empathy\n",
      " is particularly relevant\n",
      " in modern childhood\n",
      ", where children\n",
      " are often encouraged\n",
      " to\n",
      " focus on\n",
      " individual achievement\n",
      " and success,\n",
      " rather than building strong\n",
      " relationships with\n",
      " others.\n",
      "\n",
      "In\n",
      " conclusion,\n",
      " The Little\n",
      " Prince remains a\n",
      " remarkably relevant and\n",
      " powerful work of\n",
      " children's literature\n",
      ". Its exploration\n",
      " of universal themes\n",
      ", relatable\n",
      " characters, and\n",
      " poignant commentary on\n",
      " the human experience\n",
      " make it a\n",
      " must-read for\n",
      " children today.\n",
      " As a work\n",
      " of literature\n",
      ", it\n",
      " continues to inspire\n",
      " and comfort young\n",
      " readers, providing\n",
      " a powerful reminder\n",
      " of the importance\n",
      " of empathy,\n",
      " kindness, and\n",
      " understanding. As\n",
      " a cultural artifact\n",
      ", it serves\n",
      " as a powerful\n",
      " commentary on the\n",
      " challenges of growing\n",
      " up and navigating\n",
      " the complexities of\n",
      " modern society.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let stream = await essayGenerationChain.stream({\n",
    "  messages: [\n",
    "    request,\n",
    "    new AIMessage({ content: essay }),\n",
    "    new HumanMessage({ content: reflection }),\n",
    "  ],\n",
    "});\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk.content);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923d46a",
   "metadata": {},
   "source": [
    "## Define graph\n",
    "\n",
    "Now that we've shown each step in isolation, we can wire it up in a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ce4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, MemorySaver, StateGraph, START, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the top-level State interface\n",
    "const State = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "  })\n",
    "})\n",
    "\n",
    "const generationNode = async (state: typeof State.State) => {\n",
    "  const { messages } = state;\n",
    "  return {\n",
    "    messages: [await essayGenerationChain.invoke({ messages })],\n",
    "  };\n",
    "};\n",
    "\n",
    "const reflectionNode = async (state: typeof State.State) => {\n",
    "  const { messages } = state;\n",
    "  // Other messages we need to adjust\n",
    "  const clsMap: { [key: string]: new (content: string) => BaseMessage } = {\n",
    "    ai: HumanMessage,\n",
    "    human: AIMessage,\n",
    "  };\n",
    "  // First message is the original user request. We hold it the same for all nodes\n",
    "  const translated = [\n",
    "    messages[0],\n",
    "    ...messages\n",
    "      .slice(1)\n",
    "      .map((msg) => new clsMap[msg._getType()](msg.content.toString())),\n",
    "  ];\n",
    "  const res = await reflect.invoke({ messages: translated });\n",
    "  // We treat the output of this as human feedback for the generator\n",
    "  return {\n",
    "    messages: [new HumanMessage({ content: res.content })],\n",
    "  };\n",
    "};\n",
    "\n",
    "// Define the graph\n",
    "const workflow = new StateGraph(State)\n",
    "  .addNode(\"generate\", generationNode)\n",
    "  .addNode(\"reflect\", reflectionNode)\n",
    "  .addEdge(START, \"generate\");\n",
    "\n",
    "const shouldContinue = (state: typeof State.State) => {\n",
    "  const { messages } = state;\n",
    "  if (messages.length > 6) {\n",
    "    // End state after 3 iterations\n",
    "    return END;\n",
    "  }\n",
    "  return \"reflect\";\n",
    "};\n",
    "\n",
    "workflow\n",
    "  .addConditionalEdges(\"generate\", shouldContinue)\n",
    "  .addEdge(\"reflect\", \"generate\");\n",
    "\n",
    "const app = workflow.compile({ checkpointer: new MemorySaver() });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b8d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: generate\n",
      "\n",
      "------\n",
      "\n",
      "Event: reflect\n",
      "\n",
      "------\n",
      "\n",
      "Event: generate\n",
      "\n",
      "------\n",
      "\n",
      "Event: reflect\n",
      "\n",
      "------\n",
      "\n",
      "Event: generate\n",
      "\n",
      "------\n",
      "\n",
      "Event: reflect\n",
      "\n",
      "------\n",
      "\n",
      "Event: generate\n",
      "\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const checkpointConfig = { configurable: { thread_id: \"my-thread\" } };\n",
    "\n",
    "stream = await app.stream(\n",
    "  {\n",
    "    messages: [\n",
    "      new HumanMessage({\n",
    "        content:\n",
    "          \"Generate an essay on the topicality of The Little Prince and its message in modern life\",\n",
    "      }),\n",
    "    ]\n",
    "  },\n",
    "  checkpointConfig,\n",
    ");\n",
    "\n",
    "for await (const event of stream) {\n",
    "  for (const [key, _value] of Object.entries(event)) {\n",
    "    console.log(`Event: ${key}`);\n",
    "    // Uncomment to see the result of each step.\n",
    "    // console.log(value.map((msg) => msg.content).join(\"\\n\"));\n",
    "    console.log(\"\\n------\\n\");\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0c2ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate an essay on the topicality of The Little Prince and its message in modern life\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "The Little Prince, a novella written by Antoine de Saint-Exupéry in 1943, has remained a timeless classic, captivating readers of all ages with its poignant and thought-provoking themes. Despite being written over seven decades ago, the story's message continues to resonate with modern society, making it a topical and relevant work of literature.\n",
      "\n",
      "One of the primary reasons for The Little Prince's enduring popularity is its exploration of the human condition. The novella delves into the complexities of adult relationships, highlighting the superficiality and materialism that often characterize them. The Little Prince's encounters with various strange characters on different planets serve as a commentary on the flaws of modern society, where people often prioritize wealth, power, and status over genuine human connections. This critique of modern society remains pertinent today, as people continue to struggle with the pressures of social media, consumerism, and the pursuit of material success.\n",
      "\n",
      "Furthermore, The Little Prince's emphasis on the importance of human relationships and emotional connections is a message that resonates deeply with modern audiences. In an era where technology has made it easier to connect with others, yet simultaneously created a sense of isolation and disconnection, the novella's themes of love, friendship, and empathy are more relevant than ever. The story encourages readers to reevaluate their priorities and focus on building meaningful relationships, rather than getting caught up in the superficialities of modern life.\n",
      "\n",
      "The Little Prince's exploration of the environment and our responsibility towards it is another aspect of the novella that remains topical today. The character's concern for the well-being of his own planet and his desire to protect it from harm serves as a powerful metaphor for the environmental crises facing our world. As we grapple with the consequences of climate change, deforestation, and pollution, the novella's message about the importance of preserving our planet's natural beauty and resources is more urgent than ever.\n",
      "\n",
      "In conclusion, The Little Prince's themes of human relationships, emotional connections, and environmental responsibility continue to resonate with modern audiences, making it a topical and relevant work of literature. As we navigate the complexities of modern life, the novella's message serves as a powerful reminder of the importance of prioritizing what truly matters: love, friendship, and the well-being of our planet.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "The essay provides a good overview of the topicality of The Little Prince and its message in modern life. However, there are some areas that need improvement.\n",
      "\n",
      "Firstly, the introduction could be stronger. Instead of simply stating that the novella is a timeless classic, the writer could provide more context about its enduring popularity and why it remains relevant today.\n",
      "\n",
      "Secondly, the body paragraphs could be more detailed and nuanced. For example, the writer could provide more examples from the novella to support their arguments about the human condition, relationships, and the environment. Additionally, the writer could explore the implications of these themes in more depth, such as how they relate to contemporary issues like social media, consumerism, and climate change.\n",
      "\n",
      "Thirdly, the conclusion could be more concise and impactful. Instead of simply restating the main points, the writer could summarize the key takeaways and provide a final thought or call to action.\n",
      "\n",
      "In terms of style, the writing is clear and concise, but could benefit from more varied sentence structures and vocabulary. Additionally, the writer could use more transitions to connect the different paragraphs and ideas.\n",
      "\n",
      "Overall, the essay provides a good foundation for exploring the topicality of The Little Prince, but could benefit from more depth, nuance, and attention to style.\n",
      "\n",
      "Grade: B+\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "* Provide more context and background information in the introduction to set up the essay.\n",
      "* Use more specific examples and details from the novella to support arguments in the body paragraphs.\n",
      "* Explore the implications of the themes in more depth and relate them to contemporary issues.\n",
      "* Use more varied sentence structures and vocabulary to improve style.\n",
      "* Use transitions to connect the different paragraphs and ideas.\n",
      "* Summarize key takeaways and provide a final thought or call to action in the conclusion.\n",
      "\n",
      "Length: 500-750 words\n",
      "\n",
      "Depth: 7/10\n",
      "\n",
      "Style: 6/10\n",
      "\n",
      "Overall, the essay provides a good foundation for exploring the topicality of The Little Prince, but could benefit from more depth, nuance, and attention to style.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "Here is a revised version of the essay that addresses the critique provided:\n",
      "\n",
      "The Little Prince, a novella written by Antoine de Saint-Exupéry in 1943, has captivated readers of all ages with its poignant and thought-provoking themes. Despite being written over seven decades ago, the story's message continues to resonate with modern society, making it a timeless classic that remains relevant today. One of the primary reasons for its enduring popularity is its ability to tap into the universal human experiences of love, friendship, and the search for meaning. As a result, the novella has become a cultural touchstone, with its themes and characters continuing to inspire and influence contemporary art, literature, and popular culture.\n",
      "\n",
      "One of the most striking aspects of The Little Prince is its exploration of the human condition. The novella delves into the complexities of adult relationships, highlighting the superficiality and materialism that often characterize them. For example, the character of the businessman, who is so consumed by his own importance that he fails to see the beauty of the stars, serves as a powerful commentary on the flaws of modern society. Similarly, the character of the king, who is so obsessed with his own power and authority that he neglects the needs of his subjects, highlights the dangers of unchecked ambition and the importance of empathy and compassion. These critiques of modern society remain pertinent today, as people continue to struggle with the pressures of social media, consumerism, and the pursuit of material success.\n",
      "\n",
      "Furthermore, The Little Prince's emphasis on the importance of human relationships and emotional connections is a message that resonates deeply with modern audiences. In an era where technology has made it easier to connect with others, yet simultaneously created a sense of isolation and disconnection, the novella's themes of love, friendship, and empathy are more relevant than ever. For example, the Little Prince's relationship with the rose, which is characterized by a deep sense of love and responsibility, serves as a powerful metaphor for the importance of nurturing and caring for others. Similarly, the character of the fox, who teaches the Little Prince about the importance of human connection and the value of relationships, highlights the need for empathy and understanding in our interactions with others.\n",
      "\n",
      "The Little Prince's exploration of the environment and our responsibility towards it is another aspect of the novella that remains topical today. The character's concern for the well-being of his own planet and his desire to protect it from harm serves as a powerful metaphor for the environmental crises facing our world. As we grapple with the consequences of climate change, deforestation, and pollution, the novella's message about the importance of preserving our planet's natural beauty and resources is more urgent than ever. For example, the character of the lamplighter, who is so consumed by his own routine that he fails to see the beauty of the stars, serves as a powerful commentary on the dangers of complacency and the importance of taking action to protect our planet.\n",
      "\n",
      "In conclusion, The Little Prince's themes of human relationships, emotional connections, and environmental responsibility continue to resonate with modern audiences, making it a timeless classic that remains relevant today. As we navigate the complexities of modern life, the novella's message serves as a powerful reminder of the importance of prioritizing what truly matters: love, friendship, and the well-being of our planet. Ultimately, The Little Prince encourages us to reevaluate our priorities and focus on building meaningful relationships, rather than getting caught up in the superficialities of modern life. By doing so, we can create a more compassionate, empathetic, and sustainable world for ourselves and for future generations.\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "The revised essay provides a more detailed and nuanced exploration of the topicality of The Little Prince and its message in modern life. The writer has addressed the critique provided by adding more specific examples and details from the novella to support their arguments, and by exploring the implications of the themes in more depth.\n",
      "\n",
      "One of the strengths of the revised essay is its ability to provide a more detailed and nuanced exploration of the human condition. The writer has done a good job of highlighting the complexities of adult relationships and the dangers of superficiality and materialism. The use of specific examples from the novella, such as the character of the businessman and the king, adds depth and nuance to the argument.\n",
      "\n",
      "The revised essay also does a good job of exploring the importance of human relationships and emotional connections in modern life. The writer has provided more specific examples from the novella, such as the Little Prince's relationship with the rose and the character of the fox, to support their argument. The use of these examples adds depth and nuance to the argument and helps to make it more relatable to modern audiences.\n",
      "\n",
      "The revised essay also does a good job of exploring the environmental themes in the novella and their relevance to modern life. The writer has provided more specific examples from the novella, such as the character of the lamplighter, to support their argument. The use of these examples adds depth and nuance to the argument and helps to make it more relatable to modern audiences.\n",
      "\n",
      "One area for improvement is the conclusion. While the writer has done a good job of summarizing the main points, the conclusion could be more concise and impactful. The writer could consider ending with a more thought-provoking question or a call to action to leave the reader with a lasting impression.\n",
      "\n",
      "Overall, the revised essay provides a more detailed and nuanced exploration of the topicality of The Little Prince and its message in modern life. The writer has done a good job of addressing the critique provided and has added more depth and nuance to the argument.\n",
      "\n",
      "Grade: A-\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "* Consider ending the conclusion with a more thought-provoking question or a call to action to leave the reader with a lasting impression.\n",
      "* Use more varied sentence structures and vocabulary to improve style.\n",
      "* Consider adding more transitions to connect the different paragraphs and ideas.\n",
      "\n",
      "Length: 750-1000 words\n",
      "\n",
      "Depth: 9/10\n",
      "\n",
      "Style: 8/10\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "Here is a revised version of the essay that addresses the critique provided:\n",
      "\n",
      "The Little Prince, a novella written by Antoine de Saint-Exupéry in 1943, has captivated readers of all ages with its poignant and thought-provoking themes. Despite being written over seven decades ago, the story's message continues to resonate with modern society, making it a timeless classic that remains relevant today. One of the primary reasons for its enduring popularity is its ability to tap into the universal human experiences of love, friendship, and the search for meaning. As a result, the novella has become a cultural touchstone, with its themes and characters continuing to inspire and influence contemporary art, literature, and popular culture.\n",
      "\n",
      "One of the most striking aspects of The Little Prince is its exploration of the human condition. The novella delves into the complexities of adult relationships, highlighting the superficiality and materialism that often characterize them. For example, the character of the businessman, who is so consumed by his own importance that he fails to see the beauty of the stars, serves as a powerful commentary on the flaws of modern society. Similarly, the character of the king, who is so obsessed with his own power and authority that he neglects the needs of his subjects, highlights the dangers of unchecked ambition and the importance of empathy and compassion. These critiques of modern society remain pertinent today, as people continue to struggle with the pressures of social media, consumerism, and the pursuit of material success.\n",
      "\n",
      "Furthermore, The Little Prince's emphasis on the importance of human relationships and emotional connections is a message that resonates deeply with modern audiences. In an era where technology has made it easier to connect with others, yet simultaneously created a sense of isolation and disconnection, the novella's themes of love, friendship, and empathy are more relevant than ever. For example, the Little Prince's relationship with the rose, which is characterized by a deep sense of love and responsibility, serves as a powerful metaphor for the importance of nurturing and caring for others. Similarly, the character of the fox, who teaches the Little Prince about the importance of human connection and the value of relationships, highlights the need for empathy and understanding in our interactions with others.\n",
      "\n",
      "The Little Prince's exploration of the environment and our responsibility towards it is another aspect of the novella that remains topical today. The character's concern for the well-being of his own planet and his desire to protect it from harm serves as a powerful metaphor for the environmental crises facing our world. As we grapple with the consequences of climate change, deforestation, and pollution, the novella's message about the importance of preserving our planet's natural beauty and resources is more urgent than ever. For example, the character of the lamplighter, who is so consumed by his own routine that he fails to see the beauty of the stars, serves as a powerful commentary on the dangers of complacency and the importance of taking action to protect our planet.\n",
      "\n",
      "In conclusion, The Little Prince's themes of human relationships, emotional connections, and environmental responsibility continue to resonate with modern audiences, making it a timeless classic that remains relevant today. As we navigate the complexities of modern life, the novella's message serves as a powerful reminder of the importance of prioritizing what truly matters: love, friendship, and the well-being of our planet. Ultimately, The Little Prince encourages us to reevaluate our priorities and focus on building meaningful relationships, rather than getting caught up in the superficialities of modern life. By doing so, we can create a more compassionate, empathetic, and sustainable world for ourselves and for future generations. As we look to the future, we would do well to remember the Little Prince's wise words: \"You become responsible, forever, for what you have tamed.\"\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "The revised essay provides a more detailed and nuanced exploration of the topicality of The Little Prince and its message in modern life. The writer has addressed the critique provided by adding more specific examples and details from the novella to support their arguments, and by exploring the implications of the themes in more depth.\n",
      "\n",
      "One of the strengths of the revised essay is its ability to provide a more detailed and nuanced exploration of the human condition. The writer has done a good job of highlighting the complexities of adult relationships and the dangers of superficiality and materialism. The use of specific examples from the novella, such as the character of the businessman and the king, adds depth and nuance to the argument.\n",
      "\n",
      "The revised essay also does a good job of exploring the importance of human relationships and emotional connections in modern life. The writer has provided more specific examples from the novella, such as the Little Prince's relationship with the rose and the character of the fox, to support their argument. The use of these examples adds depth and nuance to the argument and helps to make it more relatable to modern audiences.\n",
      "\n",
      "The revised essay also does a good job of exploring the environmental themes in the novella and their relevance to modern life. The writer has provided more specific examples from the novella, such as the character of the lamplighter, to support their argument. The use of these examples adds depth and nuance to the argument and helps to make it more relatable to modern audiences.\n",
      "\n",
      "The conclusion is also well-written and effectively summarizes the main points of the essay. The use of the Little Prince's quote at the end adds a nice touch and helps to drive home the importance of prioritizing what truly matters in life.\n",
      "\n",
      "Overall, the revised essay provides a more detailed and nuanced exploration of the topicality of The Little Prince and its message in modern life. The writer has done a good job of addressing the critique provided and has added more depth and nuance to the argument.\n",
      "\n",
      "Grade: A\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "* None. The essay is well-written and effectively explores the topicality of The Little Prince and its message in modern life.\n",
      "\n",
      "Length: 750-1000 words\n",
      "\n",
      "Depth: 9/10\n",
      "\n",
      "Style: 9/10\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "I'm glad to hear that the revised essay meets your expectations. I believe that the essay provides a thorough and nuanced exploration of the topicality of The Little Prince and its message in modern life. The use of specific examples from the novella adds depth and nuance to the argument, and the conclusion effectively summarizes the main points of the essay.\n",
      "\n",
      "I'm also pleased to hear that the essay is well-written and effectively explores the themes of the human condition, human relationships, and environmental responsibility. The use of the Little Prince's quote at the end adds a nice touch and helps to drive home the importance of prioritizing what truly matters in life.\n",
      "\n",
      "Overall, I'm proud of the work that I've done on this essay, and I'm glad to hear that it meets your expectations. If you have any other requests or need further assistance, please don't hesitate to ask.\n",
      "\n",
      "Thank you for your feedback and guidance throughout this process. I appreciate your input and look forward to continuing to work with you in the future.\n"
     ]
    }
   ],
   "source": [
    "const snapshot = await app.getState(checkpointConfig);\n",
    "console.log(\n",
    "  snapshot.values.messages\n",
    "    .map((msg: BaseMessage) => msg.content)\n",
    "    .join(\"\\n\\n\\n------------------\\n\\n\\n\"),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b083da",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/9bf804f7-1df8-46ef-9b40-23b3d3c97756/r)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="rewoo/rewoo.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1523e3ff",
   "metadata": {},
   "source": [
    "# Reasoning without Observation\n",
    "\n",
    "In [ReWOO](https://arxiv.org/abs/2305.18323), Xu, et. al, propose an agent that\n",
    "combines a multi-step planner and variable substitution for effective tool use.\n",
    "It was designed to improve on the ReACT-style agent architecture in the\n",
    "following ways:\n",
    "\n",
    "1. Reduce token consumption and execution time by generating the full chain of\n",
    "   tools used in a single pass. (_ReACT-style agent architecture requires many\n",
    "   LLM calls with redundant prefixes (since the system prompt and previous steps\n",
    "   are provided to the LLM for each reasoning step_)\n",
    "2. Simplify the fine-tuning process. Since the planning data doesn't depend on\n",
    "   the outputs of the tool, models can be fine-tuned without actually invoking\n",
    "   the tools (in theory).\n",
    "\n",
    "The following diagram outlines ReWOO's overall computation graph:\n",
    "\n",
    "![ReWoo Diagram](./img/rewoo.png)\n",
    "\n",
    "ReWOO is made of 3 modules:\n",
    "\n",
    "1. 🧠**Planner**: Generate the plan in the following format:\n",
    "\n",
    "```text\n",
    "Plan: <reasoning>\n",
    "#E1 = Tool[argument for tool]\n",
    "Plan: <reasoning>\n",
    "#E2 = Tool[argument for tool with #E1 variable substitution]\n",
    "...\n",
    "```\n",
    "\n",
    "3. **Worker**: executes the tool with the provided arguments.\n",
    "4. 🧠**Solver**: generates the answer for the initial task based on the tool\n",
    "   observations.\n",
    "\n",
    "The modules with a 🧠 emoji depend on an LLM call. Notice that we avoid\n",
    "redundant calls to the planner LLM by using variable substitution.\n",
    "\n",
    "In this example, each module is represented by a LangGraph node. The end result\n",
    "will leave a trace that looks\n",
    "[like this one](https://smith.langchain.com/public/39dbdcf8-fbcc-4479-8e28-15377ca5e653/r).\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dc2f7",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "For this example, we will provide the agent with a Tavily search engine tool.\n",
    "You can get an API key [here](https://app.tavily.com/sign-in) or replace with a\n",
    "free tool option (e.g.,\n",
    "[duck duck go search](https://python.langchain.com/docs/integrations/tools/ddg)).\n",
    "\n",
    "For this notebook, you should add a `.env` file at the root of the repo with\n",
    "`TAVILY_API_KEY`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4184f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import \"dotenv/config\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73082b",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "```bash\n",
    "npm install langchain @langchain/community @langchain/openai @langchain/core\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e1e4a",
   "metadata": {},
   "source": [
    "**Graph State**: In LangGraph, every node updates a shared graph state. The\n",
    "state is the input to any node whenever it is invoked.\n",
    "\n",
    "Below, we will define a state object to contain the task, plan, steps, and other\n",
    "variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ff6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const GraphState = Annotation.Root({\n",
    "  task: Annotation<string>({\n",
    "    reducer: (x, y) => (y ?? x),\n",
    "    default: () => \"\",\n",
    "  }),\n",
    "  planString: Annotation<string>({\n",
    "    reducer: (x, y) => (y ?? x),\n",
    "    default: () => \"\",\n",
    "  }),\n",
    "  steps: Annotation<string[][]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "    default: () => [],\n",
    "  }),\n",
    "  results: Annotation<Record<string, any>>({\n",
    "    reducer: (x, y) => ({ ...x, ...y }),\n",
    "    default: () => ({}),\n",
    "  }),\n",
    "  result: Annotation<string>({\n",
    "    reducer: (x, y) => (y ?? x),\n",
    "    default: () => \"\",\n",
    "  }),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546316c",
   "metadata": {},
   "source": [
    "## 1. Planner\n",
    "\n",
    "The planner prompts an LLM to generate a plan in the form of a task list. The\n",
    "arguments to each task are strings that may contain special variables\n",
    "(`#E{{0-9}}+`) that are used for variable substitution from other task results.\n",
    "\n",
    "![ReWOO workflow](./img/rewoo-paper-workflow.png)\n",
    "\n",
    "Our example agent will have two tools:\n",
    "\n",
    "1. Google - a search engine (in this case Tavily)\n",
    "2. LLM - an LLM call to reason about previous outputs.\n",
    "\n",
    "The LLM tool receives less of the prompt context and so can be more\n",
    "token-efficient than the ReACT paradigm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9cc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o\",\n",
    "  temperature: 0,\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7bd03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9z88bDgCFkpWbYitlBSkuEaUU0YA2\",\n",
      "  \"content\": \"Plan: Identify the winner of the 2023 Australian Open.\\n#E1 = Google[\\\"winner of the 2023 Australian Open\\\"]\\n\\nPlan: Find the hometown of the winner identified in #E1.\\n#E2 = Google[\\\"hometown of #E1\\\"]\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 55,\n",
      "      \"promptTokens\": 438,\n",
      "      \"totalTokens\": 493\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_3aa7262c27\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 438,\n",
      "    \"output_tokens\": 55,\n",
      "    \"total_tokens\": 493\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const template =\n",
    "  `For the following task, make plans that can solve the problem step by step. For each plan, indicate\n",
    "which external tool together with tool input to retrieve evidence. You can store the evidence into a \n",
    "variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
    "\n",
    "Tools can be one of the following:\n",
    "(1) Google[input]: Worker that searches results from Google. Useful when you need to find short\n",
    "and succinct answers about a specific topic. The input should be a search query.\n",
    "(2) LLM[input]: A pre-trained LLM like yourself. Useful when you need to act with general \n",
    "world knowledge and common sense. Prioritize it when you are confident in solving the problem\n",
    "yourself. Input can be any instruction.\n",
    "\n",
    "For example,\n",
    "Task: Thomas, Toby, and Rebecca worked a total of 157 hours in one week. Thomas worked x \n",
    "hours. Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours \n",
    "less than Toby. How many hours did Rebecca work? \n",
    "Plan: Given Thomas worked x hours, translate the problem into algebraic expressions and solve with Wolfram Alpha.\n",
    "#E1 = WolframAlpha[Solve x + (2x - 10) + ((2x - 10) - 8) = 157]\n",
    "Plan: Find out the number of hours Thomas worked.\n",
    "#E2 = LLM[What is x, given #E1]\n",
    "Plan: Calculate the number of hours Rebecca worked.\n",
    "#E3 = Calculator[(2 * #E2 - 10) - 8]\n",
    "\n",
    "Important!\n",
    "Variables/results MUST be referenced using the # symbol!\n",
    "The plan will be executed as a program, so no coreference resolution apart from naive variable replacement is allowed.\n",
    "The ONLY way for steps to share context is by including #E<step> within the arguments of the tool.\n",
    "\n",
    "Begin! \n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\n",
    "Task: {task}`;\n",
    "\n",
    "const promptTemplate = ChatPromptTemplate.fromMessages([[\"human\", template]]);\n",
    "\n",
    "const planner = promptTemplate.pipe(model);\n",
    "\n",
    "const task = \"what is the hometown of the winner of the 2023 australian open?\";\n",
    "await planner.invoke({ task });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4700cc",
   "metadata": {},
   "source": [
    "#### Planner Node\n",
    "\n",
    "To connect the planner to our graph, we will create a `getPlan` node that\n",
    "accepts the `ReWOO` state and returns with a state update for the `steps` and\n",
    "`planString` fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a058cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "\n",
    "const regexPattern = new RegExp(\n",
    "  \"Plan\\\\s*\\\\d*:\\\\s*([^#]+)\\\\s*(#E\\\\d+)\\\\s*=\\\\s*(\\\\w+)\\\\s*\\\\[([^\\\\]]+)\\\\]\",\n",
    "  \"g\",\n",
    ");\n",
    "\n",
    "async function getPlan(state: typeof GraphState.State, config?: RunnableConfig) {\n",
    "  console.log(\"---GET PLAN---\");\n",
    "  const task = state.task;\n",
    "  const result = await planner.invoke({ task }, config);\n",
    "  // Find all matches in the sample text.\n",
    "  const matches = result.content.toString().matchAll(regexPattern);\n",
    "  let steps: string[][] = [];\n",
    "  for (const match of matches) {\n",
    "    const item = [match[1], match[2], match[3], match[4], match[0]];\n",
    "    if (item.some((i) => i === undefined)) {\n",
    "      throw new Error(\"Invalid match\");\n",
    "    }\n",
    "    steps.push(item as string[]);\n",
    "  }\n",
    "  return {\n",
    "    steps,\n",
    "    planString: result.content.toString(),\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b18588",
   "metadata": {},
   "source": [
    "## 2. Executor\n",
    "\n",
    "The executor receives the plan and executes the tools in sequence.\n",
    "\n",
    "Below, instantiate the search engine and define the tools execution node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a33c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "const search = new TavilySearchResults();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d49a2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "const _getCurrentTask = (state: typeof GraphState.State) => {\n",
    "  console.log(\"_getCurrentTask\", state);\n",
    "  if (!state.results) {\n",
    "    return 1;\n",
    "  }\n",
    "  if (Object.entries(state.results).length === state.steps.length) {\n",
    "    return null;\n",
    "  }\n",
    "  return Object.entries(state.results).length + 1;\n",
    "};\n",
    "\n",
    "const _parseResult = (input: unknown) => {\n",
    "  if (typeof input === \"string\") {\n",
    "    const parsedInput = JSON.parse(input);\n",
    "    if (Array.isArray(parsedInput) && \"content\" in parsedInput[0]) {\n",
    "      // This means it is a tool result.\n",
    "      return parsedInput.map(({ content }) => content).join(\"\\n\");\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if (input && typeof input === \"object\" && \"content\" in input) {\n",
    "    // If it's not a tool, we know it's an LLM result.\n",
    "    const { content } = input;\n",
    "    return content;\n",
    "  }\n",
    "  throw new Error(\"Invalid input received\");\n",
    "};\n",
    "\n",
    "async function toolExecution(state: typeof GraphState.State, config?: RunnableConfig) {\n",
    "  console.log(\"---EXECUTE TOOL---\");\n",
    "  const _step = _getCurrentTask(state);\n",
    "  if (_step === null) {\n",
    "    throw new Error(\"No current task found\");\n",
    "  }\n",
    "  const [_, stepName, tool, toolInputTemplate] = state.steps[_step - 1];\n",
    "  let toolInput = toolInputTemplate;\n",
    "  const _results = state.results || {};\n",
    "  for (const [k, v] of Object.entries(_results)) {\n",
    "    toolInput = toolInput.replace(k, v);\n",
    "  }\n",
    "  let result;\n",
    "  if (tool === \"Google\") {\n",
    "    result = await search.invoke(toolInput, config);\n",
    "  } else if (tool === \"LLM\") {\n",
    "    result = await model.invoke(toolInput, config);\n",
    "  } else {\n",
    "    throw new Error(\"Invalid tool specified\");\n",
    "  }\n",
    "  _results[stepName] = JSON.stringify(_parseResult(result), null, 2);\n",
    "  return { results: _results };\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995406a",
   "metadata": {},
   "source": [
    "## 3. Solver\n",
    "\n",
    "The solver receives the full plan and generates the final response based on the\n",
    "responses of the tool calls from the worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea68d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "const solvePrompt = ChatPromptTemplate.fromTemplate(\n",
    "  `Solve the following task or problem. To solve the problem, we have made step-by-step Plan and\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might\n",
    "contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. Respond with the answer\n",
    "directly with no extra words.\n",
    "\n",
    "Task: {task}\n",
    "Response:`,\n",
    ");\n",
    "\n",
    "async function solve(state: typeof GraphState.State, config?: RunnableConfig) {\n",
    "  console.log(\"---SOLVE---\");\n",
    "  let plan = \"\";\n",
    "  const _results = state.results || {};\n",
    "  for (let [_plan, stepName, tool, toolInput] of state.steps) {\n",
    "    for (const [k, v] of Object.entries(_results)) {\n",
    "      toolInput = toolInput.replace(k, v);\n",
    "    }\n",
    "    plan += `Plan: ${_plan}\\n${stepName} = ${tool}[${toolInput}]\\n`;\n",
    "  }\n",
    "  const model = new ChatOpenAI({\n",
    "    temperature: 0,\n",
    "    model: \"gpt-4o\",\n",
    "  });\n",
    "  const result = await solvePrompt\n",
    "    .pipe(model)\n",
    "    .invoke({ plan, task: state.task }, config);\n",
    "  return {\n",
    "    result: result.content.toString(),\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba164af7",
   "metadata": {},
   "source": [
    "## 4. Define Graph\n",
    "\n",
    "Our graph defines the workflow. Each of the planner, tool executor, and solver\n",
    "modules are added as nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f69da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const _route = (state: typeof GraphState.State) => {\n",
    "  console.log(\"---ROUTE TASK---\");\n",
    "  const _step = _getCurrentTask(state);\n",
    "  if (_step === null) {\n",
    "    // We have executed all tasks\n",
    "    return \"solve\";\n",
    "  }\n",
    "  // We are still executing tasks, loop back to the \"tool\" node\n",
    "  return \"tool\";\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph(GraphState)\n",
    "  .addNode(\"plan\", getPlan)\n",
    "  .addNode(\"tool\", toolExecution)\n",
    "  .addNode(\"solve\", solve)\n",
    "  .addEdge(\"plan\", \"tool\")\n",
    "  .addEdge(\"solve\", END)\n",
    "  .addConditionalEdges(\"tool\", _route)\n",
    "  .addEdge(START, \"plan\");\n",
    "\n",
    "// Compile\n",
    "const app = workflow.compile({ checkpointer: new MemorySaver() });\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea7a66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GET PLAN---\n",
      "{\n",
      "  plan: {\n",
      "    planString: 'Plan: Identify the winner of the 2023 Australian Open.\\n' +\n",
      "      '#E1 = Google[\"winner of the 2023 Australian Open\"]\\n' +\n",
      "      '\\n' +\n",
      "      'Plan: Find the hometown of the winner identified in #E1.\\n' +\n",
      "      '#E2 = Google[\"hometown of #E1\"]',\n",
      "    steps: [ [Array] ]\n",
      "  }\n",
      "}\n",
      "-----\n",
      "---EXECUTE TOOL---\n",
      "_getCurrentTask {\n",
      "  task: 'what is the hometown of the winner of the 2023 australian open?',\n",
      "  planString: 'Plan: Identify the winner of the 2023 Australian Open.\\n' +\n",
      "    '#E1 = Google[\"winner of the 2023 Australian Open\"]\\n' +\n",
      "    '\\n' +\n",
      "    'Plan: Find the hometown of the winner identified in #E1.\\n' +\n",
      "    '#E2 = Google[\"hometown of #E1\"]',\n",
      "  steps: [\n",
      "    [\n",
      "      'Identify the winner of the 2023 Australian Open.\\n',\n",
      "      '#E1',\n",
      "      'Google',\n",
      "      '\"winner of the 2023 Australian Open\"',\n",
      "      'Plan: Identify the winner of the 2023 Australian Open.\\n' +\n",
      "        '#E1 = Google[\"winner of the 2023 Australian Open\"]'\n",
      "    ]\n",
      "  ],\n",
      "  results: {},\n",
      "  result: ''\n",
      "}\n",
      "---ROUTE TASK---\n",
      "_getCurrentTask {\n",
      "  task: 'what is the hometown of the winner of the 2023 australian open?',\n",
      "  planString: 'Plan: Identify the winner of the 2023 Australian Open.\\n' +\n",
      "    '#E1 = Google[\"winner of the 2023 Australian Open\"]\\n' +\n",
      "    '\\n' +\n",
      "    'Plan: Find the hometown of the winner identified in #E1.\\n' +\n",
      "    '#E2 = Google[\"hometown of #E1\"]',\n",
      "  steps: [\n",
      "    [\n",
      "      'Identify the winner of the 2023 Australian Open.\\n',\n",
      "      '#E1',\n",
      "      'Google',\n",
      "      '\"winner of the 2023 Australian Open\"',\n",
      "      'Plan: Identify the winner of the 2023 Australian Open.\\n' +\n",
      "        '#E1 = Google[\"winner of the 2023 Australian Open\"]'\n",
      "    ]\n",
      "  ],\n",
      "  results: {\n",
      "    '#E1': `\"A one-set shoot-off to decide the winner of the 2023 Australian Open. There could not have been a better script. SECOND SET (* denotes server) Sabalenka* 6-3 Rybakina - Wide second serve into the deuce court from Sabalenka and forehand return from Rybakina is long. Deep backhand crosscourt return from Rybakina draws a shot ball from Sabalenka ...\\\\nThe Crossword Solver found 30 answers to \\\\\"Tennis player Sabalenka, winner of the 2023 Australian Open\\\\\", 5 letters crossword clue. The Crossword Solver finds answers to classic crosswords and cryptic crossword puzzles. Enter the length or pattern for better results. Click the answer to find similar crossword clues .\\\\nAccording to bet365, Djokovic has even odds of winning the title at Melbourne Park -- meaning that the 35-year-old has a 50% chance of being the winner of the 2023 Australian Open men's singles ...\\\\nWe found 40 solutions for Tennis player Sabalenka, winner of the 2023 Australian Open. The top solutions are determined by popularity, ratings and frequency of searches. The most likely answer for the clue is ARYNA. How many solutions does Tennis player Sabalenka, winner of the 2023 Australian Open have?\"`\n",
      "  },\n",
      "  result: ''\n",
      "}\n",
      "{\n",
      "  tool: {\n",
      "    results: {\n",
      "      '#E1': `\"A one-set shoot-off to decide the winner of the 2023 Australian Open. There could not have been a better script. SECOND SET (* denotes server) Sabalenka* 6-3 Rybakina - Wide second serve into the deuce court from Sabalenka and forehand return from Rybakina is long. Deep backhand crosscourt return from Rybakina draws a shot ball from Sabalenka ...\\\\nThe Crossword Solver found 30 answers to \\\\\"Tennis player Sabalenka, winner of the 2023 Australian Open\\\\\", 5 letters crossword clue. The Crossword Solver finds answers to classic crosswords and cryptic crossword puzzles. Enter the length or pattern for better results. Click the answer to find similar crossword clues .\\\\nAccording to bet365, Djokovic has even odds of winning the title at Melbourne Park -- meaning that the 35-year-old has a 50% chance of being the winner of the 2023 Australian Open men's singles ...\\\\nWe found 40 solutions for Tennis player Sabalenka, winner of the 2023 Australian Open. The top solutions are determined by popularity, ratings and frequency of searches. The most likely answer for the clue is ARYNA. How many solutions does Tennis player Sabalenka, winner of the 2023 Australian Open have?\"`\n",
      "    }\n",
      "  }\n",
      "}\n",
      "-----\n",
      "---SOLVE---\n",
      "{ solve: { result: 'Belgrade, Serbia' } }\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "const threadConfig = { configurable: { thread_id: \"123\" } };\n",
    "let finalResult;\n",
    "const stream = await app.stream(\n",
    "  {\n",
    "    task: \"what is the hometown of the winner of the 2023 australian open?\",\n",
    "  },\n",
    "  threadConfig,\n",
    ");\n",
    "for await (const item of stream) {\n",
    "  console.log(item);\n",
    "  console.log(\"-----\");\n",
    "  finalResult = item;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef614005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belgrade, Serbia\n"
     ]
    }
   ],
   "source": [
    "const snapshot = await app.getState(threadConfig);\n",
    "console.log(snapshot.values.result);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6291f",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/730ea730-d896-450e-ac85-3c7e228c79f4/r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0b424",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on implementing ReWOO! Before you leave, I'll leave you with a\n",
    "couple limitations of the current implementation from the paper:\n",
    "\n",
    "1. If little context of the environment is available, the planner will be\n",
    "   ineffective in its tool use. This can typically be ameliorated through\n",
    "   few-shot prompting and/or fine-tuning.\n",
    "2. The tasks are still executed in sequence, meaning the total execution time is\n",
    "   impacted by _every_ tool call, not just the longest-running in a given step.\n",
    "\n",
    "```\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="src/index.ts">
import path from "path";
import url from "url";
import { awaitAllCallbacks } from "@langchain/core/callbacks/promises";

const [exampleName, ...args] = process.argv.slice(2);

if (!exampleName) {
  console.error("Please provide path to example to run");
  process.exit(1);
}

// Allow people to pass all possible variations of a path to an example
// ./src/foo.ts, ./dist/foo.js, src/foo.ts, dist/foo.js, foo.ts
let exampleRelativePath = exampleName;

if (exampleRelativePath.startsWith("./examples/")) {
  exampleRelativePath = exampleName.slice(11);
} else if (exampleRelativePath.startsWith("examples/")) {
  exampleRelativePath = exampleName.slice(9);
}

if (exampleRelativePath.startsWith("./src/")) {
  exampleRelativePath = exampleRelativePath.slice(6);
} else if (exampleRelativePath.startsWith("./dist/")) {
  exampleRelativePath = exampleRelativePath.slice(7);
} else if (exampleRelativePath.startsWith("src/")) {
  exampleRelativePath = exampleRelativePath.slice(4);
} else if (exampleRelativePath.startsWith("dist/")) {
  exampleRelativePath = exampleRelativePath.slice(5);
}

let runExample;
try {
  ({ run: runExample } = await import(
    path.join(
      path.dirname(url.fileURLToPath(import.meta.url)),
      exampleRelativePath
    )
  ));
} catch (e) {
  console.log(e);
  throw new Error(`Could not load example ${exampleName}: ${e}`);
}

if (runExample) {
  const maybePromise = runExample(args);

  if (maybePromise instanceof Promise) {
    maybePromise
      .catch((e) => {
        console.error(`Example failed with:`);
        console.error(e);
      })
      .finally(() => awaitAllCallbacks());
  }
}
</file>

<file path="tsconfig.json">
{
  "extends": "@tsconfig/recommended",
  "compilerOptions": {
    "outDir": "dist",
    "lib": [
      "ES2021",
      "ES2022.Object",
      "ES2022.Error",
      "DOM"
    ],
    "target": "ES2021",
    "module": "nodenext",
    "sourceMap": true,
    "allowSyntheticDefaultImports": true,
    "baseUrl": "./src",
    "declaration": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true,
    "noUnusedParameters": true,
    "useDefineForClassFields": true,
    "strictPropertyInitialization": false
  },
  "exclude": [
    "node_modules/",
    "dist/",
    "tests/"
  ],
  "include": [
    "*.ts",
    "./src"
  ]
}
</file>

</files>
